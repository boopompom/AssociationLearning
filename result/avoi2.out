Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
Avoidant
... data manager created. project_root: ExperimentADBN_avoi
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105693 minutes
Weight histogram
[ 281 1452 2270 3194 2931 3395 3139 3157 1801  655] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
[  142   150   208   300   454   703   786  1476  2404 15652] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.95803
Epoch 1, cost is  3.91752
Epoch 2, cost is  3.896
Epoch 3, cost is  3.87283
Epoch 4, cost is  3.85
Training took 0.073440 minutes
Weight histogram
[6072 7442 2973 2492  411  421 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2467 1852 1768 1885 2220 1963 2058 2380 2756 2926] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-1.83405
1.94596
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148871 minutes
Weight histogram
[   95   349   614  1079  1197  3396 11492  5627   446     5] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 11530  5883   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.114896 minutes
Weight histogram
[4279 3704 3308 2711 1982 1605 1931 2198 2032  550] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
[4415 1328 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
-1.18162
1.30045
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.99009
Epoch 1, cost is  5.64815
Epoch 2, cost is  5.52508
Epoch 3, cost is  5.41134
Epoch 4, cost is  5.22494
Epoch 5, cost is  4.96822
Epoch 6, cost is  4.73003
Epoch 7, cost is  4.52856
Epoch 8, cost is  4.35091
Epoch 9, cost is  4.19452
Training took 0.178484 minutes
Weight histogram
[ 711  804 1269  474  292  206  172   73   33   16] [-0.0301243  -0.02712327 -0.02412224 -0.02112122 -0.01812019 -0.01511916
 -0.01211814 -0.00911711 -0.00611608 -0.00311506 -0.00011403]
[326 609 748 372 300 302 319 337 353 384] [-0.0301243  -0.02712327 -0.02412224 -0.02112122 -0.01812019 -0.01511916
 -0.01211814 -0.00911711 -0.00611608 -0.00311506 -0.00011403]
-0.400339
0.437275
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042568 minutes
Epoch 0
Fine tuning took 0.041907 minutes
Epoch 0
Fine tuning took 0.041190 minutes
Epoch 0
Fine tuning took 0.041087 minutes
Epoch 0
Fine tuning took 0.043932 minutes
Epoch 0
Fine tuning took 0.042462 minutes
Epoch 0
Fine tuning took 0.044222 minutes
Epoch 0
Fine tuning took 0.044280 minutes
Epoch 0
Fine tuning took 0.043813 minutes
Epoch 0
Fine tuning took 0.040452 minutes
{'zero': {0: [0.24384236453201971, 0.7426108374384236, 0.64039408866995073, 0.51970443349753692, 0.64408866995073888, 0.5788177339901478, 0.37684729064039407, 0.37438423645320196, 0.52216748768472909, 0.42364532019704432, 0.49014778325123154], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44088669950738918, 0.14655172413793102, 0.24753694581280788, 0.33004926108374383, 0.19581280788177341, 0.32019704433497537, 0.37931034482758619, 0.45443349753694579, 0.30665024630541871, 0.42364532019704432, 0.3817733990147783], 5: [0.31527093596059114, 0.11083743842364532, 0.11206896551724138, 0.15024630541871922, 0.16009852216748768, 0.10098522167487685, 0.24384236453201971, 0.17118226600985223, 0.17118226600985223, 0.15270935960591134, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24384236453201971, 0.7857142857142857, 0.68472906403940892, 0.55911330049261088, 0.71059113300492616, 0.44827586206896552, 0.48645320197044334, 0.48645320197044334, 0.56650246305418717, 0.56896551724137934, 0.57019704433497542], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44088669950738918, 0.16995073891625614, 0.15763546798029557, 0.3682266009852217, 0.23152709359605911, 0.48152709359605911, 0.41748768472906406, 0.26970443349753692, 0.33004926108374383, 0.31034482758620691, 0.34482758620689657], 5: [0.31527093596059114, 0.044334975369458129, 0.15763546798029557, 0.072660098522167482, 0.057881773399014777, 0.070197044334975367, 0.096059113300492605, 0.24384236453201971, 0.10344827586206896, 0.1206896551724138, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24384236453201971, 0.7857142857142857, 0.67487684729064035, 0.56280788177339902, 0.73891625615763545, 0.47167487684729065, 0.46674876847290642, 0.45935960591133007, 0.5923645320197044, 0.5714285714285714, 0.5923645320197044], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44088669950738918, 0.1625615763546798, 0.15886699507389163, 0.36206896551724138, 0.20935960591133004, 0.45689655172413796, 0.42241379310344829, 0.27709359605911332, 0.33004926108374383, 0.30049261083743845, 0.31773399014778325], 5: [0.31527093596059114, 0.051724137931034482, 0.16625615763546797, 0.075123152709359611, 0.051724137931034482, 0.071428571428571425, 0.11083743842364532, 0.26354679802955666, 0.077586206896551727, 0.12807881773399016, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24384236453201971, 0.80172413793103448, 0.76108374384236455, 0.60837438423645318, 0.81157635467980294, 0.48399014778325122, 0.5357142857142857, 0.56157635467980294, 0.61576354679802958, 0.64162561576354682, 0.61576354679802958], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44088669950738918, 0.16133004926108374, 0.098522167487684734, 0.35467980295566504, 0.15024630541871922, 0.44334975369458129, 0.39901477832512317, 0.19581280788177341, 0.31034482758620691, 0.2536945812807882, 0.31157635467980294], 5: [0.31527093596059114, 0.036945812807881777, 0.14039408866995073, 0.036945812807881777, 0.038177339901477834, 0.072660098522167482, 0.065270935960591137, 0.24261083743842365, 0.073891625615763554, 0.10467980295566502, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108341 minutes
Weight histogram
[ 281 1452 2270 3194 2931 3395 3139 3157 1801  655] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
[  142   150   208   300   454   703   786  1476  2404 15652] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.95803
Epoch 1, cost is  3.91752
Epoch 2, cost is  3.896
Epoch 3, cost is  3.87283
Epoch 4, cost is  3.85
Training took 0.071137 minutes
Weight histogram
[6072 7442 2973 2492  411  421 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2467 1852 1768 1885 2220 1963 2058 2380 2756 2926] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-1.83405
1.94596
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148268 minutes
Weight histogram
[   95   349   614  1079  1197  3396 11492  5627   446     5] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 11530  5883   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.115204 minutes
Weight histogram
[4279 3704 3308 2711 1982 1605 1931 2198 2032  550] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
[4415 1328 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
-1.18162
1.30045
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.49302
Epoch 1, cost is  5.25843
Epoch 2, cost is  5.17978
Epoch 3, cost is  5.01072
Epoch 4, cost is  4.69903
Epoch 5, cost is  4.40049
Epoch 6, cost is  4.17512
Epoch 7, cost is  3.99275
Epoch 8, cost is  3.83632
Epoch 9, cost is  3.68492
Training took 0.242604 minutes
Weight histogram
[1031 2065  411  204  145   85   49   32   17   11] [-0.01972586 -0.01776985 -0.01581383 -0.01385782 -0.01190181 -0.0099458
 -0.00798978 -0.00603377 -0.00407776 -0.00212174 -0.00016573]
[1014  482  263  247  253  289  320  369  392  421] [-0.01972586 -0.01776985 -0.01581383 -0.01385782 -0.01190181 -0.0099458
 -0.00798978 -0.00603377 -0.00407776 -0.00212174 -0.00016573]
-0.323797
0.271731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046683 minutes
Epoch 0
Fine tuning took 0.044430 minutes
Epoch 0
Fine tuning took 0.046617 minutes
Epoch 0
Fine tuning took 0.046515 minutes
Epoch 0
Fine tuning took 0.048102 minutes
Epoch 0
Fine tuning took 0.045744 minutes
Epoch 0
Fine tuning took 0.045957 minutes
Epoch 0
Fine tuning took 0.045360 minutes
Epoch 0
Fine tuning took 0.044873 minutes
Epoch 0
Fine tuning took 0.047026 minutes
{'zero': {0: [0.41748768472906406, 0.40763546798029554, 0.43103448275862066, 0.44088669950738918, 0.39901477832512317, 0.39408866995073893, 0.46921182266009853, 0.40024630541871919, 0.39408866995073893, 0.4248768472906404, 0.42241379310344829], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33497536945812806, 0.37561576354679804, 0.30172413793103448, 0.35467980295566504, 0.36576354679802958, 0.36206896551724138, 0.28448275862068967, 0.35221674876847292, 0.34729064039408869, 0.31773399014778325, 0.37807881773399016], 5: [0.24753694581280788, 0.21674876847290642, 0.26724137931034481, 0.20443349753694581, 0.23522167487684728, 0.24384236453201971, 0.24630541871921183, 0.24753694581280788, 0.25862068965517243, 0.25738916256157635, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.41748768472906406, 0.45197044334975367, 0.44458128078817732, 0.44088669950738918, 0.3817733990147783, 0.38423645320197042, 0.41871921182266009, 0.43472906403940886, 0.40886699507389163, 0.41009852216748771, 0.40886699507389163], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33497536945812806, 0.34852216748768472, 0.30665024630541871, 0.40640394088669951, 0.38054187192118227, 0.39162561576354682, 0.31403940886699505, 0.33128078817733991, 0.35467980295566504, 0.32758620689655171, 0.40640394088669951], 5: [0.24753694581280788, 0.19950738916256158, 0.24876847290640394, 0.15270935960591134, 0.2376847290640394, 0.22413793103448276, 0.26724137931034481, 0.23399014778325122, 0.23645320197044334, 0.26231527093596058, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.41748768472906406, 0.46674876847290642, 0.45812807881773399, 0.41379310344827586, 0.43472906403940886, 0.40270935960591131, 0.46551724137931033, 0.42241379310344829, 0.40763546798029554, 0.42610837438423643, 0.41009852216748771], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33497536945812806, 0.32635467980295568, 0.2857142857142857, 0.3854679802955665, 0.3608374384236453, 0.35714285714285715, 0.30172413793103448, 0.34729064039408869, 0.37192118226600984, 0.31527093596059114, 0.37438423645320196], 5: [0.24753694581280788, 0.20689655172413793, 0.25615763546798032, 0.20073891625615764, 0.20443349753694581, 0.24014778325123154, 0.23275862068965517, 0.23029556650246305, 0.22044334975369459, 0.25862068965517243, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.41748768472906406, 0.42733990147783252, 0.41871921182266009, 0.43719211822660098, 0.39162561576354682, 0.43596059113300495, 0.45320197044334976, 0.44088669950738918, 0.4039408866995074, 0.41995073891625617, 0.40270935960591131], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33497536945812806, 0.37315270935960593, 0.3251231527093596, 0.39039408866995073, 0.3854679802955665, 0.3460591133004926, 0.29187192118226601, 0.33743842364532017, 0.35714285714285715, 0.29926108374384236, 0.38300492610837439], 5: [0.24753694581280788, 0.19950738916256158, 0.25615763546798032, 0.17241379310344829, 0.2229064039408867, 0.21798029556650247, 0.25492610837438423, 0.22167487684729065, 0.23891625615763548, 0.28078817733990147, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106456 minutes
Weight histogram
[ 281 1452 2270 3194 2931 3395 3139 3157 1801  655] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
[  142   150   208   300   454   703   786  1476  2404 15652] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.95803
Epoch 1, cost is  3.91752
Epoch 2, cost is  3.896
Epoch 3, cost is  3.87283
Epoch 4, cost is  3.85
Training took 0.070769 minutes
Weight histogram
[6072 7442 2973 2492  411  421 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2467 1852 1768 1885 2220 1963 2058 2380 2756 2926] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-1.83405
1.94596
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148525 minutes
Weight histogram
[   95   349   614  1079  1197  3396 11492  5627   446     5] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 11530  5883   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.114141 minutes
Weight histogram
[4279 3704 3308 2711 1982 1605 1931 2198 2032  550] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
[4415 1328 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
-1.18162
1.30045
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.3651
Epoch 1, cost is  5.24086
Epoch 2, cost is  4.99851
Epoch 3, cost is  4.55018
Epoch 4, cost is  4.17334
Epoch 5, cost is  3.90469
Epoch 6, cost is  3.68152
Epoch 7, cost is  3.4818
Epoch 8, cost is  3.30078
Epoch 9, cost is  3.14596
Training took 0.331949 minutes
Weight histogram
[ 926  773  574  485 1134   93   33   16    9    7] [-0.01159094 -0.01044536 -0.00929979 -0.00815421 -0.00700863 -0.00586305
 -0.00471747 -0.0035719  -0.00242632 -0.00128074 -0.00013516]
[1037  257  240  256  295  345  372  392  416  440] [-0.01159094 -0.01044536 -0.00929979 -0.00815421 -0.00700863 -0.00586305
 -0.00471747 -0.0035719  -0.00242632 -0.00128074 -0.00013516]
-0.252911
0.20775
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051424 minutes
Epoch 0
Fine tuning took 0.050405 minutes
Epoch 0
Fine tuning took 0.050276 minutes
Epoch 0
Fine tuning took 0.051013 minutes
Epoch 0
Fine tuning took 0.051327 minutes
Epoch 0
Fine tuning took 0.052319 minutes
Epoch 0
Fine tuning took 0.050911 minutes
Epoch 0
Fine tuning took 0.050152 minutes
Epoch 0
Fine tuning took 0.049684 minutes
Epoch 0
Fine tuning took 0.050968 minutes
{'zero': {0: [0.37684729064039407, 0.38669950738916259, 0.39901477832512317, 0.43965517241379309, 0.41748768472906406, 0.40024630541871919, 0.44458128078817732, 0.42610837438423643, 0.35221674876847292, 0.36576354679802958, 0.45689655172413796], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36576354679802958, 0.34482758620689657, 0.35714285714285715, 0.35467980295566504, 0.38054187192118227, 0.33866995073891626, 0.3251231527093596, 0.27586206896551724, 0.27463054187192121, 0.36206896551724138, 0.29310344827586204], 5: [0.25738916256157635, 0.26847290640394089, 0.24384236453201971, 0.20566502463054187, 0.2019704433497537, 0.26108374384236455, 0.23029556650246305, 0.29802955665024633, 0.37315270935960593, 0.27216748768472904, 0.25], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37684729064039407, 0.40517241379310343, 0.41871921182266009, 0.3854679802955665, 0.39162561576354682, 0.37931034482758619, 0.43472906403940886, 0.43842364532019706, 0.37192118226600984, 0.3645320197044335, 0.48522167487684731], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36576354679802958, 0.34236453201970446, 0.37192118226600984, 0.38054187192118227, 0.35591133004926107, 0.33374384236453203, 0.33866995073891626, 0.29310344827586204, 0.28201970443349755, 0.37807881773399016, 0.27832512315270935], 5: [0.25738916256157635, 0.25246305418719212, 0.20935960591133004, 0.23399014778325122, 0.25246305418719212, 0.28694581280788178, 0.22660098522167488, 0.26847290640394089, 0.3460591133004926, 0.25738916256157635, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37684729064039407, 0.42610837438423643, 0.39778325123152708, 0.41995073891625617, 0.41502463054187194, 0.35714285714285715, 0.44458128078817732, 0.4248768472906404, 0.36945812807881773, 0.37192118226600984, 0.46305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36576354679802958, 0.32266009852216748, 0.34729064039408869, 0.33004926108374383, 0.35098522167487683, 0.33866995073891626, 0.32635467980295568, 0.29802955665024633, 0.2894088669950739, 0.36206896551724138, 0.29064039408866993], 5: [0.25738916256157635, 0.25123152709359609, 0.25492610837438423, 0.25, 0.23399014778325122, 0.30418719211822659, 0.22906403940886699, 0.27709359605911332, 0.34113300492610837, 0.26600985221674878, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37684729064039407, 0.37438423645320196, 0.40763546798029554, 0.44334975369458129, 0.40640394088669951, 0.3645320197044335, 0.44827586206896552, 0.39408866995073893, 0.37684729064039407, 0.36330049261083741, 0.48522167487684731], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36576354679802958, 0.35960591133004927, 0.35098522167487683, 0.35344827586206895, 0.34236453201970446, 0.3288177339901478, 0.30788177339901479, 0.30911330049261082, 0.28325123152709358, 0.34236453201970446, 0.27463054187192121], 5: [0.25738916256157635, 0.26600985221674878, 0.2413793103448276, 0.20320197044334976, 0.25123152709359609, 0.30665024630541871, 0.24384236453201971, 0.29679802955665024, 0.33990147783251229, 0.29433497536945813, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107130 minutes
Weight histogram
[ 281 1452 2270 3194 2931 3395 3139 3157 1801  655] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
[  142   150   208   300   454   703   786  1476  2404 15652] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.95803
Epoch 1, cost is  3.91752
Epoch 2, cost is  3.896
Epoch 3, cost is  3.87283
Epoch 4, cost is  3.85
Training took 0.071397 minutes
Weight histogram
[6072 7442 2973 2492  411  421 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2467 1852 1768 1885 2220 1963 2058 2380 2756 2926] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-1.83405
1.94596
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146353 minutes
Weight histogram
[   95   349   614  1079  1197  3396 11492  5627   446     5] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 11530  5883   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.115385 minutes
Weight histogram
[4279 3704 3308 2711 1982 1605 1931 2198 2032  550] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
[4415 1328 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
-1.18162
1.30045
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.3597
Epoch 1, cost is  5.19218
Epoch 2, cost is  4.74066
Epoch 3, cost is  4.20334
Epoch 4, cost is  3.82821
Epoch 5, cost is  3.53539
Epoch 6, cost is  3.28781
Epoch 7, cost is  3.07539
Epoch 8, cost is  2.90651
Epoch 9, cost is  2.76639
Training took 0.530946 minutes
Weight histogram
[ 854  743  598  392  379 1036   32    8    4    4] [-0.00614247 -0.00554043 -0.00493838 -0.00433634 -0.00373429 -0.00313224
 -0.0025302  -0.00192815 -0.00132611 -0.00072406 -0.00012202]
[936 253 244 276 321 345 367 397 428 483] [-0.00614247 -0.00554043 -0.00493838 -0.00433634 -0.00373429 -0.00313224
 -0.0025302  -0.00192815 -0.00132611 -0.00072406 -0.00012202]
-0.186733
0.170447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.059763 minutes
Epoch 0
Fine tuning took 0.060745 minutes
Epoch 0
Fine tuning took 0.063062 minutes
Epoch 0
Fine tuning took 0.062080 minutes
Epoch 0
Fine tuning took 0.062863 minutes
Epoch 0
Fine tuning took 0.061061 minutes
Epoch 0
Fine tuning took 0.062199 minutes
Epoch 0
Fine tuning took 0.060805 minutes
Epoch 0
Fine tuning took 0.061840 minutes
Epoch 0
Fine tuning took 0.060641 minutes
{'zero': {0: [0.40886699507389163, 0.41009852216748771, 0.39778325123152708, 0.42241379310344829, 0.36576354679802958, 0.38423645320197042, 0.45935960591133007, 0.40024630541871919, 0.3854679802955665, 0.39778325123152708, 0.42364532019704432], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37684729064039407, 0.35960591133004927, 0.34975369458128081, 0.28078817733990147, 0.32635467980295568, 0.33128078817733991, 0.3288177339901478, 0.28078817733990147, 0.28694581280788178, 0.31527093596059114, 0.28078817733990147], 5: [0.21428571428571427, 0.23029556650246305, 0.25246305418719212, 0.29679802955665024, 0.30788177339901479, 0.28448275862068967, 0.21182266009852216, 0.31896551724137934, 0.32758620689655171, 0.28694581280788178, 0.29556650246305421], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.40886699507389163, 0.40147783251231528, 0.39778325123152708, 0.41748768472906406, 0.41625615763546797, 0.41379310344827586, 0.46674876847290642, 0.41995073891625617, 0.37438423645320196, 0.41133004926108374, 0.38054187192118227], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37684729064039407, 0.36699507389162561, 0.36945812807881773, 0.33004926108374383, 0.31034482758620691, 0.31773399014778325, 0.33620689655172414, 0.27339901477832512, 0.30911330049261082, 0.28817733990147781, 0.30049261083743845], 5: [0.21428571428571427, 0.23152709359605911, 0.23275862068965517, 0.25246305418719212, 0.27339901477832512, 0.26847290640394089, 0.19704433497536947, 0.30665024630541871, 0.31650246305418717, 0.30049261083743845, 0.31896551724137934], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.40886699507389163, 0.41871921182266009, 0.39532019704433496, 0.41256157635467983, 0.38669950738916259, 0.41133004926108374, 0.50369458128078815, 0.38054187192118227, 0.37438423645320196, 0.37931034482758619, 0.39408866995073893], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37684729064039407, 0.3645320197044335, 0.35467980295566504, 0.3288177339901478, 0.33004926108374383, 0.30418719211822659, 0.32266009852216748, 0.29556650246305421, 0.30665024630541871, 0.33990147783251229, 0.29802955665024633], 5: [0.21428571428571427, 0.21674876847290642, 0.25, 0.25862068965517243, 0.28325123152709358, 0.28448275862068967, 0.17364532019704434, 0.32389162561576357, 0.31896551724137934, 0.28078817733990147, 0.30788177339901479], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.40886699507389163, 0.43472906403940886, 0.40147783251231528, 0.42241379310344829, 0.36330049261083741, 0.39778325123152708, 0.47044334975369456, 0.38423645320197042, 0.43842364532019706, 0.3854679802955665, 0.39285714285714285], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37684729064039407, 0.34482758620689657, 0.3682266009852217, 0.33374384236453203, 0.35098522167487683, 0.30418719211822659, 0.32758620689655171, 0.29187192118226601, 0.27339901477832512, 0.33866995073891626, 0.29802955665024633], 5: [0.21428571428571427, 0.22044334975369459, 0.23029556650246305, 0.24384236453201971, 0.2857142857142857, 0.29802955665024633, 0.2019704433497537, 0.32389162561576357, 0.28817733990147781, 0.27586206896551724, 0.30911330049261082], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148872 minutes
Weight histogram
[ 145  352  553  851 2849 3574 4717 5595 3223  416] [ -2.45821051e-04   4.24755417e-05   3.30772134e-04   6.19068727e-04
   9.07365320e-04   1.19566191e-03   1.48395851e-03   1.77225510e-03
   2.06055169e-03   2.34884828e-03   2.63714488e-03]
[  136   154   224   332   468   879  1025  2049  6732 10276] [ -2.45821051e-04   4.24755417e-05   3.30772134e-04   6.19068727e-04
   9.07365320e-04   1.19566191e-03   1.48395851e-03   1.77225510e-03
   2.06055169e-03   2.34884828e-03   2.63714488e-03]
-1.32597
1.08666
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.6378
Epoch 1, cost is  2.59787
Epoch 2, cost is  2.55968
Epoch 3, cost is  2.53134
Epoch 4, cost is  2.51134
Training took 0.113196 minutes
Weight histogram
[4182 3709 3092 2792 2011 1680 1982 1147 1403  277] [ -5.50307855e-02  -4.95241354e-02  -4.40174853e-02  -3.85108352e-02
  -3.30041851e-02  -2.74975350e-02  -2.19908849e-02  -1.64842348e-02
  -1.09775847e-02  -5.47093459e-03   3.57155150e-05]
[2475 1327 1291 1566 1894 2188 2471 2608 3204 3251] [ -5.50307855e-02  -4.95241354e-02  -4.40174853e-02  -3.85108352e-02
  -3.30041851e-02  -2.74975350e-02  -2.19908849e-02  -1.64842348e-02
  -1.09775847e-02  -5.47093459e-03   3.57155150e-05]
-1.24699
1.63732
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149636 minutes
Weight histogram
[   33   293   616  1073  1295  1020  5429 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  279   328   447   631   943  1568  1004  1856  5900 11344] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.116117 minutes
Weight histogram
[4279 3704 3308 2712 1981 1605 1931 1426 2804  550] [ -5.51578365e-02  -4.96384813e-02  -4.41191261e-02  -3.85997709e-02
  -3.30804157e-02  -2.75610605e-02  -2.20417053e-02  -1.65223501e-02
  -1.10029949e-02  -5.48363969e-03   3.57155150e-05]
[4445 1298 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96384813e-02  -4.41191261e-02  -3.85997709e-02
  -3.30804157e-02  -2.75610605e-02  -2.20417053e-02  -1.65223501e-02
  -1.10029949e-02  -5.48363969e-03   3.57155150e-05]
-1.18162
1.30045
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.20317
Epoch 1, cost is  5.95009
Epoch 2, cost is  5.78802
Epoch 3, cost is  5.58255
Epoch 4, cost is  5.26667
Epoch 5, cost is  4.92638
Epoch 6, cost is  4.63812
Epoch 7, cost is  4.40698
Epoch 8, cost is  4.21929
Epoch 9, cost is  4.05856
Training took 0.204303 minutes
Weight histogram
[ 489  443  460  474 1074  564  387  117   28   14] [-0.02607865 -0.02348716 -0.02089566 -0.01830417 -0.01571267 -0.01312118
 -0.01052969 -0.00793819 -0.0053467  -0.0027552  -0.00016371]
[854 674 308 255 269 284 310 335 370 391] [-0.02607865 -0.02348716 -0.02089566 -0.01830417 -0.01571267 -0.01312118
 -0.01052969 -0.00793819 -0.0053467  -0.0027552  -0.00016371]
-0.319972
0.38733
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.054629 minutes
Epoch 0
Fine tuning took 0.052965 minutes
Epoch 0
Fine tuning took 0.055047 minutes
Epoch 0
Fine tuning took 0.053719 minutes
Epoch 0
Fine tuning took 0.053552 minutes
Epoch 0
Fine tuning took 0.055087 minutes
Epoch 0
Fine tuning took 0.054505 minutes
Epoch 0
Fine tuning took 0.054768 minutes
Epoch 0
Fine tuning took 0.053004 minutes
Epoch 0
Fine tuning took 0.054213 minutes
{'zero': {0: [0.26970443349753692, 0.72290640394088668, 0.67980295566502458, 0.63177339901477836, 0.71059113300492616, 0.59729064039408863, 0.44581280788177341, 0.50246305418719217, 0.46551724137931033, 0.52709359605911332, 0.62561576354679804], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36330049261083741, 0.16379310344827586, 0.18719211822660098, 0.23029556650246305, 0.22536945812807882, 0.3288177339901478, 0.4211822660098522, 0.31157635467980294, 0.39408866995073893, 0.31650246305418717, 0.2857142857142857], 5: [0.36699507389162561, 0.11330049261083744, 0.13300492610837439, 0.13793103448275862, 0.064039408866995079, 0.073891625615763554, 0.13300492610837439, 0.18596059113300492, 0.14039408866995073, 0.15640394088669951, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26970443349753692, 0.57512315270935965, 0.71059113300492616, 0.67487684729064035, 0.82266009852216748, 0.53078817733990147, 0.45566502463054187, 0.44704433497536944, 0.3645320197044335, 0.62561576354679804, 0.51354679802955661], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36330049261083741, 0.19581280788177341, 0.11945812807881774, 0.19950738916256158, 0.11699507389162561, 0.36699507389162561, 0.47660098522167488, 0.27955665024630544, 0.52832512315270941, 0.17980295566502463, 0.35467980295566504], 5: [0.36699507389162561, 0.22906403940886699, 0.16995073891625614, 0.12561576354679804, 0.060344827586206899, 0.10221674876847291, 0.067733990147783252, 0.27339901477832512, 0.10714285714285714, 0.19458128078817735, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26970443349753692, 0.58743842364532017, 0.71182266009852213, 0.64778325123152714, 0.79433497536945807, 0.49137931034482757, 0.47536945812807879, 0.4642857142857143, 0.36206896551724138, 0.56773399014778325, 0.50369458128078815], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36330049261083741, 0.17857142857142858, 0.13423645320197045, 0.19827586206896552, 0.14162561576354679, 0.42610837438423643, 0.43965517241379309, 0.26724137931034481, 0.52216748768472909, 0.19704433497536947, 0.36206896551724138], 5: [0.36699507389162561, 0.23399014778325122, 0.1539408866995074, 0.1539408866995074, 0.064039408866995079, 0.082512315270935957, 0.084975369458128072, 0.26847290640394089, 0.11576354679802955, 0.23522167487684728, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26970443349753692, 0.57019704433497542, 0.72660098522167482, 0.66995073891625612, 0.82019704433497542, 0.53201970443349755, 0.49014778325123154, 0.45935960591133007, 0.32019704433497537, 0.58620689655172409, 0.53694581280788178], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36330049261083741, 0.19827586206896552, 0.10221674876847291, 0.20689655172413793, 0.11330049261083744, 0.38054187192118227, 0.43719211822660098, 0.24384236453201971, 0.56896551724137934, 0.18596059113300492, 0.36330049261083741], 5: [0.36699507389162561, 0.23152709359605911, 0.17118226600985223, 0.12315270935960591, 0.066502463054187194, 0.087438423645320201, 0.072660098522167482, 0.29679802955665024, 0.11083743842364532, 0.22783251231527094, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148894 minutes
Weight histogram
[ 145  352  553  851 2849 3574 4717 5595 3223  416] [ -2.45821051e-04   4.24755417e-05   3.30772134e-04   6.19068727e-04
   9.07365320e-04   1.19566191e-03   1.48395851e-03   1.77225510e-03
   2.06055169e-03   2.34884828e-03   2.63714488e-03]
[  136   154   224   332   468   879  1025  2049  6732 10276] [ -2.45821051e-04   4.24755417e-05   3.30772134e-04   6.19068727e-04
   9.07365320e-04   1.19566191e-03   1.48395851e-03   1.77225510e-03
   2.06055169e-03   2.34884828e-03   2.63714488e-03]
-1.32597
1.08666
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.6378
Epoch 1, cost is  2.59787
Epoch 2, cost is  2.55968
Epoch 3, cost is  2.53134
Epoch 4, cost is  2.51134
Training took 0.114215 minutes
Weight histogram
[4182 3709 3092 2792 2011 1680 1982 1147 1403  277] [ -5.50307855e-02  -4.95241354e-02  -4.40174853e-02  -3.85108352e-02
  -3.30041851e-02  -2.74975350e-02  -2.19908849e-02  -1.64842348e-02
  -1.09775847e-02  -5.47093459e-03   3.57155150e-05]
[2475 1327 1291 1566 1894 2188 2471 2608 3204 3251] [ -5.50307855e-02  -4.95241354e-02  -4.40174853e-02  -3.85108352e-02
  -3.30041851e-02  -2.74975350e-02  -2.19908849e-02  -1.64842348e-02
  -1.09775847e-02  -5.47093459e-03   3.57155150e-05]
-1.24699
1.63732
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146888 minutes
Weight histogram
[   33   293   616  1073  1295  1020  5429 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  279   328   447   631   943  1568  1004  1856  5900 11344] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.113474 minutes
Weight histogram
[4279 3704 3308 2712 1981 1605 1931 1426 2804  550] [ -5.51578365e-02  -4.96384813e-02  -4.41191261e-02  -3.85997709e-02
  -3.30804157e-02  -2.75610605e-02  -2.20417053e-02  -1.65223501e-02
  -1.10029949e-02  -5.48363969e-03   3.57155150e-05]
[4445 1298 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96384813e-02  -4.41191261e-02  -3.85997709e-02
  -3.30804157e-02  -2.75610605e-02  -2.20417053e-02  -1.65223501e-02
  -1.10029949e-02  -5.48363969e-03   3.57155150e-05]
-1.18162
1.30045
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.56722
Epoch 1, cost is  5.32952
Epoch 2, cost is  5.20539
Epoch 3, cost is  4.95584
Epoch 4, cost is  4.55559
Epoch 5, cost is  4.20596
Epoch 6, cost is  3.9385
Epoch 7, cost is  3.71914
Epoch 8, cost is  3.53029
Epoch 9, cost is  3.36566
Training took 0.296919 minutes
Weight histogram
[ 749  625  559 1588  300  124   53   28   15    9] [-0.01807489 -0.01628352 -0.01449215 -0.01270079 -0.01090942 -0.00911805
 -0.00732668 -0.00553532 -0.00374395 -0.00195258 -0.00016121]
[1247  327  233  237  260  294  328  347  379  398] [-0.01807489 -0.01628352 -0.01449215 -0.01270079 -0.01090942 -0.00911805
 -0.00732668 -0.00553532 -0.00374395 -0.00195258 -0.00016121]
-0.27419
0.314438
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.058228 minutes
Epoch 0
Fine tuning took 0.059505 minutes
Epoch 0
Fine tuning took 0.060099 minutes
Epoch 0
Fine tuning took 0.060577 minutes
Epoch 0
Fine tuning took 0.059085 minutes
Epoch 0
Fine tuning took 0.059987 minutes
Epoch 0
Fine tuning took 0.060540 minutes
Epoch 0
Fine tuning took 0.059692 minutes
Epoch 0
Fine tuning took 0.058499 minutes
Epoch 0
Fine tuning took 0.059693 minutes
{'zero': {0: [0.35344827586206895, 0.51231527093596063, 0.48891625615763545, 0.40886699507389163, 0.46798029556650245, 0.45320197044334976, 0.41748768472906406, 0.42733990147783252, 0.47044334975369456, 0.4039408866995074, 0.44334975369458129], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.39162561576354682, 0.29064039408866993, 0.28325123152709358, 0.42364532019704432, 0.34236453201970446, 0.40763546798029554, 0.35344827586206895, 0.31650246305418717, 0.31896551724137934, 0.39901477832512317, 0.41256157635467983], 5: [0.25492610837438423, 0.19704433497536947, 0.22783251231527094, 0.16748768472906403, 0.18965517241379309, 0.13916256157635468, 0.22906403940886699, 0.25615763546798032, 0.2105911330049261, 0.19704433497536947, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.35344827586206895, 0.59359605911330049, 0.48152709359605911, 0.42980295566502463, 0.44088669950738918, 0.40517241379310343, 0.4605911330049261, 0.41995073891625617, 0.48275862068965519, 0.41625615763546797, 0.45073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.39162561576354682, 0.22536945812807882, 0.29310344827586204, 0.3891625615763547, 0.4039408866995074, 0.44950738916256155, 0.3288177339901478, 0.33251231527093594, 0.30172413793103448, 0.38300492610837439, 0.38054187192118227], 5: [0.25492610837438423, 0.18103448275862069, 0.22536945812807882, 0.18103448275862069, 0.15517241379310345, 0.14532019704433496, 0.2105911330049261, 0.24753694581280788, 0.21551724137931033, 0.20073891625615764, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.35344827586206895, 0.59359605911330049, 0.45443349753694579, 0.42733990147783252, 0.48399014778325122, 0.37807881773399016, 0.43842364532019706, 0.41748768472906406, 0.46551724137931033, 0.43226600985221675, 0.43226600985221675], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.39162561576354682, 0.25862068965517243, 0.29926108374384236, 0.40886699507389163, 0.36699507389162561, 0.47536945812807879, 0.36206896551724138, 0.34359605911330049, 0.32389162561576357, 0.3891625615763547, 0.39162561576354682], 5: [0.25492610837438423, 0.14778325123152711, 0.24630541871921183, 0.16379310344827586, 0.14901477832512317, 0.14655172413793102, 0.19950738916256158, 0.23891625615763548, 0.2105911330049261, 0.17857142857142858, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.35344827586206895, 0.55911330049261088, 0.48768472906403942, 0.43842364532019706, 0.44581280788177341, 0.36576354679802958, 0.45812807881773399, 0.44950738916256155, 0.46798029556650245, 0.43103448275862066, 0.42733990147783252], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.39162561576354682, 0.27709359605911332, 0.27339901477832512, 0.39039408866995073, 0.39408866995073893, 0.45935960591133007, 0.33004926108374383, 0.30172413793103448, 0.30172413793103448, 0.37684729064039407, 0.41009852216748771], 5: [0.25492610837438423, 0.16379310344827586, 0.23891625615763548, 0.17118226600985223, 0.16009852216748768, 0.1748768472906404, 0.21182266009852216, 0.24876847290640394, 0.23029556650246305, 0.19211822660098521, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149016 minutes
Weight histogram
[ 145  352  553  851 2849 3574 4717 5595 3223  416] [ -2.45821051e-04   4.24755417e-05   3.30772134e-04   6.19068727e-04
   9.07365320e-04   1.19566191e-03   1.48395851e-03   1.77225510e-03
   2.06055169e-03   2.34884828e-03   2.63714488e-03]
[  136   154   224   332   468   879  1025  2049  6732 10276] [ -2.45821051e-04   4.24755417e-05   3.30772134e-04   6.19068727e-04
   9.07365320e-04   1.19566191e-03   1.48395851e-03   1.77225510e-03
   2.06055169e-03   2.34884828e-03   2.63714488e-03]
-1.32597
1.08666
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.6378
Epoch 1, cost is  2.59787
Epoch 2, cost is  2.55968
Epoch 3, cost is  2.53134
Epoch 4, cost is  2.51134
Training took 0.114588 minutes
Weight histogram
[4182 3709 3092 2792 2011 1680 1982 1147 1403  277] [ -5.50307855e-02  -4.95241354e-02  -4.40174853e-02  -3.85108352e-02
  -3.30041851e-02  -2.74975350e-02  -2.19908849e-02  -1.64842348e-02
  -1.09775847e-02  -5.47093459e-03   3.57155150e-05]
[2475 1327 1291 1566 1894 2188 2471 2608 3204 3251] [ -5.50307855e-02  -4.95241354e-02  -4.40174853e-02  -3.85108352e-02
  -3.30041851e-02  -2.74975350e-02  -2.19908849e-02  -1.64842348e-02
  -1.09775847e-02  -5.47093459e-03   3.57155150e-05]
-1.24699
1.63732
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.145848 minutes
Weight histogram
[   33   293   616  1073  1295  1020  5429 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  279   328   447   631   943  1568  1004  1856  5900 11344] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.115600 minutes
Weight histogram
[4279 3704 3308 2712 1981 1605 1931 1426 2804  550] [ -5.51578365e-02  -4.96384813e-02  -4.41191261e-02  -3.85997709e-02
  -3.30804157e-02  -2.75610605e-02  -2.20417053e-02  -1.65223501e-02
  -1.10029949e-02  -5.48363969e-03   3.57155150e-05]
[4445 1298 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96384813e-02  -4.41191261e-02  -3.85997709e-02
  -3.30804157e-02  -2.75610605e-02  -2.20417053e-02  -1.65223501e-02
  -1.10029949e-02  -5.48363969e-03   3.57155150e-05]
-1.18162
1.30045
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.1906
Epoch 1, cost is  5.041
Epoch 2, cost is  4.88986
Epoch 3, cost is  4.50116
Epoch 4, cost is  4.0811
Epoch 5, cost is  3.76887
Epoch 6, cost is  3.51774
Epoch 7, cost is  3.29371
Epoch 8, cost is  3.1016
Epoch 9, cost is  2.93396
Training took 0.410597 minutes
Weight histogram
[1102  838 1697  208   97   48   29   16    9    6] [-0.01216332 -0.01096072 -0.00975813 -0.00855554 -0.00735294 -0.00615035
 -0.00494775 -0.00374516 -0.00254257 -0.00133997 -0.00013738]
[1165  264  228  260  285  324  343  368  394  419] [-0.01216332 -0.01096072 -0.00975813 -0.00855554 -0.00735294 -0.00615035
 -0.00494775 -0.00374516 -0.00254257 -0.00133997 -0.00013738]
-0.221466
0.25687
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.062944 minutes
Epoch 0
Fine tuning took 0.064624 minutes
Epoch 0
Fine tuning took 0.062474 minutes
Epoch 0
Fine tuning took 0.064818 minutes
Epoch 0
Fine tuning took 0.063051 minutes
Epoch 0
Fine tuning took 0.065075 minutes
Epoch 0
Fine tuning took 0.063383 minutes
Epoch 0
Fine tuning took 0.064455 minutes
Epoch 0
Fine tuning took 0.063394 minutes
Epoch 0
Fine tuning took 0.065000 minutes
{'zero': {0: [0.39162561576354682, 0.40640394088669951, 0.38423645320197042, 0.42733990147783252, 0.3608374384236453, 0.38793103448275862, 0.41502463054187194, 0.42241379310344829, 0.37561576354679804, 0.39532019704433496, 0.50246305418719217], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.35098522167487683, 0.33620689655172414, 0.3288177339901478, 0.39162561576354682, 0.37931034482758619, 0.35837438423645318, 0.36206896551724138, 0.32389162561576357, 0.34975369458128081, 0.3608374384236453, 0.31034482758620691], 5: [0.25738916256157635, 0.25738916256157635, 0.28694581280788178, 0.18103448275862069, 0.25985221674876846, 0.2536945812807882, 0.2229064039408867, 0.2536945812807882, 0.27463054187192121, 0.24384236453201971, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.39162561576354682, 0.3817733990147783, 0.41009852216748771, 0.38300492610837439, 0.40517241379310343, 0.40517241379310343, 0.39655172413793105, 0.40640394088669951, 0.3608374384236453, 0.3608374384236453, 0.45689655172413796], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.35098522167487683, 0.3608374384236453, 0.34729064039408869, 0.39285714285714285, 0.3460591133004926, 0.3817733990147783, 0.35714285714285715, 0.31896551724137934, 0.3854679802955665, 0.39901477832512317, 0.34359605911330049], 5: [0.25738916256157635, 0.25738916256157635, 0.24261083743842365, 0.22413793103448276, 0.24876847290640394, 0.21305418719211822, 0.24630541871921183, 0.27463054187192121, 0.2536945812807882, 0.24014778325123154, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.39162561576354682, 0.41256157635467983, 0.39162561576354682, 0.38793103448275862, 0.45197044334975367, 0.4039408866995074, 0.45320197044334976, 0.41502463054187194, 0.35591133004926107, 0.39778325123152708, 0.47413793103448276], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.35098522167487683, 0.33374384236453203, 0.3251231527093596, 0.41256157635467983, 0.34852216748768472, 0.36699507389162561, 0.31280788177339902, 0.33004926108374383, 0.38054187192118227, 0.3608374384236453, 0.30788177339901479], 5: [0.25738916256157635, 0.2536945812807882, 0.28325123152709358, 0.19950738916256158, 0.19950738916256158, 0.22906403940886699, 0.23399014778325122, 0.25492610837438423, 0.26354679802955666, 0.2413793103448276, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.39162561576354682, 0.40270935960591131, 0.3891625615763547, 0.40147783251231528, 0.3854679802955665, 0.39039408866995073, 0.43103448275862066, 0.42857142857142855, 0.39532019704433496, 0.41133004926108374, 0.47906403940886699], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.35098522167487683, 0.37315270935960593, 0.33128078817733991, 0.40640394088669951, 0.39655172413793105, 0.37068965517241381, 0.35714285714285715, 0.29926108374384236, 0.34113300492610837, 0.35344827586206895, 0.32635467980295568], 5: [0.25738916256157635, 0.22413793103448276, 0.27955665024630544, 0.19211822660098521, 0.21798029556650247, 0.23891625615763548, 0.21182266009852216, 0.27216748768472904, 0.26354679802955666, 0.23522167487684728, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148490 minutes
Weight histogram
[ 145  352  553  851 2849 3574 4717 5595 3223  416] [ -2.45821051e-04   4.24755417e-05   3.30772134e-04   6.19068727e-04
   9.07365320e-04   1.19566191e-03   1.48395851e-03   1.77225510e-03
   2.06055169e-03   2.34884828e-03   2.63714488e-03]
[  136   154   224   332   468   879  1025  2049  6732 10276] [ -2.45821051e-04   4.24755417e-05   3.30772134e-04   6.19068727e-04
   9.07365320e-04   1.19566191e-03   1.48395851e-03   1.77225510e-03
   2.06055169e-03   2.34884828e-03   2.63714488e-03]
-1.32597
1.08666
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.6378
Epoch 1, cost is  2.59787
Epoch 2, cost is  2.55968
Epoch 3, cost is  2.53134
Epoch 4, cost is  2.51134
Training took 0.114234 minutes
Weight histogram
[4182 3709 3092 2792 2011 1680 1982 1147 1403  277] [ -5.50307855e-02  -4.95241354e-02  -4.40174853e-02  -3.85108352e-02
  -3.30041851e-02  -2.74975350e-02  -2.19908849e-02  -1.64842348e-02
  -1.09775847e-02  -5.47093459e-03   3.57155150e-05]
[2475 1327 1291 1566 1894 2188 2471 2608 3204 3251] [ -5.50307855e-02  -4.95241354e-02  -4.40174853e-02  -3.85108352e-02
  -3.30041851e-02  -2.74975350e-02  -2.19908849e-02  -1.64842348e-02
  -1.09775847e-02  -5.47093459e-03   3.57155150e-05]
-1.24699
1.63732
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147396 minutes
Weight histogram
[   33   293   616  1073  1295  1020  5429 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  279   328   447   631   943  1568  1004  1856  5900 11344] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.116140 minutes
Weight histogram
[4279 3704 3308 2712 1981 1605 1931 1426 2804  550] [ -5.51578365e-02  -4.96384813e-02  -4.41191261e-02  -3.85997709e-02
  -3.30804157e-02  -2.75610605e-02  -2.20417053e-02  -1.65223501e-02
  -1.10029949e-02  -5.48363969e-03   3.57155150e-05]
[4445 1298 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96384813e-02  -4.41191261e-02  -3.85997709e-02
  -3.30804157e-02  -2.75610605e-02  -2.20417053e-02  -1.65223501e-02
  -1.10029949e-02  -5.48363969e-03   3.57155150e-05]
-1.18162
1.30045
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.1536
Epoch 1, cost is  4.97612
Epoch 2, cost is  4.49621
Epoch 3, cost is  3.92143
Epoch 4, cost is  3.49509
Epoch 5, cost is  3.17105
Epoch 6, cost is  2.91773
Epoch 7, cost is  2.71666
Epoch 8, cost is  2.55798
Epoch 9, cost is  2.4351
Training took 0.684932 minutes
Weight histogram
[844 733 603 452 447 933  22   8   4   4] [-0.00679093 -0.00612546 -0.00545999 -0.00479452 -0.00412904 -0.00346357
 -0.0027981  -0.00213263 -0.00146716 -0.00080168 -0.00013621]
[940 249 244 269 296 324 357 399 453 519] [-0.00679093 -0.00612546 -0.00545999 -0.00479452 -0.00412904 -0.00346357
 -0.0027981  -0.00213263 -0.00146716 -0.00080168 -0.00013621]
-0.189124
0.202377
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077803 minutes
Epoch 0
Fine tuning took 0.077158 minutes
Epoch 0
Fine tuning took 0.077202 minutes
Epoch 0
Fine tuning took 0.078383 minutes
Epoch 0
Fine tuning took 0.078120 minutes
Epoch 0
Fine tuning took 0.077323 minutes
Epoch 0
Fine tuning took 0.078661 minutes
Epoch 0
Fine tuning took 0.076469 minutes
Epoch 0
Fine tuning took 0.076392 minutes
Epoch 0
Fine tuning took 0.077818 minutes
{'zero': {0: [0.39901477832512317, 0.3608374384236453, 0.39778325123152708, 0.38669950738916259, 0.41995073891625617, 0.39408866995073893, 0.38669950738916259, 0.42857142857142855, 0.3682266009852217, 0.34236453201970446, 0.44704433497536944], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33497536945812806, 0.35467980295566504, 0.34113300492610837, 0.37438423645320196, 0.30911330049261082, 0.33251231527093594, 0.34113300492610837, 0.27339901477832512, 0.29679802955665024, 0.33620689655172414, 0.27216748768472904], 5: [0.26600985221674878, 0.28448275862068967, 0.26108374384236455, 0.23891625615763548, 0.27093596059113301, 0.27339901477832512, 0.27216748768472904, 0.29802955665024633, 0.33497536945812806, 0.32142857142857145, 0.28078817733990147], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.39901477832512317, 0.40886699507389163, 0.4211822660098522, 0.38669950738916259, 0.42364532019704432, 0.39039408866995073, 0.39655172413793105, 0.40886699507389163, 0.3891625615763547, 0.37068965517241381, 0.45443349753694579], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33497536945812806, 0.34359605911330049, 0.35344827586206895, 0.35344827586206895, 0.30788177339901479, 0.34359605911330049, 0.35467980295566504, 0.27093596059113301, 0.31157635467980294, 0.31034482758620691, 0.26724137931034481], 5: [0.26600985221674878, 0.24753694581280788, 0.22536945812807882, 0.25985221674876846, 0.26847290640394089, 0.26600985221674878, 0.24876847290640394, 0.32019704433497537, 0.29926108374384236, 0.31896551724137934, 0.27832512315270935], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.39901477832512317, 0.41379310344827586, 0.39532019704433496, 0.40886699507389163, 0.41256157635467983, 0.37561576354679804, 0.42241379310344829, 0.40640394088669951, 0.38054187192118227, 0.36699507389162561, 0.44211822660098521], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33497536945812806, 0.34482758620689657, 0.37684729064039407, 0.33251231527093594, 0.29556650246305421, 0.32266009852216748, 0.33128078817733991, 0.26354679802955666, 0.29187192118226601, 0.33866995073891626, 0.28448275862068967], 5: [0.26600985221674878, 0.2413793103448276, 0.22783251231527094, 0.25862068965517243, 0.29187192118226601, 0.30172413793103448, 0.24630541871921183, 0.33004926108374383, 0.32758620689655171, 0.29433497536945813, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.39901477832512317, 0.3817733990147783, 0.39778325123152708, 0.42241379310344829, 0.41871921182266009, 0.37068965517241381, 0.44334975369458129, 0.39532019704433496, 0.38300492610837439, 0.34482758620689657, 0.43103448275862066], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33497536945812806, 0.34236453201970446, 0.35221674876847292, 0.35714285714285715, 0.32266009852216748, 0.34236453201970446, 0.32635467980295568, 0.2857142857142857, 0.30295566502463056, 0.3288177339901478, 0.27955665024630544], 5: [0.26600985221674878, 0.27586206896551724, 0.25, 0.22044334975369459, 0.25862068965517243, 0.28694581280788178, 0.23029556650246305, 0.31896551724137934, 0.31403940886699505, 0.32635467980295568, 0.2894088669950739], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236425 minutes
Weight histogram
[ 159  363  627  699 1253 3202 6130 7411 2230  201] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[ 155  247  341  630  838  998 2208 3535 5540 7783] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.21877
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.67726
Epoch 1, cost is  1.64337
Epoch 2, cost is  1.61644
Epoch 3, cost is  1.59514
Epoch 4, cost is  1.57558
Training took 0.225558 minutes
Weight histogram
[4273 3831 3027 2493 1976 1742 1391 1303 1471  768] [ -5.57319373e-02  -5.01514611e-02  -4.45709850e-02  -3.89905088e-02
  -3.34100327e-02  -2.78295565e-02  -2.22490803e-02  -1.66686042e-02
  -1.10881280e-02  -5.50765187e-03   7.28242885e-05]
[2136 1175 1397 1620 1833 2176 2507 2668 3109 3654] [ -5.57319373e-02  -5.01514611e-02  -4.45709850e-02  -3.89905088e-02
  -3.34100327e-02  -2.78295565e-02  -2.22490803e-02  -1.66686042e-02
  -1.10881280e-02  -5.50765187e-03   7.28242885e-05]
-0.848959
1.46688
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149037 minutes
Weight histogram
[   33   303  1006  1469   807   722  5419 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  466   902  1194   295   469   870  1004  1856  5901 11343] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.115956 minutes
Weight histogram
[4282 3707 3312 2703 1984 1605 1930 1088 2733  956] [ -5.51578365e-02  -4.96347704e-02  -4.41117044e-02  -3.85886383e-02
  -3.30655722e-02  -2.75425061e-02  -2.20194400e-02  -1.64963740e-02
  -1.09733079e-02  -5.45024179e-03   7.28242885e-05]
[4445 1298 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96347704e-02  -4.41117044e-02  -3.85886383e-02
  -3.30655722e-02  -2.75425061e-02  -2.20194400e-02  -1.64963740e-02
  -1.09733079e-02  -5.45024179e-03   7.28242885e-05]
-1.18162
1.30045
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.29488
Epoch 1, cost is  6.01106
Epoch 2, cost is  5.75249
Epoch 3, cost is  5.38845
Epoch 4, cost is  5.02301
Epoch 5, cost is  4.72632
Epoch 6, cost is  4.48811
Epoch 7, cost is  4.28607
Epoch 8, cost is  4.11172
Epoch 9, cost is  3.95573
Training took 0.249110 minutes
Weight histogram
[510 480 432 407 383 443 745 475 159  16] [-0.02811378 -0.02532068 -0.02252758 -0.01973448 -0.01694138 -0.01414828
 -0.01135518 -0.00856208 -0.00576898 -0.00297588 -0.00018278]
[810 441 286 281 302 331 361 388 410 440] [-0.02811378 -0.02532068 -0.02252758 -0.01973448 -0.01694138 -0.01414828
 -0.01135518 -0.00856208 -0.00576898 -0.00297588 -0.00018278]
-0.369089
0.502295
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.075749 minutes
Epoch 0
Fine tuning took 0.076575 minutes
Epoch 0
Fine tuning took 0.074984 minutes
Epoch 0
Fine tuning took 0.076753 minutes
Epoch 0
Fine tuning took 0.077869 minutes
Epoch 0
Fine tuning took 0.076996 minutes
Epoch 0
Fine tuning took 0.076757 minutes
Epoch 0
Fine tuning took 0.077638 minutes
Epoch 0
Fine tuning took 0.075671 minutes
Epoch 0
Fine tuning took 0.075122 minutes
{'zero': {0: [0.29556650246305421, 0.65270935960591137, 0.52463054187192115, 0.51231527093596063, 0.70320197044334976, 0.58004926108374388, 0.49261083743842365, 0.32142857142857145, 0.42980295566502463, 0.39778325123152708, 0.53817733990147787], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.35467980295566504, 0.22906403940886699, 0.12561576354679804, 0.32266009852216748, 0.23152709359605911, 0.37561576354679804, 0.28817733990147781, 0.29556650246305421, 0.4605911330049261, 0.36576354679802958, 0.26970443349753692], 5: [0.34975369458128081, 0.11822660098522167, 0.34975369458128081, 0.16502463054187191, 0.065270935960591137, 0.044334975369458129, 0.21921182266009853, 0.38300492610837439, 0.10960591133004927, 0.23645320197044334, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29556650246305421, 0.58497536945812811, 0.52216748768472909, 0.52955665024630538, 0.71921182266009853, 0.48399014778325122, 0.48522167487684731, 0.26847290640394089, 0.41379310344827586, 0.28817733990147781, 0.49876847290640391], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.35467980295566504, 0.26970443349753692, 0.1206896551724138, 0.24753694581280788, 0.21182266009852216, 0.46674876847290642, 0.27832512315270935, 0.24753694581280788, 0.46305418719211822, 0.42980295566502463, 0.26724137931034481], 5: [0.34975369458128081, 0.14532019704433496, 0.35714285714285715, 0.2229064039408867, 0.068965517241379309, 0.049261083743842367, 0.23645320197044334, 0.48399014778325122, 0.12315270935960591, 0.28201970443349755, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29556650246305421, 0.56773399014778325, 0.4963054187192118, 0.51970443349753692, 0.72044334975369462, 0.51724137931034486, 0.49507389162561577, 0.25985221674876846, 0.4248768472906404, 0.31280788177339902, 0.50492610837438423], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.35467980295566504, 0.26231527093596058, 0.12931034482758622, 0.22660098522167488, 0.20566502463054187, 0.43719211822660098, 0.26231527093596058, 0.2857142857142857, 0.43719211822660098, 0.38054187192118227, 0.23152709359605911], 5: [0.34975369458128081, 0.16995073891625614, 0.37438423645320196, 0.2536945812807882, 0.073891625615763554, 0.045566502463054187, 0.24261083743842365, 0.45443349753694579, 0.13793103448275862, 0.30665024630541871, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29556650246305421, 0.56773399014778325, 0.5073891625615764, 0.52216748768472909, 0.68472906403940892, 0.50123152709359609, 0.47413793103448276, 0.26847290640394089, 0.39655172413793105, 0.30172413793103448, 0.48522167487684731], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.35467980295566504, 0.26108374384236455, 0.13177339901477833, 0.24384236453201971, 0.23029556650246305, 0.44704433497536944, 0.27586206896551724, 0.22536945812807882, 0.47536945812807879, 0.43349753694581283, 0.26231527093596058], 5: [0.34975369458128081, 0.17118226600985223, 0.3608374384236453, 0.23399014778325122, 0.084975369458128072, 0.051724137931034482, 0.25, 0.50615763546798032, 0.12807881773399016, 0.26477832512315269, 0.25246305418719212], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235426 minutes
Weight histogram
[ 159  363  627  699 1253 3202 6130 7411 2230  201] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[ 155  247  341  630  838  998 2208 3535 5540 7783] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.21877
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.67726
Epoch 1, cost is  1.64337
Epoch 2, cost is  1.61644
Epoch 3, cost is  1.59514
Epoch 4, cost is  1.57558
Training took 0.222828 minutes
Weight histogram
[4273 3831 3027 2493 1976 1742 1391 1303 1471  768] [ -5.57319373e-02  -5.01514611e-02  -4.45709850e-02  -3.89905088e-02
  -3.34100327e-02  -2.78295565e-02  -2.22490803e-02  -1.66686042e-02
  -1.10881280e-02  -5.50765187e-03   7.28242885e-05]
[2136 1175 1397 1620 1833 2176 2507 2668 3109 3654] [ -5.57319373e-02  -5.01514611e-02  -4.45709850e-02  -3.89905088e-02
  -3.34100327e-02  -2.78295565e-02  -2.22490803e-02  -1.66686042e-02
  -1.10881280e-02  -5.50765187e-03   7.28242885e-05]
-0.848959
1.46688
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149261 minutes
Weight histogram
[   33   303  1006  1469   807   722  5419 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  466   902  1194   295   469   870  1004  1856  5901 11343] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.115753 minutes
Weight histogram
[4282 3707 3312 2703 1984 1605 1930 1088 2733  956] [ -5.51578365e-02  -4.96347704e-02  -4.41117044e-02  -3.85886383e-02
  -3.30655722e-02  -2.75425061e-02  -2.20194400e-02  -1.64963740e-02
  -1.09733079e-02  -5.45024179e-03   7.28242885e-05]
[4445 1298 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96347704e-02  -4.41117044e-02  -3.85886383e-02
  -3.30655722e-02  -2.75425061e-02  -2.20194400e-02  -1.64963740e-02
  -1.09733079e-02  -5.45024179e-03   7.28242885e-05]
-1.18162
1.30045
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.66337
Epoch 1, cost is  5.38256
Epoch 2, cost is  5.06946
Epoch 3, cost is  4.60236
Epoch 4, cost is  4.19351
Epoch 5, cost is  3.88852
Epoch 6, cost is  3.64524
Epoch 7, cost is  3.44424
Epoch 8, cost is  3.27164
Epoch 9, cost is  3.12191
Training took 0.352725 minutes
Weight histogram
[ 704  562  499  414  458 1011  309   64   20    9] [-0.01845267 -0.01662259 -0.0147925  -0.01296242 -0.01113233 -0.00930225
 -0.00747216 -0.00564207 -0.00381199 -0.0019819  -0.00015182]
[1010  280  253  265  301  340  365  390  412  434] [-0.01845267 -0.01662259 -0.0147925  -0.01296242 -0.01113233 -0.00930225
 -0.00747216 -0.00564207 -0.00381199 -0.0019819  -0.00015182]
-0.253126
0.338259
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081047 minutes
Epoch 0
Fine tuning took 0.082344 minutes
Epoch 0
Fine tuning took 0.081349 minutes
Epoch 0
Fine tuning took 0.081284 minutes
Epoch 0
Fine tuning took 0.082168 minutes
Epoch 0
Fine tuning took 0.080919 minutes
Epoch 0
Fine tuning took 0.081213 minutes
Epoch 0
Fine tuning took 0.080853 minutes
Epoch 0
Fine tuning took 0.081319 minutes
Epoch 0
Fine tuning took 0.082424 minutes
{'zero': {0: [0.32635467980295568, 0.58743842364532017, 0.56896551724137934, 0.51231527093596063, 0.53940886699507384, 0.41871921182266009, 0.38300492610837439, 0.45073891625615764, 0.52586206896551724, 0.41871921182266009, 0.49384236453201968], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.39778325123152708, 0.27709359605911332, 0.25123152709359609, 0.31403940886699505, 0.30788177339901479, 0.41009852216748771, 0.41379310344827586, 0.32389162561576357, 0.28694581280788178, 0.38669950738916259, 0.37807881773399016], 5: [0.27586206896551724, 0.1354679802955665, 0.17980295566502463, 0.17364532019704434, 0.15270935960591134, 0.17118226600985223, 0.20320197044334976, 0.22536945812807882, 0.18719211822660098, 0.19458128078817735, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.32635467980295568, 0.59482758620689657, 0.53325123152709364, 0.51600985221674878, 0.57019704433497542, 0.41133004926108374, 0.46798029556650245, 0.47413793103448276, 0.55295566502463056, 0.47290640394088668, 0.54802955665024633], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.39778325123152708, 0.2376847290640394, 0.2536945812807882, 0.34852216748768472, 0.31773399014778325, 0.44581280788177341, 0.37684729064039407, 0.28817733990147781, 0.28201970443349755, 0.38423645320197042, 0.34113300492610837], 5: [0.27586206896551724, 0.16748768472906403, 0.21305418719211822, 0.1354679802955665, 0.11206896551724138, 0.14285714285714285, 0.15517241379310345, 0.2376847290640394, 0.16502463054187191, 0.14285714285714285, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.32635467980295568, 0.58497536945812811, 0.52832512315270941, 0.51600985221674878, 0.59605911330049266, 0.4039408866995074, 0.47536945812807879, 0.5, 0.55541871921182262, 0.48275862068965519, 0.52586206896551724], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.39778325123152708, 0.24876847290640394, 0.22536945812807882, 0.32758620689655171, 0.25862068965517243, 0.45935960591133007, 0.38793103448275862, 0.27955665024630544, 0.27339901477832512, 0.3608374384236453, 0.35098522167487683], 5: [0.27586206896551724, 0.16625615763546797, 0.24630541871921183, 0.15640394088669951, 0.14532019704433496, 0.13669950738916256, 0.13669950738916256, 0.22044334975369459, 0.17118226600985223, 0.15640394088669951, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.32635467980295568, 0.58251231527093594, 0.55295566502463056, 0.50369458128078815, 0.59482758620689657, 0.43719211822660098, 0.48891625615763545, 0.52339901477832518, 0.5431034482758621, 0.52463054187192115, 0.55911330049261088], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.39778325123152708, 0.26108374384236455, 0.25246305418719212, 0.3460591133004926, 0.27463054187192121, 0.43719211822660098, 0.36206896551724138, 0.25246305418719212, 0.26847290640394089, 0.33866995073891626, 0.33374384236453203], 5: [0.27586206896551724, 0.15640394088669951, 0.19458128078817735, 0.15024630541871922, 0.13054187192118227, 0.12561576354679804, 0.14901477832512317, 0.22413793103448276, 0.18842364532019704, 0.13669950738916256, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234166 minutes
Weight histogram
[ 159  363  627  699 1253 3202 6130 7411 2230  201] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[ 155  247  341  630  838  998 2208 3535 5540 7783] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.21877
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.67726
Epoch 1, cost is  1.64337
Epoch 2, cost is  1.61644
Epoch 3, cost is  1.59514
Epoch 4, cost is  1.57558
Training took 0.222413 minutes
Weight histogram
[4273 3831 3027 2493 1976 1742 1391 1303 1471  768] [ -5.57319373e-02  -5.01514611e-02  -4.45709850e-02  -3.89905088e-02
  -3.34100327e-02  -2.78295565e-02  -2.22490803e-02  -1.66686042e-02
  -1.10881280e-02  -5.50765187e-03   7.28242885e-05]
[2136 1175 1397 1620 1833 2176 2507 2668 3109 3654] [ -5.57319373e-02  -5.01514611e-02  -4.45709850e-02  -3.89905088e-02
  -3.34100327e-02  -2.78295565e-02  -2.22490803e-02  -1.66686042e-02
  -1.10881280e-02  -5.50765187e-03   7.28242885e-05]
-0.848959
1.46688
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147053 minutes
Weight histogram
[   33   303  1006  1469   807   722  5419 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  466   902  1194   295   469   870  1004  1856  5901 11343] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.114558 minutes
Weight histogram
[4282 3707 3312 2703 1984 1605 1930 1088 2733  956] [ -5.51578365e-02  -4.96347704e-02  -4.41117044e-02  -3.85886383e-02
  -3.30655722e-02  -2.75425061e-02  -2.20194400e-02  -1.64963740e-02
  -1.09733079e-02  -5.45024179e-03   7.28242885e-05]
[4445 1298 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96347704e-02  -4.41117044e-02  -3.85886383e-02
  -3.30655722e-02  -2.75425061e-02  -2.20194400e-02  -1.64963740e-02
  -1.09733079e-02  -5.45024179e-03   7.28242885e-05]
-1.18162
1.30045
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.05923
Epoch 1, cost is  4.82999
Epoch 2, cost is  4.52733
Epoch 3, cost is  4.07999
Epoch 4, cost is  3.70136
Epoch 5, cost is  3.41186
Epoch 6, cost is  3.1743
Epoch 7, cost is  2.973
Epoch 8, cost is  2.80526
Epoch 9, cost is  2.6647
Training took 0.543890 minutes
Weight histogram
[ 877  766  613 1380  244   89   42   22   10    7] [-0.01305922 -0.0117676  -0.01047598 -0.00918436 -0.00789274 -0.00660113
 -0.00530951 -0.00401789 -0.00272627 -0.00143465 -0.00014304]
[1021  271  257  272  305  333  357  379  409  446] [-0.01305922 -0.0117676  -0.01047598 -0.00918436 -0.00789274 -0.00660113
 -0.00530951 -0.00401789 -0.00272627 -0.00143465 -0.00014304]
-0.204205
0.230387
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.091199 minutes
Epoch 0
Fine tuning took 0.092131 minutes
Epoch 0
Fine tuning took 0.090609 minutes
Epoch 0
Fine tuning took 0.089694 minutes
Epoch 0
Fine tuning took 0.091321 minutes
Epoch 0
Fine tuning took 0.091457 minutes
Epoch 0
Fine tuning took 0.091766 minutes
Epoch 0
Fine tuning took 0.090683 minutes
Epoch 0
Fine tuning took 0.091618 minutes
Epoch 0
Fine tuning took 0.089704 minutes
{'zero': {0: [0.3608374384236453, 0.41379310344827586, 0.3817733990147783, 0.34729064039408869, 0.43719211822660098, 0.42980295566502463, 0.48275862068965519, 0.39901477832512317, 0.43472906403940886, 0.41502463054187194, 0.49261083743842365], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.38054187192118227, 0.38793103448275862, 0.32142857142857145, 0.46921182266009853, 0.32019704433497537, 0.37438423645320196, 0.25615763546798032, 0.37068965517241381, 0.33743842364532017, 0.34729064039408869, 0.3460591133004926], 5: [0.25862068965517243, 0.19827586206896552, 0.29679802955665024, 0.18349753694581281, 0.24261083743842365, 0.19581280788177341, 0.26108374384236455, 0.23029556650246305, 0.22783251231527094, 0.2376847290640394, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3608374384236453, 0.40640394088669951, 0.42364532019704432, 0.34236453201970446, 0.40886699507389163, 0.40270935960591131, 0.43472906403940886, 0.39655172413793105, 0.37561576354679804, 0.4605911330049261, 0.5073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.38054187192118227, 0.39532019704433496, 0.31157635467980294, 0.44950738916256155, 0.37807881773399016, 0.41256157635467983, 0.29556650246305421, 0.36206896551724138, 0.40640394088669951, 0.30049261083743845, 0.31650246305418717], 5: [0.25862068965517243, 0.19827586206896552, 0.26477832512315269, 0.20812807881773399, 0.21305418719211822, 0.18472906403940886, 0.26970443349753692, 0.2413793103448276, 0.21798029556650247, 0.23891625615763548, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3608374384236453, 0.41871921182266009, 0.40270935960591131, 0.39285714285714285, 0.43472906403940886, 0.43596059113300495, 0.4642857142857143, 0.42241379310344829, 0.42733990147783252, 0.47536945812807879, 0.48891625615763545], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.38054187192118227, 0.37684729064039407, 0.3251231527093596, 0.43349753694581283, 0.36206896551724138, 0.36699507389162561, 0.27955665024630544, 0.33004926108374383, 0.3682266009852217, 0.30911330049261082, 0.3288177339901478], 5: [0.25862068965517243, 0.20443349753694581, 0.27216748768472904, 0.17364532019704434, 0.20320197044334976, 0.19704433497536947, 0.25615763546798032, 0.24753694581280788, 0.20443349753694581, 0.21551724137931033, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3608374384236453, 0.42733990147783252, 0.39901477832512317, 0.3288177339901478, 0.41502463054187194, 0.3891625615763547, 0.47044334975369456, 0.43103448275862066, 0.43349753694581283, 0.45566502463054187, 0.45689655172413796], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.38054187192118227, 0.36576354679802958, 0.33743842364532017, 0.4605911330049261, 0.35714285714285715, 0.40147783251231528, 0.27709359605911332, 0.31527093596059114, 0.3460591133004926, 0.33866995073891626, 0.31896551724137934], 5: [0.25862068965517243, 0.20689655172413793, 0.26354679802955666, 0.2105911330049261, 0.22783251231527094, 0.20935960591133004, 0.25246305418719212, 0.2536945812807882, 0.22044334975369459, 0.20566502463054187, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233709 minutes
Weight histogram
[ 159  363  627  699 1253 3202 6130 7411 2230  201] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[ 155  247  341  630  838  998 2208 3535 5540 7783] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.21877
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.67726
Epoch 1, cost is  1.64337
Epoch 2, cost is  1.61644
Epoch 3, cost is  1.59514
Epoch 4, cost is  1.57558
Training took 0.222019 minutes
Weight histogram
[4273 3831 3027 2493 1976 1742 1391 1303 1471  768] [ -5.57319373e-02  -5.01514611e-02  -4.45709850e-02  -3.89905088e-02
  -3.34100327e-02  -2.78295565e-02  -2.22490803e-02  -1.66686042e-02
  -1.10881280e-02  -5.50765187e-03   7.28242885e-05]
[2136 1175 1397 1620 1833 2176 2507 2668 3109 3654] [ -5.57319373e-02  -5.01514611e-02  -4.45709850e-02  -3.89905088e-02
  -3.34100327e-02  -2.78295565e-02  -2.22490803e-02  -1.66686042e-02
  -1.10881280e-02  -5.50765187e-03   7.28242885e-05]
-0.848959
1.46688
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147680 minutes
Weight histogram
[   33   303  1006  1469   807   722  5419 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  466   902  1194   295   469   870  1004  1856  5901 11343] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.114298 minutes
Weight histogram
[4282 3707 3312 2703 1984 1605 1930 1088 2733  956] [ -5.51578365e-02  -4.96347704e-02  -4.41117044e-02  -3.85886383e-02
  -3.30655722e-02  -2.75425061e-02  -2.20194400e-02  -1.64963740e-02
  -1.09733079e-02  -5.45024179e-03   7.28242885e-05]
[4445 1298 1294 1586 1979 2291 2497 2529 3120 3261] [ -5.51578365e-02  -4.96347704e-02  -4.41117044e-02  -3.85886383e-02
  -3.30655722e-02  -2.75425061e-02  -2.20194400e-02  -1.64963740e-02
  -1.09733079e-02  -5.45024179e-03   7.28242885e-05]
-1.18162
1.30045
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.83979
Epoch 1, cost is  4.57102
Epoch 2, cost is  3.98996
Epoch 3, cost is  3.49344
Epoch 4, cost is  3.14708
Epoch 5, cost is  2.86978
Epoch 6, cost is  2.65022
Epoch 7, cost is  2.47785
Epoch 8, cost is  2.33798
Epoch 9, cost is  2.22065
Training took 0.938318 minutes
Weight histogram
[ 921  743  690  480 1022  143   29   12    6    4] [-0.00805968 -0.00726779 -0.0064759  -0.00568401 -0.00489211 -0.00410022
 -0.00330833 -0.00251644 -0.00172455 -0.00093266 -0.00014077]
[840 246 252 289 319 341 372 413 459 519] [-0.00805968 -0.00726779 -0.0064759  -0.00568401 -0.00489211 -0.00410022
 -0.00330833 -0.00251644 -0.00172455 -0.00093266 -0.00014077]
-0.171795
0.190115
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.109490 minutes
Epoch 0
Fine tuning took 0.110160 minutes
Epoch 0
Fine tuning took 0.110250 minutes
Epoch 0
Fine tuning took 0.109451 minutes
Epoch 0
Fine tuning took 0.109709 minutes
Epoch 0
Fine tuning took 0.110208 minutes
Epoch 0
Fine tuning took 0.110569 minutes
Epoch 0
Fine tuning took 0.110787 minutes
Epoch 0
Fine tuning took 0.109315 minutes
Epoch 0
Fine tuning took 0.110370 minutes
{'zero': {0: [0.40147783251231528, 0.43842364532019706, 0.40517241379310343, 0.43349753694581283, 0.39778325123152708, 0.3891625615763547, 0.46551724137931033, 0.41502463054187194, 0.42733990147783252, 0.37561576354679804, 0.49014778325123154], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33374384236453203, 0.30172413793103448, 0.35714285714285715, 0.36206896551724138, 0.35221674876847292, 0.31650246305418717, 0.33004926108374383, 0.29310344827586204, 0.29433497536945813, 0.32389162561576357, 0.29433497536945813], 5: [0.26477832512315269, 0.25985221674876846, 0.2376847290640394, 0.20443349753694581, 0.25, 0.29433497536945813, 0.20443349753694581, 0.29187192118226601, 0.27832512315270935, 0.30049261083743845, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.40147783251231528, 0.42241379310344829, 0.39901477832512317, 0.41133004926108374, 0.36945812807881773, 0.40270935960591131, 0.45689655172413796, 0.39408866995073893, 0.35960591133004927, 0.37438423645320196, 0.46921182266009853], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33374384236453203, 0.31034482758620691, 0.3682266009852217, 0.36576354679802958, 0.37561576354679804, 0.33990147783251229, 0.32019704433497537, 0.30788177339901479, 0.32758620689655171, 0.33251231527093594, 0.30541871921182268], 5: [0.26477832512315269, 0.26724137931034481, 0.23275862068965517, 0.2229064039408867, 0.25492610837438423, 0.25738916256157635, 0.2229064039408867, 0.29802955665024633, 0.31280788177339902, 0.29310344827586204, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.40147783251231528, 0.42241379310344829, 0.38054187192118227, 0.46551724137931033, 0.36576354679802958, 0.3854679802955665, 0.44704433497536944, 0.38423645320197042, 0.42241379310344829, 0.3608374384236453, 0.45566502463054187], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33374384236453203, 0.30541871921182268, 0.38669950738916259, 0.34852216748768472, 0.40270935960591131, 0.34236453201970446, 0.3460591133004926, 0.2857142857142857, 0.32019704433497537, 0.34482758620689657, 0.28817733990147781], 5: [0.26477832512315269, 0.27216748768472904, 0.23275862068965517, 0.18596059113300492, 0.23152709359605911, 0.27216748768472904, 0.20689655172413793, 0.33004926108374383, 0.25738916256157635, 0.29433497536945813, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.40147783251231528, 0.46182266009852219, 0.41133004926108374, 0.44334975369458129, 0.3854679802955665, 0.39901477832512317, 0.45197044334975367, 0.4211822660098522, 0.38423645320197042, 0.34975369458128081, 0.46921182266009853], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33374384236453203, 0.29064039408866993, 0.36576354679802958, 0.36576354679802958, 0.36330049261083741, 0.32142857142857145, 0.36576354679802958, 0.27709359605911332, 0.2857142857142857, 0.37684729064039407, 0.2894088669950739], 5: [0.26477832512315269, 0.24753694581280788, 0.2229064039408867, 0.19088669950738915, 0.25123152709359609, 0.27955665024630544, 0.18226600985221675, 0.30172413793103448, 0.33004926108374383, 0.27339901477832512, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.419754 minutes
Weight histogram
[ 118  345  521  648  780 2894 6750 6645 3112  462] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1140  989  766 1011 1404 2394 2487 3360 3304 5420] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.16213
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.14414
Epoch 1, cost is  1.11803
Epoch 2, cost is  1.10041
Epoch 3, cost is  1.08692
Epoch 4, cost is  1.0736
Training took 0.646273 minutes
Weight histogram
[5124 4241 3053 2533 1679 1466 1313  970  936  960] [ -4.04839218e-02  -3.64407767e-02  -3.23976316e-02  -2.83544864e-02
  -2.43113413e-02  -2.02681962e-02  -1.62250511e-02  -1.21819060e-02
  -8.13876092e-03  -4.09561582e-03  -5.24707102e-05]
[1842 1256 1316 1476 1883 2051 2688 2844 3068 3851] [ -4.04839218e-02  -3.64407767e-02  -3.23976316e-02  -2.83544864e-02
  -2.43113413e-02  -2.02681962e-02  -1.62250511e-02  -1.21819060e-02
  -8.13876092e-03  -4.09561582e-03  -5.24707102e-05]
-0.793862
1.46343
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147696 minutes
Weight histogram
[   33   314  1753   890   629   721  5419 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[ 2167   173   222   295   467   871  1005  1855  5897 11348] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.114609 minutes
Weight histogram
[4279 3704 3308 2711 1982 1605 1931 1086 2126 1568] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
[4443 1299 1293 1587 1978 2292 2497 2529 3121 3261] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
-1.18162
1.30045
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.45503
Epoch 1, cost is  6.09546
Epoch 2, cost is  5.6125
Epoch 3, cost is  5.19655
Epoch 4, cost is  4.87563
Epoch 5, cost is  4.62314
Epoch 6, cost is  4.41139
Epoch 7, cost is  4.23285
Epoch 8, cost is  4.08068
Epoch 9, cost is  3.94463
Training took 0.322300 minutes
Weight histogram
[509 507 443 406 369 330 319 386 679 102] [-0.03244205 -0.02921506 -0.02598806 -0.02276106 -0.01953407 -0.01630707
 -0.01308008 -0.00985308 -0.00662609 -0.00339909 -0.00017209]
[677 294 278 303 336 370 403 429 460 500] [-0.03244205 -0.02921506 -0.02598806 -0.02276106 -0.01953407 -0.01630707
 -0.01308008 -0.00985308 -0.00662609 -0.00339909 -0.00017209]
-0.411576
0.634183
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.142389 minutes
Epoch 0
Fine tuning took 0.142407 minutes
Epoch 0
Fine tuning took 0.141979 minutes
Epoch 0
Fine tuning took 0.141104 minutes
Epoch 0
Fine tuning took 0.142593 minutes
Epoch 0
Fine tuning took 0.142813 minutes
Epoch 0
Fine tuning took 0.142853 minutes
Epoch 0
Fine tuning took 0.143431 minutes
Epoch 0
Fine tuning took 0.141454 minutes
Epoch 0
Fine tuning took 0.142037 minutes
{'zero': {0: [0.26600985221674878, 0.6071428571428571, 0.7426108374384236, 0.66009852216748766, 0.7931034482758621, 0.68226600985221675, 0.33251231527093594, 0.32389162561576357, 0.47660098522167488, 0.43842364532019706, 0.43349753694581283], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.38300492610837439, 0.23645320197044334, 0.099753694581280791, 0.27216748768472904, 0.077586206896551727, 0.2376847290640394, 0.52955665024630538, 0.25985221674876846, 0.37684729064039407, 0.41379310344827586, 0.39285714285714285], 5: [0.35098522167487683, 0.15640394088669951, 0.15763546798029557, 0.067733990147783252, 0.12931034482758622, 0.080049261083743842, 0.13793103448275862, 0.41625615763546797, 0.14655172413793102, 0.14778325123152711, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26600985221674878, 0.58004926108374388, 0.74753694581280783, 0.65024630541871919, 0.73645320197044339, 0.63054187192118227, 0.33743842364532017, 0.22044334975369459, 0.49753694581280788, 0.49014778325123154, 0.36206896551724138], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.38300492610837439, 0.2413793103448276, 0.10591133004926108, 0.24384236453201971, 0.11699507389162561, 0.28078817733990147, 0.52463054187192115, 0.21428571428571427, 0.3288177339901478, 0.35960591133004927, 0.31896551724137934], 5: [0.35098522167487683, 0.17857142857142858, 0.14655172413793102, 0.10591133004926108, 0.14655172413793102, 0.088669950738916259, 0.13793103448275862, 0.56527093596059108, 0.17364532019704434, 0.15024630541871922, 0.31896551724137934], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26600985221674878, 0.58743842364532017, 0.71921182266009853, 0.65517241379310343, 0.73522167487684731, 0.65024630541871919, 0.32758620689655171, 0.25492610837438423, 0.48768472906403942, 0.49876847290640391, 0.36576354679802958], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.38300492610837439, 0.24014778325123154, 0.10344827586206896, 0.22536945812807882, 0.14532019704433496, 0.25862068965517243, 0.53817733990147787, 0.20812807881773399, 0.32142857142857145, 0.3608374384236453, 0.30418719211822659], 5: [0.35098522167487683, 0.17241379310344829, 0.17733990147783252, 0.11945812807881774, 0.11945812807881774, 0.091133004926108374, 0.13423645320197045, 0.53694581280788178, 0.19088669950738915, 0.14039408866995073, 0.33004926108374383], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26600985221674878, 0.5714285714285714, 0.74384236453201968, 0.64532019704433496, 0.75123152709359609, 0.65886699507389157, 0.34975369458128081, 0.2229064039408867, 0.52339901477832518, 0.47660098522167488, 0.35714285714285715], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.38300492610837439, 0.22044334975369459, 0.11330049261083744, 0.25615763546798032, 0.10591133004926108, 0.24876847290640394, 0.50862068965517238, 0.22167487684729065, 0.29187192118226601, 0.37438423645320196, 0.30049261083743845], 5: [0.35098522167487683, 0.20812807881773399, 0.14285714285714285, 0.098522167487684734, 0.14285714285714285, 0.092364532019704432, 0.14162561576354679, 0.55541871921182262, 0.18472906403940886, 0.14901477832512317, 0.34236453201970446], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420746 minutes
Weight histogram
[ 118  345  521  648  780 2894 6750 6645 3112  462] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1140  989  766 1011 1404 2394 2487 3360 3304 5420] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.16213
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.14414
Epoch 1, cost is  1.11803
Epoch 2, cost is  1.10041
Epoch 3, cost is  1.08692
Epoch 4, cost is  1.0736
Training took 0.644192 minutes
Weight histogram
[5124 4241 3053 2533 1679 1466 1313  970  936  960] [ -4.04839218e-02  -3.64407767e-02  -3.23976316e-02  -2.83544864e-02
  -2.43113413e-02  -2.02681962e-02  -1.62250511e-02  -1.21819060e-02
  -8.13876092e-03  -4.09561582e-03  -5.24707102e-05]
[1842 1256 1316 1476 1883 2051 2688 2844 3068 3851] [ -4.04839218e-02  -3.64407767e-02  -3.23976316e-02  -2.83544864e-02
  -2.43113413e-02  -2.02681962e-02  -1.62250511e-02  -1.21819060e-02
  -8.13876092e-03  -4.09561582e-03  -5.24707102e-05]
-0.793862
1.46343
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146515 minutes
Weight histogram
[   33   314  1753   890   629   721  5419 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[ 2167   173   222   295   467   871  1005  1855  5897 11348] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.115342 minutes
Weight histogram
[4279 3704 3308 2711 1982 1605 1931 1086 2126 1568] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
[4443 1299 1293 1587 1978 2292 2497 2529 3121 3261] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
-1.18162
1.30045
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.95732
Epoch 1, cost is  5.48702
Epoch 2, cost is  4.88084
Epoch 3, cost is  4.42276
Epoch 4, cost is  4.06517
Epoch 5, cost is  3.77265
Epoch 6, cost is  3.53626
Epoch 7, cost is  3.34997
Epoch 8, cost is  3.19315
Epoch 9, cost is  3.05917
Training took 0.497341 minutes
Weight histogram
[594 542 495 444 405 348 333 658 217  14] [-0.02037968 -0.0183576  -0.01633552 -0.01431344 -0.01229136 -0.01026928
 -0.00824721 -0.00622513 -0.00420305 -0.00218097 -0.00015889]
[705 280 298 323 337 355 384 415 455 498] [-0.02037968 -0.0183576  -0.01633552 -0.01431344 -0.01229136 -0.01026928
 -0.00824721 -0.00622513 -0.00420305 -0.00218097 -0.00015889]
-0.258371
0.418584
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.151173 minutes
Epoch 0
Fine tuning took 0.151135 minutes
Epoch 0
Fine tuning took 0.150618 minutes
Epoch 0
Fine tuning took 0.149430 minutes
Epoch 0
Fine tuning took 0.150669 minutes
Epoch 0
Fine tuning took 0.150918 minutes
Epoch 0
Fine tuning took 0.149087 minutes
Epoch 0
Fine tuning took 0.150202 minutes
Epoch 0
Fine tuning took 0.150935 minutes
Epoch 0
Fine tuning took 0.150414 minutes
{'zero': {0: [0.31650246305418717, 0.61083743842364535, 0.5923645320197044, 0.48522167487684731, 0.62561576354679804, 0.46674876847290642, 0.50369458128078815, 0.47413793103448276, 0.47167487684729065, 0.45812807881773399, 0.49261083743842365], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.23152709359605911, 0.23891625615763548, 0.36699507389162561, 0.29310344827586204, 0.45812807881773399, 0.3251231527093596, 0.31773399014778325, 0.36330049261083741, 0.36945812807881773, 0.4039408866995074], 5: [0.31896551724137934, 0.15763546798029557, 0.16871921182266009, 0.14778325123152711, 0.081280788177339899, 0.075123152709359611, 0.17118226600985223, 0.20812807881773399, 0.16502463054187191, 0.17241379310344829, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31650246305418717, 0.61945812807881773, 0.61822660098522164, 0.5073891625615764, 0.65394088669950734, 0.47906403940886699, 0.52832512315270941, 0.45197044334975367, 0.4605911330049261, 0.43596059113300495, 0.50369458128078815], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.23399014778325122, 0.16871921182266009, 0.35960591133004927, 0.23891625615763548, 0.43596059113300495, 0.3288177339901478, 0.27339901477832512, 0.34359605911330049, 0.32019704433497537, 0.39655172413793105], 5: [0.31896551724137934, 0.14655172413793102, 0.21305418719211822, 0.13300492610837439, 0.10714285714285714, 0.084975369458128072, 0.14285714285714285, 0.27463054187192121, 0.19581280788177341, 0.24384236453201971, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31650246305418717, 0.60960591133004927, 0.66625615763546797, 0.51847290640394084, 0.65394088669950734, 0.46674876847290642, 0.53817733990147787, 0.45812807881773399, 0.45197044334975367, 0.44458128078817732, 0.49507389162561577], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.21921182266009853, 0.13916256157635468, 0.35221674876847292, 0.24384236453201971, 0.43472906403940886, 0.30665024630541871, 0.26724137931034481, 0.34975369458128081, 0.33990147783251229, 0.39532019704433496], 5: [0.31896551724137934, 0.17118226600985223, 0.19458128078817735, 0.12931034482758622, 0.10221674876847291, 0.098522167487684734, 0.15517241379310345, 0.27463054187192121, 0.19827586206896552, 0.21551724137931033, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31650246305418717, 0.56896551724137934, 0.64901477832512311, 0.52955665024630538, 0.65886699507389157, 0.45935960591133007, 0.52093596059113301, 0.45320197044334976, 0.45689655172413796, 0.46798029556650245, 0.48399014778325122], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.2413793103448276, 0.13177339901477833, 0.33866995073891626, 0.25, 0.46551724137931033, 0.32266009852216748, 0.27832512315270935, 0.35344827586206895, 0.31650246305418717, 0.41748768472906406], 5: [0.31896551724137934, 0.18965517241379309, 0.21921182266009853, 0.13177339901477833, 0.091133004926108374, 0.075123152709359611, 0.15640394088669951, 0.26847290640394089, 0.18965517241379309, 0.21551724137931033, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420609 minutes
Weight histogram
[ 118  345  521  648  780 2894 6750 6645 3112  462] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1140  989  766 1011 1404 2394 2487 3360 3304 5420] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.16213
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.14414
Epoch 1, cost is  1.11803
Epoch 2, cost is  1.10041
Epoch 3, cost is  1.08692
Epoch 4, cost is  1.0736
Training took 0.645344 minutes
Weight histogram
[5124 4241 3053 2533 1679 1466 1313  970  936  960] [ -4.04839218e-02  -3.64407767e-02  -3.23976316e-02  -2.83544864e-02
  -2.43113413e-02  -2.02681962e-02  -1.62250511e-02  -1.21819060e-02
  -8.13876092e-03  -4.09561582e-03  -5.24707102e-05]
[1842 1256 1316 1476 1883 2051 2688 2844 3068 3851] [ -4.04839218e-02  -3.64407767e-02  -3.23976316e-02  -2.83544864e-02
  -2.43113413e-02  -2.02681962e-02  -1.62250511e-02  -1.21819060e-02
  -8.13876092e-03  -4.09561582e-03  -5.24707102e-05]
-0.793862
1.46343
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148532 minutes
Weight histogram
[   33   314  1753   890   629   721  5419 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[ 2167   173   222   295   467   871  1005  1855  5897 11348] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.114324 minutes
Weight histogram
[4279 3704 3308 2711 1982 1605 1931 1086 2126 1568] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
[4443 1299 1293 1587 1978 2292 2497 2529 3121 3261] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
-1.18162
1.30045
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.33324
Epoch 1, cost is  4.81527
Epoch 2, cost is  4.20784
Epoch 3, cost is  3.75771
Epoch 4, cost is  3.41386
Epoch 5, cost is  3.14947
Epoch 6, cost is  2.94575
Epoch 7, cost is  2.7829
Epoch 8, cost is  2.65054
Epoch 9, cost is  2.54353
Training took 0.811852 minutes
Weight histogram
[749 651 535 486 431 350 716 105  19   8] [-0.01366388 -0.01231179 -0.0109597  -0.00960761 -0.00825551 -0.00690342
 -0.00555133 -0.00419924 -0.00284714 -0.00149505 -0.00014296]
[713 282 304 310 320 348 389 417 459 508] [-0.01366388 -0.01231179 -0.0109597  -0.00960761 -0.00825551 -0.00690342
 -0.00555133 -0.00419924 -0.00284714 -0.00149505 -0.00014296]
-0.240814
0.289531
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164032 minutes
Epoch 0
Fine tuning took 0.164286 minutes
Epoch 0
Fine tuning took 0.164978 minutes
Epoch 0
Fine tuning took 0.164994 minutes
Epoch 0
Fine tuning took 0.165485 minutes
Epoch 0
Fine tuning took 0.165983 minutes
Epoch 0
Fine tuning took 0.165741 minutes
Epoch 0
Fine tuning took 0.165272 minutes
Epoch 0
Fine tuning took 0.165891 minutes
Epoch 0
Fine tuning took 0.166084 minutes
{'zero': {0: [0.37438423645320196, 0.47290640394088668, 0.50615763546798032, 0.47660098522167488, 0.45812807881773399, 0.47413793103448276, 0.43103448275862066, 0.44581280788177341, 0.46551724137931033, 0.3817733990147783, 0.47660098522167488], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.33251231527093594, 0.29064039408866993, 0.31896551724137934, 0.3608374384236453, 0.34113300492610837, 0.37068965517241381, 0.26108374384236455, 0.30665024630541871, 0.41625615763546797, 0.33743842364532017], 5: [0.26108374384236455, 0.19458128078817735, 0.20320197044334976, 0.20443349753694581, 0.18103448275862069, 0.18472906403940886, 0.19827586206896552, 0.29310344827586204, 0.22783251231527094, 0.2019704433497537, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37438423645320196, 0.50123152709359609, 0.46551724137931033, 0.47167487684729065, 0.47906403940886699, 0.45197044334975367, 0.47413793103448276, 0.45073891625615764, 0.43349753694581283, 0.39655172413793105, 0.46182266009852219], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.32758620689655171, 0.29679802955665024, 0.33620689655172414, 0.34729064039408869, 0.3645320197044335, 0.32142857142857145, 0.29556650246305421, 0.33620689655172414, 0.41871921182266009, 0.3288177339901478], 5: [0.26108374384236455, 0.17118226600985223, 0.2376847290640394, 0.19211822660098521, 0.17364532019704434, 0.18349753694581281, 0.20443349753694581, 0.2536945812807882, 0.23029556650246305, 0.18472906403940886, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37438423645320196, 0.46921182266009853, 0.48275862068965519, 0.46305418719211822, 0.49137931034482757, 0.43842364532019706, 0.44334975369458129, 0.4211822660098522, 0.45566502463054187, 0.43103448275862066, 0.46551724137931033], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.32758620689655171, 0.31403940886699505, 0.33990147783251229, 0.33004926108374383, 0.37315270935960593, 0.33497536945812806, 0.32266009852216748, 0.35344827586206895, 0.39655172413793105, 0.32389162561576357], 5: [0.26108374384236455, 0.20320197044334976, 0.20320197044334976, 0.19704433497536947, 0.17857142857142858, 0.18842364532019704, 0.22167487684729065, 0.25615763546798032, 0.19088669950738915, 0.17241379310344829, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37438423645320196, 0.51108374384236455, 0.47413793103448276, 0.47660098522167488, 0.47906403940886699, 0.45935960591133007, 0.44581280788177341, 0.4605911330049261, 0.46674876847290642, 0.37192118226600984, 0.47044334975369456], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.31034482758620691, 0.28078817733990147, 0.33128078817733991, 0.34236453201970446, 0.36206896551724138, 0.34482758620689657, 0.27586206896551724, 0.31157635467980294, 0.40517241379310343, 0.33743842364532017], 5: [0.26108374384236455, 0.17857142857142858, 0.24507389162561577, 0.19211822660098521, 0.17857142857142858, 0.17857142857142858, 0.20935960591133004, 0.26354679802955666, 0.22167487684729065, 0.2229064039408867, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421320 minutes
Weight histogram
[ 118  345  521  648  780 2894 6750 6645 3112  462] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1140  989  766 1011 1404 2394 2487 3360 3304 5420] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.16213
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.14414
Epoch 1, cost is  1.11803
Epoch 2, cost is  1.10041
Epoch 3, cost is  1.08692
Epoch 4, cost is  1.0736
Training took 0.644329 minutes
Weight histogram
[5124 4241 3053 2533 1679 1466 1313  970  936  960] [ -4.04839218e-02  -3.64407767e-02  -3.23976316e-02  -2.83544864e-02
  -2.43113413e-02  -2.02681962e-02  -1.62250511e-02  -1.21819060e-02
  -8.13876092e-03  -4.09561582e-03  -5.24707102e-05]
[1842 1256 1316 1476 1883 2051 2688 2844 3068 3851] [ -4.04839218e-02  -3.64407767e-02  -3.23976316e-02  -2.83544864e-02
  -2.43113413e-02  -2.02681962e-02  -1.62250511e-02  -1.21819060e-02
  -8.13876092e-03  -4.09561582e-03  -5.24707102e-05]
-0.793862
1.46343
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147488 minutes
Weight histogram
[   33   314  1753   890   629   721  5419 10284  3921   336] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[ 2167   173   222   295   467   871  1005  1855  5897 11348] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.04893
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.83086
Epoch 1, cost is  2.77964
Epoch 2, cost is  2.74147
Epoch 3, cost is  2.71357
Epoch 4, cost is  2.68515
Training took 0.114973 minutes
Weight histogram
[4279 3704 3308 2711 1982 1605 1931 1086 2126 1568] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
[4443 1299 1293 1587 1978 2292 2497 2529 3121 3261] [ -5.51578365e-02  -4.96387341e-02  -4.41196317e-02  -3.86005292e-02
  -3.30814268e-02  -2.75623244e-02  -2.20432220e-02  -1.65241195e-02
  -1.10050171e-02  -5.48591467e-03   3.31877563e-05]
-1.18162
1.30045
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.71666
Epoch 1, cost is  4.21758
Epoch 2, cost is  3.66203
Epoch 3, cost is  3.25512
Epoch 4, cost is  2.9428
Epoch 5, cost is  2.71889
Epoch 6, cost is  2.54821
Epoch 7, cost is  2.41509
Epoch 8, cost is  2.30829
Epoch 9, cost is  2.21905
Training took 1.475352 minutes
Weight histogram
[974 791 651 602 828 121  48  21   9   5] [-0.00916891 -0.00826786 -0.00736682 -0.00646577 -0.00556473 -0.00466368
 -0.00376264 -0.00286159 -0.00196055 -0.0010595  -0.00015846]
[699 278 291 293 311 346 384 429 477 542] [-0.00916891 -0.00826786 -0.00736682 -0.00646577 -0.00556473 -0.00466368
 -0.00376264 -0.00286159 -0.00196055 -0.0010595  -0.00015846]
-0.160619
0.231263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.193584 minutes
Epoch 0
Fine tuning took 0.193914 minutes
Epoch 0
Fine tuning took 0.194905 minutes
Epoch 0
Fine tuning took 0.195303 minutes
Epoch 0
Fine tuning took 0.195216 minutes
Epoch 0
Fine tuning took 0.194985 minutes
Epoch 0
Fine tuning took 0.194646 minutes
Epoch 0
Fine tuning took 0.195062 minutes
Epoch 0
Fine tuning took 0.195365 minutes
Epoch 0
Fine tuning took 0.194623 minutes
{'zero': {0: [0.39778325123152708, 0.39532019704433496, 0.4039408866995074, 0.40517241379310343, 0.43719211822660098, 0.40270935960591131, 0.43842364532019706, 0.43349753694581283, 0.3891625615763547, 0.36330049261083741, 0.49507389162561577], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37192118226600984, 0.34852216748768472, 0.32389162561576357, 0.36576354679802958, 0.31403940886699505, 0.33620689655172414, 0.31527093596059114, 0.28078817733990147, 0.34482758620689657, 0.36699507389162561, 0.30541871921182268], 5: [0.23029556650246305, 0.25615763546798032, 0.27216748768472904, 0.22906403940886699, 0.24876847290640394, 0.26108374384236455, 0.24630541871921183, 0.2857142857142857, 0.26600985221674878, 0.26970443349753692, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.39778325123152708, 0.39408866995073893, 0.45320197044334976, 0.39408866995073893, 0.47660098522167488, 0.39408866995073893, 0.44458128078817732, 0.40517241379310343, 0.36699507389162561, 0.41009852216748771, 0.46305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37192118226600984, 0.33990147783251229, 0.32389162561576357, 0.36699507389162561, 0.31034482758620691, 0.37561576354679804, 0.3251231527093596, 0.28694581280788178, 0.36206896551724138, 0.33990147783251229, 0.29310344827586204], 5: [0.23029556650246305, 0.26600985221674878, 0.2229064039408867, 0.23891625615763548, 0.21305418719211822, 0.23029556650246305, 0.23029556650246305, 0.30788177339901479, 0.27093596059113301, 0.25, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.39778325123152708, 0.42857142857142855, 0.38054187192118227, 0.42241379310344829, 0.43103448275862066, 0.37438423645320196, 0.42980295566502463, 0.41995073891625617, 0.39285714285714285, 0.39285714285714285, 0.46182266009852219], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37192118226600984, 0.34975369458128081, 0.35837438423645318, 0.38300492610837439, 0.34975369458128081, 0.3608374384236453, 0.3251231527093596, 0.28448275862068967, 0.34113300492610837, 0.34482758620689657, 0.29556650246305421], 5: [0.23029556650246305, 0.22167487684729065, 0.26108374384236455, 0.19458128078817735, 0.21921182266009853, 0.26477832512315269, 0.24507389162561577, 0.29556650246305421, 0.26600985221674878, 0.26231527093596058, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.39778325123152708, 0.41009852216748771, 0.41748768472906406, 0.41379310344827586, 0.42610837438423643, 0.38793103448275862, 0.44950738916256155, 0.42733990147783252, 0.38793103448275862, 0.39162561576354682, 0.46182266009852219], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37192118226600984, 0.37438423645320196, 0.34113300492610837, 0.36699507389162561, 0.33004926108374383, 0.34975369458128081, 0.32758620689655171, 0.27709359605911332, 0.35960591133004927, 0.34975369458128081, 0.32142857142857145], 5: [0.23029556650246305, 0.21551724137931033, 0.2413793103448276, 0.21921182266009853, 0.24384236453201971, 0.26231527093596058, 0.2229064039408867, 0.29556650246305421, 0.25246305418719212, 0.25862068965517243, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105401 minutes
Weight histogram
[ 380 2204 3422 3873 3876 3818 3139 3157 1801  655] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
[  143   150   208   301   453   705   786  1537  2375 19667] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81703
Epoch 1, cost is  3.78377
Epoch 2, cost is  3.7679
Epoch 3, cost is  3.7427
Epoch 4, cost is  3.72171
Training took 0.072384 minutes
Weight histogram
[6072 7442 2973 4802 2151  421 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2617 2071 1975 2343 2195 2196 2466 3069 3134 4259] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-1.92377
2.16038
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147059 minutes
Weight histogram
[   95   349   614  1079  1197  3520 13562  7396   533     5] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 12887  8576   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.114526 minutes
Weight histogram
[4561 4652 4001 3419 2456 1904 2029 2259 2419  650] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
-1.31179
1.38974
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.96213
Epoch 1, cost is  5.61537
Epoch 2, cost is  5.51008
Epoch 3, cost is  5.41221
Epoch 4, cost is  5.24304
Epoch 5, cost is  4.99941
Epoch 6, cost is  4.77453
Epoch 7, cost is  4.59272
Epoch 8, cost is  4.42676
Epoch 9, cost is  4.27401
Training took 0.177463 minutes
Weight histogram
[1395 1585 2638  926  576  395  331  154   67   33] [-0.0301243  -0.02712327 -0.02412224 -0.02112122 -0.01812019 -0.01511916
 -0.01211814 -0.00911711 -0.00611608 -0.00311506 -0.00011403]
[ 597 1130 1624  767  609  605  651  707  724  686] [-0.0301243  -0.02712327 -0.02412224 -0.02112122 -0.01812019 -0.01511916
 -0.01211814 -0.00911711 -0.00611608 -0.00311506 -0.00011403]
-0.400339
0.437275
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043576 minutes
Epoch 0
Fine tuning took 0.042610 minutes
Epoch 0
Fine tuning took 0.043937 minutes
Epoch 0
Fine tuning took 0.042889 minutes
Epoch 0
Fine tuning took 0.044263 minutes
Epoch 0
Fine tuning took 0.040242 minutes
Epoch 0
Fine tuning took 0.043397 minutes
Epoch 0
Fine tuning took 0.040673 minutes
Epoch 0
Fine tuning took 0.042536 minutes
Epoch 0
Fine tuning took 0.044224 minutes
{'zero': {0: [0.41009852216748771, 0.30049261083743845, 0.26847290640394089, 0.23399014778325122, 0.25123152709359609, 0.2536945812807882, 0.29679802955665024, 0.14901477832512317, 0.22167487684729065, 0.16995073891625614, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.19458128078817735, 0.49876847290640391, 0.5788177339901478, 0.5788177339901478, 0.57389162561576357, 0.47660098522167488, 0.53325123152709364, 0.60221674876847286, 0.57635467980295563, 0.63793103448275867, 0.59482758620689657], 5: [0.39532019704433496, 0.20073891625615764, 0.15270935960591134, 0.18719211822660098, 0.1748768472906404, 0.26970443349753692, 0.16995073891625614, 0.24876847290640394, 0.2019704433497537, 0.19211822660098521, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.41009852216748771, 0.24507389162561577, 0.37192118226600984, 0.52832512315270941, 0.26724137931034481, 0.48522167487684731, 0.5431034482758621, 0.35960591133004927, 0.49261083743842365, 0.43596059113300495, 0.3891625615763547], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.19458128078817735, 0.64655172413793105, 0.50123152709359609, 0.18965517241379309, 0.43349753694581283, 0.35837438423645318, 0.31157635467980294, 0.40763546798029554, 0.26354679802955666, 0.39532019704433496, 0.37684729064039407], 5: [0.39532019704433496, 0.10837438423645321, 0.1268472906403941, 0.28201970443349755, 0.29926108374384236, 0.15640394088669951, 0.14532019704433496, 0.23275862068965517, 0.24384236453201971, 0.16871921182266009, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.41009852216748771, 0.27832512315270935, 0.36576354679802958, 0.5431034482758621, 0.25492610837438423, 0.47044334975369456, 0.53078817733990147, 0.34975369458128081, 0.50492610837438423, 0.38423645320197042, 0.35960591133004927], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.19458128078817735, 0.5923645320197044, 0.51600985221674878, 0.22044334975369459, 0.43226600985221675, 0.39285714285714285, 0.32019704433497537, 0.44334975369458129, 0.26108374384236455, 0.44581280788177341, 0.39778325123152708], 5: [0.39532019704433496, 0.12931034482758622, 0.11822660098522167, 0.23645320197044334, 0.31280788177339902, 0.13669950738916256, 0.14901477832512317, 0.20689655172413793, 0.23399014778325122, 0.16995073891625614, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.41009852216748771, 0.22413793103448276, 0.3817733990147783, 0.57389162561576357, 0.26600985221674878, 0.50862068965517238, 0.51108374384236455, 0.39408866995073893, 0.49137931034482757, 0.45812807881773399, 0.3817733990147783], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.19458128078817735, 0.66379310344827591, 0.50615763546798032, 0.14039408866995073, 0.44211822660098521, 0.39162561576354682, 0.32266009852216748, 0.39901477832512317, 0.23275862068965517, 0.37438423645320196, 0.35344827586206895], 5: [0.39532019704433496, 0.11206896551724138, 0.11206896551724138, 0.2857142857142857, 0.29187192118226601, 0.099753694581280791, 0.16625615763546797, 0.20689655172413793, 0.27586206896551724, 0.16748768472906403, 0.26477832512315269], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105684 minutes
Weight histogram
[ 380 2204 3422 3873 3876 3818 3139 3157 1801  655] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
[  143   150   208   301   453   705   786  1537  2375 19667] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81703
Epoch 1, cost is  3.78377
Epoch 2, cost is  3.7679
Epoch 3, cost is  3.7427
Epoch 4, cost is  3.72171
Training took 0.072488 minutes
Weight histogram
[6072 7442 2973 4802 2151  421 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2617 2071 1975 2343 2195 2196 2466 3069 3134 4259] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-1.92377
2.16038
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147161 minutes
Weight histogram
[   95   349   614  1079  1197  3520 13562  7396   533     5] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 12887  8576   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.115306 minutes
Weight histogram
[4561 4652 4001 3419 2456 1904 2029 2259 2419  650] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
-1.31179
1.38974
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.5058
Epoch 1, cost is  5.28118
Epoch 2, cost is  5.20869
Epoch 3, cost is  5.05432
Epoch 4, cost is  4.75469
Epoch 5, cost is  4.45568
Epoch 6, cost is  4.23273
Epoch 7, cost is  4.0558
Epoch 8, cost is  3.89889
Epoch 9, cost is  3.74699
Training took 0.242613 minutes
Weight histogram
[1815 4254  896  438  296  179  100   65   35   22] [-0.01972586 -0.01776956 -0.01581327 -0.01385698 -0.01190068 -0.00994439
 -0.00798809 -0.0060318  -0.0040755  -0.00211921 -0.00016291]
[1979 1022  532  493  505  581  646  742  792  808] [-0.01972586 -0.01776956 -0.01581327 -0.01385698 -0.01190068 -0.00994439
 -0.00798809 -0.0060318  -0.0040755  -0.00211921 -0.00016291]
-0.330322
0.271731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047941 minutes
Epoch 0
Fine tuning took 0.045045 minutes
Epoch 0
Fine tuning took 0.046465 minutes
Epoch 0
Fine tuning took 0.045324 minutes
Epoch 0
Fine tuning took 0.046966 minutes
Epoch 0
Fine tuning took 0.044523 minutes
Epoch 0
Fine tuning took 0.048227 minutes
Epoch 0
Fine tuning took 0.045748 minutes
Epoch 0
Fine tuning took 0.046110 minutes
Epoch 0
Fine tuning took 0.045799 minutes
{'zero': {0: [0.27463054187192121, 0.27955665024630544, 0.33990147783251229, 0.32635467980295568, 0.30541871921182268, 0.32019704433497537, 0.30665024630541871, 0.31773399014778325, 0.34113300492610837, 0.32758620689655171, 0.33497536945812806], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49753694581280788, 0.47906403940886699, 0.41133004926108374, 0.45320197044334976, 0.52832512315270941, 0.41748768472906406, 0.41502463054187194, 0.50246305418719217, 0.43719211822660098, 0.47290640394088668, 0.49384236453201968], 5: [0.22783251231527094, 0.2413793103448276, 0.24876847290640394, 0.22044334975369459, 0.16625615763546797, 0.26231527093596058, 0.27832512315270935, 0.17980295566502463, 0.22167487684729065, 0.19950738916256158, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27463054187192121, 0.30788177339901479, 0.38423645320197042, 0.35837438423645318, 0.33004926108374383, 0.35344827586206895, 0.3682266009852217, 0.35467980295566504, 0.32019704433497537, 0.35098522167487683, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49753694581280788, 0.44211822660098521, 0.38423645320197042, 0.44581280788177341, 0.49876847290640391, 0.41379310344827586, 0.3817733990147783, 0.46798029556650245, 0.44950738916256155, 0.46305418719211822, 0.47413793103448276], 5: [0.22783251231527094, 0.25, 0.23152709359605911, 0.19581280788177341, 0.17118226600985223, 0.23275862068965517, 0.25, 0.17733990147783252, 0.23029556650246305, 0.18596059113300492, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27463054187192121, 0.32266009852216748, 0.37807881773399016, 0.36576354679802958, 0.33497536945812806, 0.30541871921182268, 0.37438423645320196, 0.35221674876847292, 0.3817733990147783, 0.34359605911330049, 0.35591133004926107], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49753694581280788, 0.42857142857142855, 0.35837438423645318, 0.44088669950738918, 0.48768472906403942, 0.43596059113300495, 0.35098522167487683, 0.47906403940886699, 0.41995073891625617, 0.45566502463054187, 0.42241379310344829], 5: [0.22783251231527094, 0.24876847290640394, 0.26354679802955666, 0.19334975369458129, 0.17733990147783252, 0.25862068965517243, 0.27463054187192121, 0.16871921182266009, 0.19827586206896552, 0.20073891625615764, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27463054187192121, 0.32389162561576357, 0.35837438423645318, 0.35591133004926107, 0.32635467980295568, 0.37192118226600984, 0.3288177339901478, 0.32389162561576357, 0.35837438423645318, 0.34113300492610837, 0.33990147783251229], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49753694581280788, 0.41625615763546797, 0.40024630541871919, 0.46674876847290642, 0.49014778325123154, 0.40886699507389163, 0.41133004926108374, 0.48399014778325122, 0.42241379310344829, 0.43472906403940886, 0.47783251231527096], 5: [0.22783251231527094, 0.25985221674876846, 0.2413793103448276, 0.17733990147783252, 0.18349753694581281, 0.21921182266009853, 0.25985221674876846, 0.19211822660098521, 0.21921182266009853, 0.22413793103448276, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108162 minutes
Weight histogram
[ 380 2204 3422 3873 3876 3818 3139 3157 1801  655] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
[  143   150   208   301   453   705   786  1537  2375 19667] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81703
Epoch 1, cost is  3.78377
Epoch 2, cost is  3.7679
Epoch 3, cost is  3.7427
Epoch 4, cost is  3.72171
Training took 0.073383 minutes
Weight histogram
[6072 7442 2973 4802 2151  421 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2617 2071 1975 2343 2195 2196 2466 3069 3134 4259] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-1.92377
2.16038
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149146 minutes
Weight histogram
[   95   349   614  1079  1197  3520 13562  7396   533     5] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 12887  8576   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.113450 minutes
Weight histogram
[4561 4652 4001 3419 2456 1904 2029 2259 2419  650] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
-1.31179
1.38974
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.39154
Epoch 1, cost is  5.27345
Epoch 2, cost is  5.05122
Epoch 3, cost is  4.61336
Epoch 4, cost is  4.23495
Epoch 5, cost is  3.97299
Epoch 6, cost is  3.75347
Epoch 7, cost is  3.55104
Epoch 8, cost is  3.36476
Epoch 9, cost is  3.20091
Training took 0.332509 minutes
Weight histogram
[1607 1671 1148 1033 2295  210   71   33   18   14] [-0.01159094 -0.01044518 -0.00929941 -0.00815364 -0.00700788 -0.00586211
 -0.00471635 -0.00357058 -0.00242482 -0.00127905 -0.00013328]
[2091  523  478  510  591  701  748  786  823  849] [-0.01159094 -0.01044518 -0.00929941 -0.00815364 -0.00700788 -0.00586211
 -0.00471635 -0.00357058 -0.00242482 -0.00127905 -0.00013328]
-0.252911
0.214406
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051090 minutes
Epoch 0
Fine tuning took 0.049328 minutes
Epoch 0
Fine tuning took 0.052324 minutes
Epoch 0
Fine tuning took 0.049449 minutes
Epoch 0
Fine tuning took 0.050073 minutes
Epoch 0
Fine tuning took 0.050821 minutes
Epoch 0
Fine tuning took 0.050616 minutes
Epoch 0
Fine tuning took 0.051196 minutes
Epoch 0
Fine tuning took 0.049788 minutes
Epoch 0
Fine tuning took 0.051268 minutes
{'zero': {0: [0.28817733990147781, 0.33866995073891626, 0.36576354679802958, 0.34113300492610837, 0.40763546798029554, 0.39532019704433496, 0.37192118226600984, 0.40640394088669951, 0.41256157635467983, 0.39408866995073893, 0.3817733990147783], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.42980295566502463, 0.41133004926108374, 0.39162561576354682, 0.41871921182266009, 0.3817733990147783, 0.3891625615763547, 0.37807881773399016, 0.36576354679802958, 0.39408866995073893, 0.3817733990147783], 5: [0.18596059113300492, 0.23152709359605911, 0.2229064039408867, 0.26724137931034481, 0.17364532019704434, 0.2229064039408867, 0.23891625615763548, 0.21551724137931033, 0.22167487684729065, 0.21182266009852216, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28817733990147781, 0.33497536945812806, 0.37807881773399016, 0.35098522167487683, 0.4248768472906404, 0.37438423645320196, 0.3682266009852217, 0.42980295566502463, 0.42241379310344829, 0.41133004926108374, 0.45812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.41133004926108374, 0.37684729064039407, 0.36206896551724138, 0.39285714285714285, 0.42241379310344829, 0.37561576354679804, 0.3645320197044335, 0.33743842364532017, 0.39655172413793105, 0.33620689655172414], 5: [0.18596059113300492, 0.2536945812807882, 0.24507389162561577, 0.28694581280788178, 0.18226600985221675, 0.20320197044334976, 0.25615763546798032, 0.20566502463054187, 0.24014778325123154, 0.19211822660098521, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28817733990147781, 0.33743842364532017, 0.40147783251231528, 0.3645320197044335, 0.42364532019704432, 0.38423645320197042, 0.36576354679802958, 0.41748768472906406, 0.41625615763546797, 0.40147783251231528, 0.41625615763546797], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.42857142857142855, 0.3608374384236453, 0.36206896551724138, 0.39039408866995073, 0.41871921182266009, 0.38300492610837439, 0.35467980295566504, 0.34359605911330049, 0.40640394088669951, 0.36699507389162561], 5: [0.18596059113300492, 0.23399014778325122, 0.2376847290640394, 0.27339901477832512, 0.18596059113300492, 0.19704433497536947, 0.25123152709359609, 0.22783251231527094, 0.24014778325123154, 0.19211822660098521, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28817733990147781, 0.34359605911330049, 0.44827586206896552, 0.40147783251231528, 0.42733990147783252, 0.37438423645320196, 0.34729064039408869, 0.4211822660098522, 0.44088669950738918, 0.40147783251231528, 0.39285714285714285], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.41625615763546797, 0.33743842364532017, 0.34975369458128081, 0.37438423645320196, 0.38669950738916259, 0.42241379310344829, 0.36945812807881773, 0.33128078817733991, 0.40640394088669951, 0.37192118226600984], 5: [0.18596059113300492, 0.24014778325123154, 0.21428571428571427, 0.24876847290640394, 0.19827586206896552, 0.23891625615763548, 0.23029556650246305, 0.20935960591133004, 0.22783251231527094, 0.19211822660098521, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107324 minutes
Weight histogram
[ 380 2204 3422 3873 3876 3818 3139 3157 1801  655] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
[  143   150   208   301   453   705   786  1537  2375 19667] [-0.00117775 -0.00067683 -0.00017592  0.00032499  0.00082591  0.00132682
  0.00182774  0.00232865  0.00282956  0.00333048  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81703
Epoch 1, cost is  3.78377
Epoch 2, cost is  3.7679
Epoch 3, cost is  3.7427
Epoch 4, cost is  3.72171
Training took 0.075449 minutes
Weight histogram
[6072 7442 2973 4802 2151  421 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2617 2071 1975 2343 2195 2196 2466 3069 3134 4259] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-1.92377
2.16038
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146929 minutes
Weight histogram
[   95   349   614  1079  1197  3520 13562  7396   533     5] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 12887  8576   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.116154 minutes
Weight histogram
[4561 4652 4001 3419 2456 1904 2029 2259 2419  650] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
-1.31179
1.38974
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.38628
Epoch 1, cost is  5.22889
Epoch 2, cost is  4.80413
Epoch 3, cost is  4.26981
Epoch 4, cost is  3.90269
Epoch 5, cost is  3.61309
Epoch 6, cost is  3.35607
Epoch 7, cost is  3.13315
Epoch 8, cost is  2.95368
Epoch 9, cost is  2.80727
Training took 0.533065 minutes
Weight histogram
[1499 1497 1274  853  733 2118   94   16    8    8] [-0.00614247 -0.0055404  -0.00493832 -0.00433625 -0.00373417 -0.0031321
 -0.00253002 -0.00192795 -0.00132587 -0.0007238  -0.00012172]
[1890  511  488  558  650  700  736  783  842  942] [-0.00614247 -0.0055404  -0.00493832 -0.00433625 -0.00373417 -0.0031321
 -0.00253002 -0.00192795 -0.00132587 -0.0007238  -0.00012172]
-0.186733
0.179812
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.061274 minutes
Epoch 0
Fine tuning took 0.061937 minutes
Epoch 0
Fine tuning took 0.060613 minutes
Epoch 0
Fine tuning took 0.061504 minutes
Epoch 0
Fine tuning took 0.060780 minutes
Epoch 0
Fine tuning took 0.062556 minutes
Epoch 0
Fine tuning took 0.062375 minutes
Epoch 0
Fine tuning took 0.060993 minutes
Epoch 0
Fine tuning took 0.060798 minutes
Epoch 0
Fine tuning took 0.061597 minutes
{'zero': {0: [0.30665024630541871, 0.32266009852216748, 0.40024630541871919, 0.39162561576354682, 0.39408866995073893, 0.40886699507389163, 0.34113300492610837, 0.43226600985221675, 0.44704433497536944, 0.40517241379310343, 0.45935960591133007], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.45197044334975367, 0.34975369458128081, 0.35344827586206895, 0.43596059113300495, 0.40517241379310343, 0.38423645320197042, 0.35714285714285715, 0.32635467980295568, 0.39408866995073893, 0.32019704433497537], 5: [0.19458128078817735, 0.22536945812807882, 0.25, 0.25492610837438423, 0.16995073891625614, 0.18596059113300492, 0.27463054187192121, 0.2105911330049261, 0.22660098522167488, 0.20073891625615764, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30665024630541871, 0.34359605911330049, 0.37068965517241381, 0.41256157635467983, 0.43965517241379309, 0.42364532019704432, 0.39039408866995073, 0.42857142857142855, 0.44088669950738918, 0.42241379310344829, 0.43965517241379309], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.39285714285714285, 0.3682266009852217, 0.33620689655172414, 0.3682266009852217, 0.40517241379310343, 0.39408866995073893, 0.36699507389162561, 0.33004926108374383, 0.37931034482758619, 0.29926108374384236], 5: [0.19458128078817735, 0.26354679802955666, 0.26108374384236455, 0.25123152709359609, 0.19211822660098521, 0.17118226600985223, 0.21551724137931033, 0.20443349753694581, 0.22906403940886699, 0.19827586206896552, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30665024630541871, 0.36576354679802958, 0.40886699507389163, 0.36699507389162561, 0.45320197044334976, 0.40886699507389163, 0.35221674876847292, 0.43472906403940886, 0.44211822660098521, 0.40517241379310343, 0.44458128078817732], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.40763546798029554, 0.35714285714285715, 0.36576354679802958, 0.36576354679802958, 0.4039408866995074, 0.37315270935960593, 0.34852216748768472, 0.30788177339901479, 0.40147783251231528, 0.31280788177339902], 5: [0.19458128078817735, 0.22660098522167488, 0.23399014778325122, 0.26724137931034481, 0.18103448275862069, 0.18719211822660098, 0.27463054187192121, 0.21674876847290642, 0.25, 0.19334975369458129, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30665024630541871, 0.35591133004926107, 0.40640394088669951, 0.3817733990147783, 0.4211822660098522, 0.43472906403940886, 0.36576354679802958, 0.4211822660098522, 0.48152709359605911, 0.4248768472906404, 0.42610837438423643], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.41379310344827586, 0.36945812807881773, 0.3645320197044335, 0.38054187192118227, 0.37684729064039407, 0.39285714285714285, 0.38423645320197042, 0.28078817733990147, 0.39285714285714285, 0.33990147783251229], 5: [0.19458128078817735, 0.23029556650246305, 0.22413793103448276, 0.2536945812807882, 0.19827586206896552, 0.18842364532019704, 0.2413793103448276, 0.19458128078817735, 0.2376847290640394, 0.18226600985221675, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147549 minutes
Weight histogram
[ 166  382  633 1120 3417 4298 5656 7815 2600  238] [ -2.45821051e-04   5.94312471e-05   3.64683545e-04   6.69935843e-04
   9.75188141e-04   1.28044044e-03   1.58569274e-03   1.89094504e-03
   2.19619733e-03   2.50144963e-03   2.80670193e-03]
[  136   154   224   332   468   879  1025  2049  6732 14326] [ -2.45821051e-04   5.94312471e-05   3.64683545e-04   6.69935843e-04
   9.75188141e-04   1.28044044e-03   1.58569274e-03   1.89094504e-03
   2.19619733e-03   2.50144963e-03   2.80670193e-03]
-1.32597
1.08666
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.40485
Epoch 1, cost is  2.36594
Epoch 2, cost is  2.34096
Epoch 3, cost is  2.31789
Epoch 4, cost is  2.29651
Training took 0.114349 minutes
Weight histogram
[4802 4463 3776 3332 2568 1964 2061 1549 1478  332] [ -6.01105280e-02  -5.40959037e-02  -4.80812793e-02  -4.20666550e-02
  -3.60520306e-02  -3.00374063e-02  -2.40227819e-02  -1.80081575e-02
  -1.19935332e-02  -5.97890884e-03   3.57155150e-05]
[2635 1429 1483 1878 2260 2565 2912 3341 3636 4186] [ -6.01105280e-02  -5.40959037e-02  -4.80812793e-02  -4.20666550e-02
  -3.60520306e-02  -3.00374063e-02  -2.40227819e-02  -1.80081575e-02
  -1.19935332e-02  -5.97890884e-03   3.57155150e-05]
-1.33222
1.74153
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147898 minutes
Weight histogram
[   33   293   616  1073  1295  1020  5912 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  279   328   447   631   943  1568  1004  1856  5900 15394] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.117199 minutes
Weight histogram
[4562 4651 4001 3419 2457 1903 2029 1688 2974  666] [ -6.05602935e-02  -5.45006926e-02  -4.84410917e-02  -4.23814908e-02
  -3.63218899e-02  -3.02622890e-02  -2.42026881e-02  -1.81430872e-02
  -1.20834863e-02  -6.02388539e-03   3.57155150e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.45006926e-02  -4.84410917e-02  -4.23814908e-02
  -3.63218899e-02  -3.02622890e-02  -2.42026881e-02  -1.81430872e-02
  -1.20834863e-02  -6.02388539e-03   3.57155150e-05]
-1.31179
1.38974
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.19275
Epoch 1, cost is  5.93433
Epoch 2, cost is  5.7768
Epoch 3, cost is  5.58451
Epoch 4, cost is  5.28788
Epoch 5, cost is  4.95072
Epoch 6, cost is  4.6593
Epoch 7, cost is  4.43132
Epoch 8, cost is  4.24208
Epoch 9, cost is  4.07922
Training took 0.203302 minutes
Weight histogram
[ 929  893  912  978 2237 1074  758  235   56   28] [-0.02607865 -0.02348716 -0.02089566 -0.01830417 -0.01571267 -0.01312118
 -0.01052969 -0.00793819 -0.0053467  -0.0027552  -0.00016371]
[1671 1411  625  515  535  568  619  675  747  734] [-0.02607865 -0.02348716 -0.02089566 -0.01830417 -0.01571267 -0.01312118
 -0.01052969 -0.00793819 -0.0053467  -0.0027552  -0.00016371]
-0.321561
0.409203
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.054095 minutes
Epoch 0
Fine tuning took 0.053838 minutes
Epoch 0
Fine tuning took 0.054683 minutes
Epoch 0
Fine tuning took 0.054515 minutes
Epoch 0
Fine tuning took 0.052315 minutes
Epoch 0
Fine tuning took 0.053004 minutes
Epoch 0
Fine tuning took 0.054270 minutes
Epoch 0
Fine tuning took 0.052275 minutes
Epoch 0
Fine tuning took 0.052090 minutes
Epoch 0
Fine tuning took 0.053789 minutes
{'zero': {0: [0.52832512315270941, 0.37068965517241381, 0.33866995073891626, 0.5, 0.49261083743842365, 0.55295566502463056, 0.44211822660098521, 0.25985221674876846, 0.36330049261083741, 0.38300492610837439, 0.39162561576354682], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.11206896551724138, 0.48891625615763545, 0.47660098522167488, 0.3645320197044335, 0.29926108374384236, 0.25985221674876846, 0.36699507389162561, 0.49384236453201968, 0.40024630541871919, 0.46182266009852219, 0.45073891625615764], 5: [0.35960591133004927, 0.14039408866995073, 0.18472906403940886, 0.1354679802955665, 0.20812807881773399, 0.18719211822660098, 0.19088669950738915, 0.24630541871921183, 0.23645320197044334, 0.15517241379310345, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.52832512315270941, 0.35467980295566504, 0.48029556650246308, 0.60960591133004927, 0.53448275862068961, 0.60098522167487689, 0.49014778325123154, 0.45935960591133007, 0.43719211822660098, 0.56527093596059108, 0.51724137931034486], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.11206896551724138, 0.52463054187192115, 0.3460591133004926, 0.28078817733990147, 0.15270935960591134, 0.14408866995073891, 0.31280788177339902, 0.33620689655172414, 0.16502463054187191, 0.27216748768472904, 0.22536945812807882], 5: [0.35960591133004927, 0.1206896551724138, 0.17364532019704434, 0.10960591133004927, 0.31280788177339902, 0.25492610837438423, 0.19704433497536947, 0.20443349753694581, 0.39778325123152708, 0.1625615763546798, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.52832512315270941, 0.34729064039408869, 0.5073891625615764, 0.59852216748768472, 0.51231527093596063, 0.59359605911330049, 0.47906403940886699, 0.40270935960591131, 0.45320197044334976, 0.52463054187192115, 0.54064039408866993], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.11206896551724138, 0.52832512315270941, 0.30665024630541871, 0.2894088669950739, 0.16748768472906403, 0.1748768472906404, 0.34482758620689657, 0.41625615763546797, 0.20320197044334976, 0.31157635467980294, 0.23399014778325122], 5: [0.35960591133004927, 0.12438423645320197, 0.18596059113300492, 0.11206896551724138, 0.32019704433497537, 0.23152709359605911, 0.17610837438423646, 0.18103448275862069, 0.34359605911330049, 0.16379310344827586, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.52832512315270941, 0.33743842364532017, 0.53694581280788178, 0.57758620689655171, 0.55172413793103448, 0.63916256157635465, 0.48275862068965519, 0.43719211822660098, 0.43842364532019706, 0.59729064039408863, 0.54926108374384242], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.11206896551724138, 0.51477832512315269, 0.29802955665024633, 0.31650246305418717, 0.13916256157635468, 0.15270935960591134, 0.33990147783251229, 0.36206896551724138, 0.17364532019704434, 0.23522167487684728, 0.19334975369458129], 5: [0.35960591133004927, 0.14778325123152711, 0.16502463054187191, 0.10591133004926108, 0.30911330049261082, 0.20812807881773399, 0.17733990147783252, 0.20073891625615764, 0.38793103448275862, 0.16748768472906403, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149430 minutes
Weight histogram
[ 166  382  633 1120 3417 4298 5656 7815 2600  238] [ -2.45821051e-04   5.94312471e-05   3.64683545e-04   6.69935843e-04
   9.75188141e-04   1.28044044e-03   1.58569274e-03   1.89094504e-03
   2.19619733e-03   2.50144963e-03   2.80670193e-03]
[  136   154   224   332   468   879  1025  2049  6732 14326] [ -2.45821051e-04   5.94312471e-05   3.64683545e-04   6.69935843e-04
   9.75188141e-04   1.28044044e-03   1.58569274e-03   1.89094504e-03
   2.19619733e-03   2.50144963e-03   2.80670193e-03]
-1.32597
1.08666
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.40485
Epoch 1, cost is  2.36594
Epoch 2, cost is  2.34096
Epoch 3, cost is  2.31789
Epoch 4, cost is  2.29651
Training took 0.115176 minutes
Weight histogram
[4802 4463 3776 3332 2568 1964 2061 1549 1478  332] [ -6.01105280e-02  -5.40959037e-02  -4.80812793e-02  -4.20666550e-02
  -3.60520306e-02  -3.00374063e-02  -2.40227819e-02  -1.80081575e-02
  -1.19935332e-02  -5.97890884e-03   3.57155150e-05]
[2635 1429 1483 1878 2260 2565 2912 3341 3636 4186] [ -6.01105280e-02  -5.40959037e-02  -4.80812793e-02  -4.20666550e-02
  -3.60520306e-02  -3.00374063e-02  -2.40227819e-02  -1.80081575e-02
  -1.19935332e-02  -5.97890884e-03   3.57155150e-05]
-1.33222
1.74153
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147467 minutes
Weight histogram
[   33   293   616  1073  1295  1020  5912 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  279   328   447   631   943  1568  1004  1856  5900 15394] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.116006 minutes
Weight histogram
[4562 4651 4001 3419 2457 1903 2029 1688 2974  666] [ -6.05602935e-02  -5.45006926e-02  -4.84410917e-02  -4.23814908e-02
  -3.63218899e-02  -3.02622890e-02  -2.42026881e-02  -1.81430872e-02
  -1.20834863e-02  -6.02388539e-03   3.57155150e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.45006926e-02  -4.84410917e-02  -4.23814908e-02
  -3.63218899e-02  -3.02622890e-02  -2.42026881e-02  -1.81430872e-02
  -1.20834863e-02  -6.02388539e-03   3.57155150e-05]
-1.31179
1.38974
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.55694
Epoch 1, cost is  5.3205
Epoch 2, cost is  5.20181
Epoch 3, cost is  4.9763
Epoch 4, cost is  4.58965
Epoch 5, cost is  4.23575
Epoch 6, cost is  3.96818
Epoch 7, cost is  3.74873
Epoch 8, cost is  3.55569
Epoch 9, cost is  3.38722
Training took 0.296173 minutes
Weight histogram
[1446 1248 1132 3226  593  245  107   55   30   18] [-0.01807489 -0.01628334 -0.01449179 -0.01270025 -0.0109087  -0.00911715
 -0.00732561 -0.00553406 -0.00374251 -0.00195096 -0.00015942]
[2518  662  469  473  520  590  657  697  752  762] [-0.01807489 -0.01628334 -0.01449179 -0.01270025 -0.0109087  -0.00911715
 -0.00732561 -0.00553406 -0.00374251 -0.00195096 -0.00015942]
-0.27419
0.317038
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.060467 minutes
Epoch 0
Fine tuning took 0.057450 minutes
Epoch 0
Fine tuning took 0.059655 minutes
Epoch 0
Fine tuning took 0.057359 minutes
Epoch 0
Fine tuning took 0.059843 minutes
Epoch 0
Fine tuning took 0.058621 minutes
Epoch 0
Fine tuning took 0.059157 minutes
Epoch 0
Fine tuning took 0.058937 minutes
Epoch 0
Fine tuning took 0.058963 minutes
Epoch 0
Fine tuning took 0.057687 minutes
{'zero': {0: [0.30788177339901479, 0.28078817733990147, 0.34852216748768472, 0.28694581280788178, 0.24507389162561577, 0.25738916256157635, 0.33251231527093594, 0.23029556650246305, 0.25, 0.29187192118226601, 0.27955665024630544], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33128078817733991, 0.46182266009852219, 0.41133004926108374, 0.54187192118226601, 0.58251231527093594, 0.44950738916256155, 0.47167487684729065, 0.54926108374384242, 0.55295566502463056, 0.58251231527093594, 0.56157635467980294], 5: [0.3608374384236453, 0.25738916256157635, 0.24014778325123154, 0.17118226600985223, 0.17241379310344829, 0.29310344827586204, 0.19581280788177341, 0.22044334975369459, 0.19704433497536947, 0.12561576354679804, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30788177339901479, 0.35098522167487683, 0.41133004926108374, 0.34236453201970446, 0.3288177339901478, 0.4248768472906404, 0.41502463054187194, 0.30541871921182268, 0.37315270935960593, 0.40147783251231528, 0.37807881773399016], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33128078817733991, 0.40147783251231528, 0.3854679802955665, 0.47783251231527096, 0.49137931034482757, 0.39532019704433496, 0.33620689655172414, 0.4963054187192118, 0.47413793103448276, 0.45197044334975367, 0.48645320197044334], 5: [0.3608374384236453, 0.24753694581280788, 0.20320197044334976, 0.17980295566502463, 0.17980295566502463, 0.17980295566502463, 0.24876847290640394, 0.19827586206896552, 0.15270935960591134, 0.14655172413793102, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30788177339901479, 0.35591133004926107, 0.40517241379310343, 0.33374384236453203, 0.3288177339901478, 0.38793103448275862, 0.41625615763546797, 0.30541871921182268, 0.35837438423645318, 0.39778325123152708, 0.39285714285714285], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33128078817733991, 0.40517241379310343, 0.41133004926108374, 0.48522167487684731, 0.45197044334975367, 0.38669950738916259, 0.33990147783251229, 0.48152709359605911, 0.46798029556650245, 0.42857142857142855, 0.45443349753694579], 5: [0.3608374384236453, 0.23891625615763548, 0.18349753694581281, 0.18103448275862069, 0.21921182266009853, 0.22536945812807882, 0.24384236453201971, 0.21305418719211822, 0.17364532019704434, 0.17364532019704434, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30788177339901479, 0.33866995073891626, 0.41995073891625617, 0.3608374384236453, 0.33866995073891626, 0.43719211822660098, 0.43842364532019706, 0.34482758620689657, 0.41995073891625617, 0.41009852216748771, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.33128078817733991, 0.41995073891625617, 0.35344827586206895, 0.45320197044334976, 0.48768472906403942, 0.38423645320197042, 0.3460591133004926, 0.44704433497536944, 0.42610837438423643, 0.43103448275862066, 0.49137931034482757], 5: [0.3608374384236453, 0.2413793103448276, 0.22660098522167488, 0.18596059113300492, 0.17364532019704434, 0.17857142857142858, 0.21551724137931033, 0.20812807881773399, 0.1539408866995074, 0.15886699507389163, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146866 minutes
Weight histogram
[ 166  382  633 1120 3417 4298 5656 7815 2600  238] [ -2.45821051e-04   5.94312471e-05   3.64683545e-04   6.69935843e-04
   9.75188141e-04   1.28044044e-03   1.58569274e-03   1.89094504e-03
   2.19619733e-03   2.50144963e-03   2.80670193e-03]
[  136   154   224   332   468   879  1025  2049  6732 14326] [ -2.45821051e-04   5.94312471e-05   3.64683545e-04   6.69935843e-04
   9.75188141e-04   1.28044044e-03   1.58569274e-03   1.89094504e-03
   2.19619733e-03   2.50144963e-03   2.80670193e-03]
-1.32597
1.08666
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.40485
Epoch 1, cost is  2.36594
Epoch 2, cost is  2.34096
Epoch 3, cost is  2.31789
Epoch 4, cost is  2.29651
Training took 0.114449 minutes
Weight histogram
[4802 4463 3776 3332 2568 1964 2061 1549 1478  332] [ -6.01105280e-02  -5.40959037e-02  -4.80812793e-02  -4.20666550e-02
  -3.60520306e-02  -3.00374063e-02  -2.40227819e-02  -1.80081575e-02
  -1.19935332e-02  -5.97890884e-03   3.57155150e-05]
[2635 1429 1483 1878 2260 2565 2912 3341 3636 4186] [ -6.01105280e-02  -5.40959037e-02  -4.80812793e-02  -4.20666550e-02
  -3.60520306e-02  -3.00374063e-02  -2.40227819e-02  -1.80081575e-02
  -1.19935332e-02  -5.97890884e-03   3.57155150e-05]
-1.33222
1.74153
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147752 minutes
Weight histogram
[   33   293   616  1073  1295  1020  5912 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  279   328   447   631   943  1568  1004  1856  5900 15394] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.113339 minutes
Weight histogram
[4562 4651 4001 3419 2457 1903 2029 1688 2974  666] [ -6.05602935e-02  -5.45006926e-02  -4.84410917e-02  -4.23814908e-02
  -3.63218899e-02  -3.02622890e-02  -2.42026881e-02  -1.81430872e-02
  -1.20834863e-02  -6.02388539e-03   3.57155150e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.45006926e-02  -4.84410917e-02  -4.23814908e-02
  -3.63218899e-02  -3.02622890e-02  -2.42026881e-02  -1.81430872e-02
  -1.20834863e-02  -6.02388539e-03   3.57155150e-05]
-1.31179
1.38974
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.18312
Epoch 1, cost is  5.03659
Epoch 2, cost is  4.90117
Epoch 3, cost is  4.5415
Epoch 4, cost is  4.12018
Epoch 5, cost is  3.80339
Epoch 6, cost is  3.54798
Epoch 7, cost is  3.3181
Epoch 8, cost is  3.12118
Epoch 9, cost is  2.94879
Training took 0.413489 minutes
Weight histogram
[2040 1769 3461  418  196   96   58   32   18   12] [-0.01216332 -0.01096072 -0.00975813 -0.00855554 -0.00735294 -0.00615035
 -0.00494775 -0.00374516 -0.00254257 -0.00133997 -0.00013738]
[2355  534  461  516  569  646  684  730  782  823] [-0.01216332 -0.01096072 -0.00975813 -0.00855554 -0.00735294 -0.00615035
 -0.00494775 -0.00374516 -0.00254257 -0.00133997 -0.00013738]
-0.221466
0.25687
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.064283 minutes
Epoch 0
Fine tuning took 0.065022 minutes
Epoch 0
Fine tuning took 0.063315 minutes
Epoch 0
Fine tuning took 0.063535 minutes
Epoch 0
Fine tuning took 0.062831 minutes
Epoch 0
Fine tuning took 0.063959 minutes
Epoch 0
Fine tuning took 0.062992 minutes
Epoch 0
Fine tuning took 0.064316 minutes
Epoch 0
Fine tuning took 0.064026 minutes
Epoch 0
Fine tuning took 0.063124 minutes
{'zero': {0: [0.27093596059113301, 0.33497536945812806, 0.26354679802955666, 0.33128078817733991, 0.34236453201970446, 0.31773399014778325, 0.34236453201970446, 0.31773399014778325, 0.36330049261083741, 0.30911330049261082, 0.30541871921182268], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.5357142857142857, 0.41625615763546797, 0.44211822660098521, 0.45689655172413796, 0.48522167487684731, 0.45320197044334976, 0.3854679802955665, 0.45935960591133007, 0.40640394088669951, 0.48522167487684731, 0.52586206896551724], 5: [0.19334975369458129, 0.24876847290640394, 0.29433497536945813, 0.21182266009852216, 0.17241379310344829, 0.22906403940886699, 0.27216748768472904, 0.2229064039408867, 0.23029556650246305, 0.20566502463054187, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27093596059113301, 0.37068965517241381, 0.3251231527093596, 0.34975369458128081, 0.36699507389162561, 0.38300492610837439, 0.35221674876847292, 0.37561576354679804, 0.37438423645320196, 0.33004926108374383, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.5357142857142857, 0.36699507389162561, 0.42610837438423643, 0.41871921182266009, 0.43349753694581283, 0.40024630541871919, 0.42241379310344829, 0.40886699507389163, 0.41995073891625617, 0.43596059113300495, 0.45073891625615764], 5: [0.19334975369458129, 0.26231527093596058, 0.24876847290640394, 0.23152709359605911, 0.19950738916256158, 0.21674876847290642, 0.22536945812807882, 0.21551724137931033, 0.20566502463054187, 0.23399014778325122, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27093596059113301, 0.34729064039408869, 0.32019704433497537, 0.37438423645320196, 0.36330049261083741, 0.37561576354679804, 0.36206896551724138, 0.36699507389162561, 0.37438423645320196, 0.40024630541871919, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.5357142857142857, 0.40886699507389163, 0.41748768472906406, 0.41748768472906406, 0.45320197044334976, 0.42610837438423643, 0.43103448275862066, 0.41748768472906406, 0.40270935960591131, 0.41748768472906406, 0.47044334975369456], 5: [0.19334975369458129, 0.24384236453201971, 0.26231527093596058, 0.20812807881773399, 0.18349753694581281, 0.19827586206896552, 0.20689655172413793, 0.21551724137931033, 0.2229064039408867, 0.18226600985221675, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27093596059113301, 0.36699507389162561, 0.34113300492610837, 0.39408866995073893, 0.36330049261083741, 0.35221674876847292, 0.35960591133004927, 0.33374384236453203, 0.37315270935960593, 0.33743842364532017, 0.34729064039408869], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.5357142857142857, 0.41009852216748771, 0.41256157635467983, 0.41133004926108374, 0.46798029556650245, 0.45320197044334976, 0.39778325123152708, 0.42364532019704432, 0.40886699507389163, 0.47167487684729065, 0.46182266009852219], 5: [0.19334975369458129, 0.2229064039408867, 0.24630541871921183, 0.19458128078817735, 0.16871921182266009, 0.19458128078817735, 0.24261083743842365, 0.24261083743842365, 0.21798029556650247, 0.19088669950738915, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148398 minutes
Weight histogram
[ 166  382  633 1120 3417 4298 5656 7815 2600  238] [ -2.45821051e-04   5.94312471e-05   3.64683545e-04   6.69935843e-04
   9.75188141e-04   1.28044044e-03   1.58569274e-03   1.89094504e-03
   2.19619733e-03   2.50144963e-03   2.80670193e-03]
[  136   154   224   332   468   879  1025  2049  6732 14326] [ -2.45821051e-04   5.94312471e-05   3.64683545e-04   6.69935843e-04
   9.75188141e-04   1.28044044e-03   1.58569274e-03   1.89094504e-03
   2.19619733e-03   2.50144963e-03   2.80670193e-03]
-1.32597
1.08666
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.40485
Epoch 1, cost is  2.36594
Epoch 2, cost is  2.34096
Epoch 3, cost is  2.31789
Epoch 4, cost is  2.29651
Training took 0.114035 minutes
Weight histogram
[4802 4463 3776 3332 2568 1964 2061 1549 1478  332] [ -6.01105280e-02  -5.40959037e-02  -4.80812793e-02  -4.20666550e-02
  -3.60520306e-02  -3.00374063e-02  -2.40227819e-02  -1.80081575e-02
  -1.19935332e-02  -5.97890884e-03   3.57155150e-05]
[2635 1429 1483 1878 2260 2565 2912 3341 3636 4186] [ -6.01105280e-02  -5.40959037e-02  -4.80812793e-02  -4.20666550e-02
  -3.60520306e-02  -3.00374063e-02  -2.40227819e-02  -1.80081575e-02
  -1.19935332e-02  -5.97890884e-03   3.57155150e-05]
-1.33222
1.74153
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148356 minutes
Weight histogram
[   33   293   616  1073  1295  1020  5912 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  279   328   447   631   943  1568  1004  1856  5900 15394] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.115319 minutes
Weight histogram
[4562 4651 4001 3419 2457 1903 2029 1688 2974  666] [ -6.05602935e-02  -5.45006926e-02  -4.84410917e-02  -4.23814908e-02
  -3.63218899e-02  -3.02622890e-02  -2.42026881e-02  -1.81430872e-02
  -1.20834863e-02  -6.02388539e-03   3.57155150e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.45006926e-02  -4.84410917e-02  -4.23814908e-02
  -3.63218899e-02  -3.02622890e-02  -2.42026881e-02  -1.81430872e-02
  -1.20834863e-02  -6.02388539e-03   3.57155150e-05]
-1.31179
1.38974
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.14446
Epoch 1, cost is  4.97913
Epoch 2, cost is  4.53521
Epoch 3, cost is  3.96315
Epoch 4, cost is  3.53295
Epoch 5, cost is  3.20088
Epoch 6, cost is  2.93935
Epoch 7, cost is  2.72806
Epoch 8, cost is  2.55979
Epoch 9, cost is  2.42978
Training took 0.683264 minutes
Weight histogram
[1536 1507 1224  917  951 1888   45   16    8    8] [-0.00679093 -0.00612546 -0.00545999 -0.00479452 -0.00412904 -0.00346357
 -0.0027981  -0.00213263 -0.00146716 -0.00080168 -0.00013621]
[1905  503  492  540  598  650  713  800  902  997] [-0.00679093 -0.00612546 -0.00545999 -0.00479452 -0.00412904 -0.00346357
 -0.0027981  -0.00213263 -0.00146716 -0.00080168 -0.00013621]
-0.189124
0.20825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077332 minutes
Epoch 0
Fine tuning took 0.078634 minutes
Epoch 0
Fine tuning took 0.077746 minutes
Epoch 0
Fine tuning took 0.076442 minutes
Epoch 0
Fine tuning took 0.076665 minutes
Epoch 0
Fine tuning took 0.077606 minutes
Epoch 0
Fine tuning took 0.078028 minutes
Epoch 0
Fine tuning took 0.077148 minutes
Epoch 0
Fine tuning took 0.077796 minutes
Epoch 0
Fine tuning took 0.077202 minutes
{'zero': {0: [0.22413793103448276, 0.34236453201970446, 0.38669950738916259, 0.3682266009852217, 0.41871921182266009, 0.33866995073891626, 0.37931034482758619, 0.40517241379310343, 0.39901477832512317, 0.39162561576354682, 0.38793103448275862], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.41871921182266009, 0.36945812807881773, 0.39162561576354682, 0.3817733990147783, 0.46182266009852219, 0.3817733990147783, 0.37561576354679804, 0.38423645320197042, 0.4039408866995074, 0.39655172413793105], 5: [0.17241379310344829, 0.23891625615763548, 0.24384236453201971, 0.24014778325123154, 0.19950738916256158, 0.19950738916256158, 0.23891625615763548, 0.21921182266009853, 0.21674876847290642, 0.20443349753694581, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22413793103448276, 0.34852216748768472, 0.35098522167487683, 0.33497536945812806, 0.45197044334975367, 0.3608374384236453, 0.40024630541871919, 0.44581280788177341, 0.41256157635467983, 0.43103448275862066, 0.41871921182266009], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.41625615763546797, 0.39285714285714285, 0.41133004926108374, 0.3608374384236453, 0.43596059113300495, 0.3817733990147783, 0.33866995073891626, 0.37438423645320196, 0.38669950738916259, 0.36945812807881773], 5: [0.17241379310344829, 0.23522167487684728, 0.25615763546798032, 0.2536945812807882, 0.18719211822660098, 0.20320197044334976, 0.21798029556650247, 0.21551724137931033, 0.21305418719211822, 0.18226600985221675, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22413793103448276, 0.31157635467980294, 0.41995073891625617, 0.3854679802955665, 0.39778325123152708, 0.35098522167487683, 0.36206896551724138, 0.39655172413793105, 0.40024630541871919, 0.40886699507389163, 0.43472906403940886], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.46674876847290642, 0.3460591133004926, 0.36945812807881773, 0.39162561576354682, 0.47290640394088668, 0.39285714285714285, 0.39408866995073893, 0.36945812807881773, 0.3817733990147783, 0.37931034482758619], 5: [0.17241379310344829, 0.22167487684729065, 0.23399014778325122, 0.24507389162561577, 0.2105911330049261, 0.17610837438423646, 0.24507389162561577, 0.20935960591133004, 0.23029556650246305, 0.20935960591133004, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22413793103448276, 0.35098522167487683, 0.39901477832512317, 0.40517241379310343, 0.42733990147783252, 0.3682266009852217, 0.37315270935960593, 0.41133004926108374, 0.41009852216748771, 0.41009852216748771, 0.4039408866995074], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.43719211822660098, 0.34852216748768472, 0.36699507389162561, 0.40640394088669951, 0.42610837438423643, 0.40024630541871919, 0.3460591133004926, 0.34729064039408869, 0.37931034482758619, 0.39655172413793105], 5: [0.17241379310344829, 0.21182266009852216, 0.25246305418719212, 0.22783251231527094, 0.16625615763546797, 0.20566502463054187, 0.22660098522167488, 0.24261083743842365, 0.24261083743842365, 0.2105911330049261, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234637 minutes
Weight histogram
[ 159  363  627  699 1253 3378 7767 9100 2753  226] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[ 162  270  407  652  930 1188 3032 4032 5995 9657] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.30458
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.53041
Epoch 1, cost is  1.5026
Epoch 2, cost is  1.48312
Epoch 3, cost is  1.46579
Epoch 4, cost is  1.44954
Training took 0.221575 minutes
Weight histogram
[5867 4108 3957 2735 2293 1977 1577 1412 1399 1000] [ -5.95115758e-02  -5.35531358e-02  -4.75946958e-02  -4.16362558e-02
  -3.56778158e-02  -2.97193758e-02  -2.37609358e-02  -1.78024958e-02
  -1.18440557e-02  -5.88561573e-03   7.28242885e-05]
[2255 1317 1612 1872 2219 2495 3036 3216 3988 4315] [ -5.95115758e-02  -5.35531358e-02  -4.75946958e-02  -4.16362558e-02
  -3.56778158e-02  -2.97193758e-02  -2.37609358e-02  -1.78024958e-02
  -1.18440557e-02  -5.88561573e-03   7.28242885e-05]
-0.93891
1.62222
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148078 minutes
Weight histogram
[   33   303  1006  1469   807   722  5902 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  466   902  1194   295   469   870  1004  1856  5901 15393] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.113486 minutes
Weight histogram
[4565 4649 4002 3423 2460 1897 2036 1482 2470 1366] [ -6.05602935e-02  -5.44969817e-02  -4.84336699e-02  -4.23703582e-02
  -3.63070464e-02  -3.02437346e-02  -2.41804228e-02  -1.81171110e-02
  -1.20537993e-02  -5.99048749e-03   7.28242885e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.44969817e-02  -4.84336699e-02  -4.23703582e-02
  -3.63070464e-02  -3.02437346e-02  -2.41804228e-02  -1.81171110e-02
  -1.20537993e-02  -5.99048749e-03   7.28242885e-05]
-1.31179
1.38974
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.3091
Epoch 1, cost is  6.03265
Epoch 2, cost is  5.77584
Epoch 3, cost is  5.40789
Epoch 4, cost is  5.04647
Epoch 5, cost is  4.75566
Epoch 6, cost is  4.52406
Epoch 7, cost is  4.32159
Epoch 8, cost is  4.14738
Epoch 9, cost is  3.98966
Training took 0.246637 minutes
Weight histogram
[ 966  990  870  829  745  895 1491  954  327   33] [-0.02811378 -0.02532047 -0.02252716 -0.01973385 -0.01694054 -0.01414723
 -0.01135392 -0.00856061 -0.0057673  -0.00297399 -0.00018068]
[1637  870  571  566  612  670  726  782  823  843] [-0.02811378 -0.02532047 -0.02252716 -0.01973385 -0.01694054 -0.01414723
 -0.01135392 -0.00856061 -0.0057673  -0.00297399 -0.00018068]
-0.370984
0.514383
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076665 minutes
Epoch 0
Fine tuning took 0.077305 minutes
Epoch 0
Fine tuning took 0.076141 minutes
Epoch 0
Fine tuning took 0.077433 minutes
Epoch 0
Fine tuning took 0.077320 minutes
Epoch 0
Fine tuning took 0.077697 minutes
Epoch 0
Fine tuning took 0.077330 minutes
Epoch 0
Fine tuning took 0.077546 minutes
Epoch 0
Fine tuning took 0.076393 minutes
Epoch 0
Fine tuning took 0.077476 minutes
{'zero': {0: [0.48275862068965519, 0.54556650246305416, 0.34852216748768472, 0.44211822660098521, 0.29556650246305421, 0.5923645320197044, 0.38669950738916259, 0.36699507389162561, 0.50862068965517238, 0.44581280788177341, 0.31280788177339902], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.15640394088669951, 0.27586206896551724, 0.48152709359605911, 0.29556650246305421, 0.43596059113300495, 0.18226600985221675, 0.33251231527093594, 0.40886699507389163, 0.1625615763546798, 0.39162561576354682, 0.36945812807881773], 5: [0.3608374384236453, 0.17857142857142858, 0.16995073891625614, 0.26231527093596058, 0.26847290640394089, 0.22536945812807882, 0.28078817733990147, 0.22413793103448276, 0.3288177339901478, 0.1625615763546798, 0.31773399014778325], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.48275862068965519, 0.52832512315270941, 0.39408866995073893, 0.50615763546798032, 0.31403940886699505, 0.58620689655172409, 0.34975369458128081, 0.31896551724137934, 0.49753694581280788, 0.47783251231527096, 0.30418719211822659], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.15640394088669951, 0.26600985221674878, 0.50862068965517238, 0.27832512315270935, 0.41379310344827586, 0.16502463054187191, 0.33004926108374383, 0.42857142857142855, 0.092364532019704432, 0.32019704433497537, 0.33990147783251229], 5: [0.3608374384236453, 0.20566502463054187, 0.097290640394088676, 0.21551724137931033, 0.27216748768472904, 0.24876847290640394, 0.32019704433497537, 0.25246305418719212, 0.41009852216748771, 0.2019704433497537, 0.35591133004926107], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.48275862068965519, 0.52586206896551724, 0.36576354679802958, 0.48891625615763545, 0.30541871921182268, 0.59113300492610843, 0.34359605911330049, 0.31896551724137934, 0.53325123152709364, 0.49384236453201968, 0.3288177339901478], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.15640394088669951, 0.27709359605911332, 0.52832512315270941, 0.28325123152709358, 0.41871921182266009, 0.18226600985221675, 0.35714285714285715, 0.47413793103448276, 0.099753694581280791, 0.2857142857142857, 0.31157635467980294], 5: [0.3608374384236453, 0.19704433497536947, 0.10591133004926108, 0.22783251231527094, 0.27586206896551724, 0.22660098522167488, 0.29926108374384236, 0.20689655172413793, 0.36699507389162561, 0.22044334975369459, 0.35960591133004927], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.48275862068965519, 0.50369458128078815, 0.40640394088669951, 0.48029556650246308, 0.2894088669950739, 0.60098522167487689, 0.34729064039408869, 0.33128078817733991, 0.49753694581280788, 0.48891625615763545, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.15640394088669951, 0.27709359605911332, 0.49137931034482757, 0.29187192118226601, 0.43103448275862066, 0.14778325123152711, 0.33866995073891626, 0.42610837438423643, 0.099753694581280791, 0.28448275862068967, 0.33497536945812806], 5: [0.3608374384236453, 0.21921182266009853, 0.10221674876847291, 0.22783251231527094, 0.27955665024630544, 0.25123152709359609, 0.31403940886699505, 0.24261083743842365, 0.40270935960591131, 0.22660098522167488, 0.36576354679802958], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234663 minutes
Weight histogram
[ 159  363  627  699 1253 3378 7767 9100 2753  226] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[ 162  270  407  652  930 1188 3032 4032 5995 9657] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.30458
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.53041
Epoch 1, cost is  1.5026
Epoch 2, cost is  1.48312
Epoch 3, cost is  1.46579
Epoch 4, cost is  1.44954
Training took 0.221228 minutes
Weight histogram
[5867 4108 3957 2735 2293 1977 1577 1412 1399 1000] [ -5.95115758e-02  -5.35531358e-02  -4.75946958e-02  -4.16362558e-02
  -3.56778158e-02  -2.97193758e-02  -2.37609358e-02  -1.78024958e-02
  -1.18440557e-02  -5.88561573e-03   7.28242885e-05]
[2255 1317 1612 1872 2219 2495 3036 3216 3988 4315] [ -5.95115758e-02  -5.35531358e-02  -4.75946958e-02  -4.16362558e-02
  -3.56778158e-02  -2.97193758e-02  -2.37609358e-02  -1.78024958e-02
  -1.18440557e-02  -5.88561573e-03   7.28242885e-05]
-0.93891
1.62222
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147602 minutes
Weight histogram
[   33   303  1006  1469   807   722  5902 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  466   902  1194   295   469   870  1004  1856  5901 15393] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.114595 minutes
Weight histogram
[4565 4649 4002 3423 2460 1897 2036 1482 2470 1366] [ -6.05602935e-02  -5.44969817e-02  -4.84336699e-02  -4.23703582e-02
  -3.63070464e-02  -3.02437346e-02  -2.41804228e-02  -1.81171110e-02
  -1.20537993e-02  -5.99048749e-03   7.28242885e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.44969817e-02  -4.84336699e-02  -4.23703582e-02
  -3.63070464e-02  -3.02437346e-02  -2.41804228e-02  -1.81171110e-02
  -1.20537993e-02  -5.99048749e-03   7.28242885e-05]
-1.31179
1.38974
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.69261
Epoch 1, cost is  5.4185
Epoch 2, cost is  5.10641
Epoch 3, cost is  4.64144
Epoch 4, cost is  4.23597
Epoch 5, cost is  3.93019
Epoch 6, cost is  3.67815
Epoch 7, cost is  3.4705
Epoch 8, cost is  3.29545
Epoch 9, cost is  3.14846
Training took 0.351547 minutes
Weight histogram
[1328 1162  999  844  898 2047  633  130   40   19] [-0.01845267 -0.01662242 -0.01479217 -0.01296192 -0.01113167 -0.00930142
 -0.00747117 -0.00564092 -0.00381067 -0.00198042 -0.00015017]
[2026  561  514  543  614  684  732  779  827  820] [-0.01845267 -0.01662242 -0.01479217 -0.01296192 -0.01113167 -0.00930142
 -0.00747117 -0.00564092 -0.00381067 -0.00198042 -0.00015017]
-0.257221
0.338259
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080721 minutes
Epoch 0
Fine tuning took 0.081856 minutes
Epoch 0
Fine tuning took 0.080601 minutes
Epoch 0
Fine tuning took 0.081901 minutes
Epoch 0
Fine tuning took 0.082477 minutes
Epoch 0
Fine tuning took 0.082064 minutes
Epoch 0
Fine tuning took 0.082519 minutes
Epoch 0
Fine tuning took 0.081074 minutes
Epoch 0
Fine tuning took 0.079821 minutes
Epoch 0
Fine tuning took 0.081413 minutes
{'zero': {0: [0.39039408866995073, 0.31280788177339902, 0.33004926108374383, 0.33128078817733991, 0.28201970443349755, 0.41133004926108374, 0.29926108374384236, 0.30541871921182268, 0.26477832512315269, 0.26970443349753692, 0.33128078817733991], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.2894088669950739, 0.49137931034482757, 0.48399014778325122, 0.49014778325123154, 0.55665024630541871, 0.3608374384236453, 0.50369458128078815, 0.46305418719211822, 0.53694581280788178, 0.54433497536945807, 0.53201970443349755], 5: [0.32019704433497537, 0.19581280788177341, 0.18596059113300492, 0.17857142857142858, 0.16133004926108374, 0.22783251231527094, 0.19704433497536947, 0.23152709359605911, 0.19827586206896552, 0.18596059113300492, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.39039408866995073, 0.36699507389162561, 0.4039408866995074, 0.46305418719211822, 0.38793103448275862, 0.47660098522167488, 0.41995073891625617, 0.4642857142857143, 0.46798029556650245, 0.44334975369458129, 0.42364532019704432], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.2894088669950739, 0.41502463054187194, 0.37315270935960593, 0.32266009852216748, 0.38793103448275862, 0.27463054187192121, 0.31650246305418717, 0.33251231527093594, 0.37192118226600984, 0.39901477832512317, 0.39901477832512317], 5: [0.32019704433497537, 0.21798029556650247, 0.2229064039408867, 0.21428571428571427, 0.22413793103448276, 0.24876847290640394, 0.26354679802955666, 0.20320197044334976, 0.16009852216748768, 0.15763546798029557, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.39039408866995073, 0.34729064039408869, 0.4248768472906404, 0.47906403940886699, 0.37561576354679804, 0.43596059113300495, 0.40147783251231528, 0.43965517241379309, 0.45197044334975367, 0.42857142857142855, 0.3891625615763547], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.2894088669950739, 0.45073891625615764, 0.33990147783251229, 0.31650246305418717, 0.39285714285714285, 0.31034482758620691, 0.30418719211822659, 0.35591133004926107, 0.36330049261083741, 0.42733990147783252, 0.39162561576354682], 5: [0.32019704433497537, 0.2019704433497537, 0.23522167487684728, 0.20443349753694581, 0.23152709359605911, 0.2536945812807882, 0.29433497536945813, 0.20443349753694581, 0.18472906403940886, 0.14408866995073891, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.39039408866995073, 0.3608374384236453, 0.42610837438423643, 0.45935960591133007, 0.38054187192118227, 0.49137931034482757, 0.4039408866995074, 0.46674876847290642, 0.47290640394088668, 0.4605911330049261, 0.44827586206896552], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.2894088669950739, 0.40147783251231528, 0.33990147783251229, 0.30172413793103448, 0.39408866995073893, 0.26847290640394089, 0.32019704433497537, 0.33004926108374383, 0.3682266009852217, 0.37068965517241381, 0.35098522167487683], 5: [0.32019704433497537, 0.2376847290640394, 0.23399014778325122, 0.23891625615763548, 0.22536945812807882, 0.24014778325123154, 0.27586206896551724, 0.20320197044334976, 0.15886699507389163, 0.16871921182266009, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236703 minutes
Weight histogram
[ 159  363  627  699 1253 3378 7767 9100 2753  226] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[ 162  270  407  652  930 1188 3032 4032 5995 9657] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.30458
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.53041
Epoch 1, cost is  1.5026
Epoch 2, cost is  1.48312
Epoch 3, cost is  1.46579
Epoch 4, cost is  1.44954
Training took 0.221223 minutes
Weight histogram
[5867 4108 3957 2735 2293 1977 1577 1412 1399 1000] [ -5.95115758e-02  -5.35531358e-02  -4.75946958e-02  -4.16362558e-02
  -3.56778158e-02  -2.97193758e-02  -2.37609358e-02  -1.78024958e-02
  -1.18440557e-02  -5.88561573e-03   7.28242885e-05]
[2255 1317 1612 1872 2219 2495 3036 3216 3988 4315] [ -5.95115758e-02  -5.35531358e-02  -4.75946958e-02  -4.16362558e-02
  -3.56778158e-02  -2.97193758e-02  -2.37609358e-02  -1.78024958e-02
  -1.18440557e-02  -5.88561573e-03   7.28242885e-05]
-0.93891
1.62222
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147140 minutes
Weight histogram
[   33   303  1006  1469   807   722  5902 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  466   902  1194   295   469   870  1004  1856  5901 15393] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.113467 minutes
Weight histogram
[4565 4649 4002 3423 2460 1897 2036 1482 2470 1366] [ -6.05602935e-02  -5.44969817e-02  -4.84336699e-02  -4.23703582e-02
  -3.63070464e-02  -3.02437346e-02  -2.41804228e-02  -1.81171110e-02
  -1.20537993e-02  -5.99048749e-03   7.28242885e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.44969817e-02  -4.84336699e-02  -4.23703582e-02
  -3.63070464e-02  -3.02437346e-02  -2.41804228e-02  -1.81171110e-02
  -1.20537993e-02  -5.99048749e-03   7.28242885e-05]
-1.31179
1.38974
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.10308
Epoch 1, cost is  4.87546
Epoch 2, cost is  4.56904
Epoch 3, cost is  4.12449
Epoch 4, cost is  3.74569
Epoch 5, cost is  3.44517
Epoch 6, cost is  3.19795
Epoch 7, cost is  2.99703
Epoch 8, cost is  2.83233
Epoch 9, cost is  2.68988
Training took 0.542512 minutes
Weight histogram
[1673 1530 1283 2754  512  183   86   45   20   14] [-0.01305922 -0.01176748 -0.01047575 -0.00918402 -0.00789228 -0.00660055
 -0.00530882 -0.00401708 -0.00272535 -0.00143362 -0.00014188]
[2041  547  521  557  611  662  714  760  832  855] [-0.01305922 -0.01176748 -0.01047575 -0.00918402 -0.00789228 -0.00660055
 -0.00530882 -0.00401708 -0.00272535 -0.00143362 -0.00014188]
-0.209465
0.246313
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090389 minutes
Epoch 0
Fine tuning took 0.089507 minutes
Epoch 0
Fine tuning took 0.090984 minutes
Epoch 0
Fine tuning took 0.091879 minutes
Epoch 0
Fine tuning took 0.091155 minutes
Epoch 0
Fine tuning took 0.092025 minutes
Epoch 0
Fine tuning took 0.089521 minutes
Epoch 0
Fine tuning took 0.091080 minutes
Epoch 0
Fine tuning took 0.091853 minutes
Epoch 0
Fine tuning took 0.090045 minutes
{'zero': {0: [0.29310344827586204, 0.29926108374384236, 0.35221674876847292, 0.31034482758620691, 0.36699507389162561, 0.35098522167487683, 0.35467980295566504, 0.27339901477832512, 0.2857142857142857, 0.26724137931034481, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46921182266009853, 0.45073891625615764, 0.41502463054187194, 0.48645320197044334, 0.44950738916256155, 0.40147783251231528, 0.40147783251231528, 0.53694581280788178, 0.47783251231527096, 0.53694581280788178, 0.54187192118226601], 5: [0.2376847290640394, 0.25, 0.23275862068965517, 0.20320197044334976, 0.18349753694581281, 0.24753694581280788, 0.24384236453201971, 0.18965517241379309, 0.23645320197044334, 0.19581280788177341, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29310344827586204, 0.34729064039408869, 0.40763546798029554, 0.37684729064039407, 0.3608374384236453, 0.41256157635467983, 0.41256157635467983, 0.3460591133004926, 0.33990147783251229, 0.38054187192118227, 0.33990147783251229], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46921182266009853, 0.41256157635467983, 0.38300492610837439, 0.44211822660098521, 0.46921182266009853, 0.36206896551724138, 0.37684729064039407, 0.4963054187192118, 0.45443349753694579, 0.42241379310344829, 0.50246305418719217], 5: [0.2376847290640394, 0.24014778325123154, 0.20935960591133004, 0.18103448275862069, 0.16995073891625614, 0.22536945812807882, 0.2105911330049261, 0.15763546798029557, 0.20566502463054187, 0.19704433497536947, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29310344827586204, 0.33004926108374383, 0.42241379310344829, 0.38669950738916259, 0.37684729064039407, 0.44950738916256155, 0.38423645320197042, 0.36206896551724138, 0.38300492610837439, 0.3817733990147783, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46921182266009853, 0.41871921182266009, 0.3460591133004926, 0.43472906403940886, 0.44827586206896552, 0.34482758620689657, 0.40024630541871919, 0.48275862068965519, 0.41256157635467983, 0.44950738916256155, 0.49876847290640391], 5: [0.2376847290640394, 0.25123152709359609, 0.23152709359605911, 0.17857142857142858, 0.1748768472906404, 0.20566502463054187, 0.21551724137931033, 0.15517241379310345, 0.20443349753694581, 0.16871921182266009, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29310344827586204, 0.33743842364532017, 0.41256157635467983, 0.37684729064039407, 0.35714285714285715, 0.44088669950738918, 0.36699507389162561, 0.35837438423645318, 0.35837438423645318, 0.3682266009852217, 0.31403940886699505], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46921182266009853, 0.40763546798029554, 0.35837438423645318, 0.44458128078817732, 0.45812807881773399, 0.37438423645320196, 0.38793103448275862, 0.48891625615763545, 0.41625615763546797, 0.45320197044334976, 0.52586206896551724], 5: [0.2376847290640394, 0.25492610837438423, 0.22906403940886699, 0.17857142857142858, 0.18472906403940886, 0.18472906403940886, 0.24507389162561577, 0.15270935960591134, 0.22536945812807882, 0.17857142857142858, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233439 minutes
Weight histogram
[ 159  363  627  699 1253 3378 7767 9100 2753  226] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[ 162  270  407  652  930 1188 3032 4032 5995 9657] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.30458
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.53041
Epoch 1, cost is  1.5026
Epoch 2, cost is  1.48312
Epoch 3, cost is  1.46579
Epoch 4, cost is  1.44954
Training took 0.221097 minutes
Weight histogram
[5867 4108 3957 2735 2293 1977 1577 1412 1399 1000] [ -5.95115758e-02  -5.35531358e-02  -4.75946958e-02  -4.16362558e-02
  -3.56778158e-02  -2.97193758e-02  -2.37609358e-02  -1.78024958e-02
  -1.18440557e-02  -5.88561573e-03   7.28242885e-05]
[2255 1317 1612 1872 2219 2495 3036 3216 3988 4315] [ -5.95115758e-02  -5.35531358e-02  -4.75946958e-02  -4.16362558e-02
  -3.56778158e-02  -2.97193758e-02  -2.37609358e-02  -1.78024958e-02
  -1.18440557e-02  -5.88561573e-03   7.28242885e-05]
-0.93891
1.62222
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148129 minutes
Weight histogram
[   33   303  1006  1469   807   722  5902 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[  466   902  1194   295   469   870  1004  1856  5901 15393] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.117193 minutes
Weight histogram
[4565 4649 4002 3423 2460 1897 2036 1482 2470 1366] [ -6.05602935e-02  -5.44969817e-02  -4.84336699e-02  -4.23703582e-02
  -3.63070464e-02  -3.02437346e-02  -2.41804228e-02  -1.81171110e-02
  -1.20537993e-02  -5.99048749e-03   7.28242885e-05]
[4606 1422 1482 1914 2413 2656 2834 3268 3646 4109] [ -6.05602935e-02  -5.44969817e-02  -4.84336699e-02  -4.23703582e-02
  -3.63070464e-02  -3.02437346e-02  -2.41804228e-02  -1.81171110e-02
  -1.20537993e-02  -5.99048749e-03   7.28242885e-05]
-1.31179
1.38974
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.8885
Epoch 1, cost is  4.61055
Epoch 2, cost is  4.02962
Epoch 3, cost is  3.52947
Epoch 4, cost is  3.16759
Epoch 5, cost is  2.88679
Epoch 6, cost is  2.66828
Epoch 7, cost is  2.49167
Epoch 8, cost is  2.34683
Epoch 9, cost is  2.22444
Training took 0.941649 minutes
Weight histogram
[1770 1525 1364  990 1927  419   60   25   12    8] [-0.00805968 -0.00726726 -0.00647483 -0.00568241 -0.00488999 -0.00409757
 -0.00330515 -0.00251272 -0.0017203  -0.00092788 -0.00013546]
[1680  502  512  586  635  691  755  844  929  966] [-0.00805968 -0.00726726 -0.00647483 -0.00568241 -0.00488999 -0.00409757
 -0.00330515 -0.00251272 -0.0017203  -0.00092788 -0.00013546]
-0.181285
0.191414
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.109642 minutes
Epoch 0
Fine tuning took 0.109236 minutes
Epoch 0
Fine tuning took 0.110287 minutes
Epoch 0
Fine tuning took 0.108972 minutes
Epoch 0
Fine tuning took 0.109834 minutes
Epoch 0
Fine tuning took 0.109366 minutes
Epoch 0
Fine tuning took 0.109343 minutes
Epoch 0
Fine tuning took 0.109410 minutes
Epoch 0
Fine tuning took 0.110434 minutes
Epoch 0
Fine tuning took 0.109877 minutes
{'zero': {0: [0.28201970443349755, 0.30049261083743845, 0.3608374384236453, 0.34975369458128081, 0.38054187192118227, 0.35714285714285715, 0.37315270935960593, 0.36576354679802958, 0.36945812807881773, 0.35714285714285715, 0.38793103448275862], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.41133004926108374, 0.37931034482758619, 0.41748768472906406, 0.45812807881773399, 0.43719211822660098, 0.42364532019704432, 0.41009852216748771, 0.39162561576354682, 0.45320197044334976, 0.39901477832512317], 5: [0.17733990147783252, 0.28817733990147781, 0.25985221674876846, 0.23275862068965517, 0.16133004926108374, 0.20566502463054187, 0.20320197044334976, 0.22413793103448276, 0.23891625615763548, 0.18965517241379309, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28201970443349755, 0.31896551724137934, 0.40763546798029554, 0.35714285714285715, 0.38300492610837439, 0.39778325123152708, 0.36945812807881773, 0.3891625615763547, 0.35098522167487683, 0.3460591133004926, 0.39901477832512317], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.42857142857142855, 0.3608374384236453, 0.41995073891625617, 0.43226600985221675, 0.39408866995073893, 0.42733990147783252, 0.3817733990147783, 0.4248768472906404, 0.42610837438423643, 0.39408866995073893], 5: [0.17733990147783252, 0.25246305418719212, 0.23152709359605911, 0.2229064039408867, 0.18472906403940886, 0.20812807881773399, 0.20320197044334976, 0.22906403940886699, 0.22413793103448276, 0.22783251231527094, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28201970443349755, 0.2894088669950739, 0.41133004926108374, 0.3854679802955665, 0.37315270935960593, 0.37807881773399016, 0.35344827586206895, 0.40886699507389163, 0.38793103448275862, 0.36206896551724138, 0.40147783251231528], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.43226600985221675, 0.3645320197044335, 0.40147783251231528, 0.44088669950738918, 0.40640394088669951, 0.44458128078817732, 0.3817733990147783, 0.38054187192118227, 0.40886699507389163, 0.37438423645320196], 5: [0.17733990147783252, 0.27832512315270935, 0.22413793103448276, 0.21305418719211822, 0.18596059113300492, 0.21551724137931033, 0.2019704433497537, 0.20935960591133004, 0.23152709359605911, 0.22906403940886699, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28201970443349755, 0.30788177339901479, 0.42980295566502463, 0.37931034482758619, 0.4039408866995074, 0.38300492610837439, 0.35344827586206895, 0.38054187192118227, 0.40147783251231528, 0.37192118226600984, 0.38300492610837439], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.39408866995073893, 0.33743842364532017, 0.4039408866995074, 0.40763546798029554, 0.41995073891625617, 0.43596059113300495, 0.37684729064039407, 0.3682266009852217, 0.43596059113300495, 0.37684729064039407], 5: [0.17733990147783252, 0.29802955665024633, 0.23275862068965517, 0.21674876847290642, 0.18842364532019704, 0.19704433497536947, 0.2105911330049261, 0.24261083743842365, 0.23029556650246305, 0.19211822660098521, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.419175 minutes
Weight histogram
[ 118  345  521  648  780 2894 7646 8972 3917  484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1198  972  855 1200 1274 2753 3075 3718 3352 7928] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.28249
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.00189
Epoch 1, cost is  0.981075
Epoch 2, cost is  0.966994
Epoch 3, cost is  0.95462
Epoch 4, cost is  0.94516
Training took 0.645573 minutes
Weight histogram
[6361 4928 4064 2684 2105 1693 1372 1085  987 1046] [ -4.32080664e-02  -3.88925068e-02  -3.45769472e-02  -3.02613877e-02
  -2.59458281e-02  -2.16302685e-02  -1.73147090e-02  -1.29991494e-02
  -8.68358984e-03  -4.36803028e-03  -5.24707102e-05]
[1956 1372 1474 1777 2122 2614 2932 3291 4051 4736] [ -4.32080664e-02  -3.88925068e-02  -3.45769472e-02  -3.02613877e-02
  -2.59458281e-02  -2.16302685e-02  -1.73147090e-02  -1.29991494e-02
  -8.68358984e-03  -4.36803028e-03  -5.24707102e-05]
-0.801216
1.52578
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147695 minutes
Weight histogram
[   33   314  1753   890   629   721  5902 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[ 2167   173   222   295   467   871  1005  1855  5897 15398] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.113579 minutes
Weight histogram
[4561 4652 4001 3419 2456 1904 2029 1489 2103 1736] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
[4603 1423 1481 1915 2414 2655 2835 3268 3646 4110] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
-1.31179
1.38974
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.45049
Epoch 1, cost is  6.09393
Epoch 2, cost is  5.62733
Epoch 3, cost is  5.22665
Epoch 4, cost is  4.91582
Epoch 5, cost is  4.66868
Epoch 6, cost is  4.46187
Epoch 7, cost is  4.28193
Epoch 8, cost is  4.12727
Epoch 9, cost is  3.98842
Training took 0.319860 minutes
Weight histogram
[ 952 1025  918  822  745  664  641  787 1336  210] [-0.03244205 -0.02921506 -0.02598806 -0.02276106 -0.01953407 -0.01630707
 -0.01308008 -0.00985308 -0.00662609 -0.00339909 -0.00017209]
[1355  596  569  618  682  753  816  867  927  917] [-0.03244205 -0.02921506 -0.02598806 -0.02276106 -0.01953407 -0.01630707
 -0.01308008 -0.00985308 -0.00662609 -0.00339909 -0.00017209]
-0.411576
0.634183
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.141513 minutes
Epoch 0
Fine tuning took 0.141835 minutes
Epoch 0
Fine tuning took 0.142669 minutes
Epoch 0
Fine tuning took 0.140815 minutes
Epoch 0
Fine tuning took 0.141142 minutes
Epoch 0
Fine tuning took 0.141805 minutes
Epoch 0
Fine tuning took 0.142767 minutes
Epoch 0
Fine tuning took 0.140987 minutes
Epoch 0
Fine tuning took 0.141870 minutes
Epoch 0
Fine tuning took 0.141902 minutes
{'zero': {0: [0.43965517241379309, 0.21798029556650247, 0.32142857142857145, 0.65394088669950734, 0.19211822660098521, 0.67241379310344829, 0.40024630541871919, 0.21182266009852216, 0.25862068965517243, 0.17610837438423646, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.15640394088669951, 0.63423645320197042, 0.36576354679802958, 0.23275862068965517, 0.61822660098522164, 0.14532019704433496, 0.2229064039408867, 0.62068965517241381, 0.53201970443349755, 0.58374384236453203, 0.34236453201970446], 5: [0.4039408866995074, 0.14778325123152711, 0.31280788177339902, 0.11330049261083744, 0.18965517241379309, 0.18226600985221675, 0.37684729064039407, 0.16748768472906403, 0.20935960591133004, 0.24014778325123154, 0.40640394088669951], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.43965517241379309, 0.25123152709359609, 0.29679802955665024, 0.63177339901477836, 0.21551724137931033, 0.6145320197044335, 0.42733990147783252, 0.17857142857142858, 0.31034482758620691, 0.27709359605911332, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.15640394088669951, 0.59605911330049266, 0.43472906403940886, 0.25123152709359609, 0.56403940886699511, 0.20320197044334976, 0.23399014778325122, 0.66871921182266014, 0.39655172413793105, 0.47044334975369456, 0.36945812807881773], 5: [0.4039408866995074, 0.15270935960591134, 0.26847290640394089, 0.11699507389162561, 0.22044334975369459, 0.18226600985221675, 0.33866995073891626, 0.15270935960591134, 0.29310344827586204, 0.25246305418719212, 0.44458128078817732], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.43965517241379309, 0.22536945812807882, 0.30172413793103448, 0.61822660098522164, 0.20689655172413793, 0.66009852216748766, 0.45443349753694579, 0.21182266009852216, 0.29679802955665024, 0.23891625615763548, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.15640394088669951, 0.64408866995073888, 0.3817733990147783, 0.25985221674876846, 0.60344827586206895, 0.15147783251231528, 0.22413793103448276, 0.60098522167487689, 0.43349753694581283, 0.50246305418719217, 0.36330049261083741], 5: [0.4039408866995074, 0.13054187192118227, 0.31650246305418717, 0.12192118226600986, 0.18965517241379309, 0.18842364532019704, 0.32142857142857145, 0.18719211822660098, 0.26970443349753692, 0.25862068965517243, 0.42610837438423643], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.43965517241379309, 0.21305418719211822, 0.30541871921182268, 0.64901477832512311, 0.22536945812807882, 0.64532019704433496, 0.42857142857142855, 0.20812807881773399, 0.31034482758620691, 0.25, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.15640394088669951, 0.64778325123152714, 0.40763546798029554, 0.2413793103448276, 0.56896551724137934, 0.17733990147783252, 0.25492610837438423, 0.62315270935960587, 0.41256157635467983, 0.51724137931034486, 0.36330049261083741], 5: [0.4039408866995074, 0.13916256157635468, 0.28694581280788178, 0.10960591133004927, 0.20566502463054187, 0.17733990147783252, 0.31650246305418717, 0.16871921182266009, 0.27709359605911332, 0.23275862068965517, 0.42857142857142855], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420582 minutes
Weight histogram
[ 118  345  521  648  780 2894 7646 8972 3917  484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1198  972  855 1200 1274 2753 3075 3718 3352 7928] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.28249
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.00189
Epoch 1, cost is  0.981075
Epoch 2, cost is  0.966994
Epoch 3, cost is  0.95462
Epoch 4, cost is  0.94516
Training took 0.643960 minutes
Weight histogram
[6361 4928 4064 2684 2105 1693 1372 1085  987 1046] [ -4.32080664e-02  -3.88925068e-02  -3.45769472e-02  -3.02613877e-02
  -2.59458281e-02  -2.16302685e-02  -1.73147090e-02  -1.29991494e-02
  -8.68358984e-03  -4.36803028e-03  -5.24707102e-05]
[1956 1372 1474 1777 2122 2614 2932 3291 4051 4736] [ -4.32080664e-02  -3.88925068e-02  -3.45769472e-02  -3.02613877e-02
  -2.59458281e-02  -2.16302685e-02  -1.73147090e-02  -1.29991494e-02
  -8.68358984e-03  -4.36803028e-03  -5.24707102e-05]
-0.801216
1.52578
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146509 minutes
Weight histogram
[   33   314  1753   890   629   721  5902 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[ 2167   173   222   295   467   871  1005  1855  5897 15398] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.114716 minutes
Weight histogram
[4561 4652 4001 3419 2456 1904 2029 1489 2103 1736] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
[4603 1423 1481 1915 2414 2655 2835 3268 3646 4110] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
-1.31179
1.38974
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.95322
Epoch 1, cost is  5.49367
Epoch 2, cost is  4.90924
Epoch 3, cost is  4.45971
Epoch 4, cost is  4.10146
Epoch 5, cost is  3.80468
Epoch 6, cost is  3.56137
Epoch 7, cost is  3.3702
Epoch 8, cost is  3.21125
Epoch 9, cost is  3.07722
Training took 0.495947 minutes
Weight histogram
[1103 1093 1007  913  820  712  674 1326  424   28] [-0.02037968 -0.01835723 -0.01633477 -0.01431231 -0.01228985 -0.0102674
 -0.00824494 -0.00622248 -0.00420003 -0.00217757 -0.00015511]
[1416  568  608  649  674  710  764  825  905  981] [-0.02037968 -0.01835723 -0.01633477 -0.01431231 -0.01228985 -0.0102674
 -0.00824494 -0.00622248 -0.00420003 -0.00217757 -0.00015511]
-0.265771
0.418584
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.148768 minutes
Epoch 0
Fine tuning took 0.150069 minutes
Epoch 0
Fine tuning took 0.148688 minutes
Epoch 0
Fine tuning took 0.150182 minutes
Epoch 0
Fine tuning took 0.150538 minutes
Epoch 0
Fine tuning took 0.148790 minutes
Epoch 0
Fine tuning took 0.149190 minutes
Epoch 0
Fine tuning took 0.151311 minutes
Epoch 0
Fine tuning took 0.150532 minutes
Epoch 0
Fine tuning took 0.151254 minutes
{'zero': {0: [0.41871921182266009, 0.27832512315270935, 0.40024630541871919, 0.37192118226600984, 0.37438423645320196, 0.3817733990147783, 0.46182266009852219, 0.30541871921182268, 0.34113300492610837, 0.32266009852216748, 0.33374384236453203], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.24261083743842365, 0.51477832512315269, 0.36206896551724138, 0.37561576354679804, 0.46921182266009853, 0.3682266009852217, 0.33497536945812806, 0.48152709359605911, 0.44581280788177341, 0.51724137931034486, 0.4605911330049261], 5: [0.33866995073891626, 0.20689655172413793, 0.2376847290640394, 0.25246305418719212, 0.15640394088669951, 0.25, 0.20320197044334976, 0.21305418719211822, 0.21305418719211822, 0.16009852216748768, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.41871921182266009, 0.41133004926108374, 0.53940886699507384, 0.51724137931034486, 0.45320197044334976, 0.47660098522167488, 0.52339901477832518, 0.43472906403940886, 0.40640394088669951, 0.44704433497536944, 0.43719211822660098], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.24261083743842365, 0.42241379310344829, 0.26970443349753692, 0.24630541871921183, 0.31650246305418717, 0.24507389162561577, 0.27339901477832512, 0.39162561576354682, 0.3460591133004926, 0.4211822660098522, 0.3817733990147783], 5: [0.33866995073891626, 0.16625615763546797, 0.19088669950738915, 0.23645320197044334, 0.23029556650246305, 0.27832512315270935, 0.20320197044334976, 0.17364532019704434, 0.24753694581280788, 0.13177339901477833, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.41871921182266009, 0.36945812807881773, 0.57512315270935965, 0.49753694581280788, 0.45566502463054187, 0.46305418719211822, 0.53940886699507384, 0.43596059113300495, 0.45197044334975367, 0.40270935960591131, 0.44334975369458129], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.24261083743842365, 0.45812807881773399, 0.24384236453201971, 0.24507389162561577, 0.32019704433497537, 0.25492610837438423, 0.25738916256157635, 0.38669950738916259, 0.32019704433497537, 0.43965517241379309, 0.36330049261083741], 5: [0.33866995073891626, 0.17241379310344829, 0.18103448275862069, 0.25738916256157635, 0.22413793103448276, 0.28201970443349755, 0.20320197044334976, 0.17733990147783252, 0.22783251231527094, 0.15763546798029557, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.41871921182266009, 0.3817733990147783, 0.55295566502463056, 0.53694581280788178, 0.48399014778325122, 0.50985221674876846, 0.54556650246305416, 0.4642857142857143, 0.44827586206896552, 0.45812807881773399, 0.44827586206896552], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.24261083743842365, 0.44827586206896552, 0.27709359605911332, 0.24630541871921183, 0.28817733990147781, 0.25, 0.2413793103448276, 0.35344827586206895, 0.33620689655172414, 0.37931034482758619, 0.38054187192118227], 5: [0.33866995073891626, 0.16995073891625614, 0.16995073891625614, 0.21674876847290642, 0.22783251231527094, 0.24014778325123154, 0.21305418719211822, 0.18226600985221675, 0.21551724137931033, 0.1625615763546798, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.419065 minutes
Weight histogram
[ 118  345  521  648  780 2894 7646 8972 3917  484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1198  972  855 1200 1274 2753 3075 3718 3352 7928] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.28249
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.00189
Epoch 1, cost is  0.981075
Epoch 2, cost is  0.966994
Epoch 3, cost is  0.95462
Epoch 4, cost is  0.94516
Training took 0.644063 minutes
Weight histogram
[6361 4928 4064 2684 2105 1693 1372 1085  987 1046] [ -4.32080664e-02  -3.88925068e-02  -3.45769472e-02  -3.02613877e-02
  -2.59458281e-02  -2.16302685e-02  -1.73147090e-02  -1.29991494e-02
  -8.68358984e-03  -4.36803028e-03  -5.24707102e-05]
[1956 1372 1474 1777 2122 2614 2932 3291 4051 4736] [ -4.32080664e-02  -3.88925068e-02  -3.45769472e-02  -3.02613877e-02
  -2.59458281e-02  -2.16302685e-02  -1.73147090e-02  -1.29991494e-02
  -8.68358984e-03  -4.36803028e-03  -5.24707102e-05]
-0.801216
1.52578
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147412 minutes
Weight histogram
[   33   314  1753   890   629   721  5902 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[ 2167   173   222   295   467   871  1005  1855  5897 15398] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.113439 minutes
Weight histogram
[4561 4652 4001 3419 2456 1904 2029 1489 2103 1736] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
[4603 1423 1481 1915 2414 2655 2835 3268 3646 4110] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
-1.31179
1.38974
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.33384
Epoch 1, cost is  4.83079
Epoch 2, cost is  4.24194
Epoch 3, cost is  3.79254
Epoch 4, cost is  3.44328
Epoch 5, cost is  3.17361
Epoch 6, cost is  2.96697
Epoch 7, cost is  2.80211
Epoch 8, cost is  2.66955
Epoch 9, cost is  2.56073
Training took 0.816061 minutes
Weight histogram
[1427 1296 1090  987  879  721 1437  209   38   16] [-0.01366388 -0.01231168 -0.01095948 -0.00960727 -0.00825507 -0.00690287
 -0.00555067 -0.00419846 -0.00284626 -0.00149406 -0.00014186]
[1434  572  610  620  639  693  773  832  915 1012] [-0.01366388 -0.01231168 -0.01095948 -0.00960727 -0.00825507 -0.00690287
 -0.00555067 -0.00419846 -0.00284626 -0.00149406 -0.00014186]
-0.241098
0.289531
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164051 minutes
Epoch 0
Fine tuning took 0.165812 minutes
Epoch 0
Fine tuning took 0.163396 minutes
Epoch 0
Fine tuning took 0.163818 minutes
Epoch 0
Fine tuning took 0.162791 minutes
Epoch 0
Fine tuning took 0.162899 minutes
Epoch 0
Fine tuning took 0.163812 minutes
Epoch 0
Fine tuning took 0.164580 minutes
Epoch 0
Fine tuning took 0.165921 minutes
Epoch 0
Fine tuning took 0.164679 minutes
{'zero': {0: [0.30541871921182268, 0.36945812807881773, 0.39285714285714285, 0.38054187192118227, 0.31896551724137934, 0.40147783251231528, 0.35098522167487683, 0.32635467980295568, 0.35714285714285715, 0.31280788177339902, 0.34975369458128081], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41625615763546797, 0.45197044334975367, 0.39532019704433496, 0.41379310344827586, 0.50123152709359609, 0.41748768472906406, 0.42241379310344829, 0.47413793103448276, 0.41995073891625617, 0.46551724137931033, 0.4963054187192118], 5: [0.27832512315270935, 0.17857142857142858, 0.21182266009852216, 0.20566502463054187, 0.17980295566502463, 0.18103448275862069, 0.22660098522167488, 0.19950738916256158, 0.2229064039408867, 0.22167487684729065, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30541871921182268, 0.39285714285714285, 0.41748768472906406, 0.44704433497536944, 0.36330049261083741, 0.45073891625615764, 0.37931034482758619, 0.35837438423645318, 0.40763546798029554, 0.34852216748768472, 0.36330049261083741], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41625615763546797, 0.3854679802955665, 0.35467980295566504, 0.34852216748768472, 0.44458128078817732, 0.38300492610837439, 0.39039408866995073, 0.41871921182266009, 0.40147783251231528, 0.45197044334975367, 0.49876847290640391], 5: [0.27832512315270935, 0.22167487684729065, 0.22783251231527094, 0.20443349753694581, 0.19211822660098521, 0.16625615763546797, 0.23029556650246305, 0.2229064039408867, 0.19088669950738915, 0.19950738916256158, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30541871921182268, 0.43103448275862066, 0.42610837438423643, 0.46305418719211822, 0.36330049261083741, 0.45935960591133007, 0.36945812807881773, 0.39285714285714285, 0.40640394088669951, 0.39408866995073893, 0.3817733990147783], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41625615763546797, 0.38423645320197042, 0.33620689655172414, 0.34729064039408869, 0.44827586206896552, 0.36206896551724138, 0.39655172413793105, 0.44458128078817732, 0.3817733990147783, 0.39162561576354682, 0.44827586206896552], 5: [0.27832512315270935, 0.18472906403940886, 0.2376847290640394, 0.18965517241379309, 0.18842364532019704, 0.17857142857142858, 0.23399014778325122, 0.1625615763546798, 0.21182266009852216, 0.21428571428571427, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30541871921182268, 0.40147783251231528, 0.39532019704433496, 0.43349753694581283, 0.3682266009852217, 0.47536945812807879, 0.37931034482758619, 0.38423645320197042, 0.42610837438423643, 0.39162561576354682, 0.34852216748768472], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41625615763546797, 0.38300492610837439, 0.40024630541871919, 0.3682266009852217, 0.45320197044334976, 0.37192118226600984, 0.39655172413793105, 0.43103448275862066, 0.38423645320197042, 0.40763546798029554, 0.49014778325123154], 5: [0.27832512315270935, 0.21551724137931033, 0.20443349753694581, 0.19827586206896552, 0.17857142857142858, 0.15270935960591134, 0.22413793103448276, 0.18472906403940886, 0.18965517241379309, 0.20073891625615764, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422327 minutes
Weight histogram
[ 118  345  521  648  780 2894 7646 8972 3917  484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1198  972  855 1200 1274 2753 3075 3718 3352 7928] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.28249
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.00189
Epoch 1, cost is  0.981075
Epoch 2, cost is  0.966994
Epoch 3, cost is  0.95462
Epoch 4, cost is  0.94516
Training took 0.645547 minutes
Weight histogram
[6361 4928 4064 2684 2105 1693 1372 1085  987 1046] [ -4.32080664e-02  -3.88925068e-02  -3.45769472e-02  -3.02613877e-02
  -2.59458281e-02  -2.16302685e-02  -1.73147090e-02  -1.29991494e-02
  -8.68358984e-03  -4.36803028e-03  -5.24707102e-05]
[1956 1372 1474 1777 2122 2614 2932 3291 4051 4736] [ -4.32080664e-02  -3.88925068e-02  -3.45769472e-02  -3.02613877e-02
  -2.59458281e-02  -2.16302685e-02  -1.73147090e-02  -1.29991494e-02
  -8.68358984e-03  -4.36803028e-03  -5.24707102e-05]
-0.801216
1.52578
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148197 minutes
Weight histogram
[   33   314  1753   890   629   721  5902 12527  5185   396] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
[ 2167   173   222   295   467   871  1005  1855  5897 15398] [ -5.43409726e-04  -2.52902484e-04   3.76047567e-05   3.28111998e-04
   6.18619239e-04   9.09126480e-04   1.19963372e-03   1.49014096e-03
   1.78064820e-03   2.07115544e-03   2.36166269e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.55058
Epoch 1, cost is  2.50995
Epoch 2, cost is  2.47834
Epoch 3, cost is  2.45228
Epoch 4, cost is  2.43089
Training took 0.113473 minutes
Weight histogram
[4561 4652 4001 3419 2456 1904 2029 1489 2103 1736] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
[4603 1423 1481 1915 2414 2655 2835 3268 3646 4110] [ -6.05602935e-02  -5.45009454e-02  -4.84415972e-02  -4.23822491e-02
  -3.63229010e-02  -3.02635529e-02  -2.42042047e-02  -1.81448566e-02
  -1.20855085e-02  -6.02616037e-03   3.31877563e-05]
-1.31179
1.38974
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.72284
Epoch 1, cost is  4.23975
Epoch 2, cost is  3.69752
Epoch 3, cost is  3.28882
Epoch 4, cost is  2.97001
Epoch 5, cost is  2.74341
Epoch 6, cost is  2.57096
Epoch 7, cost is  2.43579
Epoch 8, cost is  2.32498
Epoch 9, cost is  2.23088
Training took 1.474195 minutes
Weight histogram
[1894 1572 1320 1234 1672  241   97   42   18   10] [-0.00916891 -0.00826768 -0.00736645 -0.00646522 -0.00556399 -0.00466276
 -0.00376153 -0.0028603  -0.00195907 -0.00105784 -0.00015661]
[1407  564  587  585  620  695  767  859  956 1060] [-0.00916891 -0.00826768 -0.00736645 -0.00646522 -0.00556399 -0.00466276
 -0.00376153 -0.0028603  -0.00195907 -0.00105784 -0.00015661]
-0.169758
0.233435
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.193110 minutes
Epoch 0
Fine tuning took 0.192793 minutes
Epoch 0
Fine tuning took 0.193231 minutes
Epoch 0
Fine tuning took 0.192662 minutes
Epoch 0
Fine tuning took 0.192363 minutes
Epoch 0
Fine tuning took 0.192364 minutes
Epoch 0
Fine tuning took 0.192946 minutes
Epoch 0
Fine tuning took 0.193355 minutes
Epoch 0
Fine tuning took 0.193329 minutes
Epoch 0
Fine tuning took 0.193436 minutes
{'zero': {0: [0.31527093596059114, 0.35591133004926107, 0.35837438423645318, 0.34852216748768472, 0.33004926108374383, 0.34482758620689657, 0.31527093596059114, 0.37931034482758619, 0.37192118226600984, 0.33866995073891626, 0.35221674876847292], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.38300492610837439, 0.38793103448275862, 0.4248768472906404, 0.47044334975369456, 0.45197044334975367, 0.46182266009852219, 0.43472906403940886, 0.4039408866995074, 0.46798029556650245, 0.41502463054187194], 5: [0.2019704433497537, 0.26108374384236455, 0.2536945812807882, 0.22660098522167488, 0.19950738916256158, 0.20320197044334976, 0.2229064039408867, 0.18596059113300492, 0.22413793103448276, 0.19334975369458129, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31527093596059114, 0.39901477832512317, 0.38669950738916259, 0.35344827586206895, 0.38423645320197042, 0.40517241379310343, 0.34975369458128081, 0.37315270935960593, 0.37684729064039407, 0.41256157635467983, 0.35467980295566504], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.36576354679802958, 0.39901477832512317, 0.39285714285714285, 0.41625615763546797, 0.41748768472906406, 0.42980295566502463, 0.42610837438423643, 0.41502463054187194, 0.3891625615763547, 0.41502463054187194], 5: [0.2019704433497537, 0.23522167487684728, 0.21428571428571427, 0.2536945812807882, 0.19950738916256158, 0.17733990147783252, 0.22044334975369459, 0.20073891625615764, 0.20812807881773399, 0.19827586206896552, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31527093596059114, 0.39901477832512317, 0.38669950738916259, 0.39901477832512317, 0.35221674876847292, 0.3645320197044335, 0.32758620689655171, 0.38669950738916259, 0.37192118226600984, 0.38300492610837439, 0.3891625615763547], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.37807881773399016, 0.3854679802955665, 0.37068965517241381, 0.4211822660098522, 0.45935960591133007, 0.4211822660098522, 0.3817733990147783, 0.41502463054187194, 0.4211822660098522, 0.37807881773399016], 5: [0.2019704433497537, 0.2229064039408867, 0.22783251231527094, 0.23029556650246305, 0.22660098522167488, 0.17610837438423646, 0.25123152709359609, 0.23152709359605911, 0.21305418719211822, 0.19581280788177341, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31527093596059114, 0.37684729064039407, 0.37068965517241381, 0.3891625615763547, 0.36206896551724138, 0.40147783251231528, 0.34113300492610837, 0.39162561576354682, 0.37068965517241381, 0.35837438423645318, 0.36330049261083741], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.36206896551724138, 0.37561576354679804, 0.40147783251231528, 0.44827586206896552, 0.40517241379310343, 0.41871921182266009, 0.38669950738916259, 0.40886699507389163, 0.43719211822660098, 0.43596059113300495], 5: [0.2019704433497537, 0.26108374384236455, 0.2536945812807882, 0.20935960591133004, 0.18965517241379309, 0.19334975369458129, 0.24014778325123154, 0.22167487684729065, 0.22044334975369459, 0.20443349753694581, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106201 minutes
Weight histogram
[ 657 4167 4791 4070 3973 3830 3185 3202 1834  666] [-0.00122387 -0.00071834 -0.00021281  0.00029271  0.00079824  0.00130376
  0.00180929  0.00231481  0.00282034  0.00332586  0.00383139]
[  145   154   210   313   467   769   797  1691  3816 22013] [-0.00122387 -0.00071834 -0.00021281  0.00029271  0.00079824  0.00130376
  0.00180929  0.00231481  0.00282034  0.00332586  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.79126
Epoch 1, cost is  3.7489
Epoch 2, cost is  3.73314
Epoch 3, cost is  3.70334
Epoch 4, cost is  3.67611
Training took 0.072842 minutes
Weight histogram
[6072 7442 2973 4802 4450 2172 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2780 2272 2190 2601 2353 2521 3177 3203 4478 4800] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.00778
2.47667
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148293 minutes
Weight histogram
[   95   349   614  1079  1204  4215 14957  8924   945    18] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 12101   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.115231 minutes
Weight histogram
[6309 5304 4022 3907 2902 2195 2129 1686 3226  720] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
-1.3976
1.51906
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.94588
Epoch 1, cost is  5.60306
Epoch 2, cost is  5.51013
Epoch 3, cost is  5.41583
Epoch 4, cost is  5.23588
Epoch 5, cost is  4.98983
Epoch 6, cost is  4.77487
Epoch 7, cost is  4.60524
Epoch 8, cost is  4.4451
Epoch 9, cost is  4.29684
Training took 0.176389 minutes
Weight histogram
[2093 2321 4039 1395  856  569  478  244  104   51] [-0.0301243 -0.0271222 -0.0241201 -0.021118  -0.0181159 -0.0151138
 -0.0121117 -0.0091096 -0.0061075 -0.0031054 -0.0001033]
[ 832 1556 2522 1191  909  912  987 1073 1099 1069] [-0.0301243 -0.0271222 -0.0241201 -0.021118  -0.0181159 -0.0151138
 -0.0121117 -0.0091096 -0.0061075 -0.0031054 -0.0001033]
-0.405522
0.437275
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042695 minutes
Epoch 0
Fine tuning took 0.041813 minutes
Epoch 0
Fine tuning took 0.041628 minutes
Epoch 0
Fine tuning took 0.042859 minutes
Epoch 0
Fine tuning took 0.041876 minutes
Epoch 0
Fine tuning took 0.043637 minutes
Epoch 0
Fine tuning took 0.041588 minutes
Epoch 0
Fine tuning took 0.040437 minutes
Epoch 0
Fine tuning took 0.043799 minutes
Epoch 0
Fine tuning took 0.040685 minutes
{'zero': {0: [0.51724137931034486, 0.26108374384236455, 0.30172413793103448, 0.28325123152709358, 0.2857142857142857, 0.15763546798029557, 0.32266009852216748, 0.10467980295566502, 0.14285714285714285, 0.12931034482758622, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.30172413793103448, 0.62438423645320196, 0.58866995073891626, 0.58990147783251234, 0.5431034482758621, 0.71182266009852213, 0.57635467980295563, 0.71798029556650245, 0.70566502463054193, 0.64655172413793105, 0.57758620689655171], 5: [0.18103448275862069, 0.1145320197044335, 0.10960591133004927, 0.1268472906403941, 0.17118226600985223, 0.13054187192118227, 0.10098522167487685, 0.17733990147783252, 0.15147783251231528, 0.22413793103448276, 0.25], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.51724137931034486, 0.50369458128078815, 0.73275862068965514, 0.73399014778325122, 0.53940886699507384, 0.55172413793103448, 0.52339901477832518, 0.41748768472906406, 0.41625615763546797, 0.59482758620689657, 0.37438423645320196], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.30172413793103448, 0.37192118226600984, 0.14778325123152711, 0.12561576354679804, 0.19704433497536947, 0.25615763546798032, 0.30295566502463056, 0.2229064039408867, 0.41748768472906406, 0.27832512315270935, 0.43226600985221675], 5: [0.18103448275862069, 0.12438423645320197, 0.11945812807881774, 0.14039408866995073, 0.26354679802955666, 0.19211822660098521, 0.17364532019704434, 0.35960591133004927, 0.16625615763546797, 0.1268472906403941, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.51724137931034486, 0.48152709359605911, 0.67364532019704437, 0.70935960591133007, 0.50492610837438423, 0.55049261083743839, 0.49261083743842365, 0.40024630541871919, 0.4211822660098522, 0.55788177339901479, 0.35221674876847292], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.30172413793103448, 0.40763546798029554, 0.20443349753694581, 0.14532019704433496, 0.18596059113300492, 0.26354679802955666, 0.32389162561576357, 0.25862068965517243, 0.44950738916256155, 0.29187192118226601, 0.43103448275862066], 5: [0.18103448275862069, 0.11083743842364532, 0.12192118226600986, 0.14532019704433496, 0.30911330049261082, 0.18596059113300492, 0.18349753694581281, 0.34113300492610837, 0.12931034482758622, 0.15024630541871922, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.51724137931034486, 0.54926108374384242, 0.78325123152709364, 0.72906403940886699, 0.55788177339901479, 0.6145320197044335, 0.56034482758620685, 0.46798029556650245, 0.49261083743842365, 0.68349753694581283, 0.43842364532019706], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.30172413793103448, 0.34113300492610837, 0.13177339901477833, 0.12192118226600986, 0.17980295566502463, 0.22167487684729065, 0.29310344827586204, 0.16379310344827586, 0.34852216748768472, 0.20812807881773399, 0.39039408866995073], 5: [0.18103448275862069, 0.10960591133004927, 0.084975369458128072, 0.14901477832512317, 0.26231527093596058, 0.16379310344827586, 0.14655172413793102, 0.3682266009852217, 0.15886699507389163, 0.10837438423645321, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105461 minutes
Weight histogram
[ 657 4167 4791 4070 3973 3830 3185 3202 1834  666] [-0.00122387 -0.00071834 -0.00021281  0.00029271  0.00079824  0.00130376
  0.00180929  0.00231481  0.00282034  0.00332586  0.00383139]
[  145   154   210   313   467   769   797  1691  3816 22013] [-0.00122387 -0.00071834 -0.00021281  0.00029271  0.00079824  0.00130376
  0.00180929  0.00231481  0.00282034  0.00332586  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.79126
Epoch 1, cost is  3.7489
Epoch 2, cost is  3.73314
Epoch 3, cost is  3.70334
Epoch 4, cost is  3.67611
Training took 0.071133 minutes
Weight histogram
[6072 7442 2973 4802 4450 2172 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2780 2272 2190 2601 2353 2521 3177 3203 4478 4800] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.00778
2.47667
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147543 minutes
Weight histogram
[   95   349   614  1079  1204  4215 14957  8924   945    18] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 12101   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.115250 minutes
Weight histogram
[6309 5304 4022 3907 2902 2195 2129 1686 3226  720] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
-1.3976
1.51906
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.54907
Epoch 1, cost is  5.33447
Epoch 2, cost is  5.25633
Epoch 3, cost is  5.06609
Epoch 4, cost is  4.74743
Epoch 5, cost is  4.46405
Epoch 6, cost is  4.24871
Epoch 7, cost is  4.07357
Epoch 8, cost is  3.91683
Epoch 9, cost is  3.76874
Training took 0.244378 minutes
Weight histogram
[2378 5350 2583  738  468  285  157  102   55   34] [-0.01972586 -0.0177691  -0.01581234 -0.01385559 -0.01189883 -0.00994207
 -0.00798532 -0.00602856 -0.0040718  -0.00211504 -0.00015829]
[2904 1555  799  747  777  890 1001 1132 1200 1145] [-0.01972586 -0.0177691  -0.01581234 -0.01385559 -0.01189883 -0.00994207
 -0.00798532 -0.00602856 -0.0040718  -0.00211504 -0.00015829]
-0.341753
0.278611
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046834 minutes
Epoch 0
Fine tuning took 0.044837 minutes
Epoch 0
Fine tuning took 0.047107 minutes
Epoch 0
Fine tuning took 0.047053 minutes
Epoch 0
Fine tuning took 0.046525 minutes
Epoch 0
Fine tuning took 0.045117 minutes
Epoch 0
Fine tuning took 0.044402 minutes
Epoch 0
Fine tuning took 0.046779 minutes
Epoch 0
Fine tuning took 0.045741 minutes
Epoch 0
Fine tuning took 0.046853 minutes
{'zero': {0: [0.31527093596059114, 0.25738916256157635, 0.29802955665024633, 0.35591133004926107, 0.29926108374384236, 0.29064039408866993, 0.33743842364532017, 0.30541871921182268, 0.34359605911330049, 0.25985221674876846, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.52709359605911332, 0.51724137931034486, 0.51108374384236455, 0.51108374384236455, 0.50985221674876846, 0.44704433497536944, 0.44950738916256155, 0.36206896551724138, 0.51970443349753692, 0.44211822660098521], 5: [0.18842364532019704, 0.21551724137931033, 0.18472906403940886, 0.13300492610837439, 0.18965517241379309, 0.19950738916256158, 0.21551724137931033, 0.24507389162561577, 0.29433497536945813, 0.22044334975369459, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31527093596059114, 0.33128078817733991, 0.30172413793103448, 0.3682266009852217, 0.29310344827586204, 0.29187192118226601, 0.3460591133004926, 0.33497536945812806, 0.34852216748768472, 0.27093596059113301, 0.37807881773399016], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.48891625615763545, 0.4963054187192118, 0.49261083743842365, 0.50246305418719217, 0.49876847290640391, 0.45935960591133007, 0.43103448275862066, 0.35837438423645318, 0.49384236453201968, 0.40886699507389163], 5: [0.18842364532019704, 0.17980295566502463, 0.2019704433497537, 0.13916256157635468, 0.20443349753694581, 0.20935960591133004, 0.19458128078817735, 0.23399014778325122, 0.29310344827586204, 0.23522167487684728, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31527093596059114, 0.3251231527093596, 0.35221674876847292, 0.41133004926108374, 0.3251231527093596, 0.29433497536945813, 0.38300492610837439, 0.33251231527093594, 0.37931034482758619, 0.30172413793103448, 0.39655172413793105], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.49753694581280788, 0.46674876847290642, 0.44827586206896552, 0.48275862068965519, 0.48275862068965519, 0.43472906403940886, 0.42241379310344829, 0.3608374384236453, 0.46551724137931033, 0.39039408866995073], 5: [0.18842364532019704, 0.17733990147783252, 0.18103448275862069, 0.14039408866995073, 0.19211822660098521, 0.2229064039408867, 0.18226600985221675, 0.24507389162561577, 0.25985221674876846, 0.23275862068965517, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31527093596059114, 0.35960591133004927, 0.32389162561576357, 0.38054187192118227, 0.31527093596059114, 0.32266009852216748, 0.37684729064039407, 0.33374384236453203, 0.33004926108374383, 0.2857142857142857, 0.35714285714285715], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.45689655172413796, 0.5, 0.47167487684729065, 0.4963054187192118, 0.45812807881773399, 0.43226600985221675, 0.45689655172413796, 0.37192118226600984, 0.47906403940886699, 0.41502463054187194], 5: [0.18842364532019704, 0.18349753694581281, 0.17610837438423646, 0.14778325123152711, 0.18842364532019704, 0.21921182266009853, 0.19088669950738915, 0.20935960591133004, 0.29802955665024633, 0.23522167487684728, 0.22783251231527094], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106302 minutes
Weight histogram
[ 657 4167 4791 4070 3973 3830 3185 3202 1834  666] [-0.00122387 -0.00071834 -0.00021281  0.00029271  0.00079824  0.00130376
  0.00180929  0.00231481  0.00282034  0.00332586  0.00383139]
[  145   154   210   313   467   769   797  1691  3816 22013] [-0.00122387 -0.00071834 -0.00021281  0.00029271  0.00079824  0.00130376
  0.00180929  0.00231481  0.00282034  0.00332586  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.79126
Epoch 1, cost is  3.7489
Epoch 2, cost is  3.73314
Epoch 3, cost is  3.70334
Epoch 4, cost is  3.67611
Training took 0.075558 minutes
Weight histogram
[6072 7442 2973 4802 4450 2172 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2780 2272 2190 2601 2353 2521 3177 3203 4478 4800] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.00778
2.47667
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146690 minutes
Weight histogram
[   95   349   614  1079  1204  4215 14957  8924   945    18] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 12101   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.114513 minutes
Weight histogram
[6309 5304 4022 3907 2902 2195 2129 1686 3226  720] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
-1.3976
1.51906
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.45672
Epoch 1, cost is  5.33503
Epoch 2, cost is  5.07367
Epoch 3, cost is  4.6183
Epoch 4, cost is  4.25361
Epoch 5, cost is  3.99929
Epoch 6, cost is  3.78091
Epoch 7, cost is  3.58164
Epoch 8, cost is  3.40435
Epoch 9, cost is  3.25242
Training took 0.333692 minutes
Weight histogram
[2041 2608 1830 1564 3078  810  116   55   27   21] [-0.01159094 -0.01044461 -0.00929828 -0.00815195 -0.00700562 -0.00585928
 -0.00471295 -0.00356662 -0.00242029 -0.00127396 -0.00012763]
[3104  782  718  770  891 1061 1128 1180 1239 1277] [-0.01159094 -0.01044461 -0.00929828 -0.00815195 -0.00700562 -0.00585928
 -0.00471295 -0.00356662 -0.00242029 -0.00127396 -0.00012763]
-0.252911
0.217595
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050796 minutes
Epoch 0
Fine tuning took 0.050337 minutes
Epoch 0
Fine tuning took 0.052253 minutes
Epoch 0
Fine tuning took 0.050909 minutes
Epoch 0
Fine tuning took 0.051589 minutes
Epoch 0
Fine tuning took 0.051552 minutes
Epoch 0
Fine tuning took 0.052530 minutes
Epoch 0
Fine tuning took 0.052460 minutes
Epoch 0
Fine tuning took 0.052273 minutes
Epoch 0
Fine tuning took 0.052201 minutes
{'zero': {0: [0.29310344827586204, 0.31896551724137934, 0.31157635467980294, 0.31403940886699505, 0.32389162561576357, 0.40886699507389163, 0.39655172413793105, 0.38793103448275862, 0.37931034482758619, 0.35467980295566504, 0.4211822660098522], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.40640394088669951, 0.48275862068965519, 0.46551724137931033, 0.43472906403940886, 0.4211822660098522, 0.36945812807881773, 0.3460591133004926, 0.39039408866995073, 0.39532019704433496, 0.3682266009852217], 5: [0.1748768472906404, 0.27463054187192121, 0.20566502463054187, 0.22044334975369459, 0.2413793103448276, 0.16995073891625614, 0.23399014778325122, 0.26600985221674878, 0.23029556650246305, 0.25, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29310344827586204, 0.3682266009852217, 0.39285714285714285, 0.30418719211822659, 0.35591133004926107, 0.43719211822660098, 0.34236453201970446, 0.36206896551724138, 0.40640394088669951, 0.36945812807881773, 0.44827586206896552], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.3645320197044335, 0.40763546798029554, 0.44950738916256155, 0.40640394088669951, 0.39901477832512317, 0.3891625615763547, 0.37068965517241381, 0.35714285714285715, 0.3891625615763547, 0.34113300492610837], 5: [0.1748768472906404, 0.26724137931034481, 0.19950738916256158, 0.24630541871921183, 0.2376847290640394, 0.16379310344827586, 0.26847290640394089, 0.26724137931034481, 0.23645320197044334, 0.2413793103448276, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29310344827586204, 0.32019704433497537, 0.35591133004926107, 0.33743842364532017, 0.34236453201970446, 0.41379310344827586, 0.39039408866995073, 0.35960591133004927, 0.3891625615763547, 0.35467980295566504, 0.40024630541871919], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.41133004926108374, 0.45197044334975367, 0.43226600985221675, 0.41625615763546797, 0.43226600985221675, 0.35591133004926107, 0.35837438423645318, 0.37931034482758619, 0.39778325123152708, 0.34359605911330049], 5: [0.1748768472906404, 0.26847290640394089, 0.19211822660098521, 0.23029556650246305, 0.2413793103448276, 0.1539408866995074, 0.2536945812807882, 0.28201970443349755, 0.23152709359605911, 0.24753694581280788, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29310344827586204, 0.36576354679802958, 0.37068965517241381, 0.35344827586206895, 0.34852216748768472, 0.40886699507389163, 0.39408866995073893, 0.3854679802955665, 0.41625615763546797, 0.38300492610837439, 0.44458128078817732], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.38669950738916259, 0.43965517241379309, 0.43965517241379309, 0.40270935960591131, 0.40763546798029554, 0.36576354679802958, 0.37068965517241381, 0.3608374384236453, 0.39162561576354682, 0.33128078817733991], 5: [0.1748768472906404, 0.24753694581280788, 0.18965517241379309, 0.20689655172413793, 0.24876847290640394, 0.18349753694581281, 0.24014778325123154, 0.24384236453201971, 0.2229064039408867, 0.22536945812807882, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.104955 minutes
Weight histogram
[ 657 4167 4791 4070 3973 3830 3185 3202 1834  666] [-0.00122387 -0.00071834 -0.00021281  0.00029271  0.00079824  0.00130376
  0.00180929  0.00231481  0.00282034  0.00332586  0.00383139]
[  145   154   210   313   467   769   797  1691  3816 22013] [-0.00122387 -0.00071834 -0.00021281  0.00029271  0.00079824  0.00130376
  0.00180929  0.00231481  0.00282034  0.00332586  0.00383139]
-2.29874
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.79126
Epoch 1, cost is  3.7489
Epoch 2, cost is  3.73314
Epoch 3, cost is  3.70334
Epoch 4, cost is  3.67611
Training took 0.076126 minutes
Weight histogram
[6072 7442 2973 4802 4450 2172 1380  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2780 2272 2190 2601 2353 2521 3177 3203 4478 4800] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.00778
2.47667
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147429 minutes
Weight histogram
[   95   349   614  1079  1204  4215 14957  8924   945    18] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 12101   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.116387 minutes
Weight histogram
[6309 5304 4022 3907 2902 2195 2129 1686 3226  720] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
-1.3976
1.51906
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.45219
Epoch 1, cost is  5.27882
Epoch 2, cost is  4.81117
Epoch 3, cost is  4.28532
Epoch 4, cost is  3.93078
Epoch 5, cost is  3.6419
Epoch 6, cost is  3.39673
Epoch 7, cost is  3.18431
Epoch 8, cost is  3.01814
Epoch 9, cost is  2.87189
Training took 0.532942 minutes
Weight histogram
[1965 2321 2028 1307 1131 2870  477   26   13   12] [-0.00614247 -0.00553966 -0.00493685 -0.00433405 -0.00373124 -0.00312843
 -0.00252562 -0.00192281 -0.00132    -0.00071719 -0.00011438]
[2815  763  735  844  983 1059 1108 1181 1280 1382] [-0.00614247 -0.00553966 -0.00493685 -0.00433405 -0.00373124 -0.00312843
 -0.00252562 -0.00192281 -0.00132    -0.00071719 -0.00011438]
-0.186733
0.179812
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.060711 minutes
Epoch 0
Fine tuning took 0.061220 minutes
Epoch 0
Fine tuning took 0.062773 minutes
Epoch 0
Fine tuning took 0.060094 minutes
Epoch 0
Fine tuning took 0.062854 minutes
Epoch 0
Fine tuning took 0.062731 minutes
Epoch 0
Fine tuning took 0.062017 minutes
Epoch 0
Fine tuning took 0.061864 minutes
Epoch 0
Fine tuning took 0.059919 minutes
Epoch 0
Fine tuning took 0.060797 minutes
{'zero': {0: [0.31034482758620691, 0.32758620689655171, 0.33866995073891626, 0.34236453201970446, 0.3251231527093596, 0.41133004926108374, 0.4211822660098522, 0.36330049261083741, 0.42241379310344829, 0.38793103448275862, 0.48275862068965519], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.43719211822660098, 0.47660098522167488, 0.44581280788177341, 0.41379310344827586, 0.3891625615763547, 0.31034482758620691, 0.4211822660098522, 0.30049261083743845, 0.3460591133004926, 0.27955665024630544], 5: [0.19334975369458129, 0.23522167487684728, 0.18472906403940886, 0.21182266009852216, 0.26108374384236455, 0.19950738916256158, 0.26847290640394089, 0.21551724137931033, 0.27709359605911332, 0.26600985221674878, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31034482758620691, 0.33743842364532017, 0.33743842364532017, 0.37068965517241381, 0.3682266009852217, 0.39778325123152708, 0.42980295566502463, 0.37561576354679804, 0.41009852216748771, 0.44950738916256155, 0.47906403940886699], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.42241379310344829, 0.45566502463054187, 0.42733990147783252, 0.37684729064039407, 0.40270935960591131, 0.31896551724137934, 0.43472906403940886, 0.31034482758620691, 0.33743842364532017, 0.29310344827586204], 5: [0.19334975369458129, 0.24014778325123154, 0.20689655172413793, 0.2019704433497537, 0.25492610837438423, 0.19950738916256158, 0.25123152709359609, 0.18965517241379309, 0.27955665024630544, 0.21305418719211822, 0.22783251231527094], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31034482758620691, 0.37807881773399016, 0.38793103448275862, 0.35714285714285715, 0.31896551724137934, 0.37931034482758619, 0.3891625615763547, 0.37315270935960593, 0.41009852216748771, 0.43349753694581283, 0.5], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.41379310344827586, 0.41995073891625617, 0.45197044334975367, 0.40763546798029554, 0.3854679802955665, 0.30911330049261082, 0.40147783251231528, 0.32758620689655171, 0.33743842364532017, 0.27093596059113301], 5: [0.19334975369458129, 0.20812807881773399, 0.19211822660098521, 0.19088669950738915, 0.27339901477832512, 0.23522167487684728, 0.30172413793103448, 0.22536945812807882, 0.26231527093596058, 0.22906403940886699, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31034482758620691, 0.33620689655172414, 0.35221674876847292, 0.37192118226600984, 0.34113300492610837, 0.44334975369458129, 0.40270935960591131, 0.39039408866995073, 0.43965517241379309, 0.43103448275862066, 0.47413793103448276], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.43965517241379309, 0.46921182266009853, 0.4248768472906404, 0.3682266009852217, 0.36576354679802958, 0.34113300492610837, 0.38054187192118227, 0.29679802955665024, 0.32142857142857145, 0.26847290640394089], 5: [0.19334975369458129, 0.22413793103448276, 0.17857142857142858, 0.20320197044334976, 0.29064039408866993, 0.19088669950738915, 0.25615763546798032, 0.22906403940886699, 0.26354679802955666, 0.24753694581280788, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147510 minutes
Weight histogram
[ 185  432  798 1935 3917 5704 9709 5760 1797  138] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
[  139   158   232   347   512   883  1136  2176  8349 16443] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
-1.32597
1.19368
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.33502
Epoch 1, cost is  2.30004
Epoch 2, cost is  2.27645
Epoch 3, cost is  2.25315
Epoch 4, cost is  2.23607
Training took 0.115242 minutes
Weight histogram
[5927 4835 4269 3947 3190 2244 2068 1945 1558  392] [ -6.45189360e-02  -5.80634708e-02  -5.16080057e-02  -4.51525405e-02
  -3.86970754e-02  -3.22416102e-02  -2.57861451e-02  -1.93306799e-02
  -1.28752148e-02  -6.41974963e-03   3.57155150e-05]
[2789 1526 1702 2210 2622 3053 3354 3995 4368 4756] [ -6.45189360e-02  -5.80634708e-02  -5.16080057e-02  -4.51525405e-02
  -3.86970754e-02  -3.22416102e-02  -2.57861451e-02  -1.93306799e-02
  -1.28752148e-02  -6.41974963e-03   3.57155150e-05]
-1.39671
1.86374
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146186 minutes
Weight histogram
[   36   324   649  1156  1318  1382  9553 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  283   333   456   652  1017  1512  1032  1959  6785 18371] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.116282 minutes
Weight histogram
[6309 5304 4023 3908 2900 2195 2130 1779 3097  755] [ -6.39048368e-02  -5.75107815e-02  -5.11167263e-02  -4.47226711e-02
  -3.83286159e-02  -3.19345606e-02  -2.55405054e-02  -1.91464502e-02
  -1.27523949e-02  -6.35833971e-03   3.57155150e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75107815e-02  -5.11167263e-02  -4.47226711e-02
  -3.83286159e-02  -3.19345606e-02  -2.55405054e-02  -1.91464502e-02
  -1.27523949e-02  -6.35833971e-03   3.57155150e-05]
-1.3976
1.51906
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.21319
Epoch 1, cost is  5.95934
Epoch 2, cost is  5.80104
Epoch 3, cost is  5.59621
Epoch 4, cost is  5.28411
Epoch 5, cost is  4.95172
Epoch 6, cost is  4.66813
Epoch 7, cost is  4.449
Epoch 8, cost is  4.26692
Epoch 9, cost is  4.10699
Training took 0.202878 minutes
Weight histogram
[1386 1349 1371 1477 3356 1593 1122  368   86   42] [-0.02607865 -0.02348716 -0.02089566 -0.01830417 -0.01571267 -0.01312118
 -0.01052969 -0.00793819 -0.0053467  -0.0027552  -0.00016371]
[2467 2111  940  779  805  853  932 1015 1124 1124] [-0.02607865 -0.02348716 -0.02089566 -0.01830417 -0.01571267 -0.01312118
 -0.01052969 -0.00793819 -0.0053467  -0.0027552  -0.00016371]
-0.329143
0.422927
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.054194 minutes
Epoch 0
Fine tuning took 0.054146 minutes
Epoch 0
Fine tuning took 0.053399 minutes
Epoch 0
Fine tuning took 0.052053 minutes
Epoch 0
Fine tuning took 0.052867 minutes
Epoch 0
Fine tuning took 0.055070 minutes
Epoch 0
Fine tuning took 0.054089 minutes
Epoch 0
Fine tuning took 0.052954 minutes
Epoch 0
Fine tuning took 0.052786 minutes
Epoch 0
Fine tuning took 0.055330 minutes
{'zero': {0: [0.49753694581280788, 0.56773399014778325, 0.54926108374384242, 0.72044334975369462, 0.50862068965517238, 0.38423645320197042, 0.44211822660098521, 0.29679802955665024, 0.41748768472906406, 0.28448275862068967, 0.37684729064039407], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.31280788177339902, 0.23399014778325122, 0.24630541871921183, 0.1206896551724138, 0.33866995073891626, 0.3854679802955665, 0.35098522167487683, 0.51108374384236455, 0.44211822660098521, 0.50862068965517238, 0.41748768472906406], 5: [0.18965517241379309, 0.19827586206896552, 0.20443349753694581, 0.15886699507389163, 0.15270935960591134, 0.23029556650246305, 0.20689655172413793, 0.19211822660098521, 0.14039408866995073, 0.20689655172413793, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.49753694581280788, 0.52339901477832518, 0.56034482758620685, 0.83374384236453203, 0.53078817733990147, 0.51108374384236455, 0.51231527093596063, 0.52586206896551724, 0.63916256157635465, 0.50246305418719217, 0.51477832512315269], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.31280788177339902, 0.18596059113300492, 0.13423645320197045, 0.033251231527093597, 0.19827586206896552, 0.11083743842364532, 0.097290640394088676, 0.22660098522167488, 0.21551724137931033, 0.25123152709359609, 0.24014778325123154], 5: [0.18965517241379309, 0.29064039408866993, 0.30541871921182268, 0.13300492610837439, 0.27093596059113301, 0.37807881773399016, 0.39039408866995073, 0.24753694581280788, 0.14532019704433496, 0.24630541871921183, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.49753694581280788, 0.50369458128078815, 0.54187192118226601, 0.80911330049261088, 0.49876847290640391, 0.50369458128078815, 0.51724137931034486, 0.54556650246305416, 0.65147783251231528, 0.48891625615763545, 0.49261083743842365], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.31280788177339902, 0.21428571428571427, 0.14901477832512317, 0.049261083743842367, 0.21182266009852216, 0.1206896551724138, 0.10960591133004927, 0.2019704433497537, 0.2229064039408867, 0.23029556650246305, 0.22413793103448276], 5: [0.18965517241379309, 0.28201970443349755, 0.30911330049261082, 0.14162561576354679, 0.2894088669950739, 0.37561576354679804, 0.37315270935960593, 0.25246305418719212, 0.12561576354679804, 0.28078817733990147, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.49753694581280788, 0.56157635467980294, 0.60837438423645318, 0.79926108374384242, 0.52586206896551724, 0.52955665024630538, 0.56773399014778325, 0.56034482758620685, 0.68103448275862066, 0.55172413793103448, 0.53694581280788178], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.31280788177339902, 0.17364532019704434, 0.10714285714285714, 0.055418719211822662, 0.2019704433497537, 0.098522167487684734, 0.070197044334975367, 0.1748768472906404, 0.19950738916256158, 0.21182266009852216, 0.23399014778325122], 5: [0.18965517241379309, 0.26477832512315269, 0.28448275862068967, 0.14532019704433496, 0.27216748768472904, 0.37192118226600984, 0.36206896551724138, 0.26477832512315269, 0.11945812807881774, 0.23645320197044334, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147542 minutes
Weight histogram
[ 185  432  798 1935 3917 5704 9709 5760 1797  138] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
[  139   158   232   347   512   883  1136  2176  8349 16443] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
-1.32597
1.19368
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.33502
Epoch 1, cost is  2.30004
Epoch 2, cost is  2.27645
Epoch 3, cost is  2.25315
Epoch 4, cost is  2.23607
Training took 0.113980 minutes
Weight histogram
[5927 4835 4269 3947 3190 2244 2068 1945 1558  392] [ -6.45189360e-02  -5.80634708e-02  -5.16080057e-02  -4.51525405e-02
  -3.86970754e-02  -3.22416102e-02  -2.57861451e-02  -1.93306799e-02
  -1.28752148e-02  -6.41974963e-03   3.57155150e-05]
[2789 1526 1702 2210 2622 3053 3354 3995 4368 4756] [ -6.45189360e-02  -5.80634708e-02  -5.16080057e-02  -4.51525405e-02
  -3.86970754e-02  -3.22416102e-02  -2.57861451e-02  -1.93306799e-02
  -1.28752148e-02  -6.41974963e-03   3.57155150e-05]
-1.39671
1.86374
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148751 minutes
Weight histogram
[   36   324   649  1156  1318  1382  9553 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  283   333   456   652  1017  1512  1032  1959  6785 18371] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.113467 minutes
Weight histogram
[6309 5304 4023 3908 2900 2195 2130 1779 3097  755] [ -6.39048368e-02  -5.75107815e-02  -5.11167263e-02  -4.47226711e-02
  -3.83286159e-02  -3.19345606e-02  -2.55405054e-02  -1.91464502e-02
  -1.27523949e-02  -6.35833971e-03   3.57155150e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75107815e-02  -5.11167263e-02  -4.47226711e-02
  -3.83286159e-02  -3.19345606e-02  -2.55405054e-02  -1.91464502e-02
  -1.27523949e-02  -6.35833971e-03   3.57155150e-05]
-1.3976
1.51906
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.59995
Epoch 1, cost is  5.36873
Epoch 2, cost is  5.24662
Epoch 3, cost is  4.99453
Epoch 4, cost is  4.59449
Epoch 5, cost is  4.25023
Epoch 6, cost is  3.99023
Epoch 7, cost is  3.77532
Epoch 8, cost is  3.58336
Epoch 9, cost is  3.4155
Training took 0.296373 minutes
Weight histogram
[2128 1879 1687 4861  901  375  162   84   45   28] [-0.01807489 -0.016283   -0.01449112 -0.01269923 -0.01090734 -0.00911546
 -0.00732357 -0.00553169 -0.0037398  -0.00194791 -0.00015603]
[3752  989  706  711  781  888  989 1053 1129 1152] [-0.01807489 -0.016283   -0.01449112 -0.01269923 -0.01090734 -0.00911546
 -0.00732357 -0.00553169 -0.0037398  -0.00194791 -0.00015603]
-0.293211
0.319262
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.059971 minutes
Epoch 0
Fine tuning took 0.060044 minutes
Epoch 0
Fine tuning took 0.060420 minutes
Epoch 0
Fine tuning took 0.058928 minutes
Epoch 0
Fine tuning took 0.058985 minutes
Epoch 0
Fine tuning took 0.059634 minutes
Epoch 0
Fine tuning took 0.057986 minutes
Epoch 0
Fine tuning took 0.058705 minutes
Epoch 0
Fine tuning took 0.058235 minutes
Epoch 0
Fine tuning took 0.057605 minutes
{'zero': {0: [0.37807881773399016, 0.27586206896551724, 0.29433497536945813, 0.31896551724137934, 0.22783251231527094, 0.23522167487684728, 0.26108374384236455, 0.20443349753694581, 0.23152709359605911, 0.20812807881773399, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.58004926108374388, 0.53448275862068961, 0.52586206896551724, 0.59852216748768472, 0.58990147783251234, 0.61083743842364535, 0.5431034482758621, 0.50246305418719217, 0.62684729064039413, 0.47906403940886699], 5: [0.20320197044334976, 0.14408866995073891, 0.17118226600985223, 0.15517241379310345, 0.17364532019704434, 0.1748768472906404, 0.12807881773399016, 0.25246305418719212, 0.26600985221674878, 0.16502463054187191, 0.32142857142857145], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37807881773399016, 0.44088669950738918, 0.45566502463054187, 0.43103448275862066, 0.37931034482758619, 0.39162561576354682, 0.43596059113300495, 0.31280788177339902, 0.33743842364532017, 0.30295566502463056, 0.31896551724137934], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.39655172413793105, 0.33497536945812806, 0.39162561576354682, 0.41009852216748771, 0.3608374384236453, 0.44827586206896552, 0.3854679802955665, 0.44827586206896552, 0.50123152709359609, 0.41256157635467983], 5: [0.20320197044334976, 0.1625615763546798, 0.20935960591133004, 0.17733990147783252, 0.2105911330049261, 0.24753694581280788, 0.11576354679802955, 0.30172413793103448, 0.21428571428571427, 0.19581280788177341, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37807881773399016, 0.43472906403940886, 0.41133004926108374, 0.47044334975369456, 0.39778325123152708, 0.38300492610837439, 0.4039408866995074, 0.34729064039408869, 0.36576354679802958, 0.30295566502463056, 0.35714285714285715], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.41133004926108374, 0.3854679802955665, 0.34359605911330049, 0.41748768472906406, 0.40024630541871919, 0.47044334975369456, 0.38669950738916259, 0.41871921182266009, 0.49876847290640391, 0.39285714285714285], 5: [0.20320197044334976, 0.1539408866995074, 0.20320197044334976, 0.18596059113300492, 0.18472906403940886, 0.21674876847290642, 0.12561576354679804, 0.26600985221674878, 0.21551724137931033, 0.19827586206896552, 0.25], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37807881773399016, 0.43596059113300495, 0.43472906403940886, 0.44950738916256155, 0.40147783251231528, 0.40270935960591131, 0.40517241379310343, 0.34975369458128081, 0.3608374384236453, 0.31527093596059114, 0.31034482758620691], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.37561576354679804, 0.35714285714285715, 0.37315270935960593, 0.39532019704433496, 0.39408866995073893, 0.47783251231527096, 0.40270935960591131, 0.44704433497536944, 0.49753694581280788, 0.41133004926108374], 5: [0.20320197044334976, 0.18842364532019704, 0.20812807881773399, 0.17733990147783252, 0.20320197044334976, 0.20320197044334976, 0.11699507389162561, 0.24753694581280788, 0.19211822660098521, 0.18719211822660098, 0.27832512315270935], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147537 minutes
Weight histogram
[ 185  432  798 1935 3917 5704 9709 5760 1797  138] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
[  139   158   232   347   512   883  1136  2176  8349 16443] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
-1.32597
1.19368
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.33502
Epoch 1, cost is  2.30004
Epoch 2, cost is  2.27645
Epoch 3, cost is  2.25315
Epoch 4, cost is  2.23607
Training took 0.113370 minutes
Weight histogram
[5927 4835 4269 3947 3190 2244 2068 1945 1558  392] [ -6.45189360e-02  -5.80634708e-02  -5.16080057e-02  -4.51525405e-02
  -3.86970754e-02  -3.22416102e-02  -2.57861451e-02  -1.93306799e-02
  -1.28752148e-02  -6.41974963e-03   3.57155150e-05]
[2789 1526 1702 2210 2622 3053 3354 3995 4368 4756] [ -6.45189360e-02  -5.80634708e-02  -5.16080057e-02  -4.51525405e-02
  -3.86970754e-02  -3.22416102e-02  -2.57861451e-02  -1.93306799e-02
  -1.28752148e-02  -6.41974963e-03   3.57155150e-05]
-1.39671
1.86374
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147283 minutes
Weight histogram
[   36   324   649  1156  1318  1382  9553 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  283   333   456   652  1017  1512  1032  1959  6785 18371] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.116988 minutes
Weight histogram
[6309 5304 4023 3908 2900 2195 2130 1779 3097  755] [ -6.39048368e-02  -5.75107815e-02  -5.11167263e-02  -4.47226711e-02
  -3.83286159e-02  -3.19345606e-02  -2.55405054e-02  -1.91464502e-02
  -1.27523949e-02  -6.35833971e-03   3.57155150e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75107815e-02  -5.11167263e-02  -4.47226711e-02
  -3.83286159e-02  -3.19345606e-02  -2.55405054e-02  -1.91464502e-02
  -1.27523949e-02  -6.35833971e-03   3.57155150e-05]
-1.3976
1.51906
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.24093
Epoch 1, cost is  5.09297
Epoch 2, cost is  4.93255
Epoch 3, cost is  4.53525
Epoch 4, cost is  4.12752
Epoch 5, cost is  3.82132
Epoch 6, cost is  3.56773
Epoch 7, cost is  3.33857
Epoch 8, cost is  3.14213
Epoch 9, cost is  2.96831
Training took 0.410752 minutes
Weight histogram
[2883 2746 5236  650  305  147   89   48   27   19] [-0.01216332 -0.01096072 -0.00975813 -0.00855554 -0.00735294 -0.00615035
 -0.00494775 -0.00374516 -0.00254257 -0.00133997 -0.00013738]
[3508  799  701  788  866  978 1037 1099 1179 1195] [-0.01216332 -0.01096072 -0.00975813 -0.00855554 -0.00735294 -0.00615035
 -0.00494775 -0.00374516 -0.00254257 -0.00133997 -0.00013738]
-0.22167
0.25687
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.065352 minutes
Epoch 0
Fine tuning took 0.063084 minutes
Epoch 0
Fine tuning took 0.063425 minutes
Epoch 0
Fine tuning took 0.064759 minutes
Epoch 0
Fine tuning took 0.063378 minutes
Epoch 0
Fine tuning took 0.064416 minutes
Epoch 0
Fine tuning took 0.063830 minutes
Epoch 0
Fine tuning took 0.063494 minutes
Epoch 0
Fine tuning took 0.063749 minutes
Epoch 0
Fine tuning took 0.063080 minutes
{'zero': {0: [0.30665024630541871, 0.2894088669950739, 0.31034482758620691, 0.29679802955665024, 0.26970443349753692, 0.26970443349753692, 0.35714285714285715, 0.22167487684729065, 0.26231527093596058, 0.25246305418719212, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.4963054187192118, 0.51108374384236455, 0.53078817733990147, 0.4963054187192118, 0.56896551724137934, 0.43596059113300495, 0.52832512315270941, 0.47906403940886699, 0.47290640394088668, 0.46182266009852219], 5: [0.17980295566502463, 0.21428571428571427, 0.17857142857142858, 0.17241379310344829, 0.23399014778325122, 0.16133004926108374, 0.20689655172413793, 0.25, 0.25862068965517243, 0.27463054187192121, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30665024630541871, 0.34975369458128081, 0.33743842364532017, 0.34236453201970446, 0.34359605911330049, 0.29926108374384236, 0.36945812807881773, 0.28078817733990147, 0.31527093596059114, 0.3460591133004926, 0.32266009852216748], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.44088669950738918, 0.47783251231527096, 0.52339901477832518, 0.45073891625615764, 0.54187192118226601, 0.40147783251231528, 0.49014778325123154, 0.44581280788177341, 0.41625615763546797, 0.42857142857142855], 5: [0.17980295566502463, 0.20935960591133004, 0.18472906403940886, 0.13423645320197045, 0.20566502463054187, 0.15886699507389163, 0.22906403940886699, 0.22906403940886699, 0.23891625615763548, 0.2376847290640394, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30665024630541871, 0.3460591133004926, 0.35837438423645318, 0.32142857142857145, 0.3251231527093596, 0.30172413793103448, 0.37438423645320196, 0.30295566502463056, 0.32019704433497537, 0.3251231527093596, 0.32389162561576357], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.45320197044334976, 0.46674876847290642, 0.52586206896551724, 0.45689655172413796, 0.51724137931034486, 0.42364532019704432, 0.45689655172413796, 0.46182266009852219, 0.47167487684729065, 0.39532019704433496], 5: [0.17980295566502463, 0.20073891625615764, 0.1748768472906404, 0.15270935960591134, 0.21798029556650247, 0.18103448275862069, 0.2019704433497537, 0.24014778325123154, 0.21798029556650247, 0.20320197044334976, 0.28078817733990147], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30665024630541871, 0.32758620689655171, 0.33251231527093594, 0.34359605911330049, 0.33620689655172414, 0.30541871921182268, 0.38669950738916259, 0.26231527093596058, 0.32389162561576357, 0.35221674876847292, 0.32758620689655171], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.45566502463054187, 0.48522167487684731, 0.50369458128078815, 0.44827586206896552, 0.5431034482758621, 0.40147783251231528, 0.50615763546798032, 0.41995073891625617, 0.40763546798029554, 0.40886699507389163], 5: [0.17980295566502463, 0.21674876847290642, 0.18226600985221675, 0.15270935960591134, 0.21551724137931033, 0.15147783251231528, 0.21182266009852216, 0.23152709359605911, 0.25615763546798032, 0.24014778325123154, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148387 minutes
Weight histogram
[ 185  432  798 1935 3917 5704 9709 5760 1797  138] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
[  139   158   232   347   512   883  1136  2176  8349 16443] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
-1.32597
1.19368
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.33502
Epoch 1, cost is  2.30004
Epoch 2, cost is  2.27645
Epoch 3, cost is  2.25315
Epoch 4, cost is  2.23607
Training took 0.113126 minutes
Weight histogram
[5927 4835 4269 3947 3190 2244 2068 1945 1558  392] [ -6.45189360e-02  -5.80634708e-02  -5.16080057e-02  -4.51525405e-02
  -3.86970754e-02  -3.22416102e-02  -2.57861451e-02  -1.93306799e-02
  -1.28752148e-02  -6.41974963e-03   3.57155150e-05]
[2789 1526 1702 2210 2622 3053 3354 3995 4368 4756] [ -6.45189360e-02  -5.80634708e-02  -5.16080057e-02  -4.51525405e-02
  -3.86970754e-02  -3.22416102e-02  -2.57861451e-02  -1.93306799e-02
  -1.28752148e-02  -6.41974963e-03   3.57155150e-05]
-1.39671
1.86374
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147909 minutes
Weight histogram
[   36   324   649  1156  1318  1382  9553 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  283   333   456   652  1017  1512  1032  1959  6785 18371] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.115489 minutes
Weight histogram
[6309 5304 4023 3908 2900 2195 2130 1779 3097  755] [ -6.39048368e-02  -5.75107815e-02  -5.11167263e-02  -4.47226711e-02
  -3.83286159e-02  -3.19345606e-02  -2.55405054e-02  -1.91464502e-02
  -1.27523949e-02  -6.35833971e-03   3.57155150e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75107815e-02  -5.11167263e-02  -4.47226711e-02
  -3.83286159e-02  -3.19345606e-02  -2.55405054e-02  -1.91464502e-02
  -1.27523949e-02  -6.35833971e-03   3.57155150e-05]
-1.3976
1.51906
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.20545
Epoch 1, cost is  5.02161
Epoch 2, cost is  4.53156
Epoch 3, cost is  3.97052
Epoch 4, cost is  3.55068
Epoch 5, cost is  3.21999
Epoch 6, cost is  2.95725
Epoch 7, cost is  2.74337
Epoch 8, cost is  2.57284
Epoch 9, cost is  2.43886
Training took 0.680772 minutes
Weight histogram
[2157 2289 1891 1424 1429 2842   69   24   13   12] [-0.00679093 -0.0061247  -0.00545847 -0.00479224 -0.00412601 -0.00345978
 -0.00279356 -0.00212733 -0.0014611  -0.00079487 -0.00012864]
[2845  759  747  824  908  988 1082 1207 1359 1431] [-0.00679093 -0.0061247  -0.00545847 -0.00479224 -0.00412601 -0.00345978
 -0.00279356 -0.00212733 -0.0014611  -0.00079487 -0.00012864]
-0.189124
0.20825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.078841 minutes
Epoch 0
Fine tuning took 0.076479 minutes
Epoch 0
Fine tuning took 0.078285 minutes
Epoch 0
Fine tuning took 0.077669 minutes
Epoch 0
Fine tuning took 0.077644 minutes
Epoch 0
Fine tuning took 0.077436 minutes
Epoch 0
Fine tuning took 0.076709 minutes
Epoch 0
Fine tuning took 0.076973 minutes
Epoch 0
Fine tuning took 0.076520 minutes
Epoch 0
Fine tuning took 0.077956 minutes
{'zero': {0: [0.27832512315270935, 0.26600985221674878, 0.31034482758620691, 0.31896551724137934, 0.32635467980295568, 0.35344827586206895, 0.39285714285714285, 0.37315270935960593, 0.45197044334975367, 0.3891625615763547, 0.45073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.47906403940886699, 0.51477832512315269, 0.47044334975369456, 0.4211822660098522, 0.45566502463054187, 0.37807881773399016, 0.41256157635467983, 0.30788177339901479, 0.41379310344827586, 0.33497536945812806], 5: [0.21798029556650247, 0.25492610837438423, 0.1748768472906404, 0.2105911330049261, 0.25246305418719212, 0.19088669950738915, 0.22906403940886699, 0.21428571428571427, 0.24014778325123154, 0.19704433497536947, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27832512315270935, 0.31403940886699505, 0.34482758620689657, 0.33743842364532017, 0.37315270935960593, 0.34359605911330049, 0.35221674876847292, 0.36945812807881773, 0.43719211822660098, 0.37068965517241381, 0.44704433497536944], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.45566502463054187, 0.47167487684729065, 0.45566502463054187, 0.39532019704433496, 0.45812807881773399, 0.39285714285714285, 0.42364532019704432, 0.34113300492610837, 0.40147783251231528, 0.32635467980295568], 5: [0.21798029556650247, 0.23029556650246305, 0.18349753694581281, 0.20689655172413793, 0.23152709359605911, 0.19827586206896552, 0.25492610837438423, 0.20689655172413793, 0.22167487684729065, 0.22783251231527094, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27832512315270935, 0.2894088669950739, 0.37315270935960593, 0.35221674876847292, 0.35344827586206895, 0.33620689655172414, 0.40270935960591131, 0.38300492610837439, 0.43965517241379309, 0.39901477832512317, 0.41379310344827586], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.46798029556650245, 0.44334975369458129, 0.4605911330049261, 0.39039408866995073, 0.47536945812807879, 0.35344827586206895, 0.39778325123152708, 0.34729064039408869, 0.40640394088669951, 0.33866995073891626], 5: [0.21798029556650247, 0.24261083743842365, 0.18349753694581281, 0.18719211822660098, 0.25615763546798032, 0.18842364532019704, 0.24384236453201971, 0.21921182266009853, 0.21305418719211822, 0.19458128078817735, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27832512315270935, 0.30665024630541871, 0.3682266009852217, 0.34236453201970446, 0.34729064039408869, 0.41379310344827586, 0.3891625615763547, 0.37684729064039407, 0.44211822660098521, 0.41379310344827586, 0.42610837438423643], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.46674876847290642, 0.43842364532019706, 0.44704433497536944, 0.39408866995073893, 0.40024630541871919, 0.37561576354679804, 0.40640394088669951, 0.31403940886699505, 0.38054187192118227, 0.35098522167487683], 5: [0.21798029556650247, 0.22660098522167488, 0.19334975369458129, 0.2105911330049261, 0.25862068965517243, 0.18596059113300492, 0.23522167487684728, 0.21674876847290642, 0.24384236453201971, 0.20566502463054187, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236129 minutes
Weight histogram
[  159   363   627   699  1253  3383  8118 11210  4256   307] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[  165   278   431   711   938  1283  3236  4993  8283 10057] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.3214
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.40587
Epoch 1, cost is  1.38454
Epoch 2, cost is  1.36679
Epoch 3, cost is  1.35404
Epoch 4, cost is  1.33901
Training took 0.221698 minutes
Weight histogram
[7253 4880 4382 3256 2526 2222 1768 1539 1438 1111] [ -6.30783513e-02  -5.67632338e-02  -5.04481162e-02  -4.41329986e-02
  -3.78178811e-02  -3.15027635e-02  -2.51876460e-02  -1.88725284e-02
  -1.25574108e-02  -6.24229327e-03   7.28242885e-05]
[2367 1456 1828 2110 2570 3064 3277 4059 4666 4978] [ -6.30783513e-02  -5.67632338e-02  -5.04481162e-02  -4.41329986e-02
  -3.78178811e-02  -3.15027635e-02  -2.51876460e-02  -1.88725284e-02
  -1.25574108e-02  -6.24229327e-03   7.28242885e-05]
-1.02704
1.72372
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.145875 minutes
Weight histogram
[   35   342  1082  1497   754  1160  9548 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  473   947  1150   308   502   873  1032  1959  6785 18371] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.116948 minutes
Weight histogram
[6310 5316 4028 3895 2898 2195 2129 1686 2440 1503] [ -6.39048368e-02  -5.75070707e-02  -5.11093046e-02  -4.47115385e-02
  -3.83137723e-02  -3.19160062e-02  -2.55182401e-02  -1.91204740e-02
  -1.27227079e-02  -6.32494182e-03   7.28242885e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75070707e-02  -5.11093046e-02  -4.47115385e-02
  -3.83137723e-02  -3.19160062e-02  -2.55182401e-02  -1.91204740e-02
  -1.27227079e-02  -6.32494182e-03   7.28242885e-05]
-1.3976
1.51906
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.31021
Epoch 1, cost is  6.02952
Epoch 2, cost is  5.77634
Epoch 3, cost is  5.41303
Epoch 4, cost is  5.05245
Epoch 5, cost is  4.76166
Epoch 6, cost is  4.52789
Epoch 7, cost is  4.33151
Epoch 8, cost is  4.15808
Epoch 9, cost is  4.00292
Training took 0.245450 minutes
Weight histogram
[1426 1496 1303 1251 1132 1341 2246 1407  498   50] [-0.02811378 -0.02532027 -0.02252675 -0.01973324 -0.01693973 -0.01414621
 -0.0113527  -0.00855918 -0.00576567 -0.00297216 -0.00017864]
[2438 1321  860  851  921 1010 1096 1179 1242 1232] [-0.02811378 -0.02532027 -0.02252675 -0.01973324 -0.01693973 -0.01414621
 -0.0113527  -0.00855918 -0.00576567 -0.00297216 -0.00017864]
-0.370984
0.520956
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077461 minutes
Epoch 0
Fine tuning took 0.076634 minutes
Epoch 0
Fine tuning took 0.076500 minutes
Epoch 0
Fine tuning took 0.078160 minutes
Epoch 0
Fine tuning took 0.076380 minutes
Epoch 0
Fine tuning took 0.076570 minutes
Epoch 0
Fine tuning took 0.077914 minutes
Epoch 0
Fine tuning took 0.076532 minutes
Epoch 0
Fine tuning took 0.076226 minutes
Epoch 0
Fine tuning took 0.076194 minutes
{'zero': {0: [0.44581280788177341, 0.59729064039408863, 0.57266009852216748, 0.52339901477832518, 0.39532019704433496, 0.30665024630541871, 0.56773399014778325, 0.48522167487684731, 0.41379310344827586, 0.39039408866995073, 0.41009852216748771], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.32758620689655171, 0.20443349753694581, 0.17241379310344829, 0.3460591133004926, 0.35960591133004927, 0.3817733990147783, 0.29926108374384236, 0.26477832512315269, 0.23522167487684728, 0.41133004926108374, 0.24876847290640394], 5: [0.22660098522167488, 0.19827586206896552, 0.25492610837438423, 0.13054187192118227, 0.24507389162561577, 0.31157635467980294, 0.13300492610837439, 0.25, 0.35098522167487683, 0.19827586206896552, 0.34113300492610837], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.44581280788177341, 0.54802955665024633, 0.47413793103448276, 0.50246305418719217, 0.38423645320197042, 0.28078817733990147, 0.59113300492610843, 0.51477832512315269, 0.43842364532019706, 0.3817733990147783, 0.48152709359605911], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.32758620689655171, 0.20443349753694581, 0.17118226600985223, 0.32635467980295568, 0.26847290640394089, 0.3608374384236453, 0.24261083743842365, 0.21428571428571427, 0.17118226600985223, 0.34852216748768472, 0.18719211822660098], 5: [0.22660098522167488, 0.24753694581280788, 0.35467980295566504, 0.17118226600985223, 0.34729064039408869, 0.35837438423645318, 0.16625615763546797, 0.27093596059113301, 0.39039408866995073, 0.26970443349753692, 0.33128078817733991], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.44581280788177341, 0.51108374384236455, 0.46305418719211822, 0.51600985221674878, 0.44211822660098521, 0.27339901477832512, 0.55665024630541871, 0.53448275862068961, 0.43719211822660098, 0.41009852216748771, 0.45443349753694579], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.32758620689655171, 0.24384236453201971, 0.19950738916256158, 0.35714285714285715, 0.24630541871921183, 0.34975369458128081, 0.27586206896551724, 0.17118226600985223, 0.18842364532019704, 0.35714285714285715, 0.21305418719211822], 5: [0.22660098522167488, 0.24507389162561577, 0.33743842364532017, 0.1268472906403941, 0.31157635467980294, 0.37684729064039407, 0.16748768472906403, 0.29433497536945813, 0.37438423645320196, 0.23275862068965517, 0.33251231527093594], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.44581280788177341, 0.50862068965517238, 0.45443349753694579, 0.48768472906403942, 0.43719211822660098, 0.2105911330049261, 0.5357142857142857, 0.53940886699507384, 0.43965517241379309, 0.38669950738916259, 0.4642857142857143], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.32758620689655171, 0.21551724137931033, 0.16871921182266009, 0.34359605911330049, 0.24014778325123154, 0.39532019704433496, 0.30541871921182268, 0.20566502463054187, 0.16625615763546797, 0.34113300492610837, 0.2105911330049261], 5: [0.22660098522167488, 0.27586206896551724, 0.37684729064039407, 0.16871921182266009, 0.32266009852216748, 0.39408866995073893, 0.15886699507389163, 0.25492610837438423, 0.39408866995073893, 0.27216748768472904, 0.3251231527093596], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235147 minutes
Weight histogram
[  159   363   627   699  1253  3383  8118 11210  4256   307] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[  165   278   431   711   938  1283  3236  4993  8283 10057] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.3214
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.40587
Epoch 1, cost is  1.38454
Epoch 2, cost is  1.36679
Epoch 3, cost is  1.35404
Epoch 4, cost is  1.33901
Training took 0.223482 minutes
Weight histogram
[7253 4880 4382 3256 2526 2222 1768 1539 1438 1111] [ -6.30783513e-02  -5.67632338e-02  -5.04481162e-02  -4.41329986e-02
  -3.78178811e-02  -3.15027635e-02  -2.51876460e-02  -1.88725284e-02
  -1.25574108e-02  -6.24229327e-03   7.28242885e-05]
[2367 1456 1828 2110 2570 3064 3277 4059 4666 4978] [ -6.30783513e-02  -5.67632338e-02  -5.04481162e-02  -4.41329986e-02
  -3.78178811e-02  -3.15027635e-02  -2.51876460e-02  -1.88725284e-02
  -1.25574108e-02  -6.24229327e-03   7.28242885e-05]
-1.02704
1.72372
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146885 minutes
Weight histogram
[   35   342  1082  1497   754  1160  9548 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  473   947  1150   308   502   873  1032  1959  6785 18371] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.113074 minutes
Weight histogram
[6310 5316 4028 3895 2898 2195 2129 1686 2440 1503] [ -6.39048368e-02  -5.75070707e-02  -5.11093046e-02  -4.47115385e-02
  -3.83137723e-02  -3.19160062e-02  -2.55182401e-02  -1.91204740e-02
  -1.27227079e-02  -6.32494182e-03   7.28242885e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75070707e-02  -5.11093046e-02  -4.47115385e-02
  -3.83137723e-02  -3.19160062e-02  -2.55182401e-02  -1.91204740e-02
  -1.27227079e-02  -6.32494182e-03   7.28242885e-05]
-1.3976
1.51906
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.70496
Epoch 1, cost is  5.43104
Epoch 2, cost is  5.11845
Epoch 3, cost is  4.64868
Epoch 4, cost is  4.24145
Epoch 5, cost is  3.9395
Epoch 6, cost is  3.69084
Epoch 7, cost is  3.48746
Epoch 8, cost is  3.31293
Epoch 9, cost is  3.16355
Training took 0.350853 minutes
Weight histogram
[1945 1765 1496 1274 1338 3091  954  198   61   28] [-0.01845267 -0.01662195 -0.01479123 -0.0129605  -0.01112978 -0.00929905
 -0.00746833 -0.00563761 -0.00380688 -0.00197616 -0.00014544]
[3033  842  768  816  927 1034 1098 1173 1242 1217] [-0.01845267 -0.01662195 -0.01479123 -0.0129605  -0.01112978 -0.00929905
 -0.00746833 -0.00563761 -0.00380688 -0.00197616 -0.00014544]
-0.257221
0.364602
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081787 minutes
Epoch 0
Fine tuning took 0.081215 minutes
Epoch 0
Fine tuning took 0.082362 minutes
Epoch 0
Fine tuning took 0.081287 minutes
Epoch 0
Fine tuning took 0.081425 minutes
Epoch 0
Fine tuning took 0.082196 minutes
Epoch 0
Fine tuning took 0.081996 minutes
Epoch 0
Fine tuning took 0.081456 minutes
Epoch 0
Fine tuning took 0.082060 minutes
Epoch 0
Fine tuning took 0.081578 minutes
{'zero': {0: [0.42364532019704432, 0.35344827586206895, 0.35467980295566504, 0.3288177339901478, 0.35591133004926107, 0.23645320197044334, 0.27832512315270935, 0.19827586206896552, 0.26108374384236455, 0.18965517241379309, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36206896551724138, 0.46182266009852219, 0.52216748768472909, 0.52832512315270941, 0.41995073891625617, 0.58374384236453203, 0.53325123152709364, 0.61330049261083741, 0.62315270935960587, 0.58743842364532017, 0.48768472906403942], 5: [0.21428571428571427, 0.18472906403940886, 0.12315270935960591, 0.14285714285714285, 0.22413793103448276, 0.17980295566502463, 0.18842364532019704, 0.18842364532019704, 0.11576354679802955, 0.2229064039408867, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.42364532019704432, 0.53078817733990147, 0.52339901477832518, 0.59975369458128081, 0.47044334975369456, 0.3854679802955665, 0.41256157635467983, 0.39162561576354682, 0.46921182266009853, 0.4039408866995074, 0.45935960591133007], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36206896551724138, 0.23275862068965517, 0.26477832512315269, 0.23275862068965517, 0.21798029556650247, 0.29433497536945813, 0.39901477832512317, 0.28325123152709358, 0.37931034482758619, 0.38423645320197042, 0.30788177339901479], 5: [0.21428571428571427, 0.23645320197044334, 0.21182266009852216, 0.16748768472906403, 0.31157635467980294, 0.32019704433497537, 0.18842364532019704, 0.3251231527093596, 0.15147783251231528, 0.21182266009852216, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.42364532019704432, 0.48645320197044334, 0.51724137931034486, 0.57635467980295563, 0.45443349753694579, 0.37807881773399016, 0.39778325123152708, 0.40024630541871919, 0.46551724137931033, 0.43596059113300495, 0.42857142857142855], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36206896551724138, 0.26354679802955666, 0.24261083743842365, 0.22660098522167488, 0.23275862068965517, 0.31034482758620691, 0.37315270935960593, 0.31773399014778325, 0.3891625615763547, 0.3645320197044335, 0.32019704433497537], 5: [0.21428571428571427, 0.25, 0.24014778325123154, 0.19704433497536947, 0.31280788177339902, 0.31157635467980294, 0.22906403940886699, 0.28201970443349755, 0.14532019704433496, 0.19950738916256158, 0.25123152709359609], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.42364532019704432, 0.54679802955665024, 0.58620689655172409, 0.60467980295566504, 0.51108374384236455, 0.40763546798029554, 0.45197044334975367, 0.43596059113300495, 0.51724137931034486, 0.45566502463054187, 0.49507389162561577], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36206896551724138, 0.22536945812807882, 0.20320197044334976, 0.23029556650246305, 0.1748768472906404, 0.27955665024630544, 0.34113300492610837, 0.25738916256157635, 0.33743842364532017, 0.35221674876847292, 0.26108374384236455], 5: [0.21428571428571427, 0.22783251231527094, 0.2105911330049261, 0.16502463054187191, 0.31403940886699505, 0.31280788177339902, 0.20689655172413793, 0.30665024630541871, 0.14532019704433496, 0.19211822660098521, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239764 minutes
Weight histogram
[  159   363   627   699  1253  3383  8118 11210  4256   307] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[  165   278   431   711   938  1283  3236  4993  8283 10057] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.3214
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.40587
Epoch 1, cost is  1.38454
Epoch 2, cost is  1.36679
Epoch 3, cost is  1.35404
Epoch 4, cost is  1.33901
Training took 0.223311 minutes
Weight histogram
[7253 4880 4382 3256 2526 2222 1768 1539 1438 1111] [ -6.30783513e-02  -5.67632338e-02  -5.04481162e-02  -4.41329986e-02
  -3.78178811e-02  -3.15027635e-02  -2.51876460e-02  -1.88725284e-02
  -1.25574108e-02  -6.24229327e-03   7.28242885e-05]
[2367 1456 1828 2110 2570 3064 3277 4059 4666 4978] [ -6.30783513e-02  -5.67632338e-02  -5.04481162e-02  -4.41329986e-02
  -3.78178811e-02  -3.15027635e-02  -2.51876460e-02  -1.88725284e-02
  -1.25574108e-02  -6.24229327e-03   7.28242885e-05]
-1.02704
1.72372
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147130 minutes
Weight histogram
[   35   342  1082  1497   754  1160  9548 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  473   947  1150   308   502   873  1032  1959  6785 18371] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.115316 minutes
Weight histogram
[6310 5316 4028 3895 2898 2195 2129 1686 2440 1503] [ -6.39048368e-02  -5.75070707e-02  -5.11093046e-02  -4.47115385e-02
  -3.83137723e-02  -3.19160062e-02  -2.55182401e-02  -1.91204740e-02
  -1.27227079e-02  -6.32494182e-03   7.28242885e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75070707e-02  -5.11093046e-02  -4.47115385e-02
  -3.83137723e-02  -3.19160062e-02  -2.55182401e-02  -1.91204740e-02
  -1.27227079e-02  -6.32494182e-03   7.28242885e-05]
-1.3976
1.51906
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.12729
Epoch 1, cost is  4.90193
Epoch 2, cost is  4.59086
Epoch 3, cost is  4.13434
Epoch 4, cost is  3.74998
Epoch 5, cost is  3.45016
Epoch 6, cost is  3.20815
Epoch 7, cost is  3.00653
Epoch 8, cost is  2.83747
Epoch 9, cost is  2.68865
Training took 0.541749 minutes
Weight histogram
[2434 2306 1924 4167  788  281  130   68   31   21] [-0.01305922 -0.01176748 -0.01047574 -0.00918401 -0.00789227 -0.00660054
 -0.0053088  -0.00401706 -0.00272533 -0.00143359 -0.00014185]
[3060  822  785  838  925 1003 1078 1158 1255 1226] [-0.01305922 -0.01176748 -0.01047574 -0.00918401 -0.00789227 -0.00660054
 -0.0053088  -0.00401706 -0.00272533 -0.00143359 -0.00014185]
-0.209465
0.246313
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090745 minutes
Epoch 0
Fine tuning took 0.090665 minutes
Epoch 0
Fine tuning took 0.089823 minutes
Epoch 0
Fine tuning took 0.091802 minutes
Epoch 0
Fine tuning took 0.092115 minutes
Epoch 0
Fine tuning took 0.090766 minutes
Epoch 0
Fine tuning took 0.090309 minutes
Epoch 0
Fine tuning took 0.090860 minutes
Epoch 0
Fine tuning took 0.090772 minutes
Epoch 0
Fine tuning took 0.092076 minutes
{'zero': {0: [0.37684729064039407, 0.30172413793103448, 0.32019704433497537, 0.2857142857142857, 0.26231527093596058, 0.26600985221674878, 0.27832512315270935, 0.25123152709359609, 0.28817733990147781, 0.25738916256157635, 0.30172413793103448], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45443349753694579, 0.47660098522167488, 0.47536945812807879, 0.53201970443349755, 0.5431034482758621, 0.58128078817733986, 0.51847290640394084, 0.48029556650246308, 0.45443349753694579, 0.51724137931034486, 0.4211822660098522], 5: [0.16871921182266009, 0.22167487684729065, 0.20443349753694581, 0.18226600985221675, 0.19458128078817735, 0.15270935960591134, 0.20320197044334976, 0.26847290640394089, 0.25738916256157635, 0.22536945812807882, 0.27709359605911332], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37684729064039407, 0.39532019704433496, 0.38669950738916259, 0.33374384236453203, 0.34236453201970446, 0.33990147783251229, 0.36330049261083741, 0.28078817733990147, 0.34729064039408869, 0.33866995073891626, 0.37068965517241381], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45443349753694579, 0.43349753694581283, 0.39655172413793105, 0.50615763546798032, 0.46551724137931033, 0.47536945812807879, 0.43472906403940886, 0.42857142857142855, 0.40270935960591131, 0.44704433497536944, 0.36576354679802958], 5: [0.16871921182266009, 0.17118226600985223, 0.21674876847290642, 0.16009852216748768, 0.19211822660098521, 0.18472906403940886, 0.2019704433497537, 0.29064039408866993, 0.25, 0.21428571428571427, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37684729064039407, 0.40024630541871919, 0.3682266009852217, 0.36206896551724138, 0.34729064039408869, 0.36206896551724138, 0.36945812807881773, 0.32758620689655171, 0.38669950738916259, 0.35837438423645318, 0.39162561576354682], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45443349753694579, 0.40886699507389163, 0.43965517241379309, 0.45812807881773399, 0.45443349753694579, 0.47044334975369456, 0.44827586206896552, 0.42610837438423643, 0.40886699507389163, 0.45935960591133007, 0.3854679802955665], 5: [0.16871921182266009, 0.19088669950738915, 0.19211822660098521, 0.17980295566502463, 0.19827586206896552, 0.16748768472906403, 0.18226600985221675, 0.24630541871921183, 0.20443349753694581, 0.18226600985221675, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37684729064039407, 0.4248768472906404, 0.3608374384236453, 0.34975369458128081, 0.33866995073891626, 0.32142857142857145, 0.37438423645320196, 0.32266009852216748, 0.3645320197044335, 0.33374384236453203, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45443349753694579, 0.41748768472906406, 0.3891625615763547, 0.47044334975369456, 0.48645320197044334, 0.51724137931034486, 0.44581280788177341, 0.44088669950738918, 0.40147783251231528, 0.44334975369458129, 0.38793103448275862], 5: [0.16871921182266009, 0.15763546798029557, 0.25, 0.17980295566502463, 0.1748768472906404, 0.16133004926108374, 0.17980295566502463, 0.23645320197044334, 0.23399014778325122, 0.2229064039408867, 0.26970443349753692], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234113 minutes
Weight histogram
[  159   363   627   699  1253  3383  8118 11210  4256   307] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[  165   278   431   711   938  1283  3236  4993  8283 10057] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.3214
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.40587
Epoch 1, cost is  1.38454
Epoch 2, cost is  1.36679
Epoch 3, cost is  1.35404
Epoch 4, cost is  1.33901
Training took 0.222374 minutes
Weight histogram
[7253 4880 4382 3256 2526 2222 1768 1539 1438 1111] [ -6.30783513e-02  -5.67632338e-02  -5.04481162e-02  -4.41329986e-02
  -3.78178811e-02  -3.15027635e-02  -2.51876460e-02  -1.88725284e-02
  -1.25574108e-02  -6.24229327e-03   7.28242885e-05]
[2367 1456 1828 2110 2570 3064 3277 4059 4666 4978] [ -6.30783513e-02  -5.67632338e-02  -5.04481162e-02  -4.41329986e-02
  -3.78178811e-02  -3.15027635e-02  -2.51876460e-02  -1.88725284e-02
  -1.25574108e-02  -6.24229327e-03   7.28242885e-05]
-1.02704
1.72372
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148069 minutes
Weight histogram
[   35   342  1082  1497   754  1160  9548 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  473   947  1150   308   502   873  1032  1959  6785 18371] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.114384 minutes
Weight histogram
[6310 5316 4028 3895 2898 2195 2129 1686 2440 1503] [ -6.39048368e-02  -5.75070707e-02  -5.11093046e-02  -4.47115385e-02
  -3.83137723e-02  -3.19160062e-02  -2.55182401e-02  -1.91204740e-02
  -1.27227079e-02  -6.32494182e-03   7.28242885e-05]
[4754 1544 1724 2322 2799 3097 3313 3993 4389 4465] [ -6.39048368e-02  -5.75070707e-02  -5.11093046e-02  -4.47115385e-02
  -3.83137723e-02  -3.19160062e-02  -2.55182401e-02  -1.91204740e-02
  -1.27227079e-02  -6.32494182e-03   7.28242885e-05]
-1.3976
1.51906
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.91734
Epoch 1, cost is  4.63348
Epoch 2, cost is  4.03497
Epoch 3, cost is  3.53102
Epoch 4, cost is  3.17318
Epoch 5, cost is  2.89211
Epoch 6, cost is  2.66852
Epoch 7, cost is  2.48551
Epoch 8, cost is  2.33948
Epoch 9, cost is  2.21717
Training took 0.944166 minutes
Weight histogram
[2539 2361 2051 1488 2694  857   92   37   19   12] [-0.00805968 -0.00726726 -0.00647483 -0.00568241 -0.00488999 -0.00409757
 -0.00330515 -0.00251272 -0.0017203  -0.00092788 -0.00013546]
[2516  758  773  887  962 1045 1148 1276 1403 1382] [-0.00805968 -0.00726726 -0.00647483 -0.00568241 -0.00488999 -0.00409757
 -0.00330515 -0.00251272 -0.0017203  -0.00092788 -0.00013546]
-0.186806
0.19206
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.110800 minutes
Epoch 0
Fine tuning took 0.109171 minutes
Epoch 0
Fine tuning took 0.109682 minutes
Epoch 0
Fine tuning took 0.110717 minutes
Epoch 0
Fine tuning took 0.110729 minutes
Epoch 0
Fine tuning took 0.108983 minutes
Epoch 0
Fine tuning took 0.109941 minutes
Epoch 0
Fine tuning took 0.109989 minutes
Epoch 0
Fine tuning took 0.108853 minutes
Epoch 0
Fine tuning took 0.109143 minutes
{'zero': {0: [0.30418719211822659, 0.30418719211822659, 0.3288177339901478, 0.27216748768472904, 0.33620689655172414, 0.30172413793103448, 0.37931034482758619, 0.32019704433497537, 0.3817733990147783, 0.29187192118226601, 0.3817733990147783], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.45073891625615764, 0.51847290640394084, 0.53078817733990147, 0.49137931034482757, 0.5357142857142857, 0.39655172413793105, 0.44088669950738918, 0.37192118226600984, 0.43103448275862066, 0.3891625615763547], 5: [0.15763546798029557, 0.24507389162561577, 0.15270935960591134, 0.19704433497536947, 0.17241379310344829, 0.1625615763546798, 0.22413793103448276, 0.23891625615763548, 0.24630541871921183, 0.27709359605911332, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30418719211822659, 0.33251231527093594, 0.35098522167487683, 0.32142857142857145, 0.35344827586206895, 0.32635467980295568, 0.40763546798029554, 0.35714285714285715, 0.37315270935960593, 0.35098522167487683, 0.41133004926108374], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.43472906403940886, 0.47290640394088668, 0.48891625615763545, 0.45320197044334976, 0.48645320197044334, 0.36330049261083741, 0.40640394088669951, 0.37807881773399016, 0.37192118226600984, 0.37561576354679804], 5: [0.15763546798029557, 0.23275862068965517, 0.17610837438423646, 0.18965517241379309, 0.19334975369458129, 0.18719211822660098, 0.22906403940886699, 0.23645320197044334, 0.24876847290640394, 0.27709359605911332, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30418719211822659, 0.31280788177339902, 0.37192118226600984, 0.30418719211822659, 0.37068965517241381, 0.29187192118226601, 0.41379310344827586, 0.36206896551724138, 0.37931034482758619, 0.34236453201970446, 0.42364532019704432], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.44950738916256155, 0.46305418719211822, 0.50246305418719217, 0.44704433497536944, 0.53078817733990147, 0.34975369458128081, 0.40640394088669951, 0.36206896551724138, 0.38300492610837439, 0.34236453201970446], 5: [0.15763546798029557, 0.2376847290640394, 0.16502463054187191, 0.19334975369458129, 0.18226600985221675, 0.17733990147783252, 0.23645320197044334, 0.23152709359605911, 0.25862068965517243, 0.27463054187192121, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30418719211822659, 0.32635467980295568, 0.36699507389162561, 0.29926108374384236, 0.37438423645320196, 0.36576354679802958, 0.38423645320197042, 0.34729064039408869, 0.35837438423645318, 0.3608374384236453, 0.41748768472906406], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.43719211822660098, 0.47783251231527096, 0.51477832512315269, 0.43103448275862066, 0.47536945812807879, 0.38300492610837439, 0.40270935960591131, 0.38300492610837439, 0.38793103448275862, 0.35591133004926107], 5: [0.15763546798029557, 0.23645320197044334, 0.15517241379310345, 0.18596059113300492, 0.19458128078817735, 0.15886699507389163, 0.23275862068965517, 0.25, 0.25862068965517243, 0.25123152709359609, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420123 minutes
Weight histogram
[ 118  345  521  655  939 4421 9776 9199 3917  484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1270 1000  987 1403 2331 2115 4049 3653 8435 5132] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.28282
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.889355
Epoch 1, cost is  0.870816
Epoch 2, cost is  0.859792
Epoch 3, cost is  0.850568
Epoch 4, cost is  0.841564
Training took 0.646427 minutes
Weight histogram
[7690 5627 4633 3361 2509 1754 1467 1236 1005 1093] [ -4.54398394e-02  -4.09011025e-02  -3.63623656e-02  -3.18236288e-02
  -2.72848919e-02  -2.27461550e-02  -1.82074182e-02  -1.36686813e-02
  -9.12994444e-03  -4.59120758e-03  -5.24707102e-05]
[2074 1477 1635 2056 2370 3113 3429 3881 4986 5354] [ -4.54398394e-02  -4.09011025e-02  -3.63623656e-02  -3.18236288e-02
  -2.72848919e-02  -2.27461550e-02  -1.82074182e-02  -1.36686813e-02
  -9.12994444e-03  -4.59120758e-03  -5.24707102e-05]
-0.802485
1.59314
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148330 minutes
Weight histogram
[   35   356  1835   831   653  1160  9548 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[ 2168   176   226   307   503   872  1031  1961  6773 18383] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.114297 minutes
Weight histogram
[6309 5304 4022 3907 2902 2195 2129 1686 2091 1855] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
[4753 1544 1723 2322 2799 3098 3313 3993 4389 4466] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
-1.3976
1.51906
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46071
Epoch 1, cost is  6.09485
Epoch 2, cost is  5.61422
Epoch 3, cost is  5.21114
Epoch 4, cost is  4.90046
Epoch 5, cost is  4.65556
Epoch 6, cost is  4.44828
Epoch 7, cost is  4.27062
Epoch 8, cost is  4.12049
Epoch 9, cost is  3.9862
Training took 0.320449 minutes
Weight histogram
[1409 1542 1392 1244 1117  997  956 1179 1981  333] [-0.03244205 -0.02921506 -0.02598806 -0.02276106 -0.01953407 -0.01630707
 -0.01308008 -0.00985308 -0.00662609 -0.00339909 -0.00017209]
[2021  888  853  927 1030 1132 1229 1297 1385 1388] [-0.03244205 -0.02921506 -0.02598806 -0.02276106 -0.01953407 -0.01630707
 -0.01308008 -0.00985308 -0.00662609 -0.00339909 -0.00017209]
-0.418978
0.643293
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.140750 minutes
Epoch 0
Fine tuning took 0.140192 minutes
Epoch 0
Fine tuning took 0.140367 minutes
Epoch 0
Fine tuning took 0.141454 minutes
Epoch 0
Fine tuning took 0.142331 minutes
Epoch 0
Fine tuning took 0.142589 minutes
Epoch 0
Fine tuning took 0.142330 minutes
Epoch 0
Fine tuning took 0.142842 minutes
Epoch 0
Fine tuning took 0.141585 minutes
Epoch 0
Fine tuning took 0.141234 minutes
{'zero': {0: [0.52832512315270941, 0.51600985221674878, 0.6071428571428571, 0.64039408866995073, 0.52709359605911332, 0.33866995073891626, 0.55418719211822665, 0.56896551724137934, 0.31280788177339902, 0.40517241379310343, 0.42610837438423643], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.17118226600985223, 0.27463054187192121, 0.34359605911330049, 0.29310344827586204, 0.53817733990147787, 0.22167487684729065, 0.2857142857142857, 0.5788177339901478, 0.44950738916256155, 0.2857142857142857], 5: [0.14901477832512317, 0.31280788177339902, 0.11822660098522167, 0.01600985221674877, 0.17980295566502463, 0.12315270935960591, 0.22413793103448276, 0.14532019704433496, 0.10837438423645321, 0.14532019704433496, 0.28817733990147781], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.52832512315270941, 0.54679802955665024, 0.59605911330049266, 0.6071428571428571, 0.53817733990147787, 0.32635467980295568, 0.55665024630541871, 0.57389162561576357, 0.29187192118226601, 0.35591133004926107, 0.34359605911330049], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.16379310344827586, 0.25738916256157635, 0.3645320197044335, 0.27586206896551724, 0.53078817733990147, 0.20812807881773399, 0.28694581280788178, 0.55911330049261088, 0.49876847290640391, 0.26108374384236455], 5: [0.14901477832512317, 0.2894088669950739, 0.14655172413793102, 0.02832512315270936, 0.18596059113300492, 0.14285714285714285, 0.23522167487684728, 0.13916256157635468, 0.14901477832512317, 0.14532019704433496, 0.39532019704433496], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.52832512315270941, 0.47906403940886699, 0.60591133004926112, 0.6071428571428571, 0.55665024630541871, 0.34359605911330049, 0.53448275862068961, 0.58004926108374388, 0.29556650246305421, 0.3288177339901478, 0.37931034482758619], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.17857142857142858, 0.24753694581280788, 0.37438423645320196, 0.25, 0.54433497536945807, 0.21182266009852216, 0.28201970443349755, 0.53694581280788178, 0.50985221674876846, 0.23891625615763548], 5: [0.14901477832512317, 0.34236453201970446, 0.14655172413793102, 0.018472906403940888, 0.19334975369458129, 0.11206896551724138, 0.2536945812807882, 0.13793103448275862, 0.16748768472906403, 0.16133004926108374, 0.3817733990147783], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.52832512315270941, 0.52463054187192115, 0.6071428571428571, 0.63054187192118227, 0.55665024630541871, 0.32389162561576357, 0.54187192118226601, 0.56896551724137934, 0.30172413793103448, 0.37931034482758619, 0.38669950738916259], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.1625615763546798, 0.24014778325123154, 0.35714285714285715, 0.24507389162561577, 0.55911330049261088, 0.22536945812807882, 0.31034482758620691, 0.54556650246305416, 0.47290640394088668, 0.25246305418719212], 5: [0.14901477832512317, 0.31280788177339902, 0.15270935960591134, 0.012315270935960592, 0.19827586206896552, 0.11699507389162561, 0.23275862068965517, 0.1206896551724138, 0.15270935960591134, 0.14778325123152711, 0.3608374384236453], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420432 minutes
Weight histogram
[ 118  345  521  655  939 4421 9776 9199 3917  484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1270 1000  987 1403 2331 2115 4049 3653 8435 5132] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.28282
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.889355
Epoch 1, cost is  0.870816
Epoch 2, cost is  0.859792
Epoch 3, cost is  0.850568
Epoch 4, cost is  0.841564
Training took 0.646482 minutes
Weight histogram
[7690 5627 4633 3361 2509 1754 1467 1236 1005 1093] [ -4.54398394e-02  -4.09011025e-02  -3.63623656e-02  -3.18236288e-02
  -2.72848919e-02  -2.27461550e-02  -1.82074182e-02  -1.36686813e-02
  -9.12994444e-03  -4.59120758e-03  -5.24707102e-05]
[2074 1477 1635 2056 2370 3113 3429 3881 4986 5354] [ -4.54398394e-02  -4.09011025e-02  -3.63623656e-02  -3.18236288e-02
  -2.72848919e-02  -2.27461550e-02  -1.82074182e-02  -1.36686813e-02
  -9.12994444e-03  -4.59120758e-03  -5.24707102e-05]
-0.802485
1.59314
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147460 minutes
Weight histogram
[   35   356  1835   831   653  1160  9548 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[ 2168   176   226   307   503   872  1031  1961  6773 18383] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.116050 minutes
Weight histogram
[6309 5304 4022 3907 2902 2195 2129 1686 2091 1855] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
[4753 1544 1723 2322 2799 3098 3313 3993 4389 4466] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
-1.3976
1.51906
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.97839
Epoch 1, cost is  5.50466
Epoch 2, cost is  4.90702
Epoch 3, cost is  4.46033
Epoch 4, cost is  4.10567
Epoch 5, cost is  3.81745
Epoch 6, cost is  3.58014
Epoch 7, cost is  3.39366
Epoch 8, cost is  3.23547
Epoch 9, cost is  3.10493
Training took 0.497401 minutes
Weight histogram
[1643 1653 1511 1379 1229 1068 1006 1974  645   42] [-0.02037968 -0.01835723 -0.01633477 -0.01431231 -0.01228985 -0.0102674
 -0.00824494 -0.00622248 -0.00420003 -0.00217757 -0.00015511]
[2118  857  918  983 1019 1074 1162 1255 1376 1388] [-0.02037968 -0.01835723 -0.01633477 -0.01431231 -0.01228985 -0.0102674
 -0.00824494 -0.00622248 -0.00420003 -0.00217757 -0.00015511]
-0.265771
0.418584
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.149536 minutes
Epoch 0
Fine tuning took 0.150428 minutes
Epoch 0
Fine tuning took 0.150605 minutes
Epoch 0
Fine tuning took 0.151724 minutes
Epoch 0
Fine tuning took 0.149696 minutes
Epoch 0
Fine tuning took 0.150408 minutes
Epoch 0
Fine tuning took 0.150502 minutes
Epoch 0
Fine tuning took 0.150894 minutes
Epoch 0
Fine tuning took 0.151397 minutes
Epoch 0
Fine tuning took 0.149733 minutes
{'zero': {0: [0.41748768472906406, 0.37807881773399016, 0.39532019704433496, 0.4605911330049261, 0.34729064039408869, 0.27586206896551724, 0.32635467980295568, 0.26847290640394089, 0.31650246305418717, 0.31157635467980294, 0.33374384236453203], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36945812807881773, 0.42241379310344829, 0.40517241379310343, 0.35467980295566504, 0.44458128078817732, 0.46674876847290642, 0.45320197044334976, 0.54556650246305416, 0.5357142857142857, 0.53448275862068961, 0.41995073891625617], 5: [0.21305418719211822, 0.19950738916256158, 0.19950738916256158, 0.18472906403940886, 0.20812807881773399, 0.25738916256157635, 0.22044334975369459, 0.18596059113300492, 0.14778325123152711, 0.1539408866995074, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.41748768472906406, 0.45812807881773399, 0.39901477832512317, 0.54556650246305416, 0.4039408866995074, 0.36699507389162561, 0.46305418719211822, 0.43226600985221675, 0.44704433497536944, 0.48029556650246308, 0.48029556650246308], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36945812807881773, 0.30788177339901479, 0.23029556650246305, 0.21551724137931033, 0.34975369458128081, 0.29802955665024633, 0.29556650246305421, 0.34359605911330049, 0.3891625615763547, 0.3608374384236453, 0.27216748768472904], 5: [0.21305418719211822, 0.23399014778325122, 0.37068965517241381, 0.23891625615763548, 0.24630541871921183, 0.33497536945812806, 0.2413793103448276, 0.22413793103448276, 0.16379310344827586, 0.15886699507389163, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.41748768472906406, 0.3891625615763547, 0.4211822660098522, 0.51847290640394084, 0.39778325123152708, 0.31034482758620691, 0.46182266009852219, 0.40886699507389163, 0.43349753694581283, 0.47660098522167488, 0.44581280788177341], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36945812807881773, 0.33866995073891626, 0.23891625615763548, 0.19704433497536947, 0.32142857142857145, 0.31773399014778325, 0.26231527093596058, 0.36206896551724138, 0.4039408866995074, 0.36330049261083741, 0.28201970443349755], 5: [0.21305418719211822, 0.27216748768472904, 0.33990147783251229, 0.28448275862068967, 0.28078817733990147, 0.37192118226600984, 0.27586206896551724, 0.22906403940886699, 0.1625615763546798, 0.16009852216748768, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.41748768472906406, 0.44950738916256155, 0.42857142857142855, 0.58497536945812811, 0.39655172413793105, 0.34236453201970446, 0.50615763546798032, 0.47290640394088668, 0.46182266009852219, 0.51477832512315269, 0.48522167487684731], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.36945812807881773, 0.30172413793103448, 0.22044334975369459, 0.19088669950738915, 0.3608374384236453, 0.30418719211822659, 0.28694581280788178, 0.31773399014778325, 0.39039408866995073, 0.33374384236453203, 0.25985221674876846], 5: [0.21305418719211822, 0.24876847290640394, 0.35098522167487683, 0.22413793103448276, 0.24261083743842365, 0.35344827586206895, 0.20689655172413793, 0.20935960591133004, 0.14778325123152711, 0.15147783251231528, 0.25492610837438423], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.419551 minutes
Weight histogram
[ 118  345  521  655  939 4421 9776 9199 3917  484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1270 1000  987 1403 2331 2115 4049 3653 8435 5132] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.28282
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.889355
Epoch 1, cost is  0.870816
Epoch 2, cost is  0.859792
Epoch 3, cost is  0.850568
Epoch 4, cost is  0.841564
Training took 0.644805 minutes
Weight histogram
[7690 5627 4633 3361 2509 1754 1467 1236 1005 1093] [ -4.54398394e-02  -4.09011025e-02  -3.63623656e-02  -3.18236288e-02
  -2.72848919e-02  -2.27461550e-02  -1.82074182e-02  -1.36686813e-02
  -9.12994444e-03  -4.59120758e-03  -5.24707102e-05]
[2074 1477 1635 2056 2370 3113 3429 3881 4986 5354] [ -4.54398394e-02  -4.09011025e-02  -3.63623656e-02  -3.18236288e-02
  -2.72848919e-02  -2.27461550e-02  -1.82074182e-02  -1.36686813e-02
  -9.12994444e-03  -4.59120758e-03  -5.24707102e-05]
-0.802485
1.59314
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147385 minutes
Weight histogram
[   35   356  1835   831   653  1160  9548 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[ 2168   176   226   307   503   872  1031  1961  6773 18383] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.113280 minutes
Weight histogram
[6309 5304 4022 3907 2902 2195 2129 1686 2091 1855] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
[4753 1544 1723 2322 2799 3098 3313 3993 4389 4466] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
-1.3976
1.51906
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.37749
Epoch 1, cost is  4.85917
Epoch 2, cost is  4.25731
Epoch 3, cost is  3.8088
Epoch 4, cost is  3.46318
Epoch 5, cost is  3.19746
Epoch 6, cost is  2.99345
Epoch 7, cost is  2.8278
Epoch 8, cost is  2.69579
Epoch 9, cost is  2.58425
Training took 0.820132 minutes
Weight histogram
[2105 1950 1653 1479 1323 1087 2145  326   58   24] [-0.01366388 -0.01231151 -0.01095913 -0.00960675 -0.00825437 -0.006902
 -0.00554962 -0.00419724 -0.00284486 -0.00149249 -0.00014011]
[2151  864  927  939  970 1061 1175 1270 1394 1399] [-0.01366388 -0.01231151 -0.01095913 -0.00960675 -0.00825437 -0.006902
 -0.00554962 -0.00419724 -0.00284486 -0.00149249 -0.00014011]
-0.246877
0.295594
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164469 minutes
Epoch 0
Fine tuning took 0.164340 minutes
Epoch 0
Fine tuning took 0.164673 minutes
Epoch 0
Fine tuning took 0.164681 minutes
Epoch 0
Fine tuning took 0.164051 minutes
Epoch 0
Fine tuning took 0.165608 minutes
Epoch 0
Fine tuning took 0.165618 minutes
Epoch 0
Fine tuning took 0.165618 minutes
Epoch 0
Fine tuning took 0.163123 minutes
Epoch 0
Fine tuning took 0.164988 minutes
{'zero': {0: [0.44704433497536944, 0.36699507389162561, 0.38300492610837439, 0.33004926108374383, 0.33990147783251229, 0.30911330049261082, 0.34359605911330049, 0.27339901477832512, 0.35098522167487683, 0.22906403940886699, 0.34113300492610837], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.34852216748768472, 0.47044334975369456, 0.47783251231527096, 0.5, 0.46674876847290642, 0.5073891625615764, 0.4963054187192118, 0.50862068965517238, 0.42733990147783252, 0.5788177339901478, 0.39901477832512317], 5: [0.20443349753694581, 0.1625615763546798, 0.13916256157635468, 0.16995073891625614, 0.19334975369458129, 0.18349753694581281, 0.16009852216748768, 0.21798029556650247, 0.22167487684729065, 0.19211822660098521, 0.25985221674876846], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.44704433497536944, 0.3854679802955665, 0.41379310344827586, 0.41379310344827586, 0.37315270935960593, 0.34729064039408869, 0.40640394088669951, 0.35221674876847292, 0.37684729064039407, 0.29926108374384236, 0.41995073891625617], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.34852216748768472, 0.44334975369458129, 0.39532019704433496, 0.41995073891625617, 0.43349753694581283, 0.45320197044334976, 0.4211822660098522, 0.40886699507389163, 0.39285714285714285, 0.52463054187192115, 0.37438423645320196], 5: [0.20443349753694581, 0.17118226600985223, 0.19088669950738915, 0.16625615763546797, 0.19334975369458129, 0.19950738916256158, 0.17241379310344829, 0.23891625615763548, 0.23029556650246305, 0.17610837438423646, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.44704433497536944, 0.41871921182266009, 0.4211822660098522, 0.41379310344827586, 0.40270935960591131, 0.36699507389162561, 0.43349753694581283, 0.35221674876847292, 0.39901477832512317, 0.31773399014778325, 0.44704433497536944], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.34852216748768472, 0.40024630541871919, 0.40640394088669951, 0.42610837438423643, 0.41256157635467983, 0.43472906403940886, 0.40517241379310343, 0.41256157635467983, 0.39408866995073893, 0.4642857142857143, 0.34113300492610837], 5: [0.20443349753694581, 0.18103448275862069, 0.17241379310344829, 0.16009852216748768, 0.18472906403940886, 0.19827586206896552, 0.16133004926108374, 0.23522167487684728, 0.20689655172413793, 0.21798029556650247, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.44704433497536944, 0.42364532019704432, 0.43472906403940886, 0.39285714285714285, 0.39778325123152708, 0.42857142857142855, 0.41133004926108374, 0.3645320197044335, 0.41256157635467983, 0.32142857142857145, 0.37192118226600984], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.34852216748768472, 0.4039408866995074, 0.40024630541871919, 0.45320197044334976, 0.42980295566502463, 0.40024630541871919, 0.43103448275862066, 0.41995073891625617, 0.37684729064039407, 0.48768472906403942, 0.39778325123152708], 5: [0.20443349753694581, 0.17241379310344829, 0.16502463054187191, 0.1539408866995074, 0.17241379310344829, 0.17118226600985223, 0.15763546798029557, 0.21551724137931033, 0.2105911330049261, 0.19088669950738915, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.419820 minutes
Weight histogram
[ 118  345  521  655  939 4421 9776 9199 3917  484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1270 1000  987 1403 2331 2115 4049 3653 8435 5132] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.0693
1.28282
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.889355
Epoch 1, cost is  0.870816
Epoch 2, cost is  0.859792
Epoch 3, cost is  0.850568
Epoch 4, cost is  0.841564
Training took 0.643917 minutes
Weight histogram
[7690 5627 4633 3361 2509 1754 1467 1236 1005 1093] [ -4.54398394e-02  -4.09011025e-02  -3.63623656e-02  -3.18236288e-02
  -2.72848919e-02  -2.27461550e-02  -1.82074182e-02  -1.36686813e-02
  -9.12994444e-03  -4.59120758e-03  -5.24707102e-05]
[2074 1477 1635 2056 2370 3113 3429 3881 4986 5354] [ -4.54398394e-02  -4.09011025e-02  -3.63623656e-02  -3.18236288e-02
  -2.72848919e-02  -2.27461550e-02  -1.82074182e-02  -1.36686813e-02
  -9.12994444e-03  -4.59120758e-03  -5.24707102e-05]
-0.802485
1.59314
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148388 minutes
Weight histogram
[   35   356  1835   831   653  1160  9548 13104  4538   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[ 2168   176   226   307   503   872  1031  1961  6773 18383] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48913
Epoch 1, cost is  2.4481
Epoch 2, cost is  2.41772
Epoch 3, cost is  2.3978
Epoch 4, cost is  2.38152
Training took 0.114789 minutes
Weight histogram
[6309 5304 4022 3907 2902 2195 2129 1686 2091 1855] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
[4753 1544 1723 2322 2799 3098 3313 3993 4389 4466] [ -6.39048368e-02  -5.75110343e-02  -5.11172319e-02  -4.47234294e-02
  -3.83296270e-02  -3.19358245e-02  -2.55420221e-02  -1.91482196e-02
  -1.27544171e-02  -6.36061470e-03   3.31877563e-05]
-1.3976
1.51906
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.78296
Epoch 1, cost is  4.27414
Epoch 2, cost is  3.71807
Epoch 3, cost is  3.30419
Epoch 4, cost is  2.9891
Epoch 5, cost is  2.76321
Epoch 6, cost is  2.59032
Epoch 7, cost is  2.45375
Epoch 8, cost is  2.34114
Epoch 9, cost is  2.24487
Training took 1.469858 minutes
Weight histogram
[2742 2391 2002 1848 2538  375  147   64   28   15] [-0.00916891 -0.00826768 -0.00736645 -0.00646522 -0.00556399 -0.00466276
 -0.00376153 -0.0028603  -0.00195907 -0.00105784 -0.00015661]
[2109  855  891  885  945 1056 1176 1308 1464 1461] [-0.00916891 -0.00826768 -0.00736645 -0.00646522 -0.00556399 -0.00466276
 -0.00376153 -0.0028603  -0.00195907 -0.00105784 -0.00015661]
-0.169758
0.233435
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.193646 minutes
Epoch 0
Fine tuning took 0.193384 minutes
Epoch 0
Fine tuning took 0.193565 minutes
Epoch 0
Fine tuning took 0.193413 minutes
Epoch 0
Fine tuning took 0.195315 minutes
Epoch 0
Fine tuning took 0.192751 minutes
Epoch 0
Fine tuning took 0.193881 minutes
Epoch 0
Fine tuning took 0.192639 minutes
Epoch 0
Fine tuning took 0.195436 minutes
Epoch 0
Fine tuning took 0.193054 minutes
{'zero': {0: [0.3817733990147783, 0.34359605911330049, 0.35591133004926107, 0.31280788177339902, 0.33990147783251229, 0.34482758620689657, 0.34729064039408869, 0.32266009852216748, 0.34852216748768472, 0.30418719211822659, 0.43349753694581283], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42980295566502463, 0.41379310344827586, 0.43103448275862066, 0.53325123152709364, 0.45320197044334976, 0.48275862068965519, 0.43719211822660098, 0.41748768472906406, 0.40763546798029554, 0.43226600985221675, 0.34113300492610837], 5: [0.18842364532019704, 0.24261083743842365, 0.21305418719211822, 0.1539408866995074, 0.20689655172413793, 0.17241379310344829, 0.21551724137931033, 0.25985221674876846, 0.24384236453201971, 0.26354679802955666, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3817733990147783, 0.3460591133004926, 0.39778325123152708, 0.33620689655172414, 0.34975369458128081, 0.36206896551724138, 0.39162561576354682, 0.34975369458128081, 0.33866995073891626, 0.34852216748768472, 0.44950738916256155], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42980295566502463, 0.41871921182266009, 0.41133004926108374, 0.50369458128078815, 0.43472906403940886, 0.4605911330049261, 0.36330049261083741, 0.41256157635467983, 0.39901477832512317, 0.3854679802955665, 0.32758620689655171], 5: [0.18842364532019704, 0.23522167487684728, 0.19088669950738915, 0.16009852216748768, 0.21551724137931033, 0.17733990147783252, 0.24507389162561577, 0.2376847290640394, 0.26231527093596058, 0.26600985221674878, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3817733990147783, 0.3608374384236453, 0.41133004926108374, 0.33743842364532017, 0.35098522167487683, 0.3682266009852217, 0.40640394088669951, 0.37192118226600984, 0.37192118226600984, 0.39162561576354682, 0.46798029556650245], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42980295566502463, 0.42364532019704432, 0.40517241379310343, 0.50369458128078815, 0.44211822660098521, 0.46551724137931033, 0.37315270935960593, 0.41379310344827586, 0.4039408866995074, 0.37315270935960593, 0.29064039408866993], 5: [0.18842364532019704, 0.21551724137931033, 0.18349753694581281, 0.15886699507389163, 0.20689655172413793, 0.16625615763546797, 0.22044334975369459, 0.21428571428571427, 0.22413793103448276, 0.23522167487684728, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3817733990147783, 0.34852216748768472, 0.39039408866995073, 0.35098522167487683, 0.33866995073891626, 0.37068965517241381, 0.37684729064039407, 0.32019704433497537, 0.37684729064039407, 0.35960591133004927, 0.45812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42980295566502463, 0.42857142857142855, 0.43103448275862066, 0.4963054187192118, 0.45197044334975367, 0.47660098522167488, 0.39901477832512317, 0.40517241379310343, 0.38300492610837439, 0.3854679802955665, 0.2857142857142857], 5: [0.18842364532019704, 0.2229064039408867, 0.17857142857142858, 0.15270935960591134, 0.20935960591133004, 0.15270935960591134, 0.22413793103448276, 0.27463054187192121, 0.24014778325123154, 0.25492610837438423, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105197 minutes
Weight histogram
[ 743 5354 6299 5015 4134 3831 3237 3286 1843  683] [-0.00127393 -0.0007634  -0.00025287  0.00025767  0.0007682   0.00127873
  0.00178926  0.00229979  0.00281033  0.00332086  0.00383139]
[  146   155   210   318   474   776   809  1726  5017 24794] [-0.00127393 -0.0007634  -0.00025287  0.00025767  0.0007682   0.00127873
  0.00178926  0.00229979  0.00281033  0.00332086  0.00383139]
-2.33524
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82874
Epoch 1, cost is  3.78181
Epoch 2, cost is  3.75937
Epoch 3, cost is  3.74162
Epoch 4, cost is  3.7197
Training took 0.072463 minutes
Weight histogram
[6072 7442 2973 4802 4450 4148 3454  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2976 2471 2432 2829 2621 2968 3586 4483 5336 4723] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.12908
2.65158
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148233 minutes
Weight histogram
[   95   349   614  1079  1220  4537 17461 10107   970    18] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 16151   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.115127 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 3269  814] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
-1.52949
1.62376
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.90527
Epoch 1, cost is  5.55922
Epoch 2, cost is  5.47332
Epoch 3, cost is  5.36836
Epoch 4, cost is  5.16327
Epoch 5, cost is  4.91236
Epoch 6, cost is  4.70321
Epoch 7, cost is  4.53457
Epoch 8, cost is  4.37772
Epoch 9, cost is  4.23663
Training took 0.175812 minutes
Weight histogram
[2821 3087 5366 1886 1135  733  608  345  149   70] [-0.0301243  -0.02712195 -0.02411961 -0.02111726 -0.01811492 -0.01511258
 -0.01211023 -0.00910789 -0.00610555 -0.0031032  -0.00010086]
[1099 2197 3347 1579 1270 1308 1424 1541 1596  839] [-0.0301243  -0.02712195 -0.02411961 -0.02111726 -0.01811492 -0.01511258
 -0.01211023 -0.00910789 -0.00610555 -0.0031032  -0.00010086]
-0.430818
0.437275
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044271 minutes
Epoch 0
Fine tuning took 0.041604 minutes
Epoch 0
Fine tuning took 0.042938 minutes
Epoch 0
Fine tuning took 0.043766 minutes
Epoch 0
Fine tuning took 0.042718 minutes
Epoch 0
Fine tuning took 0.041968 minutes
Epoch 0
Fine tuning took 0.041481 minutes
Epoch 0
Fine tuning took 0.041892 minutes
Epoch 0
Fine tuning took 0.042789 minutes
Epoch 0
Fine tuning took 0.042516 minutes
{'zero': {0: [0.37192118226600984, 0.29310344827586204, 0.23029556650246305, 0.30295566502463056, 0.3682266009852217, 0.2894088669950739, 0.28201970443349755, 0.27586206896551724, 0.23399014778325122, 0.18226600985221675, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41133004926108374, 0.5923645320197044, 0.64162561576354682, 0.56896551724137934, 0.51847290640394084, 0.56034482758620685, 0.55788177339901479, 0.59482758620689657, 0.59975369458128081, 0.70812807881773399, 0.65517241379310343], 5: [0.21674876847290642, 0.1145320197044335, 0.12807881773399016, 0.12807881773399016, 0.11330049261083744, 0.15024630541871922, 0.16009852216748768, 0.12931034482758622, 0.16625615763546797, 0.10960591133004927, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37192118226600984, 0.29064039408866993, 0.44950738916256155, 0.4248768472906404, 0.59359605911330049, 0.5073891625615764, 0.47536945812807879, 0.47783251231527096, 0.35098522167487683, 0.44581280788177341, 0.40763546798029554], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41133004926108374, 0.62068965517241381, 0.4211822660098522, 0.45443349753694579, 0.33866995073891626, 0.36699507389162561, 0.31773399014778325, 0.38793103448275862, 0.49753694581280788, 0.42241379310344829, 0.45812807881773399], 5: [0.21674876847290642, 0.088669950738916259, 0.12931034482758622, 0.1206896551724138, 0.067733990147783252, 0.12561576354679804, 0.20689655172413793, 0.13423645320197045, 0.15147783251231528, 0.13177339901477833, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37192118226600984, 0.29556650246305421, 0.4211822660098522, 0.44827586206896552, 0.58743842364532017, 0.44827586206896552, 0.44211822660098521, 0.4605911330049261, 0.34482758620689657, 0.41133004926108374, 0.34359605911330049], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41133004926108374, 0.57389162561576357, 0.44827586206896552, 0.43472906403940886, 0.33004926108374383, 0.4211822660098522, 0.32758620689655171, 0.41871921182266009, 0.5073891625615764, 0.47660098522167488, 0.48275862068965519], 5: [0.21674876847290642, 0.13054187192118227, 0.13054187192118227, 0.11699507389162561, 0.082512315270935957, 0.13054187192118227, 0.23029556650246305, 0.1206896551724138, 0.14778325123152711, 0.11206896551724138, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37192118226600984, 0.26600985221674878, 0.48891625615763545, 0.46551724137931033, 0.62315270935960587, 0.54926108374384242, 0.51354679802955661, 0.55418719211822665, 0.3854679802955665, 0.51354679802955661, 0.39285714285714285], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.41133004926108374, 0.63793103448275867, 0.38793103448275862, 0.44334975369458129, 0.32266009852216748, 0.37068965517241381, 0.26970443349753692, 0.34359605911330049, 0.48645320197044334, 0.38054187192118227, 0.48645320197044334], 5: [0.21674876847290642, 0.096059113300492605, 0.12315270935960591, 0.091133004926108374, 0.054187192118226604, 0.080049261083743842, 0.21674876847290642, 0.10221674876847291, 0.12807881773399016, 0.10591133004926108, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106527 minutes
Weight histogram
[ 743 5354 6299 5015 4134 3831 3237 3286 1843  683] [-0.00127393 -0.0007634  -0.00025287  0.00025767  0.0007682   0.00127873
  0.00178926  0.00229979  0.00281033  0.00332086  0.00383139]
[  146   155   210   318   474   776   809  1726  5017 24794] [-0.00127393 -0.0007634  -0.00025287  0.00025767  0.0007682   0.00127873
  0.00178926  0.00229979  0.00281033  0.00332086  0.00383139]
-2.33524
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82874
Epoch 1, cost is  3.78181
Epoch 2, cost is  3.75937
Epoch 3, cost is  3.74162
Epoch 4, cost is  3.7197
Training took 0.071170 minutes
Weight histogram
[6072 7442 2973 4802 4450 4148 3454  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2976 2471 2432 2829 2621 2968 3586 4483 5336 4723] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.12908
2.65158
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146645 minutes
Weight histogram
[   95   349   614  1079  1220  4537 17461 10107   970    18] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 16151   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.113215 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 3269  814] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
-1.52949
1.62376
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.56569
Epoch 1, cost is  5.36127
Epoch 2, cost is  5.2691
Epoch 3, cost is  5.03336
Epoch 4, cost is  4.69606
Epoch 5, cost is  4.41355
Epoch 6, cost is  4.21109
Epoch 7, cost is  4.04219
Epoch 8, cost is  3.88569
Epoch 9, cost is  3.73351
Training took 0.242971 minutes
Weight histogram
[2698 6411 4404 1112  675  409  223  145   76   47] [-0.01972586 -0.01776895 -0.01581205 -0.01385514 -0.01189824 -0.00994133
 -0.00798442 -0.00602752 -0.00407061 -0.00211371 -0.0001568 ]
[3868 2029 1078 1014 1092 1244 1418 1575 1680 1202] [-0.01972586 -0.01776895 -0.01581205 -0.01385514 -0.01189824 -0.00994133
 -0.00798442 -0.00602752 -0.00407061 -0.00211371 -0.0001568 ]
-0.341753
0.28969
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044340 minutes
Epoch 0
Fine tuning took 0.047807 minutes
Epoch 0
Fine tuning took 0.047156 minutes
Epoch 0
Fine tuning took 0.047494 minutes
Epoch 0
Fine tuning took 0.044603 minutes
Epoch 0
Fine tuning took 0.048279 minutes
Epoch 0
Fine tuning took 0.045318 minutes
Epoch 0
Fine tuning took 0.045041 minutes
Epoch 0
Fine tuning took 0.047064 minutes
Epoch 0
Fine tuning took 0.046775 minutes
{'zero': {0: [0.2894088669950739, 0.22413793103448276, 0.24630541871921183, 0.27339901477832512, 0.37807881773399016, 0.31650246305418717, 0.2894088669950739, 0.29187192118226601, 0.30788177339901479, 0.32266009852216748, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.60960591133004927, 0.5788177339901478, 0.52463054187192115, 0.42610837438423643, 0.47536945812807879, 0.50862068965517238, 0.51354679802955661, 0.51354679802955661, 0.47783251231527096, 0.49261083743842365], 5: [0.22536945812807882, 0.16625615763546797, 0.1748768472906404, 0.2019704433497537, 0.19581280788177341, 0.20812807881773399, 0.2019704433497537, 0.19458128078817735, 0.17857142857142858, 0.19950738916256158, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2894088669950739, 0.23029556650246305, 0.31403940886699505, 0.29926108374384236, 0.36945812807881773, 0.34729064039408869, 0.26724137931034481, 0.33251231527093594, 0.33497536945812806, 0.33990147783251229, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.60837438423645318, 0.50246305418719217, 0.52832512315270941, 0.44088669950738918, 0.44088669950738918, 0.5, 0.45812807881773399, 0.51477832512315269, 0.43226600985221675, 0.44704433497536944], 5: [0.22536945812807882, 0.16133004926108374, 0.18349753694581281, 0.17241379310344829, 0.18965517241379309, 0.21182266009852216, 0.23275862068965517, 0.20935960591133004, 0.15024630541871922, 0.22783251231527094, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2894088669950739, 0.29556650246305421, 0.28694581280788178, 0.34113300492610837, 0.36330049261083741, 0.33990147783251229, 0.29679802955665024, 0.35960591133004927, 0.33866995073891626, 0.35960591133004927, 0.35467980295566504], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.54802955665024633, 0.50862068965517238, 0.47536945812807879, 0.43596059113300495, 0.45197044334975367, 0.50123152709359609, 0.45443349753694579, 0.49753694581280788, 0.4248768472906404, 0.43965517241379309], 5: [0.22536945812807882, 0.15640394088669951, 0.20443349753694581, 0.18349753694581281, 0.20073891625615764, 0.20812807881773399, 0.2019704433497537, 0.18596059113300492, 0.16379310344827586, 0.21551724137931033, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2894088669950739, 0.28201970443349755, 0.31773399014778325, 0.33374384236453203, 0.40024630541871919, 0.34482758620689657, 0.26477832512315269, 0.33004926108374383, 0.30665024630541871, 0.32266009852216748, 0.31896551724137934], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.55665024630541871, 0.50492610837438423, 0.52586206896551724, 0.42241379310344829, 0.42733990147783252, 0.50862068965517238, 0.48029556650246308, 0.50985221674876846, 0.45073891625615764, 0.49507389162561577], 5: [0.22536945812807882, 0.16133004926108374, 0.17733990147783252, 0.14039408866995073, 0.17733990147783252, 0.22783251231527094, 0.22660098522167488, 0.18965517241379309, 0.18349753694581281, 0.22660098522167488, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.104401 minutes
Weight histogram
[ 743 5354 6299 5015 4134 3831 3237 3286 1843  683] [-0.00127393 -0.0007634  -0.00025287  0.00025767  0.0007682   0.00127873
  0.00178926  0.00229979  0.00281033  0.00332086  0.00383139]
[  146   155   210   318   474   776   809  1726  5017 24794] [-0.00127393 -0.0007634  -0.00025287  0.00025767  0.0007682   0.00127873
  0.00178926  0.00229979  0.00281033  0.00332086  0.00383139]
-2.33524
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82874
Epoch 1, cost is  3.78181
Epoch 2, cost is  3.75937
Epoch 3, cost is  3.74162
Epoch 4, cost is  3.7197
Training took 0.070950 minutes
Weight histogram
[6072 7442 2973 4802 4450 4148 3454  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2976 2471 2432 2829 2621 2968 3586 4483 5336 4723] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.12908
2.65158
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148200 minutes
Weight histogram
[   95   349   614  1079  1220  4537 17461 10107   970    18] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 16151   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.115056 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 3269  814] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
-1.52949
1.62376
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.49563
Epoch 1, cost is  5.36742
Epoch 2, cost is  5.07156
Epoch 3, cost is  4.60122
Epoch 4, cost is  4.23902
Epoch 5, cost is  3.98559
Epoch 6, cost is  3.75695
Epoch 7, cost is  3.54931
Epoch 8, cost is  3.36761
Epoch 9, cost is  3.21629
Training took 0.333855 minutes
Weight histogram
[2267 3510 2606 2137 3647 1712  177   78   37   29] [-0.01159094 -0.01044447 -0.00929799 -0.00815152 -0.00700504 -0.00585857
 -0.00471209 -0.00356562 -0.00241915 -0.00127267 -0.0001262 ]
[4099 1057  976 1053 1230 1451 1535 1598 1682 1519] [-0.01159094 -0.01044447 -0.00929799 -0.00815152 -0.00700504 -0.00585857
 -0.00471209 -0.00356562 -0.00241915 -0.00127267 -0.0001262 ]
-0.252911
0.220349
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050357 minutes
Epoch 0
Fine tuning took 0.050385 minutes
Epoch 0
Fine tuning took 0.051105 minutes
Epoch 0
Fine tuning took 0.050182 minutes
Epoch 0
Fine tuning took 0.051334 minutes
Epoch 0
Fine tuning took 0.051692 minutes
Epoch 0
Fine tuning took 0.049944 minutes
Epoch 0
Fine tuning took 0.050120 minutes
Epoch 0
Fine tuning took 0.050732 minutes
Epoch 0
Fine tuning took 0.049765 minutes
{'zero': {0: [0.25862068965517243, 0.32019704433497537, 0.27709359605911332, 0.30541871921182268, 0.33374384236453203, 0.33866995073891626, 0.35960591133004927, 0.31773399014778325, 0.43472906403940886, 0.3645320197044335, 0.34975369458128081], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.48275862068965519, 0.46921182266009853, 0.49876847290640391, 0.42241379310344829, 0.43472906403940886, 0.37684729064039407, 0.41256157635467983, 0.38054187192118227, 0.43103448275862066, 0.40640394088669951], 5: [0.2376847290640394, 0.19704433497536947, 0.2536945812807882, 0.19581280788177341, 0.24384236453201971, 0.22660098522167488, 0.26354679802955666, 0.26970443349753692, 0.18472906403940886, 0.20443349753694581, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25862068965517243, 0.33866995073891626, 0.29802955665024633, 0.3251231527093596, 0.34975369458128081, 0.38793103448275862, 0.37561576354679804, 0.37684729064039407, 0.44950738916256155, 0.3608374384236453, 0.38054187192118227], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.46674876847290642, 0.43103448275862066, 0.49137931034482757, 0.40270935960591131, 0.41009852216748771, 0.35467980295566504, 0.39778325123152708, 0.35344827586206895, 0.39778325123152708, 0.37192118226600984], 5: [0.2376847290640394, 0.19458128078817735, 0.27093596059113301, 0.18349753694581281, 0.24753694581280788, 0.2019704433497537, 0.26970443349753692, 0.22536945812807882, 0.19704433497536947, 0.2413793103448276, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25862068965517243, 0.36699507389162561, 0.32389162561576357, 0.27709359605911332, 0.35344827586206895, 0.36206896551724138, 0.39408866995073893, 0.35591133004926107, 0.40640394088669951, 0.4248768472906404, 0.38793103448275862], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.44088669950738918, 0.43965517241379309, 0.51108374384236455, 0.39532019704433496, 0.42364532019704432, 0.33990147783251229, 0.41133004926108374, 0.40147783251231528, 0.35467980295566504, 0.37684729064039407], 5: [0.2376847290640394, 0.19211822660098521, 0.23645320197044334, 0.21182266009852216, 0.25123152709359609, 0.21428571428571427, 0.26600985221674878, 0.23275862068965517, 0.19211822660098521, 0.22044334975369459, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25862068965517243, 0.35344827586206895, 0.35344827586206895, 0.31403940886699505, 0.36206896551724138, 0.35344827586206895, 0.3891625615763547, 0.35591133004926107, 0.4039408866995074, 0.40517241379310343, 0.39408866995073893], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.4248768472906404, 0.40024630541871919, 0.50615763546798032, 0.38300492610837439, 0.40640394088669951, 0.34482758620689657, 0.38793103448275862, 0.39162561576354682, 0.36945812807881773, 0.36699507389162561], 5: [0.2376847290640394, 0.22167487684729065, 0.24630541871921183, 0.17980295566502463, 0.25492610837438423, 0.24014778325123154, 0.26600985221674878, 0.25615763546798032, 0.20443349753694581, 0.22536945812807882, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106207 minutes
Weight histogram
[ 743 5354 6299 5015 4134 3831 3237 3286 1843  683] [-0.00127393 -0.0007634  -0.00025287  0.00025767  0.0007682   0.00127873
  0.00178926  0.00229979  0.00281033  0.00332086  0.00383139]
[  146   155   210   318   474   776   809  1726  5017 24794] [-0.00127393 -0.0007634  -0.00025287  0.00025767  0.0007682   0.00127873
  0.00178926  0.00229979  0.00281033  0.00332086  0.00383139]
-2.33524
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82874
Epoch 1, cost is  3.78181
Epoch 2, cost is  3.75937
Epoch 3, cost is  3.74162
Epoch 4, cost is  3.7197
Training took 0.074162 minutes
Weight histogram
[6072 7442 2973 4802 4450 4148 3454  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[2976 2471 2432 2829 2621 2968 3586 4483 5336 4723] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.12908
2.65158
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146868 minutes
Weight histogram
[   95   349   614  1079  1220  4537 17461 10107   970    18] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 16151   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.114837 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 3269  814] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
-1.52949
1.62376
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.49001
Epoch 1, cost is  5.30111
Epoch 2, cost is  4.79911
Epoch 3, cost is  4.27237
Epoch 4, cost is  3.91786
Epoch 5, cost is  3.6191
Epoch 6, cost is  3.36475
Epoch 7, cost is  3.15693
Epoch 8, cost is  2.99553
Epoch 9, cost is  2.85907
Training took 0.534933 minutes
Weight histogram
[2391 3124 2738 1849 1536 3309 1181   37   19   16] [-0.00614247 -0.00553869 -0.00493491 -0.00433113 -0.00372735 -0.00312357
 -0.00251979 -0.00191601 -0.00131223 -0.00070844 -0.00010466]
[3716 1013  988 1136 1319 1409 1480 1577 1736 1826] [-0.00614247 -0.00553869 -0.00493491 -0.00433113 -0.00372735 -0.00312357
 -0.00251979 -0.00191601 -0.00131223 -0.00070844 -0.00010466]
-0.188173
0.181475
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.059562 minutes
Epoch 0
Fine tuning took 0.061996 minutes
Epoch 0
Fine tuning took 0.061526 minutes
Epoch 0
Fine tuning took 0.061888 minutes
Epoch 0
Fine tuning took 0.060994 minutes
Epoch 0
Fine tuning took 0.060665 minutes
Epoch 0
Fine tuning took 0.061180 minutes
Epoch 0
Fine tuning took 0.062398 minutes
Epoch 0
Fine tuning took 0.060871 minutes
Epoch 0
Fine tuning took 0.062160 minutes
{'zero': {0: [0.26847290640394089, 0.33004926108374383, 0.30911330049261082, 0.31773399014778325, 0.31896551724137934, 0.3682266009852217, 0.35098522167487683, 0.36206896551724138, 0.40147783251231528, 0.39162561576354682, 0.41995073891625617], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50246305418719217, 0.43596059113300495, 0.42857142857142855, 0.46921182266009853, 0.44458128078817732, 0.39285714285714285, 0.37315270935960593, 0.40763546798029554, 0.38423645320197042, 0.33990147783251229, 0.37931034482758619], 5: [0.22906403940886699, 0.23399014778325122, 0.26231527093596058, 0.21305418719211822, 0.23645320197044334, 0.23891625615763548, 0.27586206896551724, 0.23029556650246305, 0.21428571428571427, 0.26847290640394089, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26847290640394089, 0.3608374384236453, 0.34482758620689657, 0.35714285714285715, 0.37315270935960593, 0.44211822660098521, 0.39408866995073893, 0.3645320197044335, 0.41871921182266009, 0.40517241379310343, 0.4039408866995074], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50246305418719217, 0.39655172413793105, 0.3817733990147783, 0.41748768472906406, 0.40024630541871919, 0.33251231527093594, 0.34729064039408869, 0.39285714285714285, 0.38423645320197042, 0.35098522167487683, 0.35960591133004927], 5: [0.22906403940886699, 0.24261083743842365, 0.27339901477832512, 0.22536945812807882, 0.22660098522167488, 0.22536945812807882, 0.25862068965517243, 0.24261083743842365, 0.19704433497536947, 0.24384236453201971, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26847290640394089, 0.3608374384236453, 0.33374384236453203, 0.35221674876847292, 0.33251231527093594, 0.37315270935960593, 0.39655172413793105, 0.35960591133004927, 0.41133004926108374, 0.40886699507389163, 0.38669950738916259], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50246305418719217, 0.39778325123152708, 0.37438423645320196, 0.42980295566502463, 0.43103448275862066, 0.37931034482758619, 0.32266009852216748, 0.38669950738916259, 0.39039408866995073, 0.34482758620689657, 0.38300492610837439], 5: [0.22906403940886699, 0.2413793103448276, 0.29187192118226601, 0.21798029556650247, 0.23645320197044334, 0.24753694581280788, 0.28078817733990147, 0.2536945812807882, 0.19827586206896552, 0.24630541871921183, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26847290640394089, 0.33004926108374383, 0.3288177339901478, 0.37807881773399016, 0.34236453201970446, 0.40640394088669951, 0.4039408866995074, 0.37192118226600984, 0.45566502463054187, 0.41379310344827586, 0.41871921182266009], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.50246305418719217, 0.41748768472906406, 0.41133004926108374, 0.42241379310344829, 0.4248768472906404, 0.35960591133004927, 0.33004926108374383, 0.3682266009852217, 0.33866995073891626, 0.31773399014778325, 0.36699507389162561], 5: [0.22906403940886699, 0.25246305418719212, 0.25985221674876846, 0.19950738916256158, 0.23275862068965517, 0.23399014778325122, 0.26600985221674878, 0.25985221674876846, 0.20566502463054187, 0.26847290640394089, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150349 minutes
Weight histogram
[  185   432   798  1935  3917  5814 11852  7468  1886   138] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
[  141   161   244   362   509   922  1204  2716 10483 17683] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
-1.32597
1.19368
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.29387
Epoch 1, cost is  2.26558
Epoch 2, cost is  2.24325
Epoch 3, cost is  2.22694
Epoch 4, cost is  2.20994
Training took 0.115319 minutes
Weight histogram
[6957 5405 5099 4243 3669 2624 2082 2269 1619  458] [ -6.84255809e-02  -6.15794512e-02  -5.47333216e-02  -4.78871919e-02
  -4.10410623e-02  -3.41949327e-02  -2.73488030e-02  -2.05026734e-02
  -1.36565438e-02  -6.81041412e-03   3.57155150e-05]
[2935 1656 1974 2542 2991 3459 4201 4364 5314 4989] [ -6.84255809e-02  -6.15794512e-02  -5.47333216e-02  -4.78871919e-02
  -4.10410623e-02  -3.41949327e-02  -2.73488030e-02  -2.05026734e-02
  -1.36565438e-02  -6.81041412e-03   3.57155150e-05]
-1.48763
2.0454
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147744 minutes
Weight histogram
[   36   324   649  1156  1318  1434 10922 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  293   351   489   738  1067  1470  1252  2311 12818 15661] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.116568 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 3200  883] [ -6.79385737e-02  -6.11411447e-02  -5.43437158e-02  -4.75462869e-02
  -4.07488580e-02  -3.39514291e-02  -2.71540002e-02  -2.03565712e-02
  -1.35591423e-02  -6.76171340e-03   3.57155150e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11411447e-02  -5.43437158e-02  -4.75462869e-02
  -4.07488580e-02  -3.39514291e-02  -2.71540002e-02  -2.03565712e-02
  -1.35591423e-02  -6.76171340e-03   3.57155150e-05]
-1.52949
1.62376
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.22834
Epoch 1, cost is  5.98079
Epoch 2, cost is  5.82506
Epoch 3, cost is  5.61524
Epoch 4, cost is  5.29782
Epoch 5, cost is  4.96871
Epoch 6, cost is  4.68848
Epoch 7, cost is  4.46696
Epoch 8, cost is  4.28392
Epoch 9, cost is  4.12455
Training took 0.202499 minutes
Weight histogram
[1819 1806 1852 1970 4426 2149 1497  508  117   56] [-0.02607865 -0.02348691 -0.02089517 -0.01830342 -0.01571168 -0.01311993
 -0.01052819 -0.00793645 -0.0053447  -0.00275296 -0.00016122]
[3306 2786 1249 1063 1089 1155 1262 1377 1521 1392] [-0.02607865 -0.02348691 -0.02089517 -0.01830342 -0.01571168 -0.01311993
 -0.01052819 -0.00793645 -0.0053447  -0.00275296 -0.00016122]
-0.329143
0.427422
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.053373 minutes
Epoch 0
Fine tuning took 0.054549 minutes
Epoch 0
Fine tuning took 0.053526 minutes
Epoch 0
Fine tuning took 0.052989 minutes
Epoch 0
Fine tuning took 0.054306 minutes
Epoch 0
Fine tuning took 0.054679 minutes
Epoch 0
Fine tuning took 0.052427 minutes
Epoch 0
Fine tuning took 0.052284 minutes
Epoch 0
Fine tuning took 0.053933 minutes
Epoch 0
Fine tuning took 0.054160 minutes
{'zero': {0: [0.28078817733990147, 0.3460591133004926, 0.37931034482758619, 0.51231527093596063, 0.5714285714285714, 0.42364532019704432, 0.34729064039408869, 0.31650246305418717, 0.34482758620689657, 0.37684729064039407, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43965517241379309, 0.53940886699507384, 0.47290640394088668, 0.3645320197044335, 0.34852216748768472, 0.41009852216748771, 0.5073891625615764, 0.47536945812807879, 0.49014778325123154, 0.45197044334975367, 0.56896551724137934], 5: [0.27955665024630544, 0.1145320197044335, 0.14778325123152711, 0.12315270935960591, 0.080049261083743842, 0.16625615763546797, 0.14532019704433496, 0.20812807881773399, 0.16502463054187191, 0.17118226600985223, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28078817733990147, 0.3645320197044335, 0.42241379310344829, 0.51108374384236455, 0.66871921182266014, 0.59482758620689657, 0.37931034482758619, 0.28201970443349755, 0.34236453201970446, 0.51231527093596063, 0.36945812807881773], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43965517241379309, 0.39039408866995073, 0.34236453201970446, 0.17118226600985223, 0.16871921182266009, 0.25492610837438423, 0.39778325123152708, 0.44211822660098521, 0.43226600985221675, 0.29187192118226601, 0.37931034482758619], 5: [0.27955665024630544, 0.24507389162561577, 0.23522167487684728, 0.31773399014778325, 0.1625615763546798, 0.15024630541871922, 0.2229064039408867, 0.27586206896551724, 0.22536945812807882, 0.19581280788177341, 0.25123152709359609], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28078817733990147, 0.3682266009852217, 0.41502463054187194, 0.50615763546798032, 0.6354679802955665, 0.5357142857142857, 0.37438423645320196, 0.24507389162561577, 0.33128078817733991, 0.49753694581280788, 0.33374384236453203], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43965517241379309, 0.3682266009852217, 0.33743842364532017, 0.20443349753694581, 0.18965517241379309, 0.26600985221674878, 0.3608374384236453, 0.43596059113300495, 0.40147783251231528, 0.30911330049261082, 0.41379310344827586], 5: [0.27955665024630544, 0.26354679802955666, 0.24753694581280788, 0.2894088669950739, 0.1748768472906404, 0.19827586206896552, 0.26477832512315269, 0.31896551724137934, 0.26724137931034481, 0.19334975369458129, 0.25246305418719212], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28078817733990147, 0.40763546798029554, 0.41379310344827586, 0.52586206896551724, 0.70073891625615758, 0.58374384236453203, 0.43349753694581283, 0.31280788177339902, 0.32019704433497537, 0.56896551724137934, 0.40763546798029554], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43965517241379309, 0.3645320197044335, 0.34852216748768472, 0.16625615763546797, 0.15147783251231528, 0.27093596059113301, 0.33866995073891626, 0.42610837438423643, 0.45812807881773399, 0.29433497536945813, 0.3682266009852217], 5: [0.27955665024630544, 0.22783251231527094, 0.2376847290640394, 0.30788177339901479, 0.14778325123152711, 0.14532019704433496, 0.22783251231527094, 0.26108374384236455, 0.22167487684729065, 0.13669950738916256, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148554 minutes
Weight histogram
[  185   432   798  1935  3917  5814 11852  7468  1886   138] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
[  141   161   244   362   509   922  1204  2716 10483 17683] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
-1.32597
1.19368
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.29387
Epoch 1, cost is  2.26558
Epoch 2, cost is  2.24325
Epoch 3, cost is  2.22694
Epoch 4, cost is  2.20994
Training took 0.116616 minutes
Weight histogram
[6957 5405 5099 4243 3669 2624 2082 2269 1619  458] [ -6.84255809e-02  -6.15794512e-02  -5.47333216e-02  -4.78871919e-02
  -4.10410623e-02  -3.41949327e-02  -2.73488030e-02  -2.05026734e-02
  -1.36565438e-02  -6.81041412e-03   3.57155150e-05]
[2935 1656 1974 2542 2991 3459 4201 4364 5314 4989] [ -6.84255809e-02  -6.15794512e-02  -5.47333216e-02  -4.78871919e-02
  -4.10410623e-02  -3.41949327e-02  -2.73488030e-02  -2.05026734e-02
  -1.36565438e-02  -6.81041412e-03   3.57155150e-05]
-1.48763
2.0454
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148566 minutes
Weight histogram
[   36   324   649  1156  1318  1434 10922 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  293   351   489   738  1067  1470  1252  2311 12818 15661] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.115396 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 3200  883] [ -6.79385737e-02  -6.11411447e-02  -5.43437158e-02  -4.75462869e-02
  -4.07488580e-02  -3.39514291e-02  -2.71540002e-02  -2.03565712e-02
  -1.35591423e-02  -6.76171340e-03   3.57155150e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11411447e-02  -5.43437158e-02  -4.75462869e-02
  -4.07488580e-02  -3.39514291e-02  -2.71540002e-02  -2.03565712e-02
  -1.35591423e-02  -6.76171340e-03   3.57155150e-05]
-1.52949
1.62376
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.6321
Epoch 1, cost is  5.40658
Epoch 2, cost is  5.28142
Epoch 3, cost is  5.01632
Epoch 4, cost is  4.61519
Epoch 5, cost is  4.27256
Epoch 6, cost is  4.00963
Epoch 7, cost is  3.7914
Epoch 8, cost is  3.59903
Epoch 9, cost is  3.43377
Training took 0.295741 minutes
Weight histogram
[2727 2549 2259 6482 1238  513  220  114   60   38] [-0.01807489 -0.016283   -0.01449112 -0.01269923 -0.01090734 -0.00911546
 -0.00732357 -0.00553169 -0.0037398  -0.00194791 -0.00015603]
[4989 1314  948  968 1059 1201 1339 1428 1519 1435] [-0.01807489 -0.016283   -0.01449112 -0.01269923 -0.01090734 -0.00911546
 -0.00732357 -0.00553169 -0.0037398  -0.00194791 -0.00015603]
-0.293211
0.32349
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.058574 minutes
Epoch 0
Fine tuning took 0.058404 minutes
Epoch 0
Fine tuning took 0.057564 minutes
Epoch 0
Fine tuning took 0.057307 minutes
Epoch 0
Fine tuning took 0.057450 minutes
Epoch 0
Fine tuning took 0.059436 minutes
Epoch 0
Fine tuning took 0.057971 minutes
Epoch 0
Fine tuning took 0.059261 minutes
Epoch 0
Fine tuning took 0.057665 minutes
Epoch 0
Fine tuning took 0.060044 minutes
{'zero': {0: [0.31034482758620691, 0.26477832512315269, 0.23522167487684728, 0.26354679802955666, 0.30788177339901479, 0.25985221674876846, 0.23275862068965517, 0.25123152709359609, 0.28448275862068967, 0.20320197044334976, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43472906403940886, 0.58128078817733986, 0.61330049261083741, 0.56650246305418717, 0.58497536945812811, 0.54556650246305416, 0.62684729064039413, 0.60098522167487689, 0.55788177339901479, 0.61945812807881773, 0.56773399014778325], 5: [0.25492610837438423, 0.1539408866995074, 0.15147783251231528, 0.16995073891625614, 0.10714285714285714, 0.19458128078817735, 0.14039408866995073, 0.14778325123152711, 0.15763546798029557, 0.17733990147783252, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31034482758620691, 0.31403940886699505, 0.3817733990147783, 0.31034482758620691, 0.40640394088669951, 0.39408866995073893, 0.33497536945812806, 0.34729064039408869, 0.33251231527093594, 0.32142857142857145, 0.39162561576354682], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43472906403940886, 0.46551724137931033, 0.43719211822660098, 0.41995073891625617, 0.45197044334975367, 0.39655172413793105, 0.49014778325123154, 0.44704433497536944, 0.47906403940886699, 0.47290640394088668, 0.44581280788177341], 5: [0.25492610837438423, 0.22044334975369459, 0.18103448275862069, 0.26970443349753692, 0.14162561576354679, 0.20935960591133004, 0.1748768472906404, 0.20566502463054187, 0.18842364532019704, 0.20566502463054187, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31034482758620691, 0.32758620689655171, 0.34729064039408869, 0.31280788177339902, 0.41502463054187194, 0.4039408866995074, 0.30911330049261082, 0.34852216748768472, 0.35960591133004927, 0.32758620689655171, 0.37192118226600984], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43472906403940886, 0.48152709359605911, 0.45320197044334976, 0.43842364532019706, 0.44827586206896552, 0.39532019704433496, 0.51477832512315269, 0.45197044334975367, 0.45443349753694579, 0.48522167487684731, 0.45443349753694579], 5: [0.25492610837438423, 0.19088669950738915, 0.19950738916256158, 0.24876847290640394, 0.13669950738916256, 0.20073891625615764, 0.17610837438423646, 0.19950738916256158, 0.18596059113300492, 0.18719211822660098, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31034482758620691, 0.33990147783251229, 0.35960591133004927, 0.32389162561576357, 0.40147783251231528, 0.43719211822660098, 0.33497536945812806, 0.37561576354679804, 0.37561576354679804, 0.34482758620689657, 0.3817733990147783], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43472906403940886, 0.45689655172413796, 0.43349753694581283, 0.43103448275862066, 0.47660098522167488, 0.37192118226600984, 0.50246305418719217, 0.44827586206896552, 0.44334975369458129, 0.46551724137931033, 0.46551724137931033], 5: [0.25492610837438423, 0.20320197044334976, 0.20689655172413793, 0.24507389162561577, 0.12192118226600986, 0.19088669950738915, 0.1625615763546798, 0.17610837438423646, 0.18103448275862069, 0.18965517241379309, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150033 minutes
Weight histogram
[  185   432   798  1935  3917  5814 11852  7468  1886   138] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
[  141   161   244   362   509   922  1204  2716 10483 17683] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
-1.32597
1.19368
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.29387
Epoch 1, cost is  2.26558
Epoch 2, cost is  2.24325
Epoch 3, cost is  2.22694
Epoch 4, cost is  2.20994
Training took 0.115170 minutes
Weight histogram
[6957 5405 5099 4243 3669 2624 2082 2269 1619  458] [ -6.84255809e-02  -6.15794512e-02  -5.47333216e-02  -4.78871919e-02
  -4.10410623e-02  -3.41949327e-02  -2.73488030e-02  -2.05026734e-02
  -1.36565438e-02  -6.81041412e-03   3.57155150e-05]
[2935 1656 1974 2542 2991 3459 4201 4364 5314 4989] [ -6.84255809e-02  -6.15794512e-02  -5.47333216e-02  -4.78871919e-02
  -4.10410623e-02  -3.41949327e-02  -2.73488030e-02  -2.05026734e-02
  -1.36565438e-02  -6.81041412e-03   3.57155150e-05]
-1.48763
2.0454
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147372 minutes
Weight histogram
[   36   324   649  1156  1318  1434 10922 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  293   351   489   738  1067  1470  1252  2311 12818 15661] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.115836 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 3200  883] [ -6.79385737e-02  -6.11411447e-02  -5.43437158e-02  -4.75462869e-02
  -4.07488580e-02  -3.39514291e-02  -2.71540002e-02  -2.03565712e-02
  -1.35591423e-02  -6.76171340e-03   3.57155150e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11411447e-02  -5.43437158e-02  -4.75462869e-02
  -4.07488580e-02  -3.39514291e-02  -2.71540002e-02  -2.03565712e-02
  -1.35591423e-02  -6.76171340e-03   3.57155150e-05]
-1.52949
1.62376
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.28444
Epoch 1, cost is  5.13635
Epoch 2, cost is  4.96236
Epoch 3, cost is  4.54936
Epoch 4, cost is  4.14452
Epoch 5, cost is  3.8346
Epoch 6, cost is  3.57551
Epoch 7, cost is  3.34297
Epoch 8, cost is  3.14436
Epoch 9, cost is  2.9677
Training took 0.412633 minutes
Weight histogram
[3614 3753 7063  899  422  201  121   65   36   26] [-0.01216332 -0.01096067 -0.00975802 -0.00855537 -0.00735272 -0.00615007
 -0.00494742 -0.00374477 -0.00254212 -0.00133946 -0.00013681]
[4652 1070  955 1072 1177 1323 1409 1483 1597 1462] [-0.01216332 -0.01096067 -0.00975802 -0.00855537 -0.00735272 -0.00615007
 -0.00494742 -0.00374477 -0.00254212 -0.00133946 -0.00013681]
-0.22167
0.269775
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.064363 minutes
Epoch 0
Fine tuning took 0.065035 minutes
Epoch 0
Fine tuning took 0.064944 minutes
Epoch 0
Fine tuning took 0.063156 minutes
Epoch 0
Fine tuning took 0.063571 minutes
Epoch 0
Fine tuning took 0.065226 minutes
Epoch 0
Fine tuning took 0.064602 minutes
Epoch 0
Fine tuning took 0.063436 minutes
Epoch 0
Fine tuning took 0.063472 minutes
Epoch 0
Fine tuning took 0.064671 minutes
{'zero': {0: [0.24507389162561577, 0.27955665024630544, 0.25862068965517243, 0.28201970443349755, 0.33990147783251229, 0.27832512315270935, 0.27832512315270935, 0.25, 0.30418719211822659, 0.33004926108374383, 0.26970443349753692], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52463054187192115, 0.53448275862068961, 0.56650246305418717, 0.51600985221674878, 0.43349753694581283, 0.51354679802955661, 0.52216748768472909, 0.57758620689655171, 0.55665024630541871, 0.46305418719211822, 0.54802955665024633], 5: [0.23029556650246305, 0.18596059113300492, 0.1748768472906404, 0.2019704433497537, 0.22660098522167488, 0.20812807881773399, 0.19950738916256158, 0.17241379310344829, 0.13916256157635468, 0.20689655172413793, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24507389162561577, 0.32142857142857145, 0.28078817733990147, 0.27216748768472904, 0.39778325123152708, 0.32758620689655171, 0.31650246305418717, 0.29187192118226601, 0.33251231527093594, 0.35714285714285715, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52463054187192115, 0.48645320197044334, 0.50369458128078815, 0.55172413793103448, 0.39655172413793105, 0.49261083743842365, 0.46921182266009853, 0.50985221674876846, 0.48768472906403942, 0.41379310344827586, 0.50246305418719217], 5: [0.23029556650246305, 0.19211822660098521, 0.21551724137931033, 0.17610837438423646, 0.20566502463054187, 0.17980295566502463, 0.21428571428571427, 0.19827586206896552, 0.17980295566502463, 0.22906403940886699, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24507389162561577, 0.33004926108374383, 0.30911330049261082, 0.2894088669950739, 0.39162561576354682, 0.30665024630541871, 0.36206896551724138, 0.30541871921182268, 0.32266009852216748, 0.34852216748768472, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52463054187192115, 0.48645320197044334, 0.50123152709359609, 0.53201970443349755, 0.40640394088669951, 0.50369458128078815, 0.43349753694581283, 0.48522167487684731, 0.51354679802955661, 0.44211822660098521, 0.50123152709359609], 5: [0.23029556650246305, 0.18349753694581281, 0.18965517241379309, 0.17857142857142858, 0.2019704433497537, 0.18965517241379309, 0.20443349753694581, 0.20935960591133004, 0.16379310344827586, 0.20935960591133004, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24507389162561577, 0.30418719211822659, 0.30541871921182268, 0.25, 0.3645320197044335, 0.31896551724137934, 0.33990147783251229, 0.32758620689655171, 0.32389162561576357, 0.35960591133004927, 0.30049261083743845], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52463054187192115, 0.47906403940886699, 0.50369458128078815, 0.56527093596059108, 0.43349753694581283, 0.50985221674876846, 0.44211822660098521, 0.48645320197044334, 0.51354679802955661, 0.44211822660098521, 0.51477832512315269], 5: [0.23029556650246305, 0.21674876847290642, 0.19088669950738915, 0.18472906403940886, 0.2019704433497537, 0.17118226600985223, 0.21798029556650247, 0.18596059113300492, 0.1625615763546798, 0.19827586206896552, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148903 minutes
Weight histogram
[  185   432   798  1935  3917  5814 11852  7468  1886   138] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
[  141   161   244   362   509   922  1204  2716 10483 17683] [ -2.45821051e-04   8.52809200e-05   4.16382891e-04   7.47484862e-04
   1.07858683e-03   1.40968880e-03   1.74079078e-03   2.07189275e-03
   2.40299472e-03   2.73409669e-03   3.06519866e-03]
-1.32597
1.19368
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.29387
Epoch 1, cost is  2.26558
Epoch 2, cost is  2.24325
Epoch 3, cost is  2.22694
Epoch 4, cost is  2.20994
Training took 0.117016 minutes
Weight histogram
[6957 5405 5099 4243 3669 2624 2082 2269 1619  458] [ -6.84255809e-02  -6.15794512e-02  -5.47333216e-02  -4.78871919e-02
  -4.10410623e-02  -3.41949327e-02  -2.73488030e-02  -2.05026734e-02
  -1.36565438e-02  -6.81041412e-03   3.57155150e-05]
[2935 1656 1974 2542 2991 3459 4201 4364 5314 4989] [ -6.84255809e-02  -6.15794512e-02  -5.47333216e-02  -4.78871919e-02
  -4.10410623e-02  -3.41949327e-02  -2.73488030e-02  -2.05026734e-02
  -1.36565438e-02  -6.81041412e-03   3.57155150e-05]
-1.48763
2.0454
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148484 minutes
Weight histogram
[   36   324   649  1156  1318  1434 10922 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  293   351   489   738  1067  1470  1252  2311 12818 15661] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.114831 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 3200  883] [ -6.79385737e-02  -6.11411447e-02  -5.43437158e-02  -4.75462869e-02
  -4.07488580e-02  -3.39514291e-02  -2.71540002e-02  -2.03565712e-02
  -1.35591423e-02  -6.76171340e-03   3.57155150e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11411447e-02  -5.43437158e-02  -4.75462869e-02
  -4.07488580e-02  -3.39514291e-02  -2.71540002e-02  -2.03565712e-02
  -1.35591423e-02  -6.76171340e-03   3.57155150e-05]
-1.52949
1.62376
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.25108
Epoch 1, cost is  5.05654
Epoch 2, cost is  4.54507
Epoch 3, cost is  3.98334
Epoch 4, cost is  3.55809
Epoch 5, cost is  3.22358
Epoch 6, cost is  2.95444
Epoch 7, cost is  2.73723
Epoch 8, cost is  2.56741
Epoch 9, cost is  2.43224
Training took 0.679368 minutes
Weight histogram
[2740 3032 2618 1921 1923 3803   97   32   18   16] [-0.00679093 -0.0061247  -0.00545847 -0.00479224 -0.00412601 -0.00345978
 -0.00279356 -0.00212733 -0.0014611  -0.00079487 -0.00012864]
[3778 1019 1018 1117 1234 1336 1459 1635 1840 1764] [-0.00679093 -0.0061247  -0.00545847 -0.00479224 -0.00412601 -0.00345978
 -0.00279356 -0.00212733 -0.0014611  -0.00079487 -0.00012864]
-0.189124
0.20825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076889 minutes
Epoch 0
Fine tuning took 0.076986 minutes
Epoch 0
Fine tuning took 0.077519 minutes
Epoch 0
Fine tuning took 0.077261 minutes
Epoch 0
Fine tuning took 0.077298 minutes
Epoch 0
Fine tuning took 0.076696 minutes
Epoch 0
Fine tuning took 0.077169 minutes
Epoch 0
Fine tuning took 0.077357 minutes
Epoch 0
Fine tuning took 0.077302 minutes
Epoch 0
Fine tuning took 0.077503 minutes
{'zero': {0: [0.27832512315270935, 0.30418719211822659, 0.27955665024630544, 0.33866995073891626, 0.35960591133004927, 0.30665024630541871, 0.31034482758620691, 0.34729064039408869, 0.4039408866995074, 0.35344827586206895, 0.3608374384236453], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49014778325123154, 0.47536945812807879, 0.4963054187192118, 0.47290640394088668, 0.40640394088669951, 0.4642857142857143, 0.44458128078817732, 0.43472906403940886, 0.44704433497536944, 0.40640394088669951, 0.41625615763546797], 5: [0.23152709359605911, 0.22044334975369459, 0.22413793103448276, 0.18842364532019704, 0.23399014778325122, 0.22906403940886699, 0.24507389162561577, 0.21798029556650247, 0.14901477832512317, 0.24014778325123154, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27832512315270935, 0.34729064039408869, 0.30541871921182268, 0.35591133004926107, 0.38300492610837439, 0.37438423645320196, 0.34359605911330049, 0.37561576354679804, 0.39162561576354682, 0.37684729064039407, 0.35837438423645318], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49014778325123154, 0.45320197044334976, 0.49137931034482757, 0.44088669950738918, 0.39655172413793105, 0.3854679802955665, 0.39778325123152708, 0.37931034482758619, 0.4248768472906404, 0.39408866995073893, 0.42241379310344829], 5: [0.23152709359605911, 0.19950738916256158, 0.20320197044334976, 0.20320197044334976, 0.22044334975369459, 0.24014778325123154, 0.25862068965517243, 0.24507389162561577, 0.18349753694581281, 0.22906403940886699, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27832512315270935, 0.33374384236453203, 0.29926108374384236, 0.35221674876847292, 0.37192118226600984, 0.35098522167487683, 0.35467980295566504, 0.34852216748768472, 0.40270935960591131, 0.40270935960591131, 0.36330049261083741], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49014778325123154, 0.46182266009852219, 0.45812807881773399, 0.42364532019704432, 0.4039408866995074, 0.43103448275862066, 0.39655172413793105, 0.39778325123152708, 0.4211822660098522, 0.3854679802955665, 0.40517241379310343], 5: [0.23152709359605911, 0.20443349753694581, 0.24261083743842365, 0.22413793103448276, 0.22413793103448276, 0.21798029556650247, 0.24876847290640394, 0.2536945812807882, 0.17610837438423646, 0.21182266009852216, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27832512315270935, 0.35098522167487683, 0.28817733990147781, 0.33620689655172414, 0.38054187192118227, 0.35221674876847292, 0.34359605911330049, 0.35591133004926107, 0.38300492610837439, 0.40886699507389163, 0.35960591133004927], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49014778325123154, 0.45197044334975367, 0.48275862068965519, 0.43226600985221675, 0.41009852216748771, 0.40024630541871919, 0.38669950738916259, 0.39778325123152708, 0.4211822660098522, 0.3460591133004926, 0.42980295566502463], 5: [0.23152709359605911, 0.19704433497536947, 0.22906403940886699, 0.23152709359605911, 0.20935960591133004, 0.24753694581280788, 0.26970443349753692, 0.24630541871921183, 0.19581280788177341, 0.24507389162561577, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235149 minutes
Weight histogram
[  159   363   627   699  1253  3383  8304 12885  6118   634] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[  171   295   471   840   953  2073  3095  6482 10740  9305] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.54242
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.33463
Epoch 1, cost is  1.30867
Epoch 2, cost is  1.29031
Epoch 3, cost is  1.27885
Epoch 4, cost is  1.26829
Training took 0.221741 minutes
Weight histogram
[7335 6545 4492 4220 3132 2350 2018 1593 1514 1226] [ -6.71829507e-02  -6.04573732e-02  -5.37317957e-02  -4.70062182e-02
  -4.02806407e-02  -3.35550632e-02  -2.68294857e-02  -2.01039082e-02
  -1.33783307e-02  -6.65275321e-03   7.28242885e-05]
[2470 1600 2040 2417 2854 3488 3891 4824 5254 5587] [ -6.71829507e-02  -6.04573732e-02  -5.37317957e-02  -4.70062182e-02
  -4.02806407e-02  -3.35550632e-02  -2.68294857e-02  -2.01039082e-02
  -1.33783307e-02  -6.65275321e-03   7.28242885e-05]
-1.10226
1.82739
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149237 minutes
Weight histogram
[   35   342  1082  1497   754  1212 10917 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  507  1023  1064   343   542   929  1252  2311 12818 15661] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.114729 minutes
Weight histogram
[7605 5205 5048 4346 3612 2429 2175 1951 2396 1683] [ -6.79385737e-02  -6.11374339e-02  -5.43362941e-02  -4.75351543e-02
  -4.07340145e-02  -3.39328747e-02  -2.71317349e-02  -2.03305951e-02
  -1.35294553e-02  -6.72831551e-03   7.28242885e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11374339e-02  -5.43362941e-02  -4.75351543e-02
  -4.07340145e-02  -3.39328747e-02  -2.71317349e-02  -2.03305951e-02
  -1.35294553e-02  -6.72831551e-03   7.28242885e-05]
-1.52949
1.62376
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.32348
Epoch 1, cost is  6.04746
Epoch 2, cost is  5.78917
Epoch 3, cost is  5.41726
Epoch 4, cost is  5.05775
Epoch 5, cost is  4.7683
Epoch 6, cost is  4.54342
Epoch 7, cost is  4.34756
Epoch 8, cost is  4.17441
Epoch 9, cost is  4.02264
Training took 0.246936 minutes
Weight histogram
[1912 1977 1739 1674 1511 1777 2999 1868  676   67] [-0.02811378 -0.02532027 -0.02252675 -0.01973324 -0.01693973 -0.01414621
 -0.0113527  -0.00855918 -0.00576567 -0.00297216 -0.00017864]
[3252 1755 1150 1140 1233 1362 1473 1592 1670 1573] [-0.02811378 -0.02532027 -0.02252675 -0.01973324 -0.01693973 -0.01414621
 -0.0113527  -0.00855918 -0.00576567 -0.00297216 -0.00017864]
-0.37164
0.520956
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.075811 minutes
Epoch 0
Fine tuning took 0.076771 minutes
Epoch 0
Fine tuning took 0.077537 minutes
Epoch 0
Fine tuning took 0.075018 minutes
Epoch 0
Fine tuning took 0.077028 minutes
Epoch 0
Fine tuning took 0.075991 minutes
Epoch 0
Fine tuning took 0.077022 minutes
Epoch 0
Fine tuning took 0.076886 minutes
Epoch 0
Fine tuning took 0.076234 minutes
Epoch 0
Fine tuning took 0.075205 minutes
{'zero': {0: [0.30172413793103448, 0.34113300492610837, 0.42733990147783252, 0.52955665024630538, 0.48275862068965519, 0.51847290640394084, 0.36699507389162561, 0.38054187192118227, 0.46798029556650245, 0.4211822660098522, 0.37807881773399016], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43842364532019706, 0.46921182266009853, 0.34236453201970446, 0.21921182266009853, 0.2894088669950739, 0.3288177339901478, 0.41502463054187194, 0.43719211822660098, 0.3608374384236453, 0.47167487684729065, 0.46551724137931033], 5: [0.25985221674876846, 0.18965517241379309, 0.23029556650246305, 0.25123152709359609, 0.22783251231527094, 0.15270935960591134, 0.21798029556650247, 0.18226600985221675, 0.17118226600985223, 0.10714285714285714, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30172413793103448, 0.32142857142857145, 0.40640394088669951, 0.59359605911330049, 0.43965517241379309, 0.45812807881773399, 0.35221674876847292, 0.35960591133004927, 0.50492610837438423, 0.47167487684729065, 0.40640394088669951], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43842364532019706, 0.41748768472906406, 0.32635467980295568, 0.16871921182266009, 0.29310344827586204, 0.34236453201970446, 0.35837438423645318, 0.44334975369458129, 0.3682266009852217, 0.40640394088669951, 0.39408866995073893], 5: [0.25985221674876846, 0.26108374384236455, 0.26724137931034481, 0.2376847290640394, 0.26724137931034481, 0.19950738916256158, 0.2894088669950739, 0.19704433497536947, 0.1268472906403941, 0.12192118226600986, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30172413793103448, 0.35837438423645318, 0.41133004926108374, 0.56157635467980294, 0.43719211822660098, 0.52216748768472909, 0.29556650246305421, 0.3288177339901478, 0.43965517241379309, 0.42857142857142855, 0.35221674876847292], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43842364532019706, 0.40886699507389163, 0.32758620689655171, 0.20689655172413793, 0.25985221674876846, 0.29802955665024633, 0.3645320197044335, 0.45689655172413796, 0.3645320197044335, 0.41871921182266009, 0.43596059113300495], 5: [0.25985221674876846, 0.23275862068965517, 0.26108374384236455, 0.23152709359605911, 0.30295566502463056, 0.17980295566502463, 0.33990147783251229, 0.21428571428571427, 0.19581280788177341, 0.15270935960591134, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30172413793103448, 0.34236453201970446, 0.43965517241379309, 0.56527093596059108, 0.46551724137931033, 0.4605911330049261, 0.31157635467980294, 0.34359605911330049, 0.48275862068965519, 0.4642857142857143, 0.38669950738916259], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.43842364532019706, 0.40024630541871919, 0.32019704433497537, 0.20073891625615764, 0.24876847290640394, 0.32389162561576357, 0.3854679802955665, 0.45073891625615764, 0.38423645320197042, 0.41502463054187194, 0.40517241379310343], 5: [0.25985221674876846, 0.25738916256157635, 0.24014778325123154, 0.23399014778325122, 0.2857142857142857, 0.21551724137931033, 0.30295566502463056, 0.20566502463054187, 0.13300492610837439, 0.1206896551724138, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234524 minutes
Weight histogram
[  159   363   627   699  1253  3383  8304 12885  6118   634] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[  171   295   471   840   953  2073  3095  6482 10740  9305] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.54242
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.33463
Epoch 1, cost is  1.30867
Epoch 2, cost is  1.29031
Epoch 3, cost is  1.27885
Epoch 4, cost is  1.26829
Training took 0.223841 minutes
Weight histogram
[7335 6545 4492 4220 3132 2350 2018 1593 1514 1226] [ -6.71829507e-02  -6.04573732e-02  -5.37317957e-02  -4.70062182e-02
  -4.02806407e-02  -3.35550632e-02  -2.68294857e-02  -2.01039082e-02
  -1.33783307e-02  -6.65275321e-03   7.28242885e-05]
[2470 1600 2040 2417 2854 3488 3891 4824 5254 5587] [ -6.71829507e-02  -6.04573732e-02  -5.37317957e-02  -4.70062182e-02
  -4.02806407e-02  -3.35550632e-02  -2.68294857e-02  -2.01039082e-02
  -1.33783307e-02  -6.65275321e-03   7.28242885e-05]
-1.10226
1.82739
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148358 minutes
Weight histogram
[   35   342  1082  1497   754  1212 10917 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  507  1023  1064   343   542   929  1252  2311 12818 15661] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.114846 minutes
Weight histogram
[7605 5205 5048 4346 3612 2429 2175 1951 2396 1683] [ -6.79385737e-02  -6.11374339e-02  -5.43362941e-02  -4.75351543e-02
  -4.07340145e-02  -3.39328747e-02  -2.71317349e-02  -2.03305951e-02
  -1.35294553e-02  -6.72831551e-03   7.28242885e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11374339e-02  -5.43362941e-02  -4.75351543e-02
  -4.07340145e-02  -3.39328747e-02  -2.71317349e-02  -2.03305951e-02
  -1.35294553e-02  -6.72831551e-03   7.28242885e-05]
-1.52949
1.62376
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.73192
Epoch 1, cost is  5.4617
Epoch 2, cost is  5.1372
Epoch 3, cost is  4.6606
Epoch 4, cost is  4.25771
Epoch 5, cost is  3.96343
Epoch 6, cost is  3.71605
Epoch 7, cost is  3.50931
Epoch 8, cost is  3.33162
Epoch 9, cost is  3.18095
Training took 0.349859 minutes
Weight histogram
[2528 2389 2011 1698 1766 4125 1293  270   82   38] [-0.01845267 -0.0166218  -0.01479093 -0.01296005 -0.01112918 -0.00929831
 -0.00746743 -0.00563656 -0.00380569 -0.00197481 -0.00014394]
[4034 1123 1030 1097 1253 1402 1475 1574 1670 1542] [-0.01845267 -0.0166218  -0.01479093 -0.01296005 -0.01112918 -0.00929831
 -0.00746743 -0.00563656 -0.00380569 -0.00197481 -0.00014394]
-0.257221
0.3666
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082316 minutes
Epoch 0
Fine tuning took 0.082327 minutes
Epoch 0
Fine tuning took 0.082007 minutes
Epoch 0
Fine tuning took 0.082533 minutes
Epoch 0
Fine tuning took 0.080135 minutes
Epoch 0
Fine tuning took 0.082064 minutes
Epoch 0
Fine tuning took 0.082203 minutes
Epoch 0
Fine tuning took 0.081186 minutes
Epoch 0
Fine tuning took 0.081147 minutes
Epoch 0
Fine tuning took 0.082170 minutes
{'zero': {0: [0.33866995073891626, 0.25615763546798032, 0.25, 0.28694581280788178, 0.35344827586206895, 0.27216748768472904, 0.2376847290640394, 0.25246305418719212, 0.27832512315270935, 0.26354679802955666, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.57266009852216748, 0.59729064039408863, 0.54433497536945807, 0.51970443349753692, 0.58251231527093594, 0.57266009852216748, 0.5923645320197044, 0.56034482758620685, 0.58128078817733986, 0.60344827586206895], 5: [0.23399014778325122, 0.17118226600985223, 0.15270935960591134, 0.16871921182266009, 0.1268472906403941, 0.14532019704433496, 0.18965517241379309, 0.15517241379310345, 0.16133004926108374, 0.15517241379310345, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33866995073891626, 0.34975369458128081, 0.43349753694581283, 0.44211822660098521, 0.44827586206896552, 0.45443349753694579, 0.3891625615763547, 0.39408866995073893, 0.41133004926108374, 0.47413793103448276, 0.48768472906403942], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.41995073891625617, 0.35960591133004927, 0.34852216748768472, 0.35591133004926107, 0.40517241379310343, 0.39655172413793105, 0.41009852216748771, 0.36206896551724138, 0.39901477832512317, 0.3854679802955665], 5: [0.23399014778325122, 0.23029556650246305, 0.20689655172413793, 0.20935960591133004, 0.19581280788177341, 0.14039408866995073, 0.21428571428571427, 0.19581280788177341, 0.22660098522167488, 0.1268472906403941, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33866995073891626, 0.36699507389162561, 0.44088669950738918, 0.42857142857142855, 0.4642857142857143, 0.42364532019704432, 0.37192118226600984, 0.3608374384236453, 0.37561576354679804, 0.4248768472906404, 0.43842364532019706], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.38054187192118227, 0.33620689655172414, 0.39162561576354682, 0.33497536945812806, 0.42364532019704432, 0.42241379310344829, 0.43719211822660098, 0.41133004926108374, 0.41871921182266009, 0.4248768472906404], 5: [0.23399014778325122, 0.25246305418719212, 0.2229064039408867, 0.17980295566502463, 0.20073891625615764, 0.15270935960591134, 0.20566502463054187, 0.2019704433497537, 0.21305418719211822, 0.15640394088669951, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33866995073891626, 0.35960591133004927, 0.47413793103448276, 0.45566502463054187, 0.50862068965517238, 0.48891625615763545, 0.39039408866995073, 0.40147783251231528, 0.44088669950738918, 0.48152709359605911, 0.51600985221674878], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.42364532019704432, 0.3251231527093596, 0.36330049261083741, 0.3251231527093596, 0.36576354679802958, 0.42610837438423643, 0.40763546798029554, 0.35344827586206895, 0.38669950738916259, 0.36330049261083741], 5: [0.23399014778325122, 0.21674876847290642, 0.20073891625615764, 0.18103448275862069, 0.16625615763546797, 0.14532019704433496, 0.18349753694581281, 0.19088669950738915, 0.20566502463054187, 0.13177339901477833, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234453 minutes
Weight histogram
[  159   363   627   699  1253  3383  8304 12885  6118   634] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[  171   295   471   840   953  2073  3095  6482 10740  9305] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.54242
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.33463
Epoch 1, cost is  1.30867
Epoch 2, cost is  1.29031
Epoch 3, cost is  1.27885
Epoch 4, cost is  1.26829
Training took 0.223380 minutes
Weight histogram
[7335 6545 4492 4220 3132 2350 2018 1593 1514 1226] [ -6.71829507e-02  -6.04573732e-02  -5.37317957e-02  -4.70062182e-02
  -4.02806407e-02  -3.35550632e-02  -2.68294857e-02  -2.01039082e-02
  -1.33783307e-02  -6.65275321e-03   7.28242885e-05]
[2470 1600 2040 2417 2854 3488 3891 4824 5254 5587] [ -6.71829507e-02  -6.04573732e-02  -5.37317957e-02  -4.70062182e-02
  -4.02806407e-02  -3.35550632e-02  -2.68294857e-02  -2.01039082e-02
  -1.33783307e-02  -6.65275321e-03   7.28242885e-05]
-1.10226
1.82739
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147868 minutes
Weight histogram
[   35   342  1082  1497   754  1212 10917 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  507  1023  1064   343   542   929  1252  2311 12818 15661] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.115389 minutes
Weight histogram
[7605 5205 5048 4346 3612 2429 2175 1951 2396 1683] [ -6.79385737e-02  -6.11374339e-02  -5.43362941e-02  -4.75351543e-02
  -4.07340145e-02  -3.39328747e-02  -2.71317349e-02  -2.03305951e-02
  -1.35294553e-02  -6.72831551e-03   7.28242885e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11374339e-02  -5.43362941e-02  -4.75351543e-02
  -4.07340145e-02  -3.39328747e-02  -2.71317349e-02  -2.03305951e-02
  -1.35294553e-02  -6.72831551e-03   7.28242885e-05]
-1.52949
1.62376
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.16653
Epoch 1, cost is  4.94099
Epoch 2, cost is  4.61355
Epoch 3, cost is  4.15
Epoch 4, cost is  3.77154
Epoch 5, cost is  3.47482
Epoch 6, cost is  3.22752
Epoch 7, cost is  3.02207
Epoch 8, cost is  2.8509
Epoch 9, cost is  2.69939
Training took 0.542912 minutes
Weight histogram
[3137 3113 2578 5563 1090  382  176   91   42   28] [-0.01305922 -0.01176709 -0.01047497 -0.00918284 -0.00789072 -0.0065986
 -0.00530647 -0.00401435 -0.00272222 -0.0014301  -0.00013798]
[4070 1099 1052 1127 1249 1353 1445 1558 1687 1560] [-0.01305922 -0.01176709 -0.01047497 -0.00918284 -0.00789072 -0.0065986
 -0.00530647 -0.00401435 -0.00272222 -0.0014301  -0.00013798]
-0.209465
0.254176
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.091051 minutes
Epoch 0
Fine tuning took 0.090431 minutes
Epoch 0
Fine tuning took 0.090942 minutes
Epoch 0
Fine tuning took 0.090893 minutes
Epoch 0
Fine tuning took 0.090511 minutes
Epoch 0
Fine tuning took 0.090854 minutes
Epoch 0
Fine tuning took 0.091617 minutes
Epoch 0
Fine tuning took 0.091188 minutes
Epoch 0
Fine tuning took 0.091747 minutes
Epoch 0
Fine tuning took 0.090483 minutes
{'zero': {0: [0.35221674876847292, 0.27709359605911332, 0.2376847290640394, 0.27339901477832512, 0.3608374384236453, 0.30788177339901479, 0.26724137931034481, 0.26231527093596058, 0.29187192118226601, 0.27093596059113301, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.54433497536945807, 0.58866995073891626, 0.52463054187192115, 0.50862068965517238, 0.50492610837438423, 0.52832512315270941, 0.56157635467980294, 0.54187192118226601, 0.5357142857142857, 0.56650246305418717], 5: [0.24014778325123154, 0.17857142857142858, 0.17364532019704434, 0.2019704433497537, 0.13054187192118227, 0.18719211822660098, 0.20443349753694581, 0.17610837438423646, 0.16625615763546797, 0.19334975369458129, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.35221674876847292, 0.34236453201970446, 0.33251231527093594, 0.32758620689655171, 0.40024630541871919, 0.40147783251231528, 0.35221674876847292, 0.31527093596059114, 0.35960591133004927, 0.33743842364532017, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.47783251231527096, 0.4963054187192118, 0.47290640394088668, 0.43965517241379309, 0.39532019704433496, 0.44458128078817732, 0.5073891625615764, 0.46921182266009853, 0.44334975369458129, 0.51108374384236455], 5: [0.24014778325123154, 0.17980295566502463, 0.17118226600985223, 0.19950738916256158, 0.16009852216748768, 0.20320197044334976, 0.20320197044334976, 0.17733990147783252, 0.17118226600985223, 0.21921182266009853, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.35221674876847292, 0.3251231527093596, 0.30665024630541871, 0.32758620689655171, 0.35221674876847292, 0.36206896551724138, 0.35098522167487683, 0.32758620689655171, 0.35714285714285715, 0.34359605911330049, 0.36699507389162561], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.47290640394088668, 0.53325123152709364, 0.48522167487684731, 0.48029556650246308, 0.41748768472906406, 0.47167487684729065, 0.49384236453201968, 0.48029556650246308, 0.44704433497536944, 0.43842364532019706], 5: [0.24014778325123154, 0.2019704433497537, 0.16009852216748768, 0.18719211822660098, 0.16748768472906403, 0.22044334975369459, 0.17733990147783252, 0.17857142857142858, 0.1625615763546798, 0.20935960591133004, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.35221674876847292, 0.31896551724137934, 0.32389162561576357, 0.32019704433497537, 0.3817733990147783, 0.37438423645320196, 0.3817733990147783, 0.33251231527093594, 0.35467980295566504, 0.34975369458128081, 0.32389162561576357], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.48399014778325122, 0.49507389162561577, 0.50123152709359609, 0.45566502463054187, 0.43103448275862066, 0.44211822660098521, 0.48029556650246308, 0.47783251231527096, 0.42733990147783252, 0.49261083743842365], 5: [0.24014778325123154, 0.19704433497536947, 0.18103448275862069, 0.17857142857142858, 0.1625615763546798, 0.19458128078817735, 0.17610837438423646, 0.18719211822660098, 0.16748768472906403, 0.2229064039408867, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234702 minutes
Weight histogram
[  159   363   627   699  1253  3383  8304 12885  6118   634] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
[  171   295   471   840   953  2073  3095  6482 10740  9305] [ -1.35047070e-04   4.24188504e-05   2.19884771e-04   3.97350691e-04
   5.74816612e-04   7.52282533e-04   9.29748453e-04   1.10721437e-03
   1.28468029e-03   1.46214621e-03   1.63961214e-03]
-1.30708
1.54242
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.33463
Epoch 1, cost is  1.30867
Epoch 2, cost is  1.29031
Epoch 3, cost is  1.27885
Epoch 4, cost is  1.26829
Training took 0.222584 minutes
Weight histogram
[7335 6545 4492 4220 3132 2350 2018 1593 1514 1226] [ -6.71829507e-02  -6.04573732e-02  -5.37317957e-02  -4.70062182e-02
  -4.02806407e-02  -3.35550632e-02  -2.68294857e-02  -2.01039082e-02
  -1.33783307e-02  -6.65275321e-03   7.28242885e-05]
[2470 1600 2040 2417 2854 3488 3891 4824 5254 5587] [ -6.71829507e-02  -6.04573732e-02  -5.37317957e-02  -4.70062182e-02
  -4.02806407e-02  -3.35550632e-02  -2.68294857e-02  -2.01039082e-02
  -1.33783307e-02  -6.65275321e-03   7.28242885e-05]
-1.10226
1.82739
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146370 minutes
Weight histogram
[   35   342  1082  1497   754  1212 10917 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[  507  1023  1064   343   542   929  1252  2311 12818 15661] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.114616 minutes
Weight histogram
[7605 5205 5048 4346 3612 2429 2175 1951 2396 1683] [ -6.79385737e-02  -6.11374339e-02  -5.43362941e-02  -4.75351543e-02
  -4.07340145e-02  -3.39328747e-02  -2.71317349e-02  -2.03305951e-02
  -1.35294553e-02  -6.72831551e-03   7.28242885e-05]
[4895 1697 2033 2724 3177 3398 4249 4388 5127 4762] [ -6.79385737e-02  -6.11374339e-02  -5.43362941e-02  -4.75351543e-02
  -4.07340145e-02  -3.39328747e-02  -2.71317349e-02  -2.03305951e-02
  -1.35294553e-02  -6.72831551e-03   7.28242885e-05]
-1.52949
1.62376
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.96121
Epoch 1, cost is  4.65943
Epoch 2, cost is  4.04509
Epoch 3, cost is  3.55244
Epoch 4, cost is  3.19429
Epoch 5, cost is  2.90581
Epoch 6, cost is  2.67829
Epoch 7, cost is  2.49274
Epoch 8, cost is  2.34452
Epoch 9, cost is  2.22304
Training took 0.940461 minutes
Weight histogram
[3264 3166 2792 2010 3206 1543  127   50   25   17] [-0.00805968 -0.00726726 -0.00647483 -0.00568241 -0.00488999 -0.00409757
 -0.00330515 -0.00251272 -0.0017203  -0.00092788 -0.00013546]
[3343 1011 1046 1195 1296 1406 1542 1707 1881 1773] [-0.00805968 -0.00726726 -0.00647483 -0.00568241 -0.00488999 -0.00409757
 -0.00330515 -0.00251272 -0.0017203  -0.00092788 -0.00013546]
-0.189497
0.19206
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.109484 minutes
Epoch 0
Fine tuning took 0.109467 minutes
Epoch 0
Fine tuning took 0.108777 minutes
Epoch 0
Fine tuning took 0.110565 minutes
Epoch 0
Fine tuning took 0.110174 minutes
Epoch 0
Fine tuning took 0.110117 minutes
Epoch 0
Fine tuning took 0.108731 minutes
Epoch 0
Fine tuning took 0.110213 minutes
Epoch 0
Fine tuning took 0.109304 minutes
Epoch 0
Fine tuning took 0.109865 minutes
{'zero': {0: [0.26847290640394089, 0.31650246305418717, 0.27709359605911332, 0.26847290640394089, 0.3682266009852217, 0.2894088669950739, 0.31773399014778325, 0.28201970443349755, 0.38793103448275862, 0.39162561576354682, 0.30049261083743845], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48891625615763545, 0.48029556650246308, 0.50492610837438423, 0.54926108374384242, 0.39532019704433496, 0.48645320197044334, 0.42364532019704432, 0.49014778325123154, 0.43842364532019706, 0.41133004926108374, 0.50492610837438423], 5: [0.24261083743842365, 0.20320197044334976, 0.21798029556650247, 0.18226600985221675, 0.23645320197044334, 0.22413793103448276, 0.25862068965517243, 0.22783251231527094, 0.17364532019704434, 0.19704433497536947, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26847290640394089, 0.34852216748768472, 0.28448275862068967, 0.35591133004926107, 0.36699507389162561, 0.34482758620689657, 0.35221674876847292, 0.3251231527093596, 0.3891625615763547, 0.40024630541871919, 0.32389162561576357], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48891625615763545, 0.47536945812807879, 0.49014778325123154, 0.47413793103448276, 0.39778325123152708, 0.45566502463054187, 0.39778325123152708, 0.45320197044334976, 0.43596059113300495, 0.35837438423645318, 0.48152709359605911], 5: [0.24261083743842365, 0.17610837438423646, 0.22536945812807882, 0.16995073891625614, 0.23522167487684728, 0.19950738916256158, 0.25, 0.22167487684729065, 0.1748768472906404, 0.2413793103448276, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26847290640394089, 0.33620689655172414, 0.30049261083743845, 0.29187192118226601, 0.39285714285714285, 0.35221674876847292, 0.38793103448275862, 0.34975369458128081, 0.40147783251231528, 0.41009852216748771, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48891625615763545, 0.46182266009852219, 0.47290640394088668, 0.53325123152709364, 0.37192118226600984, 0.45689655172413796, 0.36330049261083741, 0.44334975369458129, 0.43349753694581283, 0.37315270935960593, 0.43596059113300495], 5: [0.24261083743842365, 0.2019704433497537, 0.22660098522167488, 0.1748768472906404, 0.23522167487684728, 0.19088669950738915, 0.24876847290640394, 0.20689655172413793, 0.16502463054187191, 0.21674876847290642, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26847290640394089, 0.3460591133004926, 0.29802955665024633, 0.31403940886699505, 0.35591133004926107, 0.33990147783251229, 0.38300492610837439, 0.32266009852216748, 0.35591133004926107, 0.40763546798029554, 0.36699507389162561], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.48891625615763545, 0.45566502463054187, 0.48029556650246308, 0.5073891625615764, 0.42980295566502463, 0.44950738916256155, 0.37192118226600984, 0.45197044334975367, 0.48275862068965519, 0.37192118226600984, 0.43349753694581283], 5: [0.24261083743842365, 0.19827586206896552, 0.22167487684729065, 0.17857142857142858, 0.21428571428571427, 0.2105911330049261, 0.24507389162561577, 0.22536945812807882, 0.16133004926108374, 0.22044334975369459, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420590 minutes
Weight histogram
[  118   345   521   655   976  5161 12072 10133  3960   484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1343 1013 1098 1553 2559 3303 4263 4785 7893 6615] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.10379
1.28282
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.817302
Epoch 1, cost is  0.798184
Epoch 2, cost is  0.787897
Epoch 3, cost is  0.778323
Epoch 4, cost is  0.771252
Training took 0.643682 minutes
Weight histogram
[8638 6830 5006 3879 2991 1928 1566 1403 1031 1153] [ -4.76838350e-02  -4.29206986e-02  -3.81575622e-02  -3.33944257e-02
  -2.86312893e-02  -2.38681529e-02  -1.91050164e-02  -1.43418800e-02
  -9.57874357e-03  -4.81560714e-03  -5.24707102e-05]
[2181 1574 1798 2306 2807 3369 3801 4876 5568 6145] [ -4.76838350e-02  -4.29206986e-02  -3.81575622e-02  -3.33944257e-02
  -2.86312893e-02  -2.38681529e-02  -1.91050164e-02  -1.43418800e-02
  -9.57874357e-03  -4.81560714e-03  -5.24707102e-05]
-0.871461
1.64627
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147273 minutes
Weight histogram
[   35   356  1835   831   653  1212 10917 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[ 2172   186   235   343   538   933  1251  2303 12825 15664] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.115896 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 2079 2004] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
[4893 1697 2033 2724 3177 3398 4250 4388 5128 4762] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
-1.52949
1.62376
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.4588
Epoch 1, cost is  6.09772
Epoch 2, cost is  5.63459
Epoch 3, cost is  5.23703
Epoch 4, cost is  4.92598
Epoch 5, cost is  4.67644
Epoch 6, cost is  4.46747
Epoch 7, cost is  4.29133
Epoch 8, cost is  4.13858
Epoch 9, cost is  4.0031
Training took 0.321003 minutes
Weight histogram
[1831 2046 1875 1666 1491 1338 1295 1582 2612  464] [-0.03244205 -0.02921504 -0.02598804 -0.02276103 -0.01953402 -0.01630702
 -0.01308001 -0.009853   -0.006626   -0.00339899 -0.00017198]
[2690 1192 1144 1244 1374 1510 1637 1730 1848 1831] [-0.03244205 -0.02921504 -0.02598804 -0.02276103 -0.01953402 -0.01630702
 -0.01308001 -0.009853   -0.006626   -0.00339899 -0.00017198]
-0.425022
0.683805
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.142195 minutes
Epoch 0
Fine tuning took 0.141160 minutes
Epoch 0
Fine tuning took 0.141409 minutes
Epoch 0
Fine tuning took 0.142754 minutes
Epoch 0
Fine tuning took 0.143009 minutes
Epoch 0
Fine tuning took 0.142300 minutes
Epoch 0
Fine tuning took 0.142732 minutes
Epoch 0
Fine tuning took 0.140631 minutes
Epoch 0
Fine tuning took 0.142811 minutes
Epoch 0
Fine tuning took 0.142510 minutes
{'zero': {0: [0.3891625615763547, 0.33866995073891626, 0.41502463054187194, 0.39532019704433496, 0.29310344827586204, 0.4248768472906404, 0.48029556650246308, 0.29679802955665024, 0.28325123152709358, 0.56403940886699511, 0.45073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37807881773399016, 0.39778325123152708, 0.055418719211822662, 0.21428571428571427, 0.55541871921182262, 0.44211822660098521, 0.25862068965517243, 0.46674876847290642, 0.62561576354679804, 0.24507389162561577, 0.4642857142857143], 5: [0.23275862068965517, 0.26354679802955666, 0.52955665024630538, 0.39039408866995073, 0.15147783251231528, 0.13300492610837439, 0.26108374384236455, 0.23645320197044334, 0.091133004926108374, 0.19088669950738915, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3891625615763547, 0.31896551724137934, 0.43103448275862066, 0.39162561576354682, 0.2536945812807882, 0.43103448275862066, 0.44088669950738918, 0.26847290640394089, 0.27463054187192121, 0.57389162561576357, 0.41256157635467983], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37807881773399016, 0.42733990147783252, 0.055418719211822662, 0.22536945812807882, 0.56527093596059108, 0.42857142857142855, 0.22906403940886699, 0.50492610837438423, 0.62561576354679804, 0.18596059113300492, 0.49261083743842365], 5: [0.23275862068965517, 0.2536945812807882, 0.51354679802955661, 0.38300492610837439, 0.18103448275862069, 0.14039408866995073, 0.33004926108374383, 0.22660098522167488, 0.099753694581280791, 0.24014778325123154, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3891625615763547, 0.32266009852216748, 0.41871921182266009, 0.43349753694581283, 0.26847290640394089, 0.41379310344827586, 0.41133004926108374, 0.32266009852216748, 0.27216748768472904, 0.55049261083743839, 0.43965517241379309], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37807881773399016, 0.41748768472906406, 0.051724137931034482, 0.2019704433497537, 0.56403940886699511, 0.39901477832512317, 0.25123152709359609, 0.43472906403940886, 0.63177339901477836, 0.18842364532019704, 0.44581280788177341], 5: [0.23275862068965517, 0.25985221674876846, 0.52955665024630538, 0.3645320197044335, 0.16748768472906403, 0.18719211822660098, 0.33743842364532017, 0.24261083743842365, 0.096059113300492605, 0.26108374384236455, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3891625615763547, 0.3251231527093596, 0.42610837438423643, 0.40517241379310343, 0.27709359605911332, 0.41995073891625617, 0.42241379310344829, 0.30418719211822659, 0.25492610837438423, 0.56157635467980294, 0.42857142857142855], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.37807881773399016, 0.39655172413793105, 0.039408866995073892, 0.23152709359605911, 0.55295566502463056, 0.42733990147783252, 0.26108374384236455, 0.47906403940886699, 0.62561576354679804, 0.17364532019704434, 0.46921182266009853], 5: [0.23275862068965517, 0.27832512315270935, 0.53448275862068961, 0.36330049261083741, 0.16995073891625614, 0.15270935960591134, 0.31650246305418717, 0.21674876847290642, 0.11945812807881774, 0.26477832512315269, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.419940 minutes
Weight histogram
[  118   345   521   655   976  5161 12072 10133  3960   484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1343 1013 1098 1553 2559 3303 4263 4785 7893 6615] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.10379
1.28282
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.817302
Epoch 1, cost is  0.798184
Epoch 2, cost is  0.787897
Epoch 3, cost is  0.778323
Epoch 4, cost is  0.771252
Training took 0.645635 minutes
Weight histogram
[8638 6830 5006 3879 2991 1928 1566 1403 1031 1153] [ -4.76838350e-02  -4.29206986e-02  -3.81575622e-02  -3.33944257e-02
  -2.86312893e-02  -2.38681529e-02  -1.91050164e-02  -1.43418800e-02
  -9.57874357e-03  -4.81560714e-03  -5.24707102e-05]
[2181 1574 1798 2306 2807 3369 3801 4876 5568 6145] [ -4.76838350e-02  -4.29206986e-02  -3.81575622e-02  -3.33944257e-02
  -2.86312893e-02  -2.38681529e-02  -1.91050164e-02  -1.43418800e-02
  -9.57874357e-03  -4.81560714e-03  -5.24707102e-05]
-0.871461
1.64627
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148626 minutes
Weight histogram
[   35   356  1835   831   653  1212 10917 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[ 2172   186   235   343   538   933  1251  2303 12825 15664] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.115217 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 2079 2004] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
[4893 1697 2033 2724 3177 3398 4250 4388 5128 4762] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
-1.52949
1.62376
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.98136
Epoch 1, cost is  5.52177
Epoch 2, cost is  4.93272
Epoch 3, cost is  4.47504
Epoch 4, cost is  4.11312
Epoch 5, cost is  3.82138
Epoch 6, cost is  3.58566
Epoch 7, cost is  3.40071
Epoch 8, cost is  3.24363
Epoch 9, cost is  3.11096
Training took 0.495333 minutes
Weight histogram
[2146 2205 2017 1844 1645 1426 1365 2636  860   56] [-0.02037968 -0.01835723 -0.01633477 -0.01431231 -0.01228985 -0.0102674
 -0.00824494 -0.00622248 -0.00420003 -0.00217757 -0.00015511]
[2827 1149 1225 1307 1360 1436 1561 1686 1852 1797] [-0.02037968 -0.01835723 -0.01633477 -0.01431231 -0.01228985 -0.0102674
 -0.00824494 -0.00622248 -0.00420003 -0.00217757 -0.00015511]
-0.282819
0.418584
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.150532 minutes
Epoch 0
Fine tuning took 0.150148 minutes
Epoch 0
Fine tuning took 0.150580 minutes
Epoch 0
Fine tuning took 0.149448 minutes
Epoch 0
Fine tuning took 0.149759 minutes
Epoch 0
Fine tuning took 0.148747 minutes
Epoch 0
Fine tuning took 0.149810 minutes
Epoch 0
Fine tuning took 0.150914 minutes
Epoch 0
Fine tuning took 0.151248 minutes
Epoch 0
Fine tuning took 0.150924 minutes
{'zero': {0: [0.29556650246305421, 0.25, 0.26970443349753692, 0.30295566502463056, 0.53940886699507384, 0.39285714285714285, 0.28325123152709358, 0.34236453201970446, 0.29926108374384236, 0.35591133004926107, 0.33128078817733991], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42364532019704432, 0.55295566502463056, 0.50862068965517238, 0.44211822660098521, 0.32389162561576357, 0.36576354679802958, 0.53325123152709364, 0.51108374384236455, 0.51231527093596063, 0.51600985221674878, 0.4963054187192118], 5: [0.28078817733990147, 0.19704433497536947, 0.22167487684729065, 0.25492610837438423, 0.13669950738916256, 0.2413793103448276, 0.18349753694581281, 0.14655172413793102, 0.18842364532019704, 0.12807881773399016, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29556650246305421, 0.30665024630541871, 0.34852216748768472, 0.4211822660098522, 0.54802955665024633, 0.55788177339901479, 0.37807881773399016, 0.42241379310344829, 0.46551724137931033, 0.57019704433497542, 0.48522167487684731], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42364532019704432, 0.4642857142857143, 0.41133004926108374, 0.26477832512315269, 0.24753694581280788, 0.2413793103448276, 0.41502463054187194, 0.40763546798029554, 0.33620689655172414, 0.29433497536945813, 0.33990147783251229], 5: [0.28078817733990147, 0.22906403940886699, 0.24014778325123154, 0.31403940886699505, 0.20443349753694581, 0.20073891625615764, 0.20689655172413793, 0.16995073891625614, 0.19827586206896552, 0.1354679802955665, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29556650246305421, 0.33743842364532017, 0.33620689655172414, 0.44950738916256155, 0.55911330049261088, 0.52463054187192115, 0.41871921182266009, 0.42857142857142855, 0.44704433497536944, 0.56157635467980294, 0.4211822660098522], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42364532019704432, 0.43965517241379309, 0.37561576354679804, 0.24630541871921183, 0.21428571428571427, 0.25246305418719212, 0.37561576354679804, 0.3891625615763547, 0.3460591133004926, 0.31034482758620691, 0.37931034482758619], 5: [0.28078817733990147, 0.2229064039408867, 0.28817733990147781, 0.30418719211822659, 0.22660098522167488, 0.2229064039408867, 0.20566502463054187, 0.18226600985221675, 0.20689655172413793, 0.12807881773399016, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29556650246305421, 0.3645320197044335, 0.37807881773399016, 0.4642857142857143, 0.56527093596059108, 0.57758620689655171, 0.43719211822660098, 0.45320197044334976, 0.47413793103448276, 0.58743842364532017, 0.4642857142857143], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42364532019704432, 0.44088669950738918, 0.38300492610837439, 0.25615763546798032, 0.25, 0.22660098522167488, 0.34729064039408869, 0.36576354679802958, 0.35714285714285715, 0.28817733990147781, 0.34975369458128081], 5: [0.28078817733990147, 0.19458128078817735, 0.23891625615763548, 0.27955665024630544, 0.18472906403940886, 0.19581280788177341, 0.21551724137931033, 0.18103448275862069, 0.16871921182266009, 0.12438423645320197, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421889 minutes
Weight histogram
[  118   345   521   655   976  5161 12072 10133  3960   484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1343 1013 1098 1553 2559 3303 4263 4785 7893 6615] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.10379
1.28282
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.817302
Epoch 1, cost is  0.798184
Epoch 2, cost is  0.787897
Epoch 3, cost is  0.778323
Epoch 4, cost is  0.771252
Training took 0.644749 minutes
Weight histogram
[8638 6830 5006 3879 2991 1928 1566 1403 1031 1153] [ -4.76838350e-02  -4.29206986e-02  -3.81575622e-02  -3.33944257e-02
  -2.86312893e-02  -2.38681529e-02  -1.91050164e-02  -1.43418800e-02
  -9.57874357e-03  -4.81560714e-03  -5.24707102e-05]
[2181 1574 1798 2306 2807 3369 3801 4876 5568 6145] [ -4.76838350e-02  -4.29206986e-02  -3.81575622e-02  -3.33944257e-02
  -2.86312893e-02  -2.38681529e-02  -1.91050164e-02  -1.43418800e-02
  -9.57874357e-03  -4.81560714e-03  -5.24707102e-05]
-0.871461
1.64627
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150214 minutes
Weight histogram
[   35   356  1835   831   653  1212 10917 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[ 2172   186   235   343   538   933  1251  2303 12825 15664] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.117949 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 2079 2004] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
[4893 1697 2033 2724 3177 3398 4250 4388 5128 4762] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
-1.52949
1.62376
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.39018
Epoch 1, cost is  4.88552
Epoch 2, cost is  4.27869
Epoch 3, cost is  3.8151
Epoch 4, cost is  3.46666
Epoch 5, cost is  3.20563
Epoch 6, cost is  2.99973
Epoch 7, cost is  2.83268
Epoch 8, cost is  2.69803
Epoch 9, cost is  2.58476
Training took 0.816272 minutes
Weight histogram
[2734 2618 2213 1971 1772 1462 2875  445   78   32] [-0.01366388 -0.01231151 -0.01095913 -0.00960675 -0.00825437 -0.006902
 -0.00554962 -0.00419724 -0.00284486 -0.00149249 -0.00014011]
[2874 1157 1234 1253 1304 1432 1582 1708 1877 1779] [-0.01366388 -0.01231151 -0.01095913 -0.00960675 -0.00825437 -0.006902
 -0.00554962 -0.00419724 -0.00284486 -0.00149249 -0.00014011]
-0.246877
0.295594
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.162844 minutes
Epoch 0
Fine tuning took 0.163472 minutes
Epoch 0
Fine tuning took 0.165140 minutes
Epoch 0
Fine tuning took 0.165762 minutes
Epoch 0
Fine tuning took 0.163570 minutes
Epoch 0
Fine tuning took 0.165844 minutes
Epoch 0
Fine tuning took 0.166151 minutes
Epoch 0
Fine tuning took 0.165755 minutes
Epoch 0
Fine tuning took 0.165857 minutes
Epoch 0
Fine tuning took 0.166397 minutes
{'zero': {0: [0.35837438423645318, 0.29064039408866993, 0.34729064039408869, 0.30295566502463056, 0.37561576354679804, 0.34113300492610837, 0.34359605911330049, 0.33128078817733991, 0.32635467980295568, 0.2894088669950739, 0.33128078817733991], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4039408866995074, 0.49876847290640391, 0.47044334975369456, 0.48645320197044334, 0.44704433497536944, 0.44704433497536944, 0.48152709359605911, 0.47044334975369456, 0.4963054187192118, 0.5, 0.47536945812807879], 5: [0.2376847290640394, 0.2105911330049261, 0.18226600985221675, 0.2105911330049261, 0.17733990147783252, 0.21182266009852216, 0.1748768472906404, 0.19827586206896552, 0.17733990147783252, 0.2105911330049261, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.35837438423645318, 0.29310344827586204, 0.37561576354679804, 0.33866995073891626, 0.4211822660098522, 0.35960591133004927, 0.41871921182266009, 0.38669950738916259, 0.35221674876847292, 0.31650246305418717, 0.37807881773399016], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4039408866995074, 0.48152709359605911, 0.40147783251231528, 0.44458128078817732, 0.39655172413793105, 0.41625615763546797, 0.38793103448275862, 0.41995073891625617, 0.45197044334975367, 0.47413793103448276, 0.44088669950738918], 5: [0.2376847290640394, 0.22536945812807882, 0.2229064039408867, 0.21674876847290642, 0.18226600985221675, 0.22413793103448276, 0.19334975369458129, 0.19334975369458129, 0.19581280788177341, 0.20935960591133004, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.35837438423645318, 0.33620689655172414, 0.39039408866995073, 0.33743842364532017, 0.43349753694581283, 0.40147783251231528, 0.40517241379310343, 0.34975369458128081, 0.36206896551724138, 0.31896551724137934, 0.33620689655172414], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4039408866995074, 0.42980295566502463, 0.41748768472906406, 0.46305418719211822, 0.36699507389162561, 0.3891625615763547, 0.41009852216748771, 0.42364532019704432, 0.43226600985221675, 0.43719211822660098, 0.45935960591133007], 5: [0.2376847290640394, 0.23399014778325122, 0.19211822660098521, 0.19950738916256158, 0.19950738916256158, 0.20935960591133004, 0.18472906403940886, 0.22660098522167488, 0.20566502463054187, 0.24384236453201971, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.35837438423645318, 0.33620689655172414, 0.39039408866995073, 0.35960591133004927, 0.41133004926108374, 0.44211822660098521, 0.3645320197044335, 0.37192118226600984, 0.3682266009852217, 0.31157635467980294, 0.36576354679802958], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4039408866995074, 0.47783251231527096, 0.41009852216748771, 0.43349753694581283, 0.4039408866995074, 0.37684729064039407, 0.42857142857142855, 0.40763546798029554, 0.44088669950738918, 0.44458128078817732, 0.44827586206896552], 5: [0.2376847290640394, 0.18596059113300492, 0.19950738916256158, 0.20689655172413793, 0.18472906403940886, 0.18103448275862069, 0.20689655172413793, 0.22044334975369459, 0.19088669950738915, 0.24384236453201971, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.419906 minutes
Weight histogram
[  118   345   521   655   976  5161 12072 10133  3960   484] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
[1343 1013 1098 1553 2559 3303 4263 4785 7893 6615] [ -8.25641619e-05   1.58841460e-05   1.14332454e-04   2.12780762e-04
   3.11229069e-04   4.09677377e-04   5.08125685e-04   6.06573993e-04
   7.05022301e-04   8.03470609e-04   9.01918916e-04]
-1.10379
1.28282
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.817302
Epoch 1, cost is  0.798184
Epoch 2, cost is  0.787897
Epoch 3, cost is  0.778323
Epoch 4, cost is  0.771252
Training took 0.644002 minutes
Weight histogram
[8638 6830 5006 3879 2991 1928 1566 1403 1031 1153] [ -4.76838350e-02  -4.29206986e-02  -3.81575622e-02  -3.33944257e-02
  -2.86312893e-02  -2.38681529e-02  -1.91050164e-02  -1.43418800e-02
  -9.57874357e-03  -4.81560714e-03  -5.24707102e-05]
[2181 1574 1798 2306 2807 3369 3801 4876 5568 6145] [ -4.76838350e-02  -4.29206986e-02  -3.81575622e-02  -3.33944257e-02
  -2.86312893e-02  -2.38681529e-02  -1.91050164e-02  -1.43418800e-02
  -9.57874357e-03  -4.81560714e-03  -5.24707102e-05]
-0.871461
1.64627
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148532 minutes
Weight histogram
[   35   356  1835   831   653  1212 10917 15215  5056   340] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
[ 2172   186   235   343   538   933  1251  2303 12825 15664] [ -5.43409726e-04  -2.43846374e-04   5.57169784e-05   3.55280330e-04
   6.54843682e-04   9.54407034e-04   1.25397039e-03   1.55353374e-03
   1.85309709e-03   2.15266044e-03   2.45222379e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.41124
Epoch 1, cost is  2.38919
Epoch 2, cost is  2.35968
Epoch 3, cost is  2.34256
Epoch 4, cost is  2.33062
Training took 0.115842 minutes
Weight histogram
[7604 5190 5062 4345 3612 2426 2177 1951 2079 2004] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
[4893 1697 2033 2724 3177 3398 4250 4388 5128 4762] [ -6.79385737e-02  -6.11413975e-02  -5.43442214e-02  -4.75470452e-02
  -4.07498691e-02  -3.39526930e-02  -2.71555168e-02  -2.03583407e-02
  -1.35611645e-02  -6.76398839e-03   3.31877563e-05]
-1.52949
1.62376
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_avoi/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.80349
Epoch 1, cost is  4.3066
Epoch 2, cost is  3.73371
Epoch 3, cost is  3.30698
Epoch 4, cost is  2.99382
Epoch 5, cost is  2.76756
Epoch 6, cost is  2.59229
Epoch 7, cost is  2.45304
Epoch 8, cost is  2.33724
Epoch 9, cost is  2.23822
Training took 1.472158 minutes
Weight histogram
[3552 3218 2676 2441 3456  514  199   86   38   20] [-0.00916891 -0.00826768 -0.00736645 -0.00646522 -0.00556399 -0.00466276
 -0.00376153 -0.0028603  -0.00195907 -0.00105784 -0.00015661]
[2822 1142 1191 1188 1278 1421 1591 1771 1973 1823] [-0.00916891 -0.00826768 -0.00736645 -0.00646522 -0.00556399 -0.00466276
 -0.00376153 -0.0028603  -0.00195907 -0.00105784 -0.00015661]
-0.169758
0.233435
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.195490 minutes
Epoch 0
Fine tuning took 0.194297 minutes
Epoch 0
Fine tuning took 0.195188 minutes
Epoch 0
Fine tuning took 0.194684 minutes
Epoch 0
Fine tuning took 0.195224 minutes
Epoch 0
Fine tuning took 0.194068 minutes
Epoch 0
Fine tuning took 0.193605 minutes
Epoch 0
Fine tuning took 0.193845 minutes
Epoch 0
Fine tuning took 0.194064 minutes
Epoch 0
Fine tuning took 0.194589 minutes
{'zero': {0: [0.3288177339901478, 0.32266009852216748, 0.29187192118226601, 0.30911330049261082, 0.37068965517241381, 0.31527093596059114, 0.33990147783251229, 0.26354679802955666, 0.33004926108374383, 0.32266009852216748, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.46305418719211822, 0.5, 0.49137931034482757, 0.40517241379310343, 0.44827586206896552, 0.42364532019704432, 0.52216748768472909, 0.53448275862068961, 0.48768472906403942, 0.50862068965517238], 5: [0.26354679802955666, 0.21428571428571427, 0.20812807881773399, 0.19950738916256158, 0.22413793103448276, 0.23645320197044334, 0.23645320197044334, 0.21428571428571427, 0.1354679802955665, 0.18965517241379309, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3288177339901478, 0.35591133004926107, 0.32266009852216748, 0.30911330049261082, 0.41133004926108374, 0.35591133004926107, 0.36330049261083741, 0.35714285714285715, 0.34729064039408869, 0.36945812807881773, 0.32142857142857145], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.42241379310344829, 0.46674876847290642, 0.51970443349753692, 0.37561576354679804, 0.41995073891625617, 0.42241379310344829, 0.43965517241379309, 0.47783251231527096, 0.41625615763546797, 0.47783251231527096], 5: [0.26354679802955666, 0.22167487684729065, 0.2105911330049261, 0.17118226600985223, 0.21305418719211822, 0.22413793103448276, 0.21428571428571427, 0.20320197044334976, 0.1748768472906404, 0.21428571428571427, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3288177339901478, 0.33866995073891626, 0.33866995073891626, 0.31157635467980294, 0.37684729064039407, 0.33866995073891626, 0.38669950738916259, 0.3251231527093596, 0.37931034482758619, 0.37561576354679804, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.45812807881773399, 0.44704433497536944, 0.51231527093596063, 0.37561576354679804, 0.43719211822660098, 0.40517241379310343, 0.4605911330049261, 0.45443349753694579, 0.41256157635467983, 0.45689655172413796], 5: [0.26354679802955666, 0.20320197044334976, 0.21428571428571427, 0.17610837438423646, 0.24753694581280788, 0.22413793103448276, 0.20812807881773399, 0.21428571428571427, 0.16625615763546797, 0.21182266009852216, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3288177339901478, 0.31773399014778325, 0.31280788177339902, 0.31896551724137934, 0.3608374384236453, 0.33866995073891626, 0.35591133004926107, 0.3288177339901478, 0.33866995073891626, 0.37192118226600984, 0.30295566502463056], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.47783251231527096, 0.44704433497536944, 0.47783251231527096, 0.41009852216748771, 0.43103448275862066, 0.42857142857142855, 0.48399014778325122, 0.50246305418719217, 0.41502463054187194, 0.47536945812807879], 5: [0.26354679802955666, 0.20443349753694581, 0.24014778325123154, 0.20320197044334976, 0.22906403940886699, 0.23029556650246305, 0.21551724137931033, 0.18719211822660098, 0.15886699507389163, 0.21305418719211822, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106809 minutes
Weight histogram
[ 440 6216 8247 5442 4568 3999 3419 3476 1924  744] [-0.0014592  -0.00093014 -0.00040108  0.00012798  0.00065704  0.0011861
  0.00171516  0.00224421  0.00277327  0.00330233  0.00383139]
[  146   155   210   318   474   776   809  1726  5017 28844] [-0.0014592  -0.00093014 -0.00040108  0.00012798  0.00065704  0.0011861
  0.00171516  0.00224421  0.00277327  0.00330233  0.00383139]
-2.33524
1.94971
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.63293
Epoch 1, cost is  3.60714
Epoch 2, cost is  3.57417
Epoch 3, cost is  3.56008
Epoch 4, cost is  3.53951
Training took 0.070678 minutes
Weight histogram
[6072 7442 2973 4802 4450 4277 7375  606  291  187] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
[3166 2613 2821 2862 2891 3678 3877 5625 5288 5654] [-0.03830989 -0.03450544 -0.03070099 -0.02689655 -0.0230921  -0.01928765
 -0.01548321 -0.01167876 -0.00787431 -0.00406987 -0.00026542]
-2.22028
2.86946
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149350 minutes
Weight histogram
[   95   349   614  1079  1220  4537 17606 12333  2636    31] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
[  243   314   384   582  1045  1323  2497 13412 20201   499] [ -5.62150904e-04  -2.35951360e-04   9.02481843e-05   4.16447729e-04
   7.42647273e-04   1.06884682e-03   1.39504636e-03   1.72124591e-03
   2.04744545e-03   2.37364499e-03   2.69984454e-03]
-1.31766
1.22057
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
..