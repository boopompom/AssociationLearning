Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
Secure
... data manager created. project_root: ExperimentADBN
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108173 minutes
Weight histogram
[ 178 1060  765  548 2646 4444 4869 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  129   151   213   272   445   612   783  1089  2869 15712] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.69131
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.01813
Epoch 1, cost is  3.97838
Epoch 2, cost is  3.94854
Epoch 3, cost is  3.93101
Epoch 4, cost is  3.90429
Training took 0.076552 minutes
Weight histogram
[7102 5719 4185 2183  356  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2336 2035 2080 1785 2068 1778 2078 2531 2721 2863] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.63787
2.09519
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.152482 minutes
Weight histogram
[ 120  460 1804 4809 6748 8019 2210   84   35   11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 10493   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.116481 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 2796 2728  534] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4633 1357 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.03388
Epoch 1, cost is  5.71668
Epoch 2, cost is  5.59362
Epoch 3, cost is  5.46721
Epoch 4, cost is  5.26048
Epoch 5, cost is  5.02644
Epoch 6, cost is  4.78843
Epoch 7, cost is  4.53996
Epoch 8, cost is  4.32156
Epoch 9, cost is  4.14705
Training took 0.183119 minutes
Weight histogram
[ 702  878 1244  454  295  222  151   62   27   15] [-0.02823097 -0.02541895 -0.02260692 -0.0197949  -0.01698288 -0.01417085
 -0.01135883 -0.0085468  -0.00573478 -0.00292275 -0.00011073]
[401 728 630 362 350 322 298 295 320 344] [-0.02823097 -0.02541895 -0.02260692 -0.0197949  -0.01698288 -0.01417085
 -0.01135883 -0.0085468  -0.00573478 -0.00292275 -0.00011073]
-0.353328
0.376825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041385 minutes
Epoch 0
Fine tuning took 0.041744 minutes
Epoch 0
Fine tuning took 0.041187 minutes
Epoch 0
Fine tuning took 0.041901 minutes
Epoch 0
Fine tuning took 0.041340 minutes
Epoch 0
Fine tuning took 0.042028 minutes
Epoch 0
Fine tuning took 0.042664 minutes
Epoch 0
Fine tuning took 0.040814 minutes
Epoch 0
Fine tuning took 0.044324 minutes
Epoch 0
Fine tuning took 0.043492 minutes
{'zero': {0: [0.17733990147783252, 0.15640394088669951, 0.1145320197044335, 0.12315270935960591, 0.19458128078817735, 0.15024630541871922, 0.30788177339901479, 0.15147783251231528, 0.1268472906403941, 0.19211822660098521, 0.049261083743842367], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.63300492610837433, 0.69581280788177335, 0.70812807881773399, 0.65147783251231528, 0.74876847290640391, 0.59482758620689657, 0.6280788177339901, 0.72167487684729059, 0.65147783251231528, 0.7426108374384236], 5: [0.14655172413793102, 0.2105911330049261, 0.18965517241379309, 0.16871921182266009, 0.1539408866995074, 0.10098522167487685, 0.097290640394088676, 0.22044334975369459, 0.15147783251231528, 0.15640394088669951, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17733990147783252, 0.1268472906403941, 0.075123152709359611, 0.13054187192118227, 0.082512315270935957, 0.049261083743842367, 0.13793103448275862, 0.094827586206896547, 0.11576354679802955, 0.099753694581280791, 0.055418719211822662], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.74630541871921185, 0.71798029556650245, 0.77586206896551724, 0.81650246305418717, 0.83990147783251234, 0.77955665024630538, 0.77463054187192115, 0.77216748768472909, 0.80049261083743839, 0.82019704433497542], 5: [0.14655172413793102, 0.1268472906403941, 0.20689655172413793, 0.093596059113300489, 0.10098522167487685, 0.11083743842364532, 0.082512315270935957, 0.13054187192118227, 0.11206896551724138, 0.099753694581280791, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17733990147783252, 0.14655172413793102, 0.084975369458128072, 0.15147783251231528, 0.073891625615763554, 0.075123152709359611, 0.14408866995073891, 0.064039408866995079, 0.13916256157635468, 0.088669950738916259, 0.080049261083743842], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.73152709359605916, 0.70197044334975367, 0.72413793103448276, 0.77955665024630538, 0.82635467980295563, 0.78078817733990147, 0.78694581280788178, 0.76108374384236455, 0.79926108374384242, 0.78694581280788178], 5: [0.14655172413793102, 0.12192118226600986, 0.21305418719211822, 0.12438423645320197, 0.14655172413793102, 0.098522167487684734, 0.075123152709359611, 0.14901477832512317, 0.099753694581280791, 0.11206896551724138, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17733990147783252, 0.12561576354679804, 0.071428571428571425, 0.12561576354679804, 0.041871921182266007, 0.030788177339901478, 0.10467980295566502, 0.061576354679802957, 0.078817733990147784, 0.088669950738916259, 0.046798029556650245], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.77216748768472909, 0.73152709359605916, 0.76600985221674878, 0.83743842364532017, 0.87684729064039413, 0.8214285714285714, 0.84852216748768472, 0.83497536945812811, 0.83004926108374388, 0.84729064039408863], 5: [0.14655172413793102, 0.10221674876847291, 0.19704433497536947, 0.10837438423645321, 0.1206896551724138, 0.092364532019704432, 0.073891625615763554, 0.089901477832512317, 0.086206896551724144, 0.081280788177339899, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108440 minutes
Weight histogram
[ 178 1060  765  548 2646 4444 4869 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  129   151   213   272   445   612   783  1089  2869 15712] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.69131
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.01813
Epoch 1, cost is  3.97838
Epoch 2, cost is  3.94854
Epoch 3, cost is  3.93101
Epoch 4, cost is  3.90429
Training took 0.072738 minutes
Weight histogram
[7102 5719 4185 2183  356  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2336 2035 2080 1785 2068 1778 2078 2531 2721 2863] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.63787
2.09519
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149408 minutes
Weight histogram
[ 120  460 1804 4809 6748 8019 2210   84   35   11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 10493   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.117144 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 2796 2728  534] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4633 1357 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.52179
Epoch 1, cost is  5.30384
Epoch 2, cost is  5.21312
Epoch 3, cost is  5.00801
Epoch 4, cost is  4.73541
Epoch 5, cost is  4.47268
Epoch 6, cost is  4.18973
Epoch 7, cost is  3.93655
Epoch 8, cost is  3.74197
Epoch 9, cost is  3.58641
Training took 0.245312 minutes
Weight histogram
[ 891 2259  386  216  126   74   45   27   16   10] [-0.01881931 -0.01695349 -0.01508766 -0.01322184 -0.01135602 -0.00949019
 -0.00762437 -0.00575855 -0.00389273 -0.0020269  -0.00016108]
[1085  408  290  318  304  282  289  311  360  403] [-0.01881931 -0.01695349 -0.01508766 -0.01322184 -0.01135602 -0.00949019
 -0.00762437 -0.00575855 -0.00389273 -0.0020269  -0.00016108]
-0.261765
0.236722
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044434 minutes
Epoch 0
Fine tuning took 0.045719 minutes
Epoch 0
Fine tuning took 0.047243 minutes
Epoch 0
Fine tuning took 0.048074 minutes
Epoch 0
Fine tuning took 0.045018 minutes
Epoch 0
Fine tuning took 0.045273 minutes
Epoch 0
Fine tuning took 0.044469 minutes
Epoch 0
Fine tuning took 0.044688 minutes
Epoch 0
Fine tuning took 0.044749 minutes
Epoch 0
Fine tuning took 0.046664 minutes
{'zero': {0: [0.19334975369458129, 0.16502463054187191, 0.16502463054187191, 0.15024630541871922, 0.15147783251231528, 0.25123152709359609, 0.28694581280788178, 0.25615763546798032, 0.17857142857142858, 0.16133004926108374, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63793103448275867, 0.65886699507389157, 0.63177339901477836, 0.64162561576354682, 0.58866995073891626, 0.51477832512315269, 0.52339901477832518, 0.52709359605911332, 0.5714285714285714, 0.60591133004926112, 0.55541871921182262], 5: [0.16871921182266009, 0.17610837438423646, 0.20320197044334976, 0.20812807881773399, 0.25985221674876846, 0.23399014778325122, 0.18965517241379309, 0.21674876847290642, 0.25, 0.23275862068965517, 0.29802955665024633], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19334975369458129, 0.16748768472906403, 0.16748768472906403, 0.13793103448275862, 0.14655172413793102, 0.25, 0.32142857142857145, 0.22044334975369459, 0.18965517241379309, 0.1539408866995074, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63793103448275867, 0.65394088669950734, 0.6145320197044335, 0.67364532019704437, 0.57635467980295563, 0.54064039408866993, 0.49753694581280788, 0.58251231527093594, 0.57389162561576357, 0.59975369458128081, 0.55295566502463056], 5: [0.16871921182266009, 0.17857142857142858, 0.21798029556650247, 0.18842364532019704, 0.27709359605911332, 0.20935960591133004, 0.18103448275862069, 0.19704433497536947, 0.23645320197044334, 0.24630541871921183, 0.31773399014778325], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19334975369458129, 0.15886699507389163, 0.15886699507389163, 0.14408866995073891, 0.14655172413793102, 0.22413793103448276, 0.2894088669950739, 0.22413793103448276, 0.1625615763546798, 0.15886699507389163, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63793103448275867, 0.66995073891625612, 0.63423645320197042, 0.65517241379310343, 0.63793103448275867, 0.57635467980295563, 0.54556650246305416, 0.56773399014778325, 0.61576354679802958, 0.61083743842364535, 0.58251231527093594], 5: [0.16871921182266009, 0.17118226600985223, 0.20689655172413793, 0.20073891625615764, 0.21551724137931033, 0.19950738916256158, 0.16502463054187191, 0.20812807881773399, 0.22167487684729065, 0.23029556650246305, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19334975369458129, 0.16995073891625614, 0.1625615763546798, 0.14778325123152711, 0.16625615763546797, 0.25246305418719212, 0.2894088669950739, 0.24014778325123154, 0.1748768472906404, 0.17980295566502463, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63793103448275867, 0.65517241379310343, 0.6354679802955665, 0.62931034482758619, 0.57635467980295563, 0.53078817733990147, 0.51970443349753692, 0.55418719211822665, 0.58990147783251234, 0.57635467980295563, 0.5431034482758621], 5: [0.16871921182266009, 0.1748768472906404, 0.2019704433497537, 0.2229064039408867, 0.25738916256157635, 0.21674876847290642, 0.19088669950738915, 0.20566502463054187, 0.23522167487684728, 0.24384236453201971, 0.33004926108374383], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107980 minutes
Weight histogram
[ 178 1060  765  548 2646 4444 4869 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  129   151   213   272   445   612   783  1089  2869 15712] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.69131
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.01813
Epoch 1, cost is  3.97838
Epoch 2, cost is  3.94854
Epoch 3, cost is  3.93101
Epoch 4, cost is  3.90429
Training took 0.077011 minutes
Weight histogram
[7102 5719 4185 2183  356  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2336 2035 2080 1785 2068 1778 2078 2531 2721 2863] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.63787
2.09519
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147731 minutes
Weight histogram
[ 120  460 1804 4809 6748 8019 2210   84   35   11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 10493   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.116639 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 2796 2728  534] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4633 1357 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.3762
Epoch 1, cost is  5.25559
Epoch 2, cost is  5.01747
Epoch 3, cost is  4.59786
Epoch 4, cost is  4.21504
Epoch 5, cost is  3.86743
Epoch 6, cost is  3.60715
Epoch 7, cost is  3.41886
Epoch 8, cost is  3.25555
Epoch 9, cost is  3.1031
Training took 0.335800 minutes
Weight histogram
[ 910  607  689  559 1154   73   28   15    9    6] [-0.01131206 -0.01019388 -0.0090757  -0.00795752 -0.00683933 -0.00572115
 -0.00460297 -0.00348479 -0.00236661 -0.00124842 -0.00013024]
[1049  268  263  279  276  287  332  392  435  469] [-0.01131206 -0.01019388 -0.0090757  -0.00795752 -0.00683933 -0.00572115
 -0.00460297 -0.00348479 -0.00236661 -0.00124842 -0.00013024]
-0.22507
0.194959
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052102 minutes
Epoch 0
Fine tuning took 0.049564 minutes
Epoch 0
Fine tuning took 0.050848 minutes
Epoch 0
Fine tuning took 0.048896 minutes
Epoch 0
Fine tuning took 0.050030 minutes
Epoch 0
Fine tuning took 0.051528 minutes
Epoch 0
Fine tuning took 0.050839 minutes
Epoch 0
Fine tuning took 0.050121 minutes
Epoch 0
Fine tuning took 0.049942 minutes
Epoch 0
Fine tuning took 0.051906 minutes
{'zero': {0: [0.22783251231527094, 0.22044334975369459, 0.20443349753694581, 0.1748768472906404, 0.23399014778325122, 0.27093596059113301, 0.24384236453201971, 0.23522167487684728, 0.20443349753694581, 0.1748768472906404, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.58743842364532017, 0.57512315270935965, 0.62561576354679804, 0.53448275862068961, 0.45935960591133007, 0.56527093596059108, 0.54802955665024633, 0.57635467980295563, 0.58497536945812811, 0.54802955665024633], 5: [0.19704433497536947, 0.19211822660098521, 0.22044334975369459, 0.19950738916256158, 0.23152709359605911, 0.26970443349753692, 0.19088669950738915, 0.21674876847290642, 0.21921182266009853, 0.24014778325123154, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22783251231527094, 0.19827586206896552, 0.20935960591133004, 0.18472906403940886, 0.19088669950738915, 0.27093596059113301, 0.28694581280788178, 0.25123152709359609, 0.19211822660098521, 0.16133004926108374, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.59359605911330049, 0.56527093596059108, 0.58374384236453203, 0.57635467980295563, 0.49261083743842365, 0.54926108374384242, 0.5357142857142857, 0.56773399014778325, 0.58128078817733986, 0.54802955665024633], 5: [0.19704433497536947, 0.20812807881773399, 0.22536945812807882, 0.23152709359605911, 0.23275862068965517, 0.23645320197044334, 0.16379310344827586, 0.21305418719211822, 0.24014778325123154, 0.25738916256157635, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22783251231527094, 0.18719211822660098, 0.21305418719211822, 0.16748768472906403, 0.20566502463054187, 0.27586206896551724, 0.23152709359605911, 0.23029556650246305, 0.18965517241379309, 0.1625615763546798, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.57512315270935965, 0.57266009852216748, 0.60591133004926112, 0.56034482758620685, 0.47660098522167488, 0.59359605911330049, 0.55788177339901479, 0.60221674876847286, 0.60960591133004927, 0.55172413793103448], 5: [0.19704433497536947, 0.2376847290640394, 0.21428571428571427, 0.22660098522167488, 0.23399014778325122, 0.24753694581280788, 0.1748768472906404, 0.21182266009852216, 0.20812807881773399, 0.22783251231527094, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22783251231527094, 0.23029556650246305, 0.23152709359605911, 0.16502463054187191, 0.17733990147783252, 0.27339901477832512, 0.23645320197044334, 0.25123152709359609, 0.17733990147783252, 0.18472906403940886, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.5923645320197044, 0.54802955665024633, 0.60221674876847286, 0.56157635467980294, 0.5, 0.60098522167487689, 0.52093596059113301, 0.57758620689655171, 0.56157635467980294, 0.52216748768472909], 5: [0.19704433497536947, 0.17733990147783252, 0.22044334975369459, 0.23275862068965517, 0.26108374384236455, 0.22660098522167488, 0.1625615763546798, 0.22783251231527094, 0.24507389162561577, 0.2536945812807882, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107012 minutes
Weight histogram
[ 178 1060  765  548 2646 4444 4869 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  129   151   213   272   445   612   783  1089  2869 15712] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.69131
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.01813
Epoch 1, cost is  3.97838
Epoch 2, cost is  3.94854
Epoch 3, cost is  3.93101
Epoch 4, cost is  3.90429
Training took 0.072772 minutes
Weight histogram
[7102 5719 4185 2183  356  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2336 2035 2080 1785 2068 1778 2078 2531 2721 2863] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.63787
2.09519
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148793 minutes
Weight histogram
[ 120  460 1804 4809 6748 8019 2210   84   35   11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 10493   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.114766 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 2796 2728  534] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4633 1357 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.36014
Epoch 1, cost is  5.19195
Epoch 2, cost is  4.75641
Epoch 3, cost is  4.24209
Epoch 4, cost is  3.79393
Epoch 5, cost is  3.47698
Epoch 6, cost is  3.24684
Epoch 7, cost is  3.05047
Epoch 8, cost is  2.8639
Epoch 9, cost is  2.70887
Training took 0.537200 minutes
Weight histogram
[887 688 493 473 477 998  19   8   4   3] [-0.00591584 -0.00533677 -0.0047577  -0.00417863 -0.00359956 -0.0030205
 -0.00244143 -0.00186236 -0.00128329 -0.00070422 -0.00012516]
[940 264 265 266 277 322 377 425 443 471] [-0.00591584 -0.00533677 -0.0047577  -0.00417863 -0.00359956 -0.0030205
 -0.00244143 -0.00186236 -0.00128329 -0.00070422 -0.00012516]
-0.183692
0.174146
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.060720 minutes
Epoch 0
Fine tuning took 0.061421 minutes
Epoch 0
Fine tuning took 0.062178 minutes
Epoch 0
Fine tuning took 0.060959 minutes
Epoch 0
Fine tuning took 0.062584 minutes
Epoch 0
Fine tuning took 0.060777 minutes
Epoch 0
Fine tuning took 0.062992 minutes
Epoch 0
Fine tuning took 0.061022 minutes
Epoch 0
Fine tuning took 0.061415 minutes
Epoch 0
Fine tuning took 0.061435 minutes
{'zero': {0: [0.22167487684729065, 0.19950738916256158, 0.20812807881773399, 0.19581280788177341, 0.19458128078817735, 0.24630541871921183, 0.18965517241379309, 0.20812807881773399, 0.26847290640394089, 0.17364532019704434, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.61822660098522164, 0.58990147783251234, 0.56896551724137934, 0.58128078817733986, 0.5431034482758621, 0.60467980295566504, 0.53694581280788178, 0.52463054187192115, 0.58128078817733986, 0.58990147783251234], 5: [0.2413793103448276, 0.18226600985221675, 0.2019704433497537, 0.23522167487684728, 0.22413793103448276, 0.2105911330049261, 0.20566502463054187, 0.25492610837438423, 0.20689655172413793, 0.24507389162561577, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22167487684729065, 0.18719211822660098, 0.19088669950738915, 0.17241379310344829, 0.16009852216748768, 0.22413793103448276, 0.21551724137931033, 0.21428571428571427, 0.26847290640394089, 0.17241379310344829, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.61083743842364535, 0.60837438423645318, 0.59605911330049266, 0.60344827586206895, 0.53694581280788178, 0.58374384236453203, 0.53817733990147787, 0.53694581280788178, 0.57389162561576357, 0.5923645320197044], 5: [0.2413793103448276, 0.2019704433497537, 0.20073891625615764, 0.23152709359605911, 0.23645320197044334, 0.23891625615763548, 0.20073891625615764, 0.24753694581280788, 0.19458128078817735, 0.2536945812807882, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22167487684729065, 0.18842364532019704, 0.19088669950738915, 0.2019704433497537, 0.20689655172413793, 0.24876847290640394, 0.20073891625615764, 0.24753694581280788, 0.25738916256157635, 0.13669950738916256, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.62315270935960587, 0.59605911330049266, 0.5714285714285714, 0.56896551724137934, 0.54064039408866993, 0.58004926108374388, 0.50862068965517238, 0.51600985221674878, 0.60098522167487689, 0.58128078817733986], 5: [0.2413793103448276, 0.18842364532019704, 0.21305418719211822, 0.22660098522167488, 0.22413793103448276, 0.2105911330049261, 0.21921182266009853, 0.24384236453201971, 0.22660098522167488, 0.26231527093596058, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22167487684729065, 0.19088669950738915, 0.21305418719211822, 0.18965517241379309, 0.19950738916256158, 0.24753694581280788, 0.23029556650246305, 0.19950738916256158, 0.27586206896551724, 0.14532019704433496, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.60591133004926112, 0.59729064039408863, 0.59359605911330049, 0.55049261083743839, 0.55665024630541871, 0.58374384236453203, 0.56403940886699511, 0.52216748768472909, 0.5788177339901478, 0.58251231527093594], 5: [0.2413793103448276, 0.20320197044334976, 0.18965517241379309, 0.21674876847290642, 0.25, 0.19581280788177341, 0.18596059113300492, 0.23645320197044334, 0.2019704433497537, 0.27586206896551724, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150345 minutes
Weight histogram
[ 232  297  491  722 1677 3623 5920 6126 2807  380] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
[  136   165   223   329   517   863  1078  2009  6663 10292] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56559
Epoch 1, cost is  2.52977
Epoch 2, cost is  2.49994
Epoch 3, cost is  2.47299
Epoch 4, cost is  2.45009
Training took 0.117048 minutes
Weight histogram
[5017 3236 2990 2816 1986 1630 1985  996 1355  264] [ -5.40111847e-02  -4.86067013e-02  -4.32022180e-02  -3.77977346e-02
  -3.23932512e-02  -2.69887678e-02  -2.15842844e-02  -1.61798010e-02
  -1.07753177e-02  -5.37083428e-03   3.36491030e-05]
[2473 1309 1271 1583 1965 2235 2464 2685 3196 3094] [ -5.40111847e-02  -4.86067013e-02  -4.32022180e-02  -3.77977346e-02
  -3.23932512e-02  -2.69887678e-02  -2.15842844e-02  -1.61798010e-02
  -1.07753177e-02  -5.37083428e-03   3.36491030e-05]
-1.1744
1.54119
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150839 minutes
Weight histogram
[  65   95  492 1204 2907 4128 5177 5888 3583  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 13094] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.117587 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 2065 3464  529] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4718 1272 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.21278
Epoch 1, cost is  5.95185
Epoch 2, cost is  5.7917
Epoch 3, cost is  5.59574
Epoch 4, cost is  5.27837
Epoch 5, cost is  4.95224
Epoch 6, cost is  4.67302
Epoch 7, cost is  4.42946
Epoch 8, cost is  4.22191
Epoch 9, cost is  4.03892
Training took 0.206721 minutes
Weight histogram
[ 519  498  445  510 1111  452  361  114   27   13] [-0.02539939 -0.02287609 -0.0203528  -0.01782951 -0.01530622 -0.01278292
 -0.01025963 -0.00773634 -0.00521305 -0.00268975 -0.00016646]
[805 705 322 269 283 307 321 331 343 364] [-0.02539939 -0.02287609 -0.0203528  -0.01782951 -0.01530622 -0.01278292
 -0.01025963 -0.00773634 -0.00521305 -0.00268975 -0.00016646]
-0.419286
0.470294
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.053747 minutes
Epoch 0
Fine tuning took 0.054999 minutes
Epoch 0
Fine tuning took 0.052350 minutes
Epoch 0
Fine tuning took 0.052148 minutes
Epoch 0
Fine tuning took 0.053434 minutes
Epoch 0
Fine tuning took 0.052845 minutes
Epoch 0
Fine tuning took 0.053090 minutes
Epoch 0
Fine tuning took 0.055302 minutes
Epoch 0
Fine tuning took 0.054505 minutes
Epoch 0
Fine tuning took 0.053176 minutes
{'zero': {0: [0.16133004926108374, 0.097290640394088676, 0.036945812807881777, 0.1206896551724138, 0.10221674876847291, 0.077586206896551727, 0.060344827586206899, 0.11945812807881774, 0.25738916256157635, 0.056650246305418719, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.79679802955665024, 0.82266009852216748, 0.79064039408866993, 0.81896551724137934, 0.78817733990147787, 0.75985221674876846, 0.75492610837438423, 0.64778325123152714, 0.81157635467980294, 0.70197044334975367], 5: [0.1354679802955665, 0.10591133004926108, 0.14039408866995073, 0.088669950738916259, 0.078817733990147784, 0.13423645320197045, 0.17980295566502463, 0.12561576354679804, 0.094827586206896547, 0.13177339901477833, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.11822660098522167, 0.023399014778325122, 0.094827586206896547, 0.084975369458128072, 0.050492610837438424, 0.051724137931034482, 0.11699507389162561, 0.2376847290640394, 0.044334975369458129, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.77216748768472909, 0.83004926108374388, 0.84729064039408863, 0.85098522167487689, 0.80049261083743839, 0.77463054187192115, 0.78940886699507384, 0.71798029556650245, 0.84852216748768472, 0.71059113300492616], 5: [0.1354679802955665, 0.10960591133004927, 0.14655172413793102, 0.057881773399014777, 0.064039408866995079, 0.14901477832512317, 0.17364532019704434, 0.093596059113300489, 0.044334975369458129, 0.10714285714285714, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.11083743842364532, 0.027093596059113302, 0.10591133004926108, 0.078817733990147784, 0.061576354679802957, 0.056650246305418719, 0.1354679802955665, 0.24384236453201971, 0.057881773399014777, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.77093596059113301, 0.7857142857142857, 0.81896551724137934, 0.8571428571428571, 0.76847290640394084, 0.77955665024630538, 0.76724137931034486, 0.69581280788177335, 0.85467980295566504, 0.68842364532019706], 5: [0.1354679802955665, 0.11822660098522167, 0.18719211822660098, 0.075123152709359611, 0.064039408866995079, 0.16995073891625614, 0.16379310344827586, 0.097290640394088676, 0.060344827586206899, 0.087438423645320201, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.099753694581280791, 0.030788177339901478, 0.10467980295566502, 0.062807881773399021, 0.048029556650246302, 0.071428571428571425, 0.12807881773399016, 0.25615763546798032, 0.048029556650246302, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.78694581280788178, 0.79802955665024633, 0.83497536945812811, 0.89039408866995073, 0.80295566502463056, 0.77093596059113301, 0.77586206896551724, 0.68472906403940892, 0.83866995073891626, 0.68103448275862066], 5: [0.1354679802955665, 0.11330049261083744, 0.17118226600985223, 0.060344827586206899, 0.046798029556650245, 0.14901477832512317, 0.15763546798029557, 0.096059113300492605, 0.059113300492610835, 0.11330049261083744, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150981 minutes
Weight histogram
[ 232  297  491  722 1677 3623 5920 6126 2807  380] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
[  136   165   223   329   517   863  1078  2009  6663 10292] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56559
Epoch 1, cost is  2.52977
Epoch 2, cost is  2.49994
Epoch 3, cost is  2.47299
Epoch 4, cost is  2.45009
Training took 0.116511 minutes
Weight histogram
[5017 3236 2990 2816 1986 1630 1985  996 1355  264] [ -5.40111847e-02  -4.86067013e-02  -4.32022180e-02  -3.77977346e-02
  -3.23932512e-02  -2.69887678e-02  -2.15842844e-02  -1.61798010e-02
  -1.07753177e-02  -5.37083428e-03   3.36491030e-05]
[2473 1309 1271 1583 1965 2235 2464 2685 3196 3094] [ -5.40111847e-02  -4.86067013e-02  -4.32022180e-02  -3.77977346e-02
  -3.23932512e-02  -2.69887678e-02  -2.15842844e-02  -1.61798010e-02
  -1.07753177e-02  -5.37083428e-03   3.36491030e-05]
-1.1744
1.54119
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149043 minutes
Weight histogram
[  65   95  492 1204 2907 4128 5177 5888 3583  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 13094] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.114977 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 2065 3464  529] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4718 1272 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.59413
Epoch 1, cost is  5.36291
Epoch 2, cost is  5.22692
Epoch 3, cost is  4.9389
Epoch 4, cost is  4.53979
Epoch 5, cost is  4.21624
Epoch 6, cost is  3.93242
Epoch 7, cost is  3.67588
Epoch 8, cost is  3.46781
Epoch 9, cost is  3.29917
Training took 0.298426 minutes
Weight histogram
[ 717  697  608 1517  289  119   53   26   15    9] [-0.01764149 -0.01589278 -0.01414407 -0.01239537 -0.01064666 -0.00889795
 -0.00714924 -0.00540053 -0.00365182 -0.00190312 -0.00015441]
[1208  317  244  268  294  308  319  332  361  399] [-0.01764149 -0.01589278 -0.01414407 -0.01239537 -0.01064666 -0.00889795
 -0.00714924 -0.00540053 -0.00365182 -0.00190312 -0.00015441]
-0.260454
0.331872
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.058278 minutes
Epoch 0
Fine tuning took 0.058968 minutes
Epoch 0
Fine tuning took 0.058238 minutes
Epoch 0
Fine tuning took 0.060013 minutes
Epoch 0
Fine tuning took 0.060206 minutes
Epoch 0
Fine tuning took 0.060155 minutes
Epoch 0
Fine tuning took 0.058156 minutes
Epoch 0
Fine tuning took 0.058414 minutes
Epoch 0
Fine tuning took 0.060088 minutes
Epoch 0
Fine tuning took 0.058972 minutes
{'zero': {0: [0.1625615763546798, 0.2105911330049261, 0.17857142857142858, 0.17610837438423646, 0.20689655172413793, 0.18719211822660098, 0.18226600985221675, 0.21182266009852216, 0.1539408866995074, 0.2105911330049261, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.62438423645320196, 0.65270935960591137, 0.63669950738916259, 0.60960591133004927, 0.6280788177339901, 0.59605911330049266, 0.56157635467980294, 0.66502463054187189, 0.61330049261083741, 0.68226600985221675], 5: [0.1206896551724138, 0.16502463054187191, 0.16871921182266009, 0.18719211822660098, 0.18349753694581281, 0.18472906403940886, 0.22167487684729065, 0.22660098522167488, 0.18103448275862069, 0.17610837438423646, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1625615763546798, 0.15147783251231528, 0.1748768472906404, 0.15024630541871922, 0.16009852216748768, 0.1539408866995074, 0.18226600985221675, 0.20073891625615764, 0.11822660098522167, 0.17733990147783252, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.68842364532019706, 0.68965517241379315, 0.7142857142857143, 0.68965517241379315, 0.68472906403940892, 0.6711822660098522, 0.58251231527093594, 0.68472906403940892, 0.66995073891625612, 0.69088669950738912], 5: [0.1206896551724138, 0.16009852216748768, 0.1354679802955665, 0.1354679802955665, 0.15024630541871922, 0.16133004926108374, 0.14655172413793102, 0.21674876847290642, 0.19704433497536947, 0.15270935960591134, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1625615763546798, 0.18349753694581281, 0.14655172413793102, 0.16502463054187191, 0.14655172413793102, 0.14162561576354679, 0.2105911330049261, 0.19088669950738915, 0.13669950738916256, 0.1748768472906404, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.66379310344827591, 0.72413793103448276, 0.70073891625615758, 0.70320197044334976, 0.71798029556650245, 0.64778325123152714, 0.59359605911330049, 0.70197044334975367, 0.68226600985221675, 0.69827586206896552], 5: [0.1206896551724138, 0.15270935960591134, 0.12931034482758622, 0.13423645320197045, 0.15024630541871922, 0.14039408866995073, 0.14162561576354679, 0.21551724137931033, 0.16133004926108374, 0.14285714285714285, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1625615763546798, 0.17118226600985223, 0.1539408866995074, 0.14532019704433496, 0.15024630541871922, 0.16748768472906403, 0.19827586206896552, 0.21305418719211822, 0.16502463054187191, 0.20566502463054187, 0.11945812807881774], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.70443349753694584, 0.70935960591133007, 0.69704433497536944, 0.68472906403940892, 0.67487684729064035, 0.65394088669950734, 0.58374384236453203, 0.67364532019704437, 0.66871921182266014, 0.71551724137931039], 5: [0.1206896551724138, 0.12438423645320197, 0.13669950738916256, 0.15763546798029557, 0.16502463054187191, 0.15763546798029557, 0.14778325123152711, 0.20320197044334976, 0.16133004926108374, 0.12561576354679804, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149868 minutes
Weight histogram
[ 232  297  491  722 1677 3623 5920 6126 2807  380] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
[  136   165   223   329   517   863  1078  2009  6663 10292] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56559
Epoch 1, cost is  2.52977
Epoch 2, cost is  2.49994
Epoch 3, cost is  2.47299
Epoch 4, cost is  2.45009
Training took 0.117473 minutes
Weight histogram
[5017 3236 2990 2816 1986 1630 1985  996 1355  264] [ -5.40111847e-02  -4.86067013e-02  -4.32022180e-02  -3.77977346e-02
  -3.23932512e-02  -2.69887678e-02  -2.15842844e-02  -1.61798010e-02
  -1.07753177e-02  -5.37083428e-03   3.36491030e-05]
[2473 1309 1271 1583 1965 2235 2464 2685 3196 3094] [ -5.40111847e-02  -4.86067013e-02  -4.32022180e-02  -3.77977346e-02
  -3.23932512e-02  -2.69887678e-02  -2.15842844e-02  -1.61798010e-02
  -1.07753177e-02  -5.37083428e-03   3.36491030e-05]
-1.1744
1.54119
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149742 minutes
Weight histogram
[  65   95  492 1204 2907 4128 5177 5888 3583  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 13094] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.117440 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 2065 3464  529] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4718 1272 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.22912
Epoch 1, cost is  5.07881
Epoch 2, cost is  4.89447
Epoch 3, cost is  4.48063
Epoch 4, cost is  4.08855
Epoch 5, cost is  3.75322
Epoch 6, cost is  3.44988
Epoch 7, cost is  3.21295
Epoch 8, cost is  3.02551
Epoch 9, cost is  2.87486
Training took 0.415273 minutes
Weight histogram
[ 902  874 1834  228  102   51   28   16    9    6] [-0.01201195 -0.01082388 -0.00963581 -0.00844775 -0.00725968 -0.00607161
 -0.00488354 -0.00369547 -0.0025074  -0.00131933 -0.00013126]
[1124  268  248  284  292  306  323  357  398  450] [-0.01201195 -0.01082388 -0.00963581 -0.00844775 -0.00725968 -0.00607161
 -0.00488354 -0.00369547 -0.0025074  -0.00131933 -0.00013126]
-0.222079
0.227248
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.063068 minutes
Epoch 0
Fine tuning took 0.063767 minutes
Epoch 0
Fine tuning took 0.064948 minutes
Epoch 0
Fine tuning took 0.065060 minutes
Epoch 0
Fine tuning took 0.063640 minutes
Epoch 0
Fine tuning took 0.064363 minutes
Epoch 0
Fine tuning took 0.063214 minutes
Epoch 0
Fine tuning took 0.064833 minutes
Epoch 0
Fine tuning took 0.064097 minutes
Epoch 0
Fine tuning took 0.063868 minutes
{'zero': {0: [0.21182266009852216, 0.17733990147783252, 0.21798029556650247, 0.16379310344827586, 0.16502463054187191, 0.21551724137931033, 0.20320197044334976, 0.21798029556650247, 0.22660098522167488, 0.19950738916256158, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.61699507389162567, 0.58374384236453203, 0.61083743842364535, 0.57758620689655171, 0.58620689655172409, 0.54064039408866993, 0.58990147783251234, 0.55541871921182262, 0.55911330049261088, 0.56034482758620685], 5: [0.18472906403940886, 0.20566502463054187, 0.19827586206896552, 0.22536945812807882, 0.25738916256157635, 0.19827586206896552, 0.25615763546798032, 0.19211822660098521, 0.21798029556650247, 0.2413793103448276, 0.28817733990147781], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21182266009852216, 0.15640394088669951, 0.2019704433497537, 0.17857142857142858, 0.16625615763546797, 0.21921182266009853, 0.17857142857142858, 0.21305418719211822, 0.20812807881773399, 0.14408866995073891, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.65270935960591137, 0.58990147783251234, 0.6219211822660099, 0.56280788177339902, 0.58497536945812811, 0.58251231527093594, 0.5788177339901478, 0.54802955665024633, 0.63054187192118227, 0.54187192118226601], 5: [0.18472906403940886, 0.19088669950738915, 0.20812807881773399, 0.19950738916256158, 0.27093596059113301, 0.19581280788177341, 0.23891625615763548, 0.20812807881773399, 0.24384236453201971, 0.22536945812807882, 0.29433497536945813], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21182266009852216, 0.13793103448275862, 0.1748768472906404, 0.14778325123152711, 0.14162561576354679, 0.22044334975369459, 0.20320197044334976, 0.18965517241379309, 0.20443349753694581, 0.17364532019704434, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.67610837438423643, 0.61945812807881773, 0.63177339901477836, 0.6280788177339901, 0.57635467980295563, 0.54556650246305416, 0.59729064039408863, 0.5714285714285714, 0.64778325123152714, 0.56157635467980294], 5: [0.18472906403940886, 0.18596059113300492, 0.20566502463054187, 0.22044334975369459, 0.23029556650246305, 0.20320197044334976, 0.25123152709359609, 0.21305418719211822, 0.22413793103448276, 0.17857142857142858, 0.26970443349753692], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21182266009852216, 0.14901477832512317, 0.22536945812807882, 0.16133004926108374, 0.16748768472906403, 0.24014778325123154, 0.17980295566502463, 0.19088669950738915, 0.19334975369458129, 0.17241379310344829, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.64039408866995073, 0.58620689655172409, 0.63669950738916259, 0.57758620689655171, 0.58374384236453203, 0.57019704433497542, 0.60221674876847286, 0.59975369458128081, 0.60221674876847286, 0.60221674876847286], 5: [0.18472906403940886, 0.2105911330049261, 0.18842364532019704, 0.2019704433497537, 0.25492610837438423, 0.17610837438423646, 0.25, 0.20689655172413793, 0.20689655172413793, 0.22536945812807882, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149480 minutes
Weight histogram
[ 232  297  491  722 1677 3623 5920 6126 2807  380] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
[  136   165   223   329   517   863  1078  2009  6663 10292] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56559
Epoch 1, cost is  2.52977
Epoch 2, cost is  2.49994
Epoch 3, cost is  2.47299
Epoch 4, cost is  2.45009
Training took 0.116614 minutes
Weight histogram
[5017 3236 2990 2816 1986 1630 1985  996 1355  264] [ -5.40111847e-02  -4.86067013e-02  -4.32022180e-02  -3.77977346e-02
  -3.23932512e-02  -2.69887678e-02  -2.15842844e-02  -1.61798010e-02
  -1.07753177e-02  -5.37083428e-03   3.36491030e-05]
[2473 1309 1271 1583 1965 2235 2464 2685 3196 3094] [ -5.40111847e-02  -4.86067013e-02  -4.32022180e-02  -3.77977346e-02
  -3.23932512e-02  -2.69887678e-02  -2.15842844e-02  -1.61798010e-02
  -1.07753177e-02  -5.37083428e-03   3.36491030e-05]
-1.1744
1.54119
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151164 minutes
Weight histogram
[  65   95  492 1204 2907 4128 5177 5888 3583  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 13094] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.116744 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 2065 3464  529] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4718 1272 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.18523
Epoch 1, cost is  4.99369
Epoch 2, cost is  4.48553
Epoch 3, cost is  3.92794
Epoch 4, cost is  3.46575
Epoch 5, cost is  3.12028
Epoch 6, cost is  2.87773
Epoch 7, cost is  2.69038
Epoch 8, cost is  2.53265
Epoch 9, cost is  2.3923
Training took 0.687115 minutes
Weight histogram
[897 669 541 519 488 901  19   8   4   4] [-0.00658948 -0.00594376 -0.00529804 -0.00465231 -0.00400659 -0.00336086
 -0.00271514 -0.00206941 -0.00142369 -0.00077797 -0.00013224]
[922 251 257 270 280 306 355 418 474 517] [-0.00658948 -0.00594376 -0.00529804 -0.00465231 -0.00400659 -0.00336086
 -0.00271514 -0.00206941 -0.00142369 -0.00077797 -0.00013224]
-0.183405
0.196057
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077103 minutes
Epoch 0
Fine tuning took 0.078215 minutes
Epoch 0
Fine tuning took 0.076538 minutes
Epoch 0
Fine tuning took 0.077643 minutes
Epoch 0
Fine tuning took 0.076339 minutes
Epoch 0
Fine tuning took 0.078883 minutes
Epoch 0
Fine tuning took 0.076532 minutes
Epoch 0
Fine tuning took 0.078024 minutes
Epoch 0
Fine tuning took 0.077509 minutes
Epoch 0
Fine tuning took 0.077091 minutes
{'zero': {0: [0.22044334975369459, 0.2376847290640394, 0.18349753694581281, 0.18103448275862069, 0.21798029556650247, 0.26231527093596058, 0.17364532019704434, 0.20320197044334976, 0.23645320197044334, 0.17364532019704434, 0.22044334975369459], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.53325123152709364, 0.6145320197044335, 0.59359605911330049, 0.57758620689655171, 0.50123152709359609, 0.62438423645320196, 0.53325123152709364, 0.5431034482758621, 0.55665024630541871, 0.5714285714285714], 5: [0.22783251231527094, 0.22906403940886699, 0.2019704433497537, 0.22536945812807882, 0.20443349753694581, 0.23645320197044334, 0.2019704433497537, 0.26354679802955666, 0.22044334975369459, 0.26970443349753692, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22044334975369459, 0.21551724137931033, 0.17857142857142858, 0.17610837438423646, 0.22044334975369459, 0.23152709359605911, 0.2019704433497537, 0.19704433497536947, 0.26354679802955666, 0.17733990147783252, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.54679802955665024, 0.61576354679802958, 0.61699507389162567, 0.55172413793103448, 0.51847290640394084, 0.60467980295566504, 0.56280788177339902, 0.54187192118226601, 0.55911330049261088, 0.5431034482758621], 5: [0.22783251231527094, 0.2376847290640394, 0.20566502463054187, 0.20689655172413793, 0.22783251231527094, 0.25, 0.19334975369458129, 0.24014778325123154, 0.19458128078817735, 0.26354679802955666, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22044334975369459, 0.22167487684729065, 0.17980295566502463, 0.16871921182266009, 0.21305418719211822, 0.21182266009852216, 0.18719211822660098, 0.22906403940886699, 0.2536945812807882, 0.19088669950738915, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.56527093596059108, 0.62931034482758619, 0.61576354679802958, 0.56403940886699511, 0.53201970443349755, 0.60591133004926112, 0.51108374384236455, 0.54064039408866993, 0.54926108374384242, 0.54926108374384242], 5: [0.22783251231527094, 0.21305418719211822, 0.19088669950738915, 0.21551724137931033, 0.2229064039408867, 0.25615763546798032, 0.20689655172413793, 0.25985221674876846, 0.20566502463054187, 0.25985221674876846, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22044334975369459, 0.20443349753694581, 0.18103448275862069, 0.18103448275862069, 0.20073891625615764, 0.23645320197044334, 0.17364532019704434, 0.22167487684729065, 0.21921182266009853, 0.19211822660098521, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.56896551724137934, 0.60221674876847286, 0.58866995073891626, 0.5788177339901478, 0.49137931034482757, 0.59359605911330049, 0.53201970443349755, 0.57635467980295563, 0.54926108374384242, 0.54679802955665024], 5: [0.22783251231527094, 0.22660098522167488, 0.21674876847290642, 0.23029556650246305, 0.22044334975369459, 0.27216748768472904, 0.23275862068965517, 0.24630541871921183, 0.20443349753694581, 0.25862068965517243, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.238311 minutes
Weight histogram
[ 184  569  703  472  702 1627 3978 7393 6191  456] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 151  245  352  627  822  959 2154 3086 6177 7702] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.66873
Epoch 1, cost is  1.62975
Epoch 2, cost is  1.60624
Epoch 3, cost is  1.58603
Epoch 4, cost is  1.56819
Training took 0.224583 minutes
Weight histogram
[4519 3731 2952 2491 2006 1732 1339 1253 1492  760] [ -5.56989387e-02  -5.01220619e-02  -4.45451852e-02  -3.89683085e-02
  -3.33914318e-02  -2.78145550e-02  -2.22376783e-02  -1.66608016e-02
  -1.10839249e-02  -5.50704812e-03   6.98286021e-05]
[2136 1177 1409 1636 1841 2194 2441 2705 3105 3631] [ -5.56989387e-02  -5.01220619e-02  -4.45451852e-02  -3.89683085e-02
  -3.33914318e-02  -2.78145550e-02  -2.22376783e-02  -1.66608016e-02
  -1.10839249e-02  -5.50704812e-03   6.98286021e-05]
-0.868565
1.58598
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149620 minutes
Weight histogram
[  65   94  749 1648 3108 3814 4768 5739 3554  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 13094] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.115033 minutes
Weight histogram
[4046 3110 2578 2775 2143 1832 1760 1663 3626  767] [ -5.38726188e-02  -4.84783741e-02  -4.30841293e-02  -3.76898846e-02
  -3.22956399e-02  -2.69013951e-02  -2.15071504e-02  -1.61129056e-02
  -1.07186609e-02  -5.32441614e-03   6.98286021e-05]
[4718 1272 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84783741e-02  -4.30841293e-02  -3.76898846e-02
  -3.22956399e-02  -2.69013951e-02  -2.15071504e-02  -1.61129056e-02
  -1.07186609e-02  -5.32441614e-03   6.98286021e-05]
-1.06456
1.29569
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.30143
Epoch 1, cost is  6.01037
Epoch 2, cost is  5.7538
Epoch 3, cost is  5.39076
Epoch 4, cost is  5.01343
Epoch 5, cost is  4.70825
Epoch 6, cost is  4.4614
Epoch 7, cost is  4.26386
Epoch 8, cost is  4.09508
Epoch 9, cost is  3.95118
Training took 0.250710 minutes
Weight histogram
[461 431 426 401 409 478 743 473 210  18] [-0.02929136 -0.02637985 -0.02346835 -0.02055684 -0.01764533 -0.01473382
 -0.01182231 -0.00891081 -0.0059993  -0.00308779 -0.00017628]
[784 470 282 274 296 319 352 390 423 460] [-0.02929136 -0.02637985 -0.02346835 -0.02055684 -0.01764533 -0.01473382
 -0.01182231 -0.00891081 -0.0059993  -0.00308779 -0.00017628]
-0.459907
0.641182
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076005 minutes
Epoch 0
Fine tuning took 0.077008 minutes
Epoch 0
Fine tuning took 0.075885 minutes
Epoch 0
Fine tuning took 0.076366 minutes
Epoch 0
Fine tuning took 0.077521 minutes
Epoch 0
Fine tuning took 0.077904 minutes
Epoch 0
Fine tuning took 0.077000 minutes
Epoch 0
Fine tuning took 0.077063 minutes
Epoch 0
Fine tuning took 0.076972 minutes
Epoch 0
Fine tuning took 0.076864 minutes
{'zero': {0: [0.16133004926108374, 0.12807881773399016, 0.084975369458128072, 0.057881773399014777, 0.068965517241379309, 0.059113300492610835, 0.035714285714285712, 0.089901477832512317, 0.077586206896551727, 0.099753694581280791, 0.062807881773399021], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.65886699507389157, 0.75862068965517238, 0.79679802955665024, 0.83374384236453203, 0.83004926108374388, 0.85344827586206895, 0.79433497536945807, 0.88177339901477836, 0.83497536945812811, 0.75], 5: [0.1354679802955665, 0.21305418719211822, 0.15640394088669951, 0.14532019704433496, 0.097290640394088676, 0.11083743842364532, 0.11083743842364532, 0.11576354679802955, 0.04064039408866995, 0.065270935960591137, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.12807881773399016, 0.11083743842364532, 0.094827586206896547, 0.088669950738916259, 0.048029556650246302, 0.019704433497536946, 0.13300492610837439, 0.086206896551724144, 0.098522167487684734, 0.059113300492610835], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.70566502463054193, 0.78448275862068961, 0.81403940886699511, 0.82019704433497542, 0.87438423645320196, 0.90640394088669951, 0.78078817733990147, 0.88916256157635465, 0.84359605911330049, 0.76724137931034486], 5: [0.1354679802955665, 0.16625615763546797, 0.10467980295566502, 0.091133004926108374, 0.091133004926108374, 0.077586206896551727, 0.073891625615763554, 0.086206896551724144, 0.024630541871921183, 0.057881773399014777, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.16009852216748768, 0.11576354679802955, 0.10960591133004927, 0.098522167487684734, 0.044334975369458129, 0.027093596059113302, 0.16748768472906403, 0.076354679802955669, 0.092364532019704432, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.69827586206896552, 0.77709359605911332, 0.80049261083743839, 0.80295566502463056, 0.86945812807881773, 0.89532019704433496, 0.7426108374384236, 0.89039408866995073, 0.84359605911330049, 0.76724137931034486], 5: [0.1354679802955665, 0.14162561576354679, 0.10714285714285714, 0.089901477832512317, 0.098522167487684734, 0.086206896551724144, 0.077586206896551727, 0.089901477832512317, 0.033251231527093597, 0.064039408866995079, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.15147783251231528, 0.12931034482758622, 0.088669950738916259, 0.093596059113300489, 0.041871921182266007, 0.014778325123152709, 0.12315270935960591, 0.060344827586206899, 0.086206896551724144, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.71059113300492616, 0.76354679802955661, 0.81403940886699511, 0.82635467980295563, 0.88054187192118227, 0.9211822660098522, 0.79926108374384242, 0.91625615763546797, 0.84605911330049266, 0.75369458128078815], 5: [0.1354679802955665, 0.13793103448275862, 0.10714285714285714, 0.097290640394088676, 0.080049261083743842, 0.077586206896551727, 0.064039408866995079, 0.077586206896551727, 0.023399014778325122, 0.067733990147783252, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237765 minutes
Weight histogram
[ 184  569  703  472  702 1627 3978 7393 6191  456] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 151  245  352  627  822  959 2154 3086 6177 7702] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.66873
Epoch 1, cost is  1.62975
Epoch 2, cost is  1.60624
Epoch 3, cost is  1.58603
Epoch 4, cost is  1.56819
Training took 0.224103 minutes
Weight histogram
[4519 3731 2952 2491 2006 1732 1339 1253 1492  760] [ -5.56989387e-02  -5.01220619e-02  -4.45451852e-02  -3.89683085e-02
  -3.33914318e-02  -2.78145550e-02  -2.22376783e-02  -1.66608016e-02
  -1.10839249e-02  -5.50704812e-03   6.98286021e-05]
[2136 1177 1409 1636 1841 2194 2441 2705 3105 3631] [ -5.56989387e-02  -5.01220619e-02  -4.45451852e-02  -3.89683085e-02
  -3.33914318e-02  -2.78145550e-02  -2.22376783e-02  -1.66608016e-02
  -1.10839249e-02  -5.50704812e-03   6.98286021e-05]
-0.868565
1.58598
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148630 minutes
Weight histogram
[  65   94  749 1648 3108 3814 4768 5739 3554  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 13094] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.116316 minutes
Weight histogram
[4046 3110 2578 2775 2143 1832 1760 1663 3626  767] [ -5.38726188e-02  -4.84783741e-02  -4.30841293e-02  -3.76898846e-02
  -3.22956399e-02  -2.69013951e-02  -2.15071504e-02  -1.61129056e-02
  -1.07186609e-02  -5.32441614e-03   6.98286021e-05]
[4718 1272 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84783741e-02  -4.30841293e-02  -3.76898846e-02
  -3.22956399e-02  -2.69013951e-02  -2.15071504e-02  -1.61129056e-02
  -1.07186609e-02  -5.32441614e-03   6.98286021e-05]
-1.06456
1.29569
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.67935
Epoch 1, cost is  5.39334
Epoch 2, cost is  5.05756
Epoch 3, cost is  4.56495
Epoch 4, cost is  4.15266
Epoch 5, cost is  3.85693
Epoch 6, cost is  3.62831
Epoch 7, cost is  3.425
Epoch 8, cost is  3.24277
Epoch 9, cost is  3.08375
Training took 0.354306 minutes
Weight histogram
[ 741  596  474  390  428 1018  305   68   21    9] [-0.01850437 -0.01666854 -0.01483271 -0.01299688 -0.01116105 -0.00932522
 -0.00748939 -0.00565356 -0.00381772 -0.00198189 -0.00014606]
[982 275 249 266 305 351 383 400 411 428] [-0.01850437 -0.01666854 -0.01483271 -0.01299688 -0.01116105 -0.00932522
 -0.00748939 -0.00565356 -0.00381772 -0.00198189 -0.00014606]
-0.269547
0.387319
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080747 minutes
Epoch 0
Fine tuning took 0.081624 minutes
Epoch 0
Fine tuning took 0.082403 minutes
Epoch 0
Fine tuning took 0.081911 minutes
Epoch 0
Fine tuning took 0.082702 minutes
Epoch 0
Fine tuning took 0.081856 minutes
Epoch 0
Fine tuning took 0.081279 minutes
Epoch 0
Fine tuning took 0.082349 minutes
Epoch 0
Fine tuning took 0.082645 minutes
Epoch 0
Fine tuning took 0.082012 minutes
{'zero': {0: [0.15763546798029557, 0.14778325123152711, 0.1625615763546798, 0.16502463054187191, 0.18965517241379309, 0.19088669950738915, 0.19088669950738915, 0.18226600985221675, 0.17241379310344829, 0.13423645320197045, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.63669950738916259, 0.59729064039408863, 0.6219211822660099, 0.63054187192118227, 0.6785714285714286, 0.61576354679802958, 0.62931034482758619, 0.65640394088669951, 0.69950738916256161, 0.69458128078817738], 5: [0.13300492610837439, 0.21551724137931033, 0.24014778325123154, 0.21305418719211822, 0.17980295566502463, 0.13054187192118227, 0.19334975369458129, 0.18842364532019704, 0.17118226600985223, 0.16625615763546797, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.15024630541871922, 0.1748768472906404, 0.18719211822660098, 0.12807881773399016, 0.20566502463054187, 0.13916256157635468, 0.15640394088669951, 0.15024630541871922, 0.12315270935960591, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.6711822660098522, 0.66995073891625612, 0.67241379310344829, 0.75, 0.64901477832512311, 0.70443349753694584, 0.70320197044334976, 0.69334975369458129, 0.73522167487684731, 0.69334975369458129], 5: [0.13300492610837439, 0.17857142857142858, 0.15517241379310345, 0.14039408866995073, 0.12192118226600986, 0.14532019704433496, 0.15640394088669951, 0.14039408866995073, 0.15640394088669951, 0.14162561576354679, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.14285714285714285, 0.16379310344827586, 0.17610837438423646, 0.10837438423645321, 0.21428571428571427, 0.13177339901477833, 0.11576354679802955, 0.15147783251231528, 0.096059113300492605, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.68103448275862066, 0.68472906403940892, 0.69211822660098521, 0.77463054187192115, 0.63669950738916259, 0.71305418719211822, 0.70566502463054193, 0.72044334975369462, 0.75615763546798032, 0.73029556650246308], 5: [0.13300492610837439, 0.17610837438423646, 0.15147783251231528, 0.13177339901477833, 0.11699507389162561, 0.14901477832512317, 0.15517241379310345, 0.17857142857142858, 0.12807881773399016, 0.14778325123152711, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.14532019704433496, 0.14532019704433496, 0.15147783251231528, 0.11083743842364532, 0.18596059113300492, 0.15147783251231528, 0.11576354679802955, 0.16625615763546797, 0.1206896551724138, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.69458128078817738, 0.69827586206896552, 0.71182266009852213, 0.75123152709359609, 0.66871921182266014, 0.68226600985221675, 0.73029556650246308, 0.69334975369458129, 0.73768472906403937, 0.71674876847290636], 5: [0.13300492610837439, 0.16009852216748768, 0.15640394088669951, 0.13669950738916256, 0.13793103448275862, 0.14532019704433496, 0.16625615763546797, 0.1539408866995074, 0.14039408866995073, 0.14162561576354679, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236119 minutes
Weight histogram
[ 184  569  703  472  702 1627 3978 7393 6191  456] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 151  245  352  627  822  959 2154 3086 6177 7702] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.66873
Epoch 1, cost is  1.62975
Epoch 2, cost is  1.60624
Epoch 3, cost is  1.58603
Epoch 4, cost is  1.56819
Training took 0.225183 minutes
Weight histogram
[4519 3731 2952 2491 2006 1732 1339 1253 1492  760] [ -5.56989387e-02  -5.01220619e-02  -4.45451852e-02  -3.89683085e-02
  -3.33914318e-02  -2.78145550e-02  -2.22376783e-02  -1.66608016e-02
  -1.10839249e-02  -5.50704812e-03   6.98286021e-05]
[2136 1177 1409 1636 1841 2194 2441 2705 3105 3631] [ -5.56989387e-02  -5.01220619e-02  -4.45451852e-02  -3.89683085e-02
  -3.33914318e-02  -2.78145550e-02  -2.22376783e-02  -1.66608016e-02
  -1.10839249e-02  -5.50704812e-03   6.98286021e-05]
-0.868565
1.58598
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150311 minutes
Weight histogram
[  65   94  749 1648 3108 3814 4768 5739 3554  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 13094] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.116682 minutes
Weight histogram
[4046 3110 2578 2775 2143 1832 1760 1663 3626  767] [ -5.38726188e-02  -4.84783741e-02  -4.30841293e-02  -3.76898846e-02
  -3.22956399e-02  -2.69013951e-02  -2.15071504e-02  -1.61129056e-02
  -1.07186609e-02  -5.32441614e-03   6.98286021e-05]
[4718 1272 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84783741e-02  -4.30841293e-02  -3.76898846e-02
  -3.22956399e-02  -2.69013951e-02  -2.15071504e-02  -1.61129056e-02
  -1.07186609e-02  -5.32441614e-03   6.98286021e-05]
-1.06456
1.29569
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.08343
Epoch 1, cost is  4.84216
Epoch 2, cost is  4.49273
Epoch 3, cost is  4.02045
Epoch 4, cost is  3.65447
Epoch 5, cost is  3.38502
Epoch 6, cost is  3.1462
Epoch 7, cost is  2.92921
Epoch 8, cost is  2.75106
Epoch 9, cost is  2.60403
Training took 0.543740 minutes
Weight histogram
[ 935  808  547 1333  251   95   42   22   11    6] [-0.01303397 -0.01174412 -0.01045427 -0.00916442 -0.00787457 -0.00658472
 -0.00529486 -0.00400501 -0.00271516 -0.00142531 -0.00013546]
[978 267 254 279 327 358 367 377 403 440] [-0.01303397 -0.01174412 -0.01045427 -0.00916442 -0.00787457 -0.00658472
 -0.00529486 -0.00400501 -0.00271516 -0.00142531 -0.00013546]
-0.192691
0.243983
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.091558 minutes
Epoch 0
Fine tuning took 0.090800 minutes
Epoch 0
Fine tuning took 0.090619 minutes
Epoch 0
Fine tuning took 0.090738 minutes
Epoch 0
Fine tuning took 0.091526 minutes
Epoch 0
Fine tuning took 0.091412 minutes
Epoch 0
Fine tuning took 0.091224 minutes
Epoch 0
Fine tuning took 0.090587 minutes
Epoch 0
Fine tuning took 0.090751 minutes
Epoch 0
Fine tuning took 0.090897 minutes
{'zero': {0: [0.17857142857142858, 0.11330049261083744, 0.23029556650246305, 0.18226600985221675, 0.17733990147783252, 0.26354679802955666, 0.23522167487684728, 0.24753694581280788, 0.17733990147783252, 0.14162561576354679, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.67610837438423643, 0.57512315270935965, 0.60221674876847286, 0.55665024630541871, 0.53201970443349755, 0.60098522167487689, 0.55049261083743839, 0.62438423645320196, 0.64778325123152714, 0.58620689655172409], 5: [0.1625615763546798, 0.2105911330049261, 0.19458128078817735, 0.21551724137931033, 0.26600985221674878, 0.20443349753694581, 0.16379310344827586, 0.2019704433497537, 0.19827586206896552, 0.2105911330049261, 0.25246305418719212], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17857142857142858, 0.13300492610837439, 0.19827586206896552, 0.17241379310344829, 0.16748768472906403, 0.23399014778325122, 0.20689655172413793, 0.25985221674876846, 0.15886699507389163, 0.14532019704433496, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.67733990147783252, 0.63300492610837433, 0.63669950738916259, 0.61330049261083741, 0.56280788177339902, 0.59359605911330049, 0.51108374384236455, 0.64408866995073888, 0.64039408866995073, 0.56650246305418717], 5: [0.1625615763546798, 0.18965517241379309, 0.16871921182266009, 0.19088669950738915, 0.21921182266009853, 0.20320197044334976, 0.19950738916256158, 0.22906403940886699, 0.19704433497536947, 0.21428571428571427, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17857142857142858, 0.1539408866995074, 0.19950738916256158, 0.16995073891625614, 0.1625615763546798, 0.25246305418719212, 0.23152709359605911, 0.2229064039408867, 0.14408866995073891, 0.14162561576354679, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.6576354679802956, 0.63054187192118227, 0.62684729064039413, 0.61822660098522164, 0.59359605911330049, 0.62068965517241381, 0.56034482758620685, 0.6576354679802956, 0.67487684729064035, 0.65270935960591137], 5: [0.1625615763546798, 0.18842364532019704, 0.16995073891625614, 0.20320197044334976, 0.21921182266009853, 0.1539408866995074, 0.14778325123152711, 0.21674876847290642, 0.19827586206896552, 0.18349753694581281, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17857142857142858, 0.11330049261083744, 0.18965517241379309, 0.15024630541871922, 0.15517241379310345, 0.24876847290640394, 0.22536945812807882, 0.23275862068965517, 0.16871921182266009, 0.1354679802955665, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.67241379310344829, 0.62315270935960587, 0.64655172413793105, 0.58620689655172409, 0.58004926108374388, 0.59482758620689657, 0.56650246305418717, 0.66133004926108374, 0.6576354679802956, 0.61945812807881773], 5: [0.1625615763546798, 0.21428571428571427, 0.18719211822660098, 0.20320197044334976, 0.25862068965517243, 0.17118226600985223, 0.17980295566502463, 0.20073891625615764, 0.16995073891625614, 0.20689655172413793, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234601 minutes
Weight histogram
[ 184  569  703  472  702 1627 3978 7393 6191  456] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 151  245  352  627  822  959 2154 3086 6177 7702] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.66873
Epoch 1, cost is  1.62975
Epoch 2, cost is  1.60624
Epoch 3, cost is  1.58603
Epoch 4, cost is  1.56819
Training took 0.224068 minutes
Weight histogram
[4519 3731 2952 2491 2006 1732 1339 1253 1492  760] [ -5.56989387e-02  -5.01220619e-02  -4.45451852e-02  -3.89683085e-02
  -3.33914318e-02  -2.78145550e-02  -2.22376783e-02  -1.66608016e-02
  -1.10839249e-02  -5.50704812e-03   6.98286021e-05]
[2136 1177 1409 1636 1841 2194 2441 2705 3105 3631] [ -5.56989387e-02  -5.01220619e-02  -4.45451852e-02  -3.89683085e-02
  -3.33914318e-02  -2.78145550e-02  -2.22376783e-02  -1.66608016e-02
  -1.10839249e-02  -5.50704812e-03   6.98286021e-05]
-0.868565
1.58598
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149955 minutes
Weight histogram
[  65   94  749 1648 3108 3814 4768 5739 3554  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 13094] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.116380 minutes
Weight histogram
[4046 3110 2578 2775 2143 1832 1760 1663 3626  767] [ -5.38726188e-02  -4.84783741e-02  -4.30841293e-02  -3.76898846e-02
  -3.22956399e-02  -2.69013951e-02  -2.15071504e-02  -1.61129056e-02
  -1.07186609e-02  -5.32441614e-03   6.98286021e-05]
[4718 1272 1280 1549 1859 2109 2379 2699 3173 3262] [ -5.38726188e-02  -4.84783741e-02  -4.30841293e-02  -3.76898846e-02
  -3.22956399e-02  -2.69013951e-02  -2.15071504e-02  -1.61129056e-02
  -1.07186609e-02  -5.32441614e-03   6.98286021e-05]
-1.06456
1.29569
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.86327
Epoch 1, cost is  4.56355
Epoch 2, cost is  3.94504
Epoch 3, cost is  3.45791
Epoch 4, cost is  3.10883
Epoch 5, cost is  2.81622
Epoch 6, cost is  2.58854
Epoch 7, cost is  2.41556
Epoch 8, cost is  2.2786
Epoch 9, cost is  2.16999
Training took 0.943258 minutes
Weight histogram
[918 728 706 466 999 181  29  13   6   4] [-0.0080684  -0.00727492 -0.00648143 -0.00568795 -0.00489447 -0.00410099
 -0.00330751 -0.00251403 -0.00172054 -0.00092706 -0.00013358]
[816 245 256 297 325 337 365 409 469 531] [-0.0080684  -0.00727492 -0.00648143 -0.00568795 -0.00489447 -0.00410099
 -0.00330751 -0.00251403 -0.00172054 -0.00092706 -0.00013358]
-0.170789
0.170343
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.110770 minutes
Epoch 0
Fine tuning took 0.108732 minutes
Epoch 0
Fine tuning took 0.110133 minutes
Epoch 0
Fine tuning took 0.110972 minutes
Epoch 0
Fine tuning took 0.110100 minutes
Epoch 0
Fine tuning took 0.109826 minutes
Epoch 0
Fine tuning took 0.110083 minutes
Epoch 0
Fine tuning took 0.110623 minutes
Epoch 0
Fine tuning took 0.110356 minutes
Epoch 0
Fine tuning took 0.110030 minutes
{'zero': {0: [0.17610837438423646, 0.21921182266009853, 0.16871921182266009, 0.17610837438423646, 0.2105911330049261, 0.25123152709359609, 0.22167487684729065, 0.18965517241379309, 0.21428571428571427, 0.17610837438423646, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59975369458128081, 0.57019704433497542, 0.62561576354679804, 0.61083743842364535, 0.52832512315270941, 0.49753694581280788, 0.58004926108374388, 0.54556650246305416, 0.5788177339901478, 0.61330049261083741, 0.5714285714285714], 5: [0.22413793103448276, 0.2105911330049261, 0.20566502463054187, 0.21305418719211822, 0.26108374384236455, 0.25123152709359609, 0.19827586206896552, 0.26477832512315269, 0.20689655172413793, 0.2105911330049261, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17610837438423646, 0.18226600985221675, 0.16871921182266009, 0.1748768472906404, 0.19581280788177341, 0.24507389162561577, 0.22044334975369459, 0.25246305418719212, 0.22536945812807882, 0.16625615763546797, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59975369458128081, 0.58866995073891626, 0.61945812807881773, 0.60837438423645318, 0.57512315270935965, 0.51600985221674878, 0.57019704433497542, 0.48275862068965519, 0.56280788177339902, 0.67487684729064035, 0.57019704433497542], 5: [0.22413793103448276, 0.22906403940886699, 0.21182266009852216, 0.21674876847290642, 0.22906403940886699, 0.23891625615763548, 0.20935960591133004, 0.26477832512315269, 0.21182266009852216, 0.15886699507389163, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17610837438423646, 0.22167487684729065, 0.17364532019704434, 0.19088669950738915, 0.19458128078817735, 0.26231527093596058, 0.2376847290640394, 0.25246305418719212, 0.21798029556650247, 0.17857142857142858, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59975369458128081, 0.60098522167487689, 0.6071428571428571, 0.58620689655172409, 0.55049261083743839, 0.50369458128078815, 0.58743842364532017, 0.52339901477832518, 0.56896551724137934, 0.60837438423645318, 0.56034482758620685], 5: [0.22413793103448276, 0.17733990147783252, 0.21921182266009853, 0.2229064039408867, 0.25492610837438423, 0.23399014778325122, 0.1748768472906404, 0.22413793103448276, 0.21305418719211822, 0.21305418719211822, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17610837438423646, 0.18596059113300492, 0.15517241379310345, 0.19211822660098521, 0.20812807881773399, 0.26354679802955666, 0.21674876847290642, 0.20935960591133004, 0.23275862068965517, 0.14162561576354679, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59975369458128081, 0.61206896551724133, 0.61206896551724133, 0.59729064039408863, 0.55911330049261088, 0.51231527093596063, 0.59482758620689657, 0.54802955665024633, 0.56157635467980294, 0.66871921182266014, 0.55911330049261088], 5: [0.22413793103448276, 0.2019704433497537, 0.23275862068965517, 0.2105911330049261, 0.23275862068965517, 0.22413793103448276, 0.18842364532019704, 0.24261083743842365, 0.20566502463054187, 0.18965517241379309, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420257 minutes
Weight histogram
[ 231  514  610  458  859 5370 9146 3389 1487  211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1149  972  756 1097 1297 2394 2480 3121 3621 5388] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.22702
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.13163
Epoch 1, cost is  1.10485
Epoch 2, cost is  1.08877
Epoch 3, cost is  1.07324
Epoch 4, cost is  1.05977
Training took 0.648255 minutes
Weight histogram
[5083 4204 3087 2554 1710 1457 1324  951  933  972] [ -4.07019220e-02  -3.66372561e-02  -3.25725902e-02  -2.85079243e-02
  -2.44432583e-02  -2.03785924e-02  -1.63139265e-02  -1.22492605e-02
  -8.18459462e-03  -4.11992870e-03  -5.52627716e-05]
[1834 1246 1303 1458 1885 2092 2713 2870 3043 3831] [ -4.07019220e-02  -3.66372561e-02  -3.25725902e-02  -2.85079243e-02
  -2.44432583e-02  -2.03785924e-02  -1.63139265e-02  -1.22492605e-02
  -8.18459462e-03  -4.11992870e-03  -5.52627716e-05]
-0.876214
1.7614
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151277 minutes
Weight histogram
[  65   75  931 1941 2832 3634 4768 5739 3554  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 13096] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.116146 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 1655 2871 1532] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4717 1272 1280 1548 1859 2110 2380 2698 3174 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.43997
Epoch 1, cost is  6.06881
Epoch 2, cost is  5.60083
Epoch 3, cost is  5.20471
Epoch 4, cost is  4.89716
Epoch 5, cost is  4.6509
Epoch 6, cost is  4.44835
Epoch 7, cost is  4.28245
Epoch 8, cost is  4.14117
Epoch 9, cost is  4.01674
Training took 0.325102 minutes
Weight histogram
[512 489 419 399 369 369 341 414 585 153] [-0.03341708 -0.03009224 -0.0267674  -0.02344256 -0.02011772 -0.01679288
 -0.01346805 -0.01014321 -0.00681837 -0.00349353 -0.00016869]
[653 290 279 314 332 361 395 439 472 515] [-0.03341708 -0.03009224 -0.0267674  -0.02344256 -0.02011772 -0.01679288
 -0.01346805 -0.01014321 -0.00681837 -0.00349353 -0.00016869]
-0.411392
0.49972
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.141578 minutes
Epoch 0
Fine tuning took 0.143631 minutes
Epoch 0
Fine tuning took 0.141976 minutes
Epoch 0
Fine tuning took 0.142840 minutes
Epoch 0
Fine tuning took 0.143101 minutes
Epoch 0
Fine tuning took 0.143178 minutes
Epoch 0
Fine tuning took 0.143703 minutes
Epoch 0
Fine tuning took 0.143670 minutes
Epoch 0
Fine tuning took 0.141094 minutes
Epoch 0
Fine tuning took 0.141603 minutes
{'zero': {0: [0.17241379310344829, 0.11206896551724138, 0.029556650246305417, 0.046798029556650245, 0.072660098522167482, 0.04064039408866995, 0.048029556650246302, 0.060344827586206899, 0.11945812807881774, 0.078817733990147784, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.50615763546798032, 0.91256157635467983, 0.80911330049261088, 0.8571428571428571, 0.79064039408866993, 0.74137931034482762, 0.74384236453201968, 0.76847290640394084, 0.76477832512315269, 0.6785714285714286], 5: [0.16625615763546797, 0.3817733990147783, 0.057881773399014777, 0.14408866995073891, 0.070197044334975367, 0.16871921182266009, 0.2105911330049261, 0.19581280788177341, 0.11206896551724138, 0.15640394088669951, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17241379310344829, 0.093596059113300489, 0.033251231527093597, 0.046798029556650245, 0.089901477832512317, 0.029556650246305417, 0.064039408866995079, 0.083743842364532015, 0.11206896551724138, 0.076354679802955669, 0.098522167487684734], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.52093596059113301, 0.90024630541871919, 0.83128078817733986, 0.84359605911330049, 0.80911330049261088, 0.73029556650246308, 0.75862068965517238, 0.80172413793103448, 0.78078817733990147, 0.74137931034482762], 5: [0.16625615763546797, 0.3854679802955665, 0.066502463054187194, 0.12192118226600986, 0.066502463054187194, 0.16133004926108374, 0.20566502463054187, 0.15763546798029557, 0.086206896551724144, 0.14285714285714285, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17241379310344829, 0.10714285714285714, 0.035714285714285712, 0.068965517241379309, 0.11822660098522167, 0.029556650246305417, 0.076354679802955669, 0.10344827586206896, 0.099753694581280791, 0.086206896551724144, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.50985221674876846, 0.89901477832512311, 0.82266009852216748, 0.82389162561576357, 0.77216748768472909, 0.73522167487684731, 0.7426108374384236, 0.80295566502463056, 0.77339901477832518, 0.76970443349753692], 5: [0.16625615763546797, 0.38300492610837439, 0.065270935960591137, 0.10837438423645321, 0.057881773399014777, 0.19827586206896552, 0.18842364532019704, 0.1539408866995074, 0.097290640394088676, 0.14039408866995073, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17241379310344829, 0.13177339901477833, 0.027093596059113302, 0.056650246305418719, 0.10591133004926108, 0.02832512315270936, 0.059113300492610835, 0.080049261083743842, 0.086206896551724144, 0.083743842364532015, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.5, 0.92610837438423643, 0.81773399014778325, 0.81896551724137934, 0.79679802955665024, 0.74014778325123154, 0.77093596059113301, 0.83866995073891626, 0.76847290640394084, 0.74876847290640391], 5: [0.16625615763546797, 0.3682266009852217, 0.046798029556650245, 0.12561576354679804, 0.075123152709359611, 0.1748768472906404, 0.20073891625615764, 0.14901477832512317, 0.075123152709359611, 0.14778325123152711, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421961 minutes
Weight histogram
[ 231  514  610  458  859 5370 9146 3389 1487  211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1149  972  756 1097 1297 2394 2480 3121 3621 5388] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.22702
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.13163
Epoch 1, cost is  1.10485
Epoch 2, cost is  1.08877
Epoch 3, cost is  1.07324
Epoch 4, cost is  1.05977
Training took 0.648880 minutes
Weight histogram
[5083 4204 3087 2554 1710 1457 1324  951  933  972] [ -4.07019220e-02  -3.66372561e-02  -3.25725902e-02  -2.85079243e-02
  -2.44432583e-02  -2.03785924e-02  -1.63139265e-02  -1.22492605e-02
  -8.18459462e-03  -4.11992870e-03  -5.52627716e-05]
[1834 1246 1303 1458 1885 2092 2713 2870 3043 3831] [ -4.07019220e-02  -3.66372561e-02  -3.25725902e-02  -2.85079243e-02
  -2.44432583e-02  -2.03785924e-02  -1.63139265e-02  -1.22492605e-02
  -8.18459462e-03  -4.11992870e-03  -5.52627716e-05]
-0.876214
1.7614
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151754 minutes
Weight histogram
[  65   75  931 1941 2832 3634 4768 5739 3554  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 13096] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.116823 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 1655 2871 1532] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4717 1272 1280 1548 1859 2110 2380 2698 3174 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.94026
Epoch 1, cost is  5.4726
Epoch 2, cost is  4.8806
Epoch 3, cost is  4.42065
Epoch 4, cost is  4.06691
Epoch 5, cost is  3.78278
Epoch 6, cost is  3.55528
Epoch 7, cost is  3.36923
Epoch 8, cost is  3.21067
Epoch 9, cost is  3.07807
Training took 0.497655 minutes
Weight histogram
[604 537 497 439 388 368 343 656 204  14] [-0.02047332 -0.01844112 -0.01640892 -0.01437671 -0.01234451 -0.01031231
 -0.0082801  -0.0062479  -0.0042157  -0.0021835  -0.00015129]
[705 274 309 312 337 359 390 417 454 493] [-0.02047332 -0.01844112 -0.01640892 -0.01437671 -0.01234451 -0.01031231
 -0.0082801  -0.0062479  -0.0042157  -0.0021835  -0.00015129]
-0.381438
0.375299
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.150540 minutes
Epoch 0
Fine tuning took 0.150623 minutes
Epoch 0
Fine tuning took 0.148367 minutes
Epoch 0
Fine tuning took 0.150822 minutes
Epoch 0
Fine tuning took 0.151037 minutes
Epoch 0
Fine tuning took 0.149255 minutes
Epoch 0
Fine tuning took 0.149605 minutes
Epoch 0
Fine tuning took 0.151280 minutes
Epoch 0
Fine tuning took 0.150343 minutes
Epoch 0
Fine tuning took 0.150192 minutes
{'zero': {0: [0.1625615763546798, 0.25492610837438423, 0.11206896551724138, 0.21674876847290642, 0.24384236453201971, 0.094827586206896547, 0.14778325123152711, 0.19950738916256158, 0.20320197044334976, 0.16502463054187191, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.44334975369458129, 0.53694581280788178, 0.58128078817733986, 0.63793103448275867, 0.77093596059113301, 0.66256157635467983, 0.56896551724137934, 0.64901477832512311, 0.67980295566502458, 0.60221674876847286], 5: [0.13669950738916256, 0.30172413793103448, 0.35098522167487683, 0.2019704433497537, 0.11822660098522167, 0.13423645320197045, 0.18965517241379309, 0.23152709359605911, 0.14778325123152711, 0.15517241379310345, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1625615763546798, 0.22536945812807882, 0.066502463054187194, 0.15517241379310345, 0.16133004926108374, 0.076354679802955669, 0.12807881773399016, 0.16379310344827586, 0.18103448275862069, 0.16133004926108374, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.58990147783251234, 0.65886699507389157, 0.6711822660098522, 0.72536945812807885, 0.80541871921182262, 0.69334975369458129, 0.66995073891625612, 0.70812807881773399, 0.69827586206896552, 0.68719211822660098], 5: [0.13669950738916256, 0.18472906403940886, 0.27463054187192121, 0.17364532019704434, 0.11330049261083744, 0.11822660098522167, 0.17857142857142858, 0.16625615763546797, 0.11083743842364532, 0.14039408866995073, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1625615763546798, 0.21674876847290642, 0.078817733990147784, 0.16009852216748768, 0.17364532019704434, 0.10098522167487685, 0.096059113300492605, 0.14162561576354679, 0.15270935960591134, 0.14532019704433496, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.61083743842364535, 0.71059113300492616, 0.68349753694581283, 0.72413793103448276, 0.79064039408866993, 0.7068965517241379, 0.69827586206896552, 0.70812807881773399, 0.72290640394088668, 0.67364532019704437], 5: [0.13669950738916256, 0.17241379310344829, 0.2105911330049261, 0.15640394088669951, 0.10221674876847291, 0.10837438423645321, 0.19704433497536947, 0.16009852216748768, 0.13916256157635468, 0.13177339901477833, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1625615763546798, 0.18842364532019704, 0.071428571428571425, 0.14408866995073891, 0.14039408866995073, 0.086206896551724144, 0.10221674876847291, 0.17980295566502463, 0.18472906403940886, 0.15763546798029557, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.6219211822660099, 0.72167487684729059, 0.66133004926108374, 0.73768472906403937, 0.79433497536945807, 0.7068965517241379, 0.66256157635467983, 0.71921182266009853, 0.71921182266009853, 0.66871921182266014], 5: [0.13669950738916256, 0.18965517241379309, 0.20689655172413793, 0.19458128078817735, 0.12192118226600986, 0.11945812807881774, 0.19088669950738915, 0.15763546798029557, 0.096059113300492605, 0.12315270935960591, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422287 minutes
Weight histogram
[ 231  514  610  458  859 5370 9146 3389 1487  211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1149  972  756 1097 1297 2394 2480 3121 3621 5388] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.22702
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.13163
Epoch 1, cost is  1.10485
Epoch 2, cost is  1.08877
Epoch 3, cost is  1.07324
Epoch 4, cost is  1.05977
Training took 0.647947 minutes
Weight histogram
[5083 4204 3087 2554 1710 1457 1324  951  933  972] [ -4.07019220e-02  -3.66372561e-02  -3.25725902e-02  -2.85079243e-02
  -2.44432583e-02  -2.03785924e-02  -1.63139265e-02  -1.22492605e-02
  -8.18459462e-03  -4.11992870e-03  -5.52627716e-05]
[1834 1246 1303 1458 1885 2092 2713 2870 3043 3831] [ -4.07019220e-02  -3.66372561e-02  -3.25725902e-02  -2.85079243e-02
  -2.44432583e-02  -2.03785924e-02  -1.63139265e-02  -1.22492605e-02
  -8.18459462e-03  -4.11992870e-03  -5.52627716e-05]
-0.876214
1.7614
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149584 minutes
Weight histogram
[  65   75  931 1941 2832 3634 4768 5739 3554  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 13096] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.115784 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 1655 2871 1532] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4717 1272 1280 1548 1859 2110 2380 2698 3174 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.31857
Epoch 1, cost is  4.80591
Epoch 2, cost is  4.20178
Epoch 3, cost is  3.75853
Epoch 4, cost is  3.42947
Epoch 5, cost is  3.17154
Epoch 6, cost is  2.9662
Epoch 7, cost is  2.79523
Epoch 8, cost is  2.65426
Epoch 9, cost is  2.537
Training took 0.813687 minutes
Weight histogram
[775 670 545 470 406 349 709  99  20   7] [-0.01359871 -0.01225261 -0.0109065  -0.00956039 -0.00821429 -0.00686818
 -0.00552208 -0.00417597 -0.00282986 -0.00148376 -0.00013765]
[710 283 302 316 336 360 390 416 449 488] [-0.01359871 -0.01225261 -0.0109065  -0.00956039 -0.00821429 -0.00686818
 -0.00552208 -0.00417597 -0.00282986 -0.00148376 -0.00013765]
-0.227939
0.307072
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164207 minutes
Epoch 0
Fine tuning took 0.166546 minutes
Epoch 0
Fine tuning took 0.163232 minutes
Epoch 0
Fine tuning took 0.165473 minutes
Epoch 0
Fine tuning took 0.166560 minutes
Epoch 0
Fine tuning took 0.164597 minutes
Epoch 0
Fine tuning took 0.164324 minutes
Epoch 0
Fine tuning took 0.165761 minutes
Epoch 0
Fine tuning took 0.165616 minutes
Epoch 0
Fine tuning took 0.164766 minutes
{'zero': {0: [0.16871921182266009, 0.17118226600985223, 0.20566502463054187, 0.20073891625615764, 0.17857142857142858, 0.23645320197044334, 0.25, 0.22413793103448276, 0.19458128078817735, 0.15763546798029557, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.68596059113300489, 0.64039408866995073, 0.64778325123152714, 0.62068965517241381, 0.58497536945812811, 0.58374384236453203, 0.59975369458128081, 0.57389162561576357, 0.58128078817733986, 0.63423645320197042, 0.60344827586206895], 5: [0.14532019704433496, 0.18842364532019704, 0.14655172413793102, 0.17857142857142858, 0.23645320197044334, 0.17980295566502463, 0.15024630541871922, 0.2019704433497537, 0.22413793103448276, 0.20812807881773399, 0.20320197044334976], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16871921182266009, 0.1206896551724138, 0.19581280788177341, 0.17364532019704434, 0.14778325123152711, 0.18596059113300492, 0.21798029556650247, 0.20812807881773399, 0.17241379310344829, 0.14778325123152711, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.68596059113300489, 0.66995073891625612, 0.64162561576354682, 0.6280788177339901, 0.64408866995073888, 0.64778325123152714, 0.58990147783251234, 0.61206896551724133, 0.61083743842364535, 0.6711822660098522, 0.59975369458128081], 5: [0.14532019704433496, 0.20935960591133004, 0.1625615763546798, 0.19827586206896552, 0.20812807881773399, 0.16625615763546797, 0.19211822660098521, 0.17980295566502463, 0.21674876847290642, 0.18103448275862069, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16871921182266009, 0.15517241379310345, 0.16625615763546797, 0.16009852216748768, 0.14039408866995073, 0.20812807881773399, 0.21921182266009853, 0.21551724137931033, 0.17118226600985223, 0.17610837438423646, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.68596059113300489, 0.65024630541871919, 0.68842364532019706, 0.62931034482758619, 0.6219211822660099, 0.64408866995073888, 0.61083743842364535, 0.57758620689655171, 0.6071428571428571, 0.64039408866995073, 0.58743842364532017], 5: [0.14532019704433496, 0.19458128078817735, 0.14532019704433496, 0.2105911330049261, 0.2376847290640394, 0.14778325123152711, 0.16995073891625614, 0.20689655172413793, 0.22167487684729065, 0.18349753694581281, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16871921182266009, 0.13423645320197045, 0.17610837438423646, 0.19211822660098521, 0.13423645320197045, 0.19581280788177341, 0.23522167487684728, 0.20689655172413793, 0.17118226600985223, 0.16995073891625614, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.68596059113300489, 0.65024630541871919, 0.66256157635467983, 0.62068965517241381, 0.65147783251231528, 0.63177339901477836, 0.57389162561576357, 0.62438423645320196, 0.60344827586206895, 0.62438423645320196, 0.63300492610837433], 5: [0.14532019704433496, 0.21551724137931033, 0.16133004926108374, 0.18719211822660098, 0.21428571428571427, 0.17241379310344829, 0.19088669950738915, 0.16871921182266009, 0.22536945812807882, 0.20566502463054187, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422116 minutes
Weight histogram
[ 231  514  610  458  859 5370 9146 3389 1487  211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1149  972  756 1097 1297 2394 2480 3121 3621 5388] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.22702
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.13163
Epoch 1, cost is  1.10485
Epoch 2, cost is  1.08877
Epoch 3, cost is  1.07324
Epoch 4, cost is  1.05977
Training took 0.649173 minutes
Weight histogram
[5083 4204 3087 2554 1710 1457 1324  951  933  972] [ -4.07019220e-02  -3.66372561e-02  -3.25725902e-02  -2.85079243e-02
  -2.44432583e-02  -2.03785924e-02  -1.63139265e-02  -1.22492605e-02
  -8.18459462e-03  -4.11992870e-03  -5.52627716e-05]
[1834 1246 1303 1458 1885 2092 2713 2870 3043 3831] [ -4.07019220e-02  -3.66372561e-02  -3.25725902e-02  -2.85079243e-02
  -2.44432583e-02  -2.03785924e-02  -1.63139265e-02  -1.22492605e-02
  -8.18459462e-03  -4.11992870e-03  -5.52627716e-05]
-0.876214
1.7614
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151899 minutes
Weight histogram
[  65   75  931 1941 2832 3634 4768 5739 3554  761] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 13096] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.81856
Epoch 1, cost is  2.77093
Epoch 2, cost is  2.72966
Epoch 3, cost is  2.70178
Epoch 4, cost is  2.67384
Training took 0.117164 minutes
Weight histogram
[4046 3106 2581 2766 2148 1832 1763 1655 2871 1532] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
[4717 1272 1280 1548 1859 2110 2380 2698 3174 3262] [ -5.38726188e-02  -4.84814740e-02  -4.30903292e-02  -3.76991843e-02
  -3.23080395e-02  -2.69168947e-02  -2.15257498e-02  -1.61346050e-02
  -1.07434602e-02  -5.35231536e-03   3.88294720e-05]
-1.06456
1.29569
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.70626
Epoch 1, cost is  4.20805
Epoch 2, cost is  3.65721
Epoch 3, cost is  3.25938
Epoch 4, cost is  2.9614
Epoch 5, cost is  2.73051
Epoch 6, cost is  2.54818
Epoch 7, cost is  2.40304
Epoch 8, cost is  2.29032
Epoch 9, cost is  2.20073
Training took 1.465461 minutes
Weight histogram
[971 828 669 583 801 119  45  20   9   5] [-0.00909592 -0.00820167 -0.00730741 -0.00641316 -0.00551891 -0.00462466
 -0.0037304  -0.00283615 -0.0019419  -0.00104765 -0.00015339]
[694 282 294 307 326 354 382 414 467 530] [-0.00909592 -0.00820167 -0.00730741 -0.00641316 -0.00551891 -0.00462466
 -0.0037304  -0.00283615 -0.0019419  -0.00104765 -0.00015339]
-0.192328
0.217202
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.195758 minutes
Epoch 0
Fine tuning took 0.195597 minutes
Epoch 0
Fine tuning took 0.194462 minutes
Epoch 0
Fine tuning took 0.195129 minutes
Epoch 0
Fine tuning took 0.195629 minutes
Epoch 0
Fine tuning took 0.195147 minutes
Epoch 0
Fine tuning took 0.194797 minutes
Epoch 0
Fine tuning took 0.195382 minutes
Epoch 0
Fine tuning took 0.195691 minutes
Epoch 0
Fine tuning took 0.194533 minutes
{'zero': {0: [0.17118226600985223, 0.19581280788177341, 0.21551724137931033, 0.21182266009852216, 0.17980295566502463, 0.25, 0.21305418719211822, 0.19950738916256158, 0.19950738916256158, 0.15763546798029557, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.61206896551724133, 0.59975369458128081, 0.59359605911330049, 0.55295566502463056, 0.49753694581280788, 0.58743842364532017, 0.57266009852216748, 0.58128078817733986, 0.61576354679802958, 0.60221674876847286], 5: [0.16748768472906403, 0.19211822660098521, 0.18472906403940886, 0.19458128078817735, 0.26724137931034481, 0.25246305418719212, 0.19950738916256158, 0.22783251231527094, 0.21921182266009853, 0.22660098522167488, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17118226600985223, 0.17241379310344829, 0.22167487684729065, 0.20935960591133004, 0.16625615763546797, 0.20073891625615764, 0.2019704433497537, 0.21182266009852216, 0.20073891625615764, 0.14408866995073891, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.6145320197044335, 0.61576354679802958, 0.62684729064039413, 0.57389162561576357, 0.56527093596059108, 0.59975369458128081, 0.55541871921182262, 0.5788177339901478, 0.61206896551724133, 0.57266009852216748], 5: [0.16748768472906403, 0.21305418719211822, 0.1625615763546798, 0.16379310344827586, 0.25985221674876846, 0.23399014778325122, 0.19827586206896552, 0.23275862068965517, 0.22044334975369459, 0.24384236453201971, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17118226600985223, 0.19088669950738915, 0.20320197044334976, 0.21674876847290642, 0.18472906403940886, 0.18965517241379309, 0.19334975369458129, 0.19334975369458129, 0.19334975369458129, 0.15763546798029557, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.61330049261083741, 0.61699507389162567, 0.61576354679802958, 0.59482758620689657, 0.58497536945812811, 0.60591133004926112, 0.58374384236453203, 0.59113300492610843, 0.64039408866995073, 0.5714285714285714], 5: [0.16748768472906403, 0.19581280788177341, 0.17980295566502463, 0.16748768472906403, 0.22044334975369459, 0.22536945812807882, 0.20073891625615764, 0.2229064039408867, 0.21551724137931033, 0.2019704433497537, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17118226600985223, 0.16009852216748768, 0.21798029556650247, 0.19827586206896552, 0.18103448275862069, 0.23645320197044334, 0.20689655172413793, 0.21182266009852216, 0.20443349753694581, 0.15147783251231528, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.6145320197044335, 0.6071428571428571, 0.63916256157635465, 0.58620689655172409, 0.50492610837438423, 0.58128078817733986, 0.55418719211822665, 0.59729064039408863, 0.61945812807881773, 0.56157635467980294], 5: [0.16748768472906403, 0.22536945812807882, 0.1748768472906404, 0.1625615763546798, 0.23275862068965517, 0.25862068965517243, 0.21182266009852216, 0.23399014778325122, 0.19827586206896552, 0.22906403940886699, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105914 minutes
Weight histogram
[ 213 1644 1863 1309 3513 5121 4897 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  133   161   214   297   458   731   763  1609  4499 17460] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.74522
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.73496
Epoch 1, cost is  3.69133
Epoch 2, cost is  3.66268
Epoch 3, cost is  3.64429
Epoch 4, cost is  3.62266
Training took 0.072783 minutes
Weight histogram
[7102 5719 4185 4249 2340  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2502 2396 2160 2184 2068 2154 2649 3086 3196 3930] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.80153
2.32169
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149042 minutes
Weight histogram
[  120   460  1804  4857  7422 10719  2838    84    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7737 14298   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.09706
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.63621
Epoch 1, cost is  2.58057
Epoch 2, cost is  2.54853
Epoch 3, cost is  2.51856
Epoch 4, cost is  2.49287
Training took 0.116822 minutes
Weight histogram
[4320 4455 3158 2802 3081 2039 1864 2684 3314  633] [ -5.94836064e-02  -5.35313628e-02  -4.75791192e-02  -4.16268756e-02
  -3.56746320e-02  -2.97223884e-02  -2.37701449e-02  -1.78179013e-02
  -1.18656577e-02  -5.91341411e-03   3.88294720e-05]
[4862 1393 1477 1840 2218 2497 2926 3381 3631 4125] [ -5.94836064e-02  -5.35313628e-02  -4.75791192e-02  -4.16268756e-02
  -3.56746320e-02  -2.97223884e-02  -2.37701449e-02  -1.78179013e-02
  -1.18656577e-02  -5.91341411e-03   3.88294720e-05]
-1.18714
1.3347
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.98065
Epoch 1, cost is  5.64823
Epoch 2, cost is  5.54137
Epoch 3, cost is  5.43232
Epoch 4, cost is  5.23627
Epoch 5, cost is  5.00967
Epoch 6, cost is  4.78492
Epoch 7, cost is  4.55311
Epoch 8, cost is  4.34083
Epoch 9, cost is  4.1677
Training took 0.178763 minutes
Weight histogram
[1418 1845 2475  889  554  400  301  131   56   31] [-0.02830861 -0.02548818 -0.02266775 -0.01984732 -0.01702689 -0.01420646
 -0.01138603 -0.0085656  -0.00574517 -0.00292474 -0.00010431]
[ 703 1370 1376  751  704  669  622  611  655  639] [-0.02830861 -0.02548818 -0.02266775 -0.01984732 -0.01702689 -0.01420646
 -0.01138603 -0.0085656  -0.00574517 -0.00292474 -0.00010431]
-0.359219
0.397315
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040493 minutes
Epoch 0
Fine tuning took 0.042100 minutes
Epoch 0
Fine tuning took 0.043485 minutes
Epoch 0
Fine tuning took 0.042282 minutes
Epoch 0
Fine tuning took 0.041702 minutes
Epoch 0
Fine tuning took 0.041533 minutes
Epoch 0
Fine tuning took 0.041494 minutes
Epoch 0
Fine tuning took 0.043393 minutes
Epoch 0
Fine tuning took 0.042877 minutes
Epoch 0
Fine tuning took 0.041855 minutes
{'zero': {0: [0.054187192118226604, 0.22044334975369459, 0.25246305418719212, 0.23522167487684728, 0.088669950738916259, 0.19334975369458129, 0.14901477832512317, 0.16625615763546797, 0.1748768472906404, 0.092364532019704432, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.77586206896551724, 0.63423645320197042, 0.6280788177339901, 0.5923645320197044, 0.65024630541871919, 0.56650246305418717, 0.72167487684729059, 0.68596059113300489, 0.60344827586206895, 0.7573891625615764, 0.70812807881773399], 5: [0.16995073891625614, 0.14532019704433496, 0.11945812807881774, 0.17241379310344829, 0.26108374384236455, 0.24014778325123154, 0.12931034482758622, 0.14778325123152711, 0.22167487684729065, 0.15024630541871922, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.054187192118226604, 0.24630541871921183, 0.23275862068965517, 0.3460591133004926, 0.18103448275862069, 0.19088669950738915, 0.23891625615763548, 0.15147783251231528, 0.23399014778325122, 0.16009852216748768, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.77586206896551724, 0.65147783251231528, 0.71921182266009853, 0.59975369458128081, 0.71182266009852213, 0.70812807881773399, 0.71305418719211822, 0.77709359605911332, 0.58620689655172409, 0.73152709359605916, 0.81034482758620685], 5: [0.16995073891625614, 0.10221674876847291, 0.048029556650246302, 0.054187192118226604, 0.10714285714285714, 0.10098522167487685, 0.048029556650246302, 0.071428571428571425, 0.17980295566502463, 0.10837438423645321, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.054187192118226604, 0.24630541871921183, 0.21921182266009853, 0.30788177339901479, 0.15763546798029557, 0.19458128078817735, 0.25492610837438423, 0.14039408866995073, 0.20073891625615764, 0.18965517241379309, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.77586206896551724, 0.64408866995073888, 0.74384236453201968, 0.62931034482758619, 0.73029556650246308, 0.68103448275862066, 0.7068965517241379, 0.78448275862068961, 0.6354679802955665, 0.72660098522167482, 0.78940886699507384], 5: [0.16995073891625614, 0.10960591133004927, 0.036945812807881777, 0.062807881773399021, 0.11206896551724138, 0.12438423645320197, 0.038177339901477834, 0.075123152709359611, 0.16379310344827586, 0.083743842364532015, 0.087438423645320201], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.054187192118226604, 0.22167487684729065, 0.22044334975369459, 0.29556650246305421, 0.16133004926108374, 0.19458128078817735, 0.23522167487684728, 0.15763546798029557, 0.2105911330049261, 0.17241379310344829, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.77586206896551724, 0.63054187192118227, 0.76231527093596063, 0.66009852216748766, 0.75246305418719217, 0.69704433497536944, 0.70443349753694584, 0.78448275862068961, 0.54064039408866993, 0.74630541871921185, 0.7857142857142857], 5: [0.16995073891625614, 0.14778325123152711, 0.017241379310344827, 0.044334975369458129, 0.086206896551724144, 0.10837438423645321, 0.060344827586206899, 0.057881773399014777, 0.24876847290640394, 0.081280788177339899, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106559 minutes
Weight histogram
[ 222 1649 1741  944 2651 4453 4875 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  129   151   213   272   445   612   783  1089  2869 17737] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.69131
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82963
Epoch 1, cost is  3.78158
Epoch 2, cost is  3.74917
Epoch 3, cost is  3.71669
Epoch 4, cost is  3.69093
Training took 0.072671 minutes
Weight histogram
[7102 5719 4185 4033  531  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2417 2212 2127 2006 2028 1951 2356 2933 2784 3486] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.69104
2.26806
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151661 minutes
Weight histogram
[ 120  460 1804 4815 6920 9023 3045   92   35   11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 12518   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.117595 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 2850 2938  576] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4757 1377 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.51189
Epoch 1, cost is  5.29715
Epoch 2, cost is  5.21113
Epoch 3, cost is  5.00986
Epoch 4, cost is  4.72004
Epoch 5, cost is  4.4397
Epoch 6, cost is  4.15628
Epoch 7, cost is  3.91501
Epoch 8, cost is  3.7256
Epoch 9, cost is  3.57651
Training took 0.248156 minutes
Weight histogram
[1757 4492  805  442  255  149   93   55   32   20] [-0.01881931 -0.01695333 -0.01508734 -0.01322136 -0.01135538 -0.0094894
 -0.00762341 -0.00575743 -0.00389145 -0.00202547 -0.00015948]
[2156  835  579  619  596  570  591  639  730  785] [-0.01881931 -0.01695333 -0.01508734 -0.01322136 -0.01135538 -0.0094894
 -0.00762341 -0.00575743 -0.00389145 -0.00202547 -0.00015948]
-0.276114
0.239616
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046412 minutes
Epoch 0
Fine tuning took 0.047586 minutes
Epoch 0
Fine tuning took 0.047164 minutes
Epoch 0
Fine tuning took 0.044616 minutes
Epoch 0
Fine tuning took 0.047277 minutes
Epoch 0
Fine tuning took 0.045610 minutes
Epoch 0
Fine tuning took 0.047139 minutes
Epoch 0
Fine tuning took 0.045846 minutes
Epoch 0
Fine tuning took 0.044349 minutes
Epoch 0
Fine tuning took 0.048043 minutes
{'zero': {0: [0.12561576354679804, 0.21182266009852216, 0.21674876847290642, 0.19704433497536947, 0.19458128078817735, 0.17857142857142858, 0.16133004926108374, 0.16379310344827586, 0.16748768472906403, 0.10098522167487685, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.6219211822660099, 0.54433497536945807, 0.54926108374384242, 0.58866995073891626, 0.61206896551724133, 0.5788177339901478, 0.57635467980295563, 0.60467980295566504, 0.58251231527093594, 0.56896551724137934], 5: [0.22413793103448276, 0.16625615763546797, 0.23891625615763548, 0.2536945812807882, 0.21674876847290642, 0.20935960591133004, 0.25985221674876846, 0.25985221674876846, 0.22783251231527094, 0.31650246305418717, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12561576354679804, 0.19950738916256158, 0.18965517241379309, 0.19704433497536947, 0.19458128078817735, 0.1748768472906404, 0.16133004926108374, 0.13054187192118227, 0.17364532019704434, 0.11576354679802955, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.65147783251231528, 0.57635467980295563, 0.56157635467980294, 0.60221674876847286, 0.62931034482758619, 0.60344827586206895, 0.63300492610837433, 0.6354679802955665, 0.57635467980295563, 0.54556650246305416], 5: [0.22413793103448276, 0.14901477832512317, 0.23399014778325122, 0.2413793103448276, 0.20320197044334976, 0.19581280788177341, 0.23522167487684728, 0.23645320197044334, 0.19088669950738915, 0.30788177339901479, 0.25492610837438423], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12561576354679804, 0.18103448275862069, 0.1625615763546798, 0.19088669950738915, 0.17364532019704434, 0.17980295566502463, 0.15763546798029557, 0.17118226600985223, 0.17980295566502463, 0.12931034482758622, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.66379310344827591, 0.62684729064039413, 0.58990147783251234, 0.61699507389162567, 0.6219211822660099, 0.62438423645320196, 0.60098522167487689, 0.60837438423645318, 0.61330049261083741, 0.56773399014778325], 5: [0.22413793103448276, 0.15517241379310345, 0.2105911330049261, 0.21921182266009853, 0.20935960591133004, 0.19827586206896552, 0.21798029556650247, 0.22783251231527094, 0.21182266009852216, 0.25738916256157635, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12561576354679804, 0.18349753694581281, 0.17733990147783252, 0.19458128078817735, 0.18596059113300492, 0.16133004926108374, 0.16748768472906403, 0.15517241379310345, 0.16995073891625614, 0.10960591133004927, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.65394088669950734, 0.57389162561576357, 0.56034482758620685, 0.59113300492610843, 0.6219211822660099, 0.58743842364532017, 0.58743842364532017, 0.59482758620689657, 0.5923645320197044, 0.54556650246305416], 5: [0.22413793103448276, 0.1625615763546798, 0.24876847290640394, 0.24507389162561577, 0.2229064039408867, 0.21674876847290642, 0.24507389162561577, 0.25738916256157635, 0.23522167487684728, 0.29802955665024633, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108067 minutes
Weight histogram
[ 222 1649 1741  944 2651 4453 4875 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  129   151   213   272   445   612   783  1089  2869 17737] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.69131
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82963
Epoch 1, cost is  3.78158
Epoch 2, cost is  3.74917
Epoch 3, cost is  3.71669
Epoch 4, cost is  3.69093
Training took 0.072842 minutes
Weight histogram
[7102 5719 4185 4033  531  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2417 2212 2127 2006 2028 1951 2356 2933 2784 3486] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.69104
2.26806
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147765 minutes
Weight histogram
[ 120  460 1804 4815 6920 9023 3045   92   35   11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 12518   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.115925 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 2850 2938  576] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4757 1377 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.37719
Epoch 1, cost is  5.25833
Epoch 2, cost is  5.02455
Epoch 3, cost is  4.58375
Epoch 4, cost is  4.19093
Epoch 5, cost is  3.85442
Epoch 6, cost is  3.606
Epoch 7, cost is  3.42503
Epoch 8, cost is  3.27017
Epoch 9, cost is  3.12122
Training took 0.335205 minutes
Weight histogram
[1742 1261 1312 1160 2351  156   58   30   18   12] [-0.01131206 -0.0101929  -0.00907373 -0.00795457 -0.00683541 -0.00571624
 -0.00459708 -0.00347792 -0.00235875 -0.00123959 -0.00012043]
[2103  531  508  548  554  582  671  795  885  923] [-0.01131206 -0.0101929  -0.00907373 -0.00795457 -0.00683541 -0.00571624
 -0.00459708 -0.00347792 -0.00235875 -0.00123959 -0.00012043]
-0.226358
0.201586
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051829 minutes
Epoch 0
Fine tuning took 0.052303 minutes
Epoch 0
Fine tuning took 0.049972 minutes
Epoch 0
Fine tuning took 0.051144 minutes
Epoch 0
Fine tuning took 0.049951 minutes
Epoch 0
Fine tuning took 0.050555 minutes
Epoch 0
Fine tuning took 0.049650 minutes
Epoch 0
Fine tuning took 0.051560 minutes
Epoch 0
Fine tuning took 0.050065 minutes
Epoch 0
Fine tuning took 0.050989 minutes
{'zero': {0: [0.18226600985221675, 0.15763546798029557, 0.19950738916256158, 0.24384236453201971, 0.19827586206896552, 0.18596059113300492, 0.19704433497536947, 0.22660098522167488, 0.20812807881773399, 0.18349753694581281, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.6280788177339901, 0.56650246305418717, 0.52955665024630538, 0.58620689655172409, 0.53448275862068961, 0.56896551724137934, 0.50492610837438423, 0.55172413793103448, 0.50246305418719217, 0.51847290640394084], 5: [0.20566502463054187, 0.21428571428571427, 0.23399014778325122, 0.22660098522167488, 0.21551724137931033, 0.27955665024630544, 0.23399014778325122, 0.26847290640394089, 0.24014778325123154, 0.31403940886699505, 0.2894088669950739], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18226600985221675, 0.14901477832512317, 0.20935960591133004, 0.23645320197044334, 0.19334975369458129, 0.18472906403940886, 0.19704433497536947, 0.18349753694581281, 0.19458128078817735, 0.20073891625615764, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.6219211822660099, 0.55049261083743839, 0.54187192118226601, 0.5923645320197044, 0.59113300492610843, 0.53078817733990147, 0.55172413793103448, 0.5073891625615764, 0.52339901477832518, 0.54433497536945807], 5: [0.20566502463054187, 0.22906403940886699, 0.24014778325123154, 0.22167487684729065, 0.21428571428571427, 0.22413793103448276, 0.27216748768472904, 0.26477832512315269, 0.29802955665024633, 0.27586206896551724, 0.28694581280788178], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18226600985221675, 0.15517241379310345, 0.19458128078817735, 0.25492610837438423, 0.20812807881773399, 0.17364532019704434, 0.16009852216748768, 0.21551724137931033, 0.21674876847290642, 0.18965517241379309, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.62438423645320196, 0.58004926108374388, 0.51847290640394084, 0.57019704433497542, 0.5788177339901478, 0.58374384236453203, 0.51231527093596063, 0.52339901477832518, 0.49876847290640391, 0.55295566502463056], 5: [0.20566502463054187, 0.22044334975369459, 0.22536945812807882, 0.22660098522167488, 0.22167487684729065, 0.24753694581280788, 0.25615763546798032, 0.27216748768472904, 0.25985221674876846, 0.31157635467980294, 0.25], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18226600985221675, 0.16133004926108374, 0.21921182266009853, 0.25123152709359609, 0.20443349753694581, 0.17733990147783252, 0.19334975369458129, 0.20935960591133004, 0.20935960591133004, 0.17733990147783252, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.6145320197044335, 0.54433497536945807, 0.52216748768472909, 0.56773399014778325, 0.58990147783251234, 0.54802955665024633, 0.51600985221674878, 0.5357142857142857, 0.52832512315270941, 0.52832512315270941], 5: [0.20566502463054187, 0.22413793103448276, 0.23645320197044334, 0.22660098522167488, 0.22783251231527094, 0.23275862068965517, 0.25862068965517243, 0.27463054187192121, 0.25492610837438423, 0.29433497536945813, 0.29926108374384236], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.110667 minutes
Weight histogram
[ 222 1649 1741  944 2651 4453 4875 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  129   151   213   272   445   612   783  1089  2869 17737] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.69131
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82963
Epoch 1, cost is  3.78158
Epoch 2, cost is  3.74917
Epoch 3, cost is  3.71669
Epoch 4, cost is  3.69093
Training took 0.072821 minutes
Weight histogram
[7102 5719 4185 4033  531  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2417 2212 2127 2006 2028 1951 2356 2933 2784 3486] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.69104
2.26806
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150281 minutes
Weight histogram
[ 120  460 1804 4815 6920 9023 3045   92   35   11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 12518   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.116453 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 2850 2938  576] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4757 1377 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.36084
Epoch 1, cost is  5.19801
Epoch 2, cost is  4.76073
Epoch 3, cost is  4.22508
Epoch 4, cost is  3.78744
Epoch 5, cost is  3.48316
Epoch 6, cost is  3.26237
Epoch 7, cost is  3.06639
Epoch 8, cost is  2.87585
Epoch 9, cost is  2.71172
Training took 0.538298 minutes
Weight histogram
[1660 1471  981  956  934 2028   39   17    8    6] [-0.00591584 -0.00533609 -0.00475634 -0.0041766  -0.00359685 -0.00301711
 -0.00243736 -0.00185762 -0.00127787 -0.00069812 -0.00011838]
[1886  520  518  533  559  652  767  860  881  924] [-0.00591584 -0.00533609 -0.00475634 -0.0041766  -0.00359685 -0.00301711
 -0.00243736 -0.00185762 -0.00127787 -0.00069812 -0.00011838]
-0.183692
0.174146
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.061761 minutes
Epoch 0
Fine tuning took 0.062930 minutes
Epoch 0
Fine tuning took 0.062640 minutes
Epoch 0
Fine tuning took 0.062973 minutes
Epoch 0
Fine tuning took 0.062001 minutes
Epoch 0
Fine tuning took 0.061507 minutes
Epoch 0
Fine tuning took 0.060902 minutes
Epoch 0
Fine tuning took 0.062187 minutes
Epoch 0
Fine tuning took 0.061619 minutes
Epoch 0
Fine tuning took 0.062335 minutes
{'zero': {0: [0.18226600985221675, 0.14408866995073891, 0.18596059113300492, 0.22660098522167488, 0.23152709359605911, 0.18842364532019704, 0.18719211822660098, 0.17241379310344829, 0.17733990147783252, 0.18965517241379309, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.62438423645320196, 0.58620689655172409, 0.6071428571428571, 0.51970443349753692, 0.60344827586206895, 0.60098522167487689, 0.56527093596059108, 0.55172413793103448, 0.54064039408866993, 0.55049261083743839], 5: [0.21182266009852216, 0.23152709359605911, 0.22783251231527094, 0.16625615763546797, 0.24876847290640394, 0.20812807881773399, 0.21182266009852216, 0.26231527093596058, 0.27093596059113301, 0.26970443349753692, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18226600985221675, 0.16625615763546797, 0.19458128078817735, 0.17610837438423646, 0.21428571428571427, 0.18719211822660098, 0.22660098522167488, 0.14532019704433496, 0.22044334975369459, 0.20443349753694581, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.61822660098522164, 0.57389162561576357, 0.59482758620689657, 0.57512315270935965, 0.5923645320197044, 0.57512315270935965, 0.56773399014778325, 0.52093596059113301, 0.51108374384236455, 0.56403940886699511], 5: [0.21182266009852216, 0.21551724137931033, 0.23152709359605911, 0.22906403940886699, 0.2105911330049261, 0.22044334975369459, 0.19827586206896552, 0.28694581280788178, 0.25862068965517243, 0.28448275862068967, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18226600985221675, 0.15640394088669951, 0.19827586206896552, 0.17733990147783252, 0.21674876847290642, 0.17118226600985223, 0.18842364532019704, 0.14162561576354679, 0.20935960591133004, 0.22044334975369459, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.64655172413793105, 0.5431034482758621, 0.60960591133004927, 0.57266009852216748, 0.58620689655172409, 0.59852216748768472, 0.59482758620689657, 0.55665024630541871, 0.47044334975369456, 0.58497536945812811], 5: [0.21182266009852216, 0.19704433497536947, 0.25862068965517243, 0.21305418719211822, 0.2105911330049261, 0.24261083743842365, 0.21305418719211822, 0.26354679802955666, 0.23399014778325122, 0.30911330049261082, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18226600985221675, 0.16502463054187191, 0.18719211822660098, 0.19088669950738915, 0.18349753694581281, 0.17733990147783252, 0.20566502463054187, 0.15147783251231528, 0.20689655172413793, 0.18349753694581281, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.62438423645320196, 0.55172413793103448, 0.5788177339901478, 0.54926108374384242, 0.62561576354679804, 0.58866995073891626, 0.57758620689655171, 0.54556650246305416, 0.51600985221674878, 0.58620689655172409], 5: [0.21182266009852216, 0.2105911330049261, 0.26108374384236455, 0.23029556650246305, 0.26724137931034481, 0.19704433497536947, 0.20566502463054187, 0.27093596059113301, 0.24753694581280788, 0.30049261083743845, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149515 minutes
Weight histogram
[ 232  297  491  722 1679 3668 6133 7069 3623  386] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
[  136   165   223   329   517   863  1078  2009  6663 12317] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43544
Epoch 1, cost is  2.39453
Epoch 2, cost is  2.36676
Epoch 3, cost is  2.34133
Epoch 4, cost is  2.3194
Training took 0.114897 minutes
Weight histogram
[5446 3908 2981 3086 2265 1767 1987 1179 1392  289] [ -5.63502945e-02  -5.07119001e-02  -4.50735058e-02  -3.94351114e-02
  -3.37967171e-02  -2.81583227e-02  -2.25199283e-02  -1.68815340e-02
  -1.12431396e-02  -5.60474526e-03   3.36491030e-05]
[2568 1356 1376 1773 2150 2461 2694 3059 3418 3445] [ -5.63502945e-02  -5.07119001e-02  -4.50735058e-02  -3.94351114e-02
  -3.37967171e-02  -2.81583227e-02  -2.25199283e-02  -1.68815340e-02
  -1.12431396e-02  -5.60474526e-03   3.36491030e-05]
-1.22127
1.57668
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148352 minutes
Weight histogram
[  65   95  492 1204 2907 4150 5315 6391 4583 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 15119] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.117770 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 2155 3631  578] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4796 1338 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.20901
Epoch 1, cost is  5.94684
Epoch 2, cost is  5.78888
Epoch 3, cost is  5.60231
Epoch 4, cost is  5.2991
Epoch 5, cost is  4.96807
Epoch 6, cost is  4.68039
Epoch 7, cost is  4.4288
Epoch 8, cost is  4.2181
Epoch 9, cost is  4.03634
Training took 0.205627 minutes
Weight histogram
[ 986  987  918 1051 2256  876  718  228   54   26] [-0.02539939 -0.02287585 -0.0203523  -0.01782876 -0.01530522 -0.01278168
 -0.01025814 -0.00773459 -0.00521105 -0.00268751 -0.00016397]
[1600 1447  646  542  568  607  635  655  688  712] [-0.02539939 -0.02287585 -0.0203523  -0.01782876 -0.01530522 -0.01278168
 -0.01025814 -0.00773459 -0.00521105 -0.00268751 -0.00016397]
-0.419286
0.470294
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052362 minutes
Epoch 0
Fine tuning took 0.052222 minutes
Epoch 0
Fine tuning took 0.054802 minutes
Epoch 0
Fine tuning took 0.055417 minutes
Epoch 0
Fine tuning took 0.055365 minutes
Epoch 0
Fine tuning took 0.053371 minutes
Epoch 0
Fine tuning took 0.055270 minutes
Epoch 0
Fine tuning took 0.054469 minutes
Epoch 0
Fine tuning took 0.052207 minutes
Epoch 0
Fine tuning took 0.054203 minutes
{'zero': {0: [0.33004926108374383, 0.30295566502463056, 0.11083743842364532, 0.18965517241379309, 0.30541871921182268, 0.11206896551724138, 0.12315270935960591, 0.14778325123152711, 0.073891625615763554, 0.080049261083743842, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.54802955665024633, 0.64039408866995073, 0.68472906403940892, 0.41256157635467983, 0.66748768472906406, 0.69827586206896552, 0.80541871921182262, 0.77339901477832518, 0.73891625615763545, 0.72783251231527091], 5: [0.19211822660098521, 0.14901477832512317, 0.24876847290640394, 0.12561576354679804, 0.28201970443349755, 0.22044334975369459, 0.17857142857142858, 0.046798029556650245, 0.15270935960591134, 0.18103448275862069, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33004926108374383, 0.38054187192118227, 0.17118226600985223, 0.20935960591133004, 0.36699507389162561, 0.10344827586206896, 0.20812807881773399, 0.25492610837438423, 0.16995073891625614, 0.065270935960591137, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.43349753694581283, 0.60221674876847286, 0.66379310344827591, 0.37315270935960593, 0.69581280788177335, 0.58990147783251234, 0.70197044334975367, 0.7068965517241379, 0.76354679802955661, 0.71921182266009853], 5: [0.19211822660098521, 0.18596059113300492, 0.22660098522167488, 0.1268472906403941, 0.25985221674876846, 0.20073891625615764, 0.2019704433497537, 0.043103448275862072, 0.12315270935960591, 0.17118226600985223, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33004926108374383, 0.35098522167487683, 0.15886699507389163, 0.21305418719211822, 0.43472906403940886, 0.13793103448275862, 0.23645320197044334, 0.25123152709359609, 0.15270935960591134, 0.057881773399014777, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.48275862068965519, 0.62931034482758619, 0.68596059113300489, 0.30541871921182268, 0.66995073891625612, 0.58004926108374388, 0.71182266009852213, 0.70443349753694584, 0.73645320197044339, 0.68472906403940892], 5: [0.19211822660098521, 0.16625615763546797, 0.21182266009852216, 0.10098522167487685, 0.25985221674876846, 0.19211822660098521, 0.18349753694581281, 0.036945812807881777, 0.14285714285714285, 0.20566502463054187, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33004926108374383, 0.35591133004926107, 0.12192118226600986, 0.2019704433497537, 0.37684729064039407, 0.13177339901477833, 0.2019704433497537, 0.24261083743842365, 0.15270935960591134, 0.071428571428571425, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.48275862068965519, 0.65886699507389157, 0.69950738916256161, 0.37315270935960593, 0.66995073891625612, 0.59852216748768472, 0.72660098522167482, 0.71059113300492616, 0.72906403940886699, 0.69458128078817738], 5: [0.19211822660098521, 0.16133004926108374, 0.21921182266009853, 0.098522167487684734, 0.25, 0.19827586206896552, 0.19950738916256158, 0.030788177339901478, 0.13669950738916256, 0.19950738916256158, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150007 minutes
Weight histogram
[ 232  297  491  722 1679 3668 6133 7069 3623  386] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
[  136   165   223   329   517   863  1078  2009  6663 12317] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43544
Epoch 1, cost is  2.39453
Epoch 2, cost is  2.36676
Epoch 3, cost is  2.34133
Epoch 4, cost is  2.3194
Training took 0.116359 minutes
Weight histogram
[5446 3908 2981 3086 2265 1767 1987 1179 1392  289] [ -5.63502945e-02  -5.07119001e-02  -4.50735058e-02  -3.94351114e-02
  -3.37967171e-02  -2.81583227e-02  -2.25199283e-02  -1.68815340e-02
  -1.12431396e-02  -5.60474526e-03   3.36491030e-05]
[2568 1356 1376 1773 2150 2461 2694 3059 3418 3445] [ -5.63502945e-02  -5.07119001e-02  -4.50735058e-02  -3.94351114e-02
  -3.37967171e-02  -2.81583227e-02  -2.25199283e-02  -1.68815340e-02
  -1.12431396e-02  -5.60474526e-03   3.36491030e-05]
-1.22127
1.57668
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151681 minutes
Weight histogram
[  65   95  492 1204 2907 4150 5315 6391 4583 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 15119] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.117251 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 2155 3631  578] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4796 1338 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.59253
Epoch 1, cost is  5.36124
Epoch 2, cost is  5.23379
Epoch 3, cost is  4.96345
Epoch 4, cost is  4.56304
Epoch 5, cost is  4.22455
Epoch 6, cost is  3.92809
Epoch 7, cost is  3.66605
Epoch 8, cost is  3.46084
Epoch 9, cost is  3.29504
Training took 0.299397 minutes
Weight histogram
[1393 1356 1232 3104  571  239  105   53   29   18] [-0.01764149 -0.01589161 -0.01414173 -0.01239184 -0.01064196 -0.00889208
 -0.0071422  -0.00539231 -0.00364243 -0.00189255 -0.00014267]
[2436  643  493  534  579  607  632  669  729  778] [-0.01764149 -0.01589161 -0.01414173 -0.01239184 -0.01064196 -0.00889208
 -0.0071422  -0.00539231 -0.00364243 -0.00189255 -0.00014267]
-0.261856
0.331947
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.057710 minutes
Epoch 0
Fine tuning took 0.057876 minutes
Epoch 0
Fine tuning took 0.058675 minutes
Epoch 0
Fine tuning took 0.060625 minutes
Epoch 0
Fine tuning took 0.059534 minutes
Epoch 0
Fine tuning took 0.059867 minutes
Epoch 0
Fine tuning took 0.059805 minutes
Epoch 0
Fine tuning took 0.059126 minutes
Epoch 0
Fine tuning took 0.059757 minutes
Epoch 0
Fine tuning took 0.060205 minutes
{'zero': {0: [0.18965517241379309, 0.19950738916256158, 0.1268472906403941, 0.16625615763546797, 0.17733990147783252, 0.21182266009852216, 0.16133004926108374, 0.12561576354679804, 0.092364532019704432, 0.12192118226600986, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.62561576354679804, 0.67364532019704437, 0.62684729064039413, 0.59729064039408863, 0.59113300492610843, 0.59852216748768472, 0.6785714285714286, 0.72413793103448276, 0.58004926108374388, 0.53078817733990147], 5: [0.20566502463054187, 0.1748768472906404, 0.19950738916256158, 0.20689655172413793, 0.22536945812807882, 0.19704433497536947, 0.24014778325123154, 0.19581280788177341, 0.18349753694581281, 0.29802955665024633, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18965517241379309, 0.18965517241379309, 0.077586206896551727, 0.13423645320197045, 0.14532019704433496, 0.18349753694581281, 0.1354679802955665, 0.14162561576354679, 0.10098522167487685, 0.12315270935960591, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.63054187192118227, 0.74507389162561577, 0.66748768472906406, 0.64162561576354682, 0.62931034482758619, 0.67610837438423643, 0.66995073891625612, 0.72290640394088668, 0.6354679802955665, 0.55788177339901479], 5: [0.20566502463054187, 0.17980295566502463, 0.17733990147783252, 0.19827586206896552, 0.21305418719211822, 0.18719211822660098, 0.18842364532019704, 0.18842364532019704, 0.17610837438423646, 0.2413793103448276, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18965517241379309, 0.18719211822660098, 0.092364532019704432, 0.15763546798029557, 0.15024630541871922, 0.14901477832512317, 0.1354679802955665, 0.16502463054187191, 0.1268472906403941, 0.12315270935960591, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.62561576354679804, 0.72906403940886699, 0.65024630541871919, 0.64778325123152714, 0.68719211822660098, 0.6711822660098522, 0.66748768472906406, 0.72290640394088668, 0.61576354679802958, 0.58374384236453203], 5: [0.20566502463054187, 0.18719211822660098, 0.17857142857142858, 0.19211822660098521, 0.2019704433497537, 0.16379310344827586, 0.19334975369458129, 0.16748768472906403, 0.15024630541871922, 0.26108374384236455, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18965517241379309, 0.20073891625615764, 0.076354679802955669, 0.14655172413793102, 0.1268472906403941, 0.16133004926108374, 0.14408866995073891, 0.14408866995073891, 0.10837438423645321, 0.13669950738916256, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.62068965517241381, 0.74384236453201968, 0.65886699507389157, 0.66625615763546797, 0.67364532019704437, 0.67241379310344829, 0.66133004926108374, 0.73522167487684731, 0.60591133004926112, 0.58497536945812811], 5: [0.20566502463054187, 0.17857142857142858, 0.17980295566502463, 0.19458128078817735, 0.20689655172413793, 0.16502463054187191, 0.18349753694581281, 0.19458128078817735, 0.15640394088669951, 0.25738916256157635, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150586 minutes
Weight histogram
[ 232  297  491  722 1679 3668 6133 7069 3623  386] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
[  136   165   223   329   517   863  1078  2009  6663 12317] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43544
Epoch 1, cost is  2.39453
Epoch 2, cost is  2.36676
Epoch 3, cost is  2.34133
Epoch 4, cost is  2.3194
Training took 0.115148 minutes
Weight histogram
[5446 3908 2981 3086 2265 1767 1987 1179 1392  289] [ -5.63502945e-02  -5.07119001e-02  -4.50735058e-02  -3.94351114e-02
  -3.37967171e-02  -2.81583227e-02  -2.25199283e-02  -1.68815340e-02
  -1.12431396e-02  -5.60474526e-03   3.36491030e-05]
[2568 1356 1376 1773 2150 2461 2694 3059 3418 3445] [ -5.63502945e-02  -5.07119001e-02  -4.50735058e-02  -3.94351114e-02
  -3.37967171e-02  -2.81583227e-02  -2.25199283e-02  -1.68815340e-02
  -1.12431396e-02  -5.60474526e-03   3.36491030e-05]
-1.22127
1.57668
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149825 minutes
Weight histogram
[  65   95  492 1204 2907 4150 5315 6391 4583 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 15119] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.116030 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 2155 3631  578] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4796 1338 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.22883
Epoch 1, cost is  5.08102
Epoch 2, cost is  4.90999
Epoch 3, cost is  4.50799
Epoch 4, cost is  4.09653
Epoch 5, cost is  3.74311
Epoch 6, cost is  3.43899
Epoch 7, cost is  3.20504
Epoch 8, cost is  3.01811
Epoch 9, cost is  2.86645
Training took 0.414602 minutes
Weight histogram
[1766 1709 3740  461  203  103   56   32   18   12] [-0.01201195 -0.01082364 -0.00963533 -0.00844701 -0.0072587  -0.00607038
 -0.00488207 -0.00369376 -0.00250544 -0.00131713 -0.00012881]
[2268  543  497  562  575  608  649  718  805  875] [-0.01201195 -0.01082364 -0.00963533 -0.00844701 -0.0072587  -0.00607038
 -0.00488207 -0.00369376 -0.00250544 -0.00131713 -0.00012881]
-0.222079
0.227248
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.065391 minutes
Epoch 0
Fine tuning took 0.064390 minutes
Epoch 0
Fine tuning took 0.063549 minutes
Epoch 0
Fine tuning took 0.063511 minutes
Epoch 0
Fine tuning took 0.062836 minutes
Epoch 0
Fine tuning took 0.065142 minutes
Epoch 0
Fine tuning took 0.063350 minutes
Epoch 0
Fine tuning took 0.062517 minutes
Epoch 0
Fine tuning took 0.064093 minutes
Epoch 0
Fine tuning took 0.064181 minutes
{'zero': {0: [0.14901477832512317, 0.14655172413793102, 0.16379310344827586, 0.23152709359605911, 0.17241379310344829, 0.1268472906403941, 0.17610837438423646, 0.14778325123152711, 0.11699507389162561, 0.18472906403940886, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.66379310344827591, 0.55295566502463056, 0.5357142857142857, 0.63054187192118227, 0.65270935960591137, 0.55418719211822665, 0.58620689655172409, 0.5923645320197044, 0.59852216748768472, 0.54433497536945807], 5: [0.20320197044334976, 0.18965517241379309, 0.28325123152709358, 0.23275862068965517, 0.19704433497536947, 0.22044334975369459, 0.26970443349753692, 0.26600985221674878, 0.29064039408866993, 0.21674876847290642, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14901477832512317, 0.13793103448275862, 0.13669950738916256, 0.21305418719211822, 0.17733990147783252, 0.14039408866995073, 0.20443349753694581, 0.15147783251231528, 0.14408866995073891, 0.16625615763546797, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.63669950738916259, 0.58743842364532017, 0.56034482758620685, 0.62068965517241381, 0.64408866995073888, 0.52832512315270941, 0.58374384236453203, 0.56650246305418717, 0.61206896551724133, 0.58251231527093594], 5: [0.20320197044334976, 0.22536945812807882, 0.27586206896551724, 0.22660098522167488, 0.2019704433497537, 0.21551724137931033, 0.26724137931034481, 0.26477832512315269, 0.2894088669950739, 0.22167487684729065, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14901477832512317, 0.12438423645320197, 0.14408866995073891, 0.19088669950738915, 0.16625615763546797, 0.13177339901477833, 0.19950738916256158, 0.1625615763546798, 0.13793103448275862, 0.17610837438423646, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.67487684729064035, 0.60344827586206895, 0.58374384236453203, 0.6428571428571429, 0.64532019704433496, 0.56527093596059108, 0.56157635467980294, 0.59729064039408863, 0.61822660098522164, 0.56650246305418717], 5: [0.20320197044334976, 0.20073891625615764, 0.25246305418719212, 0.22536945812807882, 0.19088669950738915, 0.2229064039408867, 0.23522167487684728, 0.27586206896551724, 0.26477832512315269, 0.20566502463054187, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14901477832512317, 0.1206896551724138, 0.15763546798029557, 0.20689655172413793, 0.15024630541871922, 0.1268472906403941, 0.1748768472906404, 0.1145320197044335, 0.13669950738916256, 0.15270935960591134, 0.22044334975369459], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.65270935960591137, 0.57512315270935965, 0.56896551724137934, 0.66009852216748766, 0.62684729064039413, 0.5788177339901478, 0.61576354679802958, 0.57266009852216748, 0.62068965517241381, 0.58743842364532017], 5: [0.20320197044334976, 0.22660098522167488, 0.26724137931034481, 0.22413793103448276, 0.18965517241379309, 0.24630541871921183, 0.24630541871921183, 0.26970443349753692, 0.29064039408866993, 0.22660098522167488, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150000 minutes
Weight histogram
[ 232  297  491  722 1679 3668 6133 7069 3623  386] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
[  136   165   223   329   517   863  1078  2009  6663 12317] [-0.0001064   0.00012902  0.00036444  0.00059986  0.00083528  0.0010707
  0.00130612  0.00154154  0.00177696  0.00201238  0.0022478 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43544
Epoch 1, cost is  2.39453
Epoch 2, cost is  2.36676
Epoch 3, cost is  2.34133
Epoch 4, cost is  2.3194
Training took 0.116311 minutes
Weight histogram
[5446 3908 2981 3086 2265 1767 1987 1179 1392  289] [ -5.63502945e-02  -5.07119001e-02  -4.50735058e-02  -3.94351114e-02
  -3.37967171e-02  -2.81583227e-02  -2.25199283e-02  -1.68815340e-02
  -1.12431396e-02  -5.60474526e-03   3.36491030e-05]
[2568 1356 1376 1773 2150 2461 2694 3059 3418 3445] [ -5.63502945e-02  -5.07119001e-02  -4.50735058e-02  -3.94351114e-02
  -3.37967171e-02  -2.81583227e-02  -2.25199283e-02  -1.68815340e-02
  -1.12431396e-02  -5.60474526e-03   3.36491030e-05]
-1.22127
1.57668
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149558 minutes
Weight histogram
[  65   95  492 1204 2907 4150 5315 6391 4583 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 15119] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.117205 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 2155 3631  578] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4796 1338 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.18582
Epoch 1, cost is  5.00254
Epoch 2, cost is  4.51318
Epoch 3, cost is  3.93368
Epoch 4, cost is  3.45643
Epoch 5, cost is  3.11097
Epoch 6, cost is  2.86905
Epoch 7, cost is  2.67779
Epoch 8, cost is  2.51266
Epoch 9, cost is  2.36582
Training took 0.681269 minutes
Weight histogram
[1756 1366 1064 1043  956 1846   37   16    8    8] [-0.00658948 -0.00594345 -0.00529742 -0.00465138 -0.00400535 -0.00335932
 -0.00271328 -0.00206725 -0.00142122 -0.00077518 -0.00012915]
[1864  506  509  536  560  622  721  848  955  979] [-0.00658948 -0.00594345 -0.00529742 -0.00465138 -0.00400535 -0.00335932
 -0.00271328 -0.00206725 -0.00142122 -0.00077518 -0.00012915]
-0.183405
0.197416
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077006 minutes
Epoch 0
Fine tuning took 0.076858 minutes
Epoch 0
Fine tuning took 0.077062 minutes
Epoch 0
Fine tuning took 0.077564 minutes
Epoch 0
Fine tuning took 0.078454 minutes
Epoch 0
Fine tuning took 0.077445 minutes
Epoch 0
Fine tuning took 0.077547 minutes
Epoch 0
Fine tuning took 0.078748 minutes
Epoch 0
Fine tuning took 0.077567 minutes
Epoch 0
Fine tuning took 0.078122 minutes
{'zero': {0: [0.18596059113300492, 0.18842364532019704, 0.18226600985221675, 0.19581280788177341, 0.23399014778325122, 0.18596059113300492, 0.15517241379310345, 0.15886699507389163, 0.23399014778325122, 0.16379310344827586, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.62931034482758619, 0.58620689655172409, 0.57266009852216748, 0.56773399014778325, 0.56896551724137934, 0.58497536945812811, 0.56280788177339902, 0.56527093596059108, 0.56896551724137934, 0.57019704433497542], 5: [0.18349753694581281, 0.18226600985221675, 0.23152709359605911, 0.23152709359605911, 0.19827586206896552, 0.24507389162561577, 0.25985221674876846, 0.27832512315270935, 0.20073891625615764, 0.26724137931034481, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18596059113300492, 0.16625615763546797, 0.19827586206896552, 0.24630541871921183, 0.24876847290640394, 0.2019704433497537, 0.16625615763546797, 0.16748768472906403, 0.19950738916256158, 0.20320197044334976, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.62315270935960587, 0.56034482758620685, 0.53201970443349755, 0.56280788177339902, 0.60221674876847286, 0.5431034482758621, 0.54187192118226601, 0.59605911330049266, 0.56650246305418717, 0.53694581280788178], 5: [0.18349753694581281, 0.2105911330049261, 0.2413793103448276, 0.22167487684729065, 0.18842364532019704, 0.19581280788177341, 0.29064039408866993, 0.29064039408866993, 0.20443349753694581, 0.23029556650246305, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18596059113300492, 0.16625615763546797, 0.18226600985221675, 0.21428571428571427, 0.2413793103448276, 0.18965517241379309, 0.1748768472906404, 0.16502463054187191, 0.20443349753694581, 0.19827586206896552, 0.21182266009852216], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.62684729064039413, 0.57512315270935965, 0.55665024630541871, 0.56034482758620685, 0.61330049261083741, 0.56403940886699511, 0.59975369458128081, 0.59605911330049266, 0.55049261083743839, 0.54187192118226601], 5: [0.18349753694581281, 0.20689655172413793, 0.24261083743842365, 0.22906403940886699, 0.19827586206896552, 0.19704433497536947, 0.26108374384236455, 0.23522167487684728, 0.19950738916256158, 0.25123152709359609, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18596059113300492, 0.15270935960591134, 0.17364532019704434, 0.23275862068965517, 0.20566502463054187, 0.18719211822660098, 0.16995073891625614, 0.15517241379310345, 0.22906403940886699, 0.18349753694581281, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.61945812807881773, 0.58251231527093594, 0.52216748768472909, 0.58497536945812811, 0.6071428571428571, 0.56157635467980294, 0.59729064039408863, 0.56157635467980294, 0.56034482758620685, 0.56527093596059108], 5: [0.18349753694581281, 0.22783251231527094, 0.24384236453201971, 0.24507389162561577, 0.20935960591133004, 0.20566502463054187, 0.26847290640394089, 0.24753694581280788, 0.20935960591133004, 0.25615763546798032, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235810 minutes
Weight histogram
[ 184  569  703  472  702 1629 3992 7774 7557  718] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 151  247  353  630  821  974 2244 3250 5952 9678] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.57147
Epoch 1, cost is  1.54488
Epoch 2, cost is  1.52405
Epoch 3, cost is  1.50723
Epoch 4, cost is  1.49011
Training took 0.223550 minutes
Weight histogram
[5238 3715 3514 2742 2158 1840 1420 1331 1425  917] [ -5.78097627e-02  -5.20218035e-02  -4.62338444e-02  -4.04458853e-02
  -3.46579262e-02  -2.88699670e-02  -2.30820079e-02  -1.72940488e-02
  -1.15060896e-02  -5.71813052e-03   6.98286021e-05]
[2192 1247 1510 1754 2045 2279 2811 2860 3418 4184] [ -5.78097627e-02  -5.20218035e-02  -4.62338444e-02  -4.04458853e-02
  -3.46579262e-02  -2.88699670e-02  -2.30820079e-02  -1.72940488e-02
  -1.15060896e-02  -5.71813052e-03   6.98286021e-05]
-0.875225
1.64895
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151375 minutes
Weight histogram
[  65   94  749 1648 3108 3836 4906 6242 4554 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 15119] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.115160 minutes
Weight histogram
[4668 3310 2936 2779 2496 2002 1782 1805 3434 1113] [ -5.63436821e-02  -5.07023310e-02  -4.50609800e-02  -3.94196289e-02
  -3.37782778e-02  -2.81369268e-02  -2.24955757e-02  -1.68542246e-02
  -1.12128735e-02  -5.57152247e-03   6.98286021e-05]
[4796 1338 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07023310e-02  -4.50609800e-02  -3.94196289e-02
  -3.37782778e-02  -2.81369268e-02  -2.24955757e-02  -1.68542246e-02
  -1.12128735e-02  -5.57152247e-03   6.98286021e-05]
-1.13506
1.30819
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.30682
Epoch 1, cost is  6.02114
Epoch 2, cost is  5.75931
Epoch 3, cost is  5.39516
Epoch 4, cost is  5.02412
Epoch 5, cost is  4.72305
Epoch 6, cost is  4.47833
Epoch 7, cost is  4.27672
Epoch 8, cost is  4.10926
Epoch 9, cost is  3.96047
Training took 0.251664 minutes
Weight histogram
[ 904  876  849  812  816  934 1497  948  428   36] [-0.02929136 -0.02637939 -0.02346741 -0.02055544 -0.01764347 -0.01473149
 -0.01181952 -0.00890754 -0.00599557 -0.00308359 -0.00017162]
[1574  932  564  553  600  641  705  780  845  906] [-0.02929136 -0.02637939 -0.02346741 -0.02055544 -0.01764347 -0.01473149
 -0.01181952 -0.00890754 -0.00599557 -0.00308359 -0.00017162]
-0.474795
0.641182
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076439 minutes
Epoch 0
Fine tuning took 0.077514 minutes
Epoch 0
Fine tuning took 0.076529 minutes
Epoch 0
Fine tuning took 0.077519 minutes
Epoch 0
Fine tuning took 0.077295 minutes
Epoch 0
Fine tuning took 0.075703 minutes
Epoch 0
Fine tuning took 0.076804 minutes
Epoch 0
Fine tuning took 0.075604 minutes
Epoch 0
Fine tuning took 0.077275 minutes
Epoch 0
Fine tuning took 0.077092 minutes
{'zero': {0: [0.36945812807881773, 0.2413793103448276, 0.20566502463054187, 0.15024630541871922, 0.24753694581280788, 0.16871921182266009, 0.12807881773399016, 0.099753694581280791, 0.16009852216748768, 0.13793103448275862, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44211822660098521, 0.54556650246305416, 0.5923645320197044, 0.75862068965517238, 0.44827586206896552, 0.67241379310344829, 0.65886699507389157, 0.70935960591133007, 0.6711822660098522, 0.61576354679802958, 0.69704433497536944], 5: [0.18842364532019704, 0.21305418719211822, 0.2019704433497537, 0.091133004926108374, 0.30418719211822659, 0.15886699507389163, 0.21305418719211822, 0.19088669950738915, 0.16871921182266009, 0.24630541871921183, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.36945812807881773, 0.26354679802955666, 0.34975369458128081, 0.23891625615763548, 0.28694581280788178, 0.19334975369458129, 0.15147783251231528, 0.10837438423645321, 0.1748768472906404, 0.11822660098522167, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44211822660098521, 0.51600985221674878, 0.51108374384236455, 0.72536945812807885, 0.43349753694581283, 0.66502463054187189, 0.66256157635467983, 0.66256157635467983, 0.64778325123152714, 0.64778325123152714, 0.75123152709359609], 5: [0.18842364532019704, 0.22044334975369459, 0.13916256157635468, 0.035714285714285712, 0.27955665024630544, 0.14162561576354679, 0.18596059113300492, 0.22906403940886699, 0.17733990147783252, 0.23399014778325122, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.36945812807881773, 0.24507389162561577, 0.32758620689655171, 0.22783251231527094, 0.28448275862068967, 0.21551724137931033, 0.18596059113300492, 0.078817733990147784, 0.19458128078817735, 0.11576354679802955, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44211822660098521, 0.53078817733990147, 0.53325123152709364, 0.73275862068965514, 0.41502463054187194, 0.63669950738916259, 0.62561576354679804, 0.69704433497536944, 0.64532019704433496, 0.64655172413793105, 0.72660098522167482], 5: [0.18842364532019704, 0.22413793103448276, 0.13916256157635468, 0.039408866995073892, 0.30049261083743845, 0.14778325123152711, 0.18842364532019704, 0.22413793103448276, 0.16009852216748768, 0.2376847290640394, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.36945812807881773, 0.27093596059113301, 0.37068965517241381, 0.24630541871921183, 0.27832512315270935, 0.22167487684729065, 0.17857142857142858, 0.10221674876847291, 0.18719211822660098, 0.11822660098522167, 0.10467980295566502], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44211822660098521, 0.49753694581280788, 0.48152709359605911, 0.72536945812807885, 0.44581280788177341, 0.63300492610837433, 0.64039408866995073, 0.6785714285714286, 0.67733990147783252, 0.62068965517241381, 0.69211822660098521], 5: [0.18842364532019704, 0.23152709359605911, 0.14778325123152711, 0.02832512315270936, 0.27586206896551724, 0.14532019704433496, 0.18103448275862069, 0.21921182266009853, 0.1354679802955665, 0.26108374384236455, 0.20320197044334976], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235897 minutes
Weight histogram
[ 184  569  703  472  702 1629 3992 7774 7557  718] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 151  247  353  630  821  974 2244 3250 5952 9678] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.57147
Epoch 1, cost is  1.54488
Epoch 2, cost is  1.52405
Epoch 3, cost is  1.50723
Epoch 4, cost is  1.49011
Training took 0.224566 minutes
Weight histogram
[5238 3715 3514 2742 2158 1840 1420 1331 1425  917] [ -5.78097627e-02  -5.20218035e-02  -4.62338444e-02  -4.04458853e-02
  -3.46579262e-02  -2.88699670e-02  -2.30820079e-02  -1.72940488e-02
  -1.15060896e-02  -5.71813052e-03   6.98286021e-05]
[2192 1247 1510 1754 2045 2279 2811 2860 3418 4184] [ -5.78097627e-02  -5.20218035e-02  -4.62338444e-02  -4.04458853e-02
  -3.46579262e-02  -2.88699670e-02  -2.30820079e-02  -1.72940488e-02
  -1.15060896e-02  -5.71813052e-03   6.98286021e-05]
-0.875225
1.64895
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148017 minutes
Weight histogram
[  65   94  749 1648 3108 3836 4906 6242 4554 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 15119] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.114641 minutes
Weight histogram
[4668 3310 2936 2779 2496 2002 1782 1805 3434 1113] [ -5.63436821e-02  -5.07023310e-02  -4.50609800e-02  -3.94196289e-02
  -3.37782778e-02  -2.81369268e-02  -2.24955757e-02  -1.68542246e-02
  -1.12128735e-02  -5.57152247e-03   6.98286021e-05]
[4796 1338 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07023310e-02  -4.50609800e-02  -3.94196289e-02
  -3.37782778e-02  -2.81369268e-02  -2.24955757e-02  -1.68542246e-02
  -1.12128735e-02  -5.57152247e-03   6.98286021e-05]
-1.13506
1.30819
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.68908
Epoch 1, cost is  5.40545
Epoch 2, cost is  5.06672
Epoch 3, cost is  4.58449
Epoch 4, cost is  4.17648
Epoch 5, cost is  3.87997
Epoch 6, cost is  3.64315
Epoch 7, cost is  3.43141
Epoch 8, cost is  3.24593
Epoch 9, cost is  3.0867
Training took 0.353936 minutes
Weight histogram
[1427 1219  961  791  865 2026  615  136   42   18] [-0.01850437 -0.01666854 -0.01483271 -0.01299688 -0.01116105 -0.00932522
 -0.00748939 -0.00565356 -0.00381772 -0.00198189 -0.00014606]
[1966  553  511  540  621  702  766  793  821  827] [-0.01850437 -0.01666854 -0.01483271 -0.01299688 -0.01116105 -0.00932522
 -0.00748939 -0.00565356 -0.00381772 -0.00198189 -0.00014606]
-0.272732
0.388563
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082513 minutes
Epoch 0
Fine tuning took 0.082171 minutes
Epoch 0
Fine tuning took 0.081139 minutes
Epoch 0
Fine tuning took 0.082698 minutes
Epoch 0
Fine tuning took 0.080838 minutes
Epoch 0
Fine tuning took 0.081732 minutes
Epoch 0
Fine tuning took 0.080558 minutes
Epoch 0
Fine tuning took 0.080933 minutes
Epoch 0
Fine tuning took 0.082027 minutes
Epoch 0
Fine tuning took 0.081169 minutes
{'zero': {0: [0.25, 0.14778325123152711, 0.093596059113300489, 0.22536945812807882, 0.098522167487684734, 0.15763546798029557, 0.12315270935960591, 0.10714285714285714, 0.12438423645320197, 0.10591133004926108, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.66009852216748766, 0.70935960591133007, 0.55911330049261088, 0.74014778325123154, 0.66871921182266014, 0.68226600985221675, 0.75123152709359609, 0.71305418719211822, 0.62931034482758619, 0.60098522167487689], 5: [0.18226600985221675, 0.19211822660098521, 0.19704433497536947, 0.21551724137931033, 0.16133004926108374, 0.17364532019704434, 0.19458128078817735, 0.14162561576354679, 0.1625615763546798, 0.26477832512315269, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25, 0.16625615763546797, 0.10591133004926108, 0.17364532019704434, 0.13300492610837439, 0.1354679802955665, 0.13177339901477833, 0.089901477832512317, 0.12315270935960591, 0.13423645320197045, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.66256157635467983, 0.70320197044334976, 0.64778325123152714, 0.72906403940886699, 0.7068965517241379, 0.69211822660098521, 0.79187192118226601, 0.75985221674876846, 0.66995073891625612, 0.66133004926108374], 5: [0.18226600985221675, 0.17118226600985223, 0.19088669950738915, 0.17857142857142858, 0.13793103448275862, 0.15763546798029557, 0.17610837438423646, 0.11822660098522167, 0.11699507389162561, 0.19581280788177341, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25, 0.18103448275862069, 0.12192118226600986, 0.21551724137931033, 0.16871921182266009, 0.14655172413793102, 0.12561576354679804, 0.1268472906403941, 0.13669950738916256, 0.12561576354679804, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.6354679802955665, 0.6711822660098522, 0.60591133004926112, 0.71551724137931039, 0.70073891625615758, 0.73891625615763545, 0.76600985221674878, 0.75123152709359609, 0.63177339901477836, 0.66871921182266014], 5: [0.18226600985221675, 0.18349753694581281, 0.20689655172413793, 0.17857142857142858, 0.11576354679802955, 0.15270935960591134, 0.1354679802955665, 0.10714285714285714, 0.11206896551724138, 0.24261083743842365, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25, 0.14655172413793102, 0.11206896551724138, 0.17118226600985223, 0.14901477832512317, 0.13916256157635468, 0.13669950738916256, 0.10098522167487685, 0.13054187192118227, 0.13423645320197045, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.66502463054187189, 0.69211822660098521, 0.64778325123152714, 0.73152709359605916, 0.73029556650246308, 0.69704433497536944, 0.78448275862068961, 0.74014778325123154, 0.6145320197044335, 0.64162561576354682], 5: [0.18226600985221675, 0.18842364532019704, 0.19581280788177341, 0.18103448275862069, 0.11945812807881774, 0.13054187192118227, 0.16625615763546797, 0.1145320197044335, 0.12931034482758622, 0.25123152709359609, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237959 minutes
Weight histogram
[ 184  569  703  472  702 1629 3992 7774 7557  718] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 151  247  353  630  821  974 2244 3250 5952 9678] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.57147
Epoch 1, cost is  1.54488
Epoch 2, cost is  1.52405
Epoch 3, cost is  1.50723
Epoch 4, cost is  1.49011
Training took 0.224239 minutes
Weight histogram
[5238 3715 3514 2742 2158 1840 1420 1331 1425  917] [ -5.78097627e-02  -5.20218035e-02  -4.62338444e-02  -4.04458853e-02
  -3.46579262e-02  -2.88699670e-02  -2.30820079e-02  -1.72940488e-02
  -1.15060896e-02  -5.71813052e-03   6.98286021e-05]
[2192 1247 1510 1754 2045 2279 2811 2860 3418 4184] [ -5.78097627e-02  -5.20218035e-02  -4.62338444e-02  -4.04458853e-02
  -3.46579262e-02  -2.88699670e-02  -2.30820079e-02  -1.72940488e-02
  -1.15060896e-02  -5.71813052e-03   6.98286021e-05]
-0.875225
1.64895
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149789 minutes
Weight histogram
[  65   94  749 1648 3108 3836 4906 6242 4554 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 15119] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.117538 minutes
Weight histogram
[4668 3310 2936 2779 2496 2002 1782 1805 3434 1113] [ -5.63436821e-02  -5.07023310e-02  -4.50609800e-02  -3.94196289e-02
  -3.37782778e-02  -2.81369268e-02  -2.24955757e-02  -1.68542246e-02
  -1.12128735e-02  -5.57152247e-03   6.98286021e-05]
[4796 1338 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07023310e-02  -4.50609800e-02  -3.94196289e-02
  -3.37782778e-02  -2.81369268e-02  -2.24955757e-02  -1.68542246e-02
  -1.12128735e-02  -5.57152247e-03   6.98286021e-05]
-1.13506
1.30819
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.09804
Epoch 1, cost is  4.85604
Epoch 2, cost is  4.50561
Epoch 3, cost is  4.04641
Epoch 4, cost is  3.68127
Epoch 5, cost is  3.40128
Epoch 6, cost is  3.15423
Epoch 7, cost is  2.93466
Epoch 8, cost is  2.75741
Epoch 9, cost is  2.60918
Training took 0.545604 minutes
Weight histogram
[1809 1647 1122 2663  504  192   85   43   22   13] [-0.01303397 -0.01174368 -0.01045338 -0.00916308 -0.00787279 -0.00658249
 -0.00529219 -0.0040019  -0.0027116  -0.0014213  -0.00013101]
[1955  542  517  566  656  712  728  754  809  861] [-0.01303397 -0.01174368 -0.01045338 -0.00916308 -0.00787279 -0.00658249
 -0.00529219 -0.0040019  -0.0027116  -0.0014213  -0.00013101]
-0.204408
0.249578
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090374 minutes
Epoch 0
Fine tuning took 0.092073 minutes
Epoch 0
Fine tuning took 0.091746 minutes
Epoch 0
Fine tuning took 0.090822 minutes
Epoch 0
Fine tuning took 0.089667 minutes
Epoch 0
Fine tuning took 0.090216 minutes
Epoch 0
Fine tuning took 0.090540 minutes
Epoch 0
Fine tuning took 0.091260 minutes
Epoch 0
Fine tuning took 0.090834 minutes
Epoch 0
Fine tuning took 0.089363 minutes
{'zero': {0: [0.17241379310344829, 0.23152709359605911, 0.18226600985221675, 0.17857142857142858, 0.14285714285714285, 0.18965517241379309, 0.19334975369458129, 0.17364532019704434, 0.15763546798029557, 0.14039408866995073, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.58497536945812811, 0.56280788177339902, 0.6071428571428571, 0.64162561576354682, 0.59482758620689657, 0.57512315270935965, 0.52586206896551724, 0.64655172413793105, 0.59729064039408863, 0.57758620689655171], 5: [0.19458128078817735, 0.18349753694581281, 0.25492610837438423, 0.21428571428571427, 0.21551724137931033, 0.21551724137931033, 0.23152709359605911, 0.30049261083743845, 0.19581280788177341, 0.26231527093596058, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17241379310344829, 0.20443349753694581, 0.16748768472906403, 0.17364532019704434, 0.14285714285714285, 0.20689655172413793, 0.19581280788177341, 0.14162561576354679, 0.16871921182266009, 0.13054187192118227, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.61699507389162567, 0.6145320197044335, 0.6145320197044335, 0.6219211822660099, 0.60591133004926112, 0.54064039408866993, 0.56403940886699511, 0.59729064039408863, 0.62931034482758619, 0.55541871921182262], 5: [0.19458128078817735, 0.17857142857142858, 0.21798029556650247, 0.21182266009852216, 0.23522167487684728, 0.18719211822660098, 0.26354679802955666, 0.29433497536945813, 0.23399014778325122, 0.24014778325123154, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17241379310344829, 0.16009852216748768, 0.15270935960591134, 0.16502463054187191, 0.14285714285714285, 0.19581280788177341, 0.20566502463054187, 0.16379310344827586, 0.14039408866995073, 0.11576354679802955, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.67241379310344829, 0.62438423645320196, 0.59975369458128081, 0.64901477832512311, 0.61576354679802958, 0.60098522167487689, 0.55049261083743839, 0.63177339901477836, 0.61576354679802958, 0.54187192118226601], 5: [0.19458128078817735, 0.16748768472906403, 0.2229064039408867, 0.23522167487684728, 0.20812807881773399, 0.18842364532019704, 0.19334975369458129, 0.2857142857142857, 0.22783251231527094, 0.26847290640394089, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17241379310344829, 0.19827586206896552, 0.16009852216748768, 0.16995073891625614, 0.11206896551724138, 0.19088669950738915, 0.2019704433497537, 0.15886699507389163, 0.13177339901477833, 0.11206896551724138, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.59852216748768472, 0.62931034482758619, 0.60591133004926112, 0.67610837438423643, 0.59729064039408863, 0.55418719211822665, 0.55418719211822665, 0.62931034482758619, 0.61330049261083741, 0.5788177339901478], 5: [0.19458128078817735, 0.20320197044334976, 0.2105911330049261, 0.22413793103448276, 0.21182266009852216, 0.21182266009852216, 0.24384236453201971, 0.28694581280788178, 0.23891625615763548, 0.27463054187192121, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234739 minutes
Weight histogram
[ 184  569  703  472  702 1629 3992 7774 7557  718] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 151  247  353  630  821  974 2244 3250 5952 9678] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.57147
Epoch 1, cost is  1.54488
Epoch 2, cost is  1.52405
Epoch 3, cost is  1.50723
Epoch 4, cost is  1.49011
Training took 0.225336 minutes
Weight histogram
[5238 3715 3514 2742 2158 1840 1420 1331 1425  917] [ -5.78097627e-02  -5.20218035e-02  -4.62338444e-02  -4.04458853e-02
  -3.46579262e-02  -2.88699670e-02  -2.30820079e-02  -1.72940488e-02
  -1.15060896e-02  -5.71813052e-03   6.98286021e-05]
[2192 1247 1510 1754 2045 2279 2811 2860 3418 4184] [ -5.78097627e-02  -5.20218035e-02  -4.62338444e-02  -4.04458853e-02
  -3.46579262e-02  -2.88699670e-02  -2.30820079e-02  -1.72940488e-02
  -1.15060896e-02  -5.71813052e-03   6.98286021e-05]
-0.875225
1.64895
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149393 minutes
Weight histogram
[  65   94  749 1648 3108 3836 4906 6242 4554 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 15119] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.114878 minutes
Weight histogram
[4668 3310 2936 2779 2496 2002 1782 1805 3434 1113] [ -5.63436821e-02  -5.07023310e-02  -4.50609800e-02  -3.94196289e-02
  -3.37782778e-02  -2.81369268e-02  -2.24955757e-02  -1.68542246e-02
  -1.12128735e-02  -5.57152247e-03   6.98286021e-05]
[4796 1338 1379 1696 2047 2306 2631 3085 3456 3591] [ -5.63436821e-02  -5.07023310e-02  -4.50609800e-02  -3.94196289e-02
  -3.37782778e-02  -2.81369268e-02  -2.24955757e-02  -1.68542246e-02
  -1.12128735e-02  -5.57152247e-03   6.98286021e-05]
-1.13506
1.30819
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.87915
Epoch 1, cost is  4.57474
Epoch 2, cost is  3.96723
Epoch 3, cost is  3.48059
Epoch 4, cost is  3.12011
Epoch 5, cost is  2.81994
Epoch 6, cost is  2.58857
Epoch 7, cost is  2.41529
Epoch 8, cost is  2.27661
Epoch 9, cost is  2.166
Training took 0.944957 minutes
Weight histogram
[1803 1442 1432  952 1920  447   58   26   12    8] [-0.0080684  -0.00727492 -0.00648143 -0.00568795 -0.00489447 -0.00410099
 -0.00330751 -0.00251403 -0.00172054 -0.00092706 -0.00013358]
[1633  498  523  597  649  669  732  825  944 1030] [-0.0080684  -0.00727492 -0.00648143 -0.00568795 -0.00489447 -0.00410099
 -0.00330751 -0.00251403 -0.00172054 -0.00092706 -0.00013358]
-0.174459
0.177601
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.110319 minutes
Epoch 0
Fine tuning took 0.108435 minutes
Epoch 0
Fine tuning took 0.110623 minutes
Epoch 0
Fine tuning took 0.111037 minutes
Epoch 0
Fine tuning took 0.110225 minutes
Epoch 0
Fine tuning took 0.110771 minutes
Epoch 0
Fine tuning took 0.109235 minutes
Epoch 0
Fine tuning took 0.109867 minutes
Epoch 0
Fine tuning took 0.109990 minutes
Epoch 0
Fine tuning took 0.110606 minutes
{'zero': {0: [0.13793103448275862, 0.1625615763546798, 0.18349753694581281, 0.23399014778325122, 0.19827586206896552, 0.17733990147783252, 0.16502463054187191, 0.17733990147783252, 0.18719211822660098, 0.19827586206896552, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66009852216748766, 0.61699507389162567, 0.5788177339901478, 0.58743842364532017, 0.57758620689655171, 0.55295566502463056, 0.55049261083743839, 0.55665024630541871, 0.58866995073891626, 0.58990147783251234, 0.54802955665024633], 5: [0.2019704433497537, 0.22044334975369459, 0.2376847290640394, 0.17857142857142858, 0.22413793103448276, 0.26970443349753692, 0.28448275862068967, 0.26600985221674878, 0.22413793103448276, 0.21182266009852216, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13793103448275862, 0.14901477832512317, 0.15886699507389163, 0.22536945812807882, 0.2229064039408867, 0.19950738916256158, 0.17118226600985223, 0.15763546798029557, 0.19704433497536947, 0.21551724137931033, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66009852216748766, 0.64162561576354682, 0.57512315270935965, 0.56650246305418717, 0.55295566502463056, 0.55295566502463056, 0.57635467980295563, 0.55665024630541871, 0.56650246305418717, 0.58128078817733986, 0.57512315270935965], 5: [0.2019704433497537, 0.20935960591133004, 0.26600985221674878, 0.20812807881773399, 0.22413793103448276, 0.24753694581280788, 0.25246305418719212, 0.2857142857142857, 0.23645320197044334, 0.20320197044334976, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13793103448275862, 0.14408866995073891, 0.17241379310344829, 0.24507389162561577, 0.18596059113300492, 0.17857142857142858, 0.14655172413793102, 0.1748768472906404, 0.21921182266009853, 0.19581280788177341, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66009852216748766, 0.63423645320197042, 0.60221674876847286, 0.53201970443349755, 0.60344827586206895, 0.55172413793103448, 0.56157635467980294, 0.54556650246305416, 0.52709359605911332, 0.60098522167487689, 0.53940886699507384], 5: [0.2019704433497537, 0.22167487684729065, 0.22536945812807882, 0.2229064039408867, 0.2105911330049261, 0.26970443349753692, 0.29187192118226601, 0.27955665024630544, 0.2536945812807882, 0.20320197044334976, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13793103448275862, 0.17364532019704434, 0.16871921182266009, 0.23399014778325122, 0.2019704433497537, 0.16871921182266009, 0.17610837438423646, 0.16502463054187191, 0.17610837438423646, 0.20689655172413793, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66009852216748766, 0.58497536945812811, 0.58374384236453203, 0.54926108374384242, 0.55911330049261088, 0.57635467980295563, 0.53940886699507384, 0.57389162561576357, 0.56157635467980294, 0.56527093596059108, 0.58743842364532017], 5: [0.2019704433497537, 0.2413793103448276, 0.24753694581280788, 0.21674876847290642, 0.23891625615763548, 0.25492610837438423, 0.28448275862068967, 0.26108374384236455, 0.26231527093596058, 0.22783251231527094, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422216 minutes
Weight histogram
[  231   514   610   458   889  6256 10200  3444  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1180  981  835 1232 1267 2522 2996 3088 3814 6385] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.03557
Epoch 1, cost is  1.01538
Epoch 2, cost is  1.00091
Epoch 3, cost is  0.989164
Epoch 4, cost is  0.977623
Training took 0.647256 minutes
Weight histogram
[5715 4404 3774 2531 1913 1643 1342 1000  955 1023] [ -4.21172865e-02  -3.79110841e-02  -3.37048817e-02  -2.94986794e-02
  -2.52924770e-02  -2.10862746e-02  -1.68800723e-02  -1.26738699e-02
  -8.46766751e-03  -4.26146514e-03  -5.52627716e-05]
[1891 1301 1389 1590 2034 2384 2823 3119 3433 4336] [ -4.21172865e-02  -3.79110841e-02  -3.37048817e-02  -2.94986794e-02
  -2.52924770e-02  -2.10862746e-02  -1.68800723e-02  -1.26738699e-02
  -8.46766751e-03  -4.26146514e-03  -5.52627716e-05]
-0.893532
1.78989
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149594 minutes
Weight histogram
[  65   75  931 1941 2832 3656 4906 6242 4554 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 15121] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.114589 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 1811 2949 1604] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4794 1338 1379 1696 2048 2305 2632 3086 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.443
Epoch 1, cost is  6.07691
Epoch 2, cost is  5.6117
Epoch 3, cost is  5.21302
Epoch 4, cost is  4.90992
Epoch 5, cost is  4.66391
Epoch 6, cost is  4.46281
Epoch 7, cost is  4.29626
Epoch 8, cost is  4.15187
Epoch 9, cost is  4.02908
Training took 0.323055 minutes
Weight histogram
[ 991  983  861  803  737  734  678  818 1189  306] [-0.03341708 -0.03009224 -0.0267674  -0.02344256 -0.02011772 -0.01679288
 -0.01346805 -0.01014321 -0.00681837 -0.00349353 -0.00016869]
[1312  579  558  628  666  726  793  876  944 1018] [-0.03341708 -0.03009224 -0.0267674  -0.02344256 -0.02011772 -0.01679288
 -0.01346805 -0.01014321 -0.00681837 -0.00349353 -0.00016869]
-0.411392
0.50831
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.142856 minutes
Epoch 0
Fine tuning took 0.141278 minutes
Epoch 0
Fine tuning took 0.140409 minutes
Epoch 0
Fine tuning took 0.143293 minutes
Epoch 0
Fine tuning took 0.142890 minutes
Epoch 0
Fine tuning took 0.141981 minutes
Epoch 0
Fine tuning took 0.141650 minutes
Epoch 0
Fine tuning took 0.141907 minutes
Epoch 0
Fine tuning took 0.141448 minutes
Epoch 0
Fine tuning took 0.142199 minutes
{'zero': {0: [0.23152709359605911, 0.066502463054187194, 0.10344827586206896, 0.099753694581280791, 0.16502463054187191, 0.094827586206896547, 0.061576354679802957, 0.22413793103448276, 0.11330049261083744, 0.22167487684729065, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54926108374384242, 0.64162561576354682, 0.53817733990147787, 0.77709359605911332, 0.66748768472906406, 0.76724137931034486, 0.85098522167487689, 0.65270935960591137, 0.69581280788177335, 0.60837438423645318, 0.41009852216748771], 5: [0.21921182266009853, 0.29187192118226601, 0.35837438423645318, 0.12315270935960591, 0.16748768472906403, 0.13793103448275862, 0.087438423645320201, 0.12315270935960591, 0.19088669950738915, 0.16995073891625614, 0.33128078817733991], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23152709359605911, 0.071428571428571425, 0.10837438423645321, 0.084975369458128072, 0.14532019704433496, 0.10098522167487685, 0.078817733990147784, 0.2857142857142857, 0.10591133004926108, 0.18596059113300492, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54926108374384242, 0.68596059113300489, 0.58251231527093594, 0.81034482758620685, 0.67733990147783252, 0.74876847290640391, 0.84729064039408863, 0.6219211822660099, 0.72413793103448276, 0.63916256157635465, 0.47660098522167488], 5: [0.21921182266009853, 0.24261083743842365, 0.30911330049261082, 0.10467980295566502, 0.17733990147783252, 0.15024630541871922, 0.073891625615763554, 0.092364532019704432, 0.16995073891625614, 0.1748768472906404, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23152709359605911, 0.070197044334975367, 0.10467980295566502, 0.097290640394088676, 0.17733990147783252, 0.13916256157635468, 0.087438423645320201, 0.31280788177339902, 0.13300492610837439, 0.16625615763546797, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54926108374384242, 0.69581280788177335, 0.55541871921182262, 0.80172413793103448, 0.66256157635467983, 0.70320197044334976, 0.83620689655172409, 0.6071428571428571, 0.7068965517241379, 0.67364532019704437, 0.47536945812807879], 5: [0.21921182266009853, 0.23399014778325122, 0.33990147783251229, 0.10098522167487685, 0.16009852216748768, 0.15763546798029557, 0.076354679802955669, 0.080049261083743842, 0.16009852216748768, 0.16009852216748768, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23152709359605911, 0.068965517241379309, 0.088669950738916259, 0.071428571428571425, 0.14408866995073891, 0.10837438423645321, 0.087438423645320201, 0.28078817733990147, 0.13054187192118227, 0.1539408866995074, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54926108374384242, 0.68596059113300489, 0.56280788177339902, 0.8214285714285714, 0.69827586206896552, 0.75862068965517238, 0.84975369458128081, 0.6280788177339901, 0.70812807881773399, 0.65270935960591137, 0.46305418719211822], 5: [0.21921182266009853, 0.24507389162561577, 0.34852216748768472, 0.10714285714285714, 0.15763546798029557, 0.13300492610837439, 0.062807881773399021, 0.091133004926108374, 0.16133004926108374, 0.19334975369458129, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420142 minutes
Weight histogram
[  231   514   610   458   889  6256 10200  3444  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1180  981  835 1232 1267 2522 2996 3088 3814 6385] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.03557
Epoch 1, cost is  1.01538
Epoch 2, cost is  1.00091
Epoch 3, cost is  0.989164
Epoch 4, cost is  0.977623
Training took 0.646344 minutes
Weight histogram
[5715 4404 3774 2531 1913 1643 1342 1000  955 1023] [ -4.21172865e-02  -3.79110841e-02  -3.37048817e-02  -2.94986794e-02
  -2.52924770e-02  -2.10862746e-02  -1.68800723e-02  -1.26738699e-02
  -8.46766751e-03  -4.26146514e-03  -5.52627716e-05]
[1891 1301 1389 1590 2034 2384 2823 3119 3433 4336] [ -4.21172865e-02  -3.79110841e-02  -3.37048817e-02  -2.94986794e-02
  -2.52924770e-02  -2.10862746e-02  -1.68800723e-02  -1.26738699e-02
  -8.46766751e-03  -4.26146514e-03  -5.52627716e-05]
-0.893532
1.78989
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149621 minutes
Weight histogram
[  65   75  931 1941 2832 3656 4906 6242 4554 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 15121] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.114880 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 1811 2949 1604] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4794 1338 1379 1696 2048 2305 2632 3086 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.94608
Epoch 1, cost is  5.47935
Epoch 2, cost is  4.89641
Epoch 3, cost is  4.44578
Epoch 4, cost is  4.08983
Epoch 5, cost is  3.80225
Epoch 6, cost is  3.57232
Epoch 7, cost is  3.38373
Epoch 8, cost is  3.22482
Epoch 9, cost is  3.09173
Training took 0.500811 minutes
Weight histogram
[1177 1073  988  894  788  747  685 1307  413   28] [-0.02047332 -0.01844112 -0.01640892 -0.01437671 -0.01234451 -0.01031231
 -0.0082801  -0.0062479  -0.0042157  -0.0021835  -0.00015129]
[1409  556  625  627  672  718  776  833  906  978] [-0.02047332 -0.01844112 -0.01640892 -0.01437671 -0.01234451 -0.01031231
 -0.0082801  -0.0062479  -0.0042157  -0.0021835  -0.00015129]
-0.387108
0.391565
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.150034 minutes
Epoch 0
Fine tuning took 0.150717 minutes
Epoch 0
Fine tuning took 0.151081 minutes
Epoch 0
Fine tuning took 0.150781 minutes
Epoch 0
Fine tuning took 0.151459 minutes
Epoch 0
Fine tuning took 0.151611 minutes
Epoch 0
Fine tuning took 0.151045 minutes
Epoch 0
Fine tuning took 0.148956 minutes
Epoch 0
Fine tuning took 0.149984 minutes
Epoch 0
Fine tuning took 0.150537 minutes
{'zero': {0: [0.31280788177339902, 0.17980295566502463, 0.17980295566502463, 0.19704433497536947, 0.18596059113300492, 0.24384236453201971, 0.20073891625615764, 0.083743842364532015, 0.10098522167487685, 0.11945812807881774, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.72290640394088668, 0.6219211822660099, 0.60960591133004927, 0.63793103448275867, 0.6071428571428571, 0.6280788177339901, 0.76354679802955661, 0.70935960591133007, 0.69211822660098521, 0.6145320197044335], 5: [0.23152709359605911, 0.097290640394088676, 0.19827586206896552, 0.19334975369458129, 0.17610837438423646, 0.14901477832512317, 0.17118226600985223, 0.15270935960591134, 0.18965517241379309, 0.18842364532019704, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31280788177339902, 0.23275862068965517, 0.16625615763546797, 0.18349753694581281, 0.24014778325123154, 0.23152709359605911, 0.17733990147783252, 0.10714285714285714, 0.13177339901477833, 0.15147783251231528, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.66871921182266014, 0.62438423645320196, 0.66502463054187189, 0.57389162561576357, 0.60837438423645318, 0.65270935960591137, 0.79064039408866993, 0.68596059113300489, 0.66502463054187189, 0.63054187192118227], 5: [0.23152709359605911, 0.098522167487684734, 0.20935960591133004, 0.15147783251231528, 0.18596059113300492, 0.16009852216748768, 0.16995073891625614, 0.10221674876847291, 0.18226600985221675, 0.18349753694581281, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31280788177339902, 0.2229064039408867, 0.16379310344827586, 0.22413793103448276, 0.24507389162561577, 0.20812807881773399, 0.17610837438423646, 0.13054187192118227, 0.12807881773399016, 0.14039408866995073, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.64162561576354682, 0.62315270935960587, 0.62438423645320196, 0.54802955665024633, 0.6145320197044335, 0.63177339901477836, 0.75492610837438423, 0.68719211822660098, 0.68226600985221675, 0.63793103448275867], 5: [0.23152709359605911, 0.1354679802955665, 0.21305418719211822, 0.15147783251231528, 0.20689655172413793, 0.17733990147783252, 0.19211822660098521, 0.1145320197044335, 0.18472906403940886, 0.17733990147783252, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31280788177339902, 0.20812807881773399, 0.16009852216748768, 0.14408866995073891, 0.20812807881773399, 0.21551724137931033, 0.19088669950738915, 0.096059113300492605, 0.11822660098522167, 0.13177339901477833, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.6711822660098522, 0.65024630541871919, 0.67733990147783252, 0.56896551724137934, 0.63916256157635465, 0.66009852216748766, 0.79926108374384242, 0.74507389162561577, 0.65886699507389157, 0.65394088669950734], 5: [0.23152709359605911, 0.1206896551724138, 0.18965517241379309, 0.17857142857142858, 0.2229064039408867, 0.14532019704433496, 0.14901477832512317, 0.10467980295566502, 0.13669950738916256, 0.20935960591133004, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421497 minutes
Weight histogram
[  231   514   610   458   889  6256 10200  3444  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1180  981  835 1232 1267 2522 2996 3088 3814 6385] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.03557
Epoch 1, cost is  1.01538
Epoch 2, cost is  1.00091
Epoch 3, cost is  0.989164
Epoch 4, cost is  0.977623
Training took 0.646734 minutes
Weight histogram
[5715 4404 3774 2531 1913 1643 1342 1000  955 1023] [ -4.21172865e-02  -3.79110841e-02  -3.37048817e-02  -2.94986794e-02
  -2.52924770e-02  -2.10862746e-02  -1.68800723e-02  -1.26738699e-02
  -8.46766751e-03  -4.26146514e-03  -5.52627716e-05]
[1891 1301 1389 1590 2034 2384 2823 3119 3433 4336] [ -4.21172865e-02  -3.79110841e-02  -3.37048817e-02  -2.94986794e-02
  -2.52924770e-02  -2.10862746e-02  -1.68800723e-02  -1.26738699e-02
  -8.46766751e-03  -4.26146514e-03  -5.52627716e-05]
-0.893532
1.78989
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150270 minutes
Weight histogram
[  65   75  931 1941 2832 3656 4906 6242 4554 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 15121] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.118839 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 1811 2949 1604] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4794 1338 1379 1696 2048 2305 2632 3086 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.3263
Epoch 1, cost is  4.81459
Epoch 2, cost is  4.22611
Epoch 3, cost is  3.78585
Epoch 4, cost is  3.44947
Epoch 5, cost is  3.18685
Epoch 6, cost is  2.97661
Epoch 7, cost is  2.80494
Epoch 8, cost is  2.66163
Epoch 9, cost is  2.54344
Training took 0.815842 minutes
Weight histogram
[1505 1331 1110  946  839  706 1409  200   40   14] [-0.01359871 -0.01225254 -0.01090637 -0.00956019 -0.00821402 -0.00686785
 -0.00552167 -0.0041755  -0.00282933 -0.00148316 -0.00013698]
[1423  576  611  634  672  721  776  832  897  958] [-0.01359871 -0.01225254 -0.01090637 -0.00956019 -0.00821402 -0.00686785
 -0.00552167 -0.0041755  -0.00282933 -0.00148316 -0.00013698]
-0.227939
0.316708
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164816 minutes
Epoch 0
Fine tuning took 0.163395 minutes
Epoch 0
Fine tuning took 0.165931 minutes
Epoch 0
Fine tuning took 0.164550 minutes
Epoch 0
Fine tuning took 0.165110 minutes
Epoch 0
Fine tuning took 0.164829 minutes
Epoch 0
Fine tuning took 0.165281 minutes
Epoch 0
Fine tuning took 0.164771 minutes
Epoch 0
Fine tuning took 0.165992 minutes
Epoch 0
Fine tuning took 0.164611 minutes
{'zero': {0: [0.24630541871921183, 0.25615763546798032, 0.14655172413793102, 0.14655172413793102, 0.14778325123152711, 0.19704433497536947, 0.14162561576354679, 0.16133004926108374, 0.15640394088669951, 0.12561576354679804, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.58128078817733986, 0.58990147783251234, 0.63177339901477836, 0.61576354679802958, 0.62561576354679804, 0.58743842364532017, 0.64039408866995073, 0.66133004926108374, 0.61822660098522164, 0.56773399014778325], 5: [0.20073891625615764, 0.1625615763546798, 0.26354679802955666, 0.22167487684729065, 0.23645320197044334, 0.17733990147783252, 0.27093596059113301, 0.19827586206896552, 0.18226600985221675, 0.25615763546798032, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24630541871921183, 0.21921182266009853, 0.11699507389162561, 0.17610837438423646, 0.17118226600985223, 0.18349753694581281, 0.17733990147783252, 0.16748768472906403, 0.1748768472906404, 0.13669950738916256, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.59113300492610843, 0.64039408866995073, 0.62068965517241381, 0.60591133004926112, 0.62931034482758619, 0.63054187192118227, 0.64039408866995073, 0.61822660098522164, 0.6145320197044335, 0.58251231527093594], 5: [0.20073891625615764, 0.18965517241379309, 0.24261083743842365, 0.20320197044334976, 0.2229064039408867, 0.18719211822660098, 0.19211822660098521, 0.19211822660098521, 0.20689655172413793, 0.24876847290640394, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24630541871921183, 0.22536945812807882, 0.13054187192118227, 0.15147783251231528, 0.12192118226600986, 0.17241379310344829, 0.14039408866995073, 0.16748768472906403, 0.15886699507389163, 0.12561576354679804, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.61945812807881773, 0.64778325123152714, 0.63177339901477836, 0.65394088669950734, 0.66502463054187189, 0.64408866995073888, 0.62068965517241381, 0.67364532019704437, 0.61699507389162567, 0.57512315270935965], 5: [0.20073891625615764, 0.15517241379310345, 0.22167487684729065, 0.21674876847290642, 0.22413793103448276, 0.1625615763546798, 0.21551724137931033, 0.21182266009852216, 0.16748768472906403, 0.25738916256157635, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24630541871921183, 0.20812807881773399, 0.12807881773399016, 0.15886699507389163, 0.15147783251231528, 0.19581280788177341, 0.1625615763546798, 0.14162561576354679, 0.14655172413793102, 0.13054187192118227, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.59975369458128081, 0.6071428571428571, 0.61330049261083741, 0.64778325123152714, 0.61822660098522164, 0.61945812807881773, 0.64162561576354682, 0.6711822660098522, 0.58743842364532017, 0.58497536945812811], 5: [0.20073891625615764, 0.19211822660098521, 0.26477832512315269, 0.22783251231527094, 0.20073891625615764, 0.18596059113300492, 0.21798029556650247, 0.21674876847290642, 0.18226600985221675, 0.28201970443349755, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421428 minutes
Weight histogram
[  231   514   610   458   889  6256 10200  3444  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1180  981  835 1232 1267 2522 2996 3088 3814 6385] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.03557
Epoch 1, cost is  1.01538
Epoch 2, cost is  1.00091
Epoch 3, cost is  0.989164
Epoch 4, cost is  0.977623
Training took 0.646798 minutes
Weight histogram
[5715 4404 3774 2531 1913 1643 1342 1000  955 1023] [ -4.21172865e-02  -3.79110841e-02  -3.37048817e-02  -2.94986794e-02
  -2.52924770e-02  -2.10862746e-02  -1.68800723e-02  -1.26738699e-02
  -8.46766751e-03  -4.26146514e-03  -5.52627716e-05]
[1891 1301 1389 1590 2034 2384 2823 3119 3433 4336] [ -4.21172865e-02  -3.79110841e-02  -3.37048817e-02  -2.94986794e-02
  -2.52924770e-02  -2.10862746e-02  -1.68800723e-02  -1.26738699e-02
  -8.46766751e-03  -4.26146514e-03  -5.52627716e-05]
-0.893532
1.78989
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149453 minutes
Weight histogram
[  65   75  931 1941 2832 3656 4906 6242 4554 1123] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 15121] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.09266
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.66713
Epoch 1, cost is  2.62264
Epoch 2, cost is  2.58902
Epoch 3, cost is  2.56116
Epoch 4, cost is  2.53734
Training took 0.116892 minutes
Weight histogram
[4660 3315 2937 2777 2497 1993 1782 1811 2949 1604] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
[4794 1338 1379 1696 2048 2305 2632 3086 3456 3591] [ -5.63436821e-02  -5.07054310e-02  -4.50671798e-02  -3.94289286e-02
  -3.37906775e-02  -2.81524263e-02  -2.25141752e-02  -1.68759240e-02
  -1.12376728e-02  -5.59942169e-03   3.88294720e-05]
-1.13506
1.30819
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.71606
Epoch 1, cost is  4.22018
Epoch 2, cost is  3.68595
Epoch 3, cost is  3.28002
Epoch 4, cost is  2.97252
Epoch 5, cost is  2.73901
Epoch 6, cost is  2.5545
Epoch 7, cost is  2.40821
Epoch 8, cost is  2.29476
Epoch 9, cost is  2.20472
Training took 1.470235 minutes
Weight histogram
[1920 1649 1345 1190 1598  240   90   40   18   10] [-0.00909592 -0.00820138 -0.00730684 -0.0064123  -0.00551776 -0.00462322
 -0.00372868 -0.00283414 -0.0019396  -0.00104506 -0.00015052]
[1392  579  589  610  652  706  765  833  940 1034] [-0.00909592 -0.00820138 -0.00730684 -0.0064123  -0.00551776 -0.00462322
 -0.00372868 -0.00283414 -0.0019396  -0.00104506 -0.00015052]
-0.193063
0.217202
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.194445 minutes
Epoch 0
Fine tuning took 0.194634 minutes
Epoch 0
Fine tuning took 0.194962 minutes
Epoch 0
Fine tuning took 0.193112 minutes
Epoch 0
Fine tuning took 0.193051 minutes
Epoch 0
Fine tuning took 0.194853 minutes
Epoch 0
Fine tuning took 0.193680 minutes
Epoch 0
Fine tuning took 0.194935 minutes
Epoch 0
Fine tuning took 0.192890 minutes
Epoch 0
Fine tuning took 0.193776 minutes
{'zero': {0: [0.1354679802955665, 0.17241379310344829, 0.18965517241379309, 0.21182266009852216, 0.17241379310344829, 0.15763546798029557, 0.18226600985221675, 0.16995073891625614, 0.19581280788177341, 0.18472906403940886, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.68103448275862066, 0.58620689655172409, 0.54926108374384242, 0.57266009852216748, 0.60098522167487689, 0.6576354679802956, 0.58374384236453203, 0.53940886699507384, 0.61699507389162567, 0.60344827586206895, 0.6428571428571429], 5: [0.18349753694581281, 0.2413793103448276, 0.26108374384236455, 0.21551724137931033, 0.22660098522167488, 0.18472906403940886, 0.23399014778325122, 0.29064039408866993, 0.18719211822660098, 0.21182266009852216, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1354679802955665, 0.1748768472906404, 0.1625615763546798, 0.24384236453201971, 0.17364532019704434, 0.14532019704433496, 0.2019704433497537, 0.17118226600985223, 0.20320197044334976, 0.18103448275862069, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.68103448275862066, 0.61576354679802958, 0.57758620689655171, 0.55665024630541871, 0.55418719211822665, 0.64532019704433496, 0.56157635467980294, 0.52709359605911332, 0.57512315270935965, 0.61330049261083741, 0.63177339901477836], 5: [0.18349753694581281, 0.20935960591133004, 0.25985221674876846, 0.19950738916256158, 0.27216748768472904, 0.20935960591133004, 0.23645320197044334, 0.30172413793103448, 0.22167487684729065, 0.20566502463054187, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1354679802955665, 0.15640394088669951, 0.15640394088669951, 0.21674876847290642, 0.18103448275862069, 0.16379310344827586, 0.18226600985221675, 0.13916256157635468, 0.20073891625615764, 0.17241379310344829, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.68103448275862066, 0.65394088669950734, 0.57512315270935965, 0.56527093596059108, 0.58251231527093594, 0.63423645320197042, 0.58990147783251234, 0.55295566502463056, 0.59605911330049266, 0.58990147783251234, 0.6280788177339901], 5: [0.18349753694581281, 0.18965517241379309, 0.26847290640394089, 0.21798029556650247, 0.23645320197044334, 0.2019704433497537, 0.22783251231527094, 0.30788177339901479, 0.20320197044334976, 0.2376847290640394, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1354679802955665, 0.1748768472906404, 0.17118226600985223, 0.2105911330049261, 0.1625615763546798, 0.13793103448275862, 0.17733990147783252, 0.14408866995073891, 0.20935960591133004, 0.17364532019704434, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.68103448275862066, 0.61083743842364535, 0.5431034482758621, 0.55418719211822665, 0.59605911330049266, 0.65517241379310343, 0.60344827586206895, 0.55665024630541871, 0.61083743842364535, 0.60467980295566504, 0.6354679802955665], 5: [0.18349753694581281, 0.21428571428571427, 0.2857142857142857, 0.23522167487684728, 0.2413793103448276, 0.20689655172413793, 0.21921182266009853, 0.29926108374384236, 0.17980295566502463, 0.22167487684729065, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107819 minutes
Weight histogram
[ 329 1334 2314 1938 1991 4853 5993 4810 4128  660] [-0.002536   -0.00200725 -0.00147851 -0.00094976 -0.00042102  0.00010772
  0.00063647  0.00116521  0.00169396  0.0022227   0.00275144]
[  133   161   214   297   458   731   763  1609  4499 19485] [-0.002536   -0.00200725 -0.00147851 -0.00094976 -0.00042102  0.00010772
  0.00063647  0.00116521  0.00169396  0.0022227   0.00275144]
-1.95951
1.7874
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.65745
Epoch 1, cost is  3.619
Epoch 2, cost is  3.58932
Epoch 3, cost is  3.5665
Epoch 4, cost is  3.53342
Training took 0.073165 minutes
Weight histogram
[7102 5719 4185 4249 2366 2383 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2576 2569 2184 2360 2084 2387 3023 3103 3777 4287] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.92103
2.42049
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149379 minutes
Weight histogram
[  120   460  1804  4857  7548 11913  3524   103    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7737 16323   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.33223
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48234
Epoch 1, cost is  2.44549
Epoch 2, cost is  2.4157
Epoch 3, cost is  2.38745
Epoch 4, cost is  2.36643
Training took 0.115553 minutes
Weight histogram
[5520 4445 3418 2873 3200 2240 1878 2103 4034  664] [ -6.10981248e-02  -5.49844294e-02  -4.88707340e-02  -4.27570385e-02
  -3.66433431e-02  -3.05296477e-02  -2.44159522e-02  -1.83022568e-02
  -1.21885614e-02  -6.07486596e-03   3.88294720e-05]
[4933 1450 1589 1985 2403 2729 3130 3787 3787 4582] [ -6.10981248e-02  -5.49844294e-02  -4.88707340e-02  -4.27570385e-02
  -3.66433431e-02  -3.05296477e-02  -2.44159522e-02  -1.83022568e-02
  -1.21885614e-02  -6.07486596e-03   3.88294720e-05]
-1.23574
1.44212
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.9376
Epoch 1, cost is  5.59441
Epoch 2, cost is  5.48932
Epoch 3, cost is  5.36174
Epoch 4, cost is  5.14093
Epoch 5, cost is  4.92442
Epoch 6, cost is  4.70547
Epoch 7, cost is  4.47983
Epoch 8, cost is  4.28064
Epoch 9, cost is  4.12395
Training took 0.179915 minutes
Weight histogram
[2236 2830 3607 1325  801  564  441  212   87   47] [-0.02834478 -0.02552074 -0.02269669 -0.01987264 -0.01704859 -0.01422454
 -0.0114005  -0.00857645 -0.0057524  -0.00292835 -0.00010431]
[1014 2024 2056 1175 1108 1055  982  991 1054  691] [-0.02834478 -0.02552074 -0.02269669 -0.01987264 -0.01704859 -0.01422454
 -0.0114005  -0.00857645 -0.0057524  -0.00292835 -0.00010431]
-0.359219
0.578597
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044117 minutes
Epoch 0
Fine tuning took 0.042095 minutes
Epoch 0
Fine tuning took 0.041598 minutes
Epoch 0
Fine tuning took 0.043172 minutes
Epoch 0
Fine tuning took 0.042631 minutes
Epoch 0
Fine tuning took 0.044099 minutes
Epoch 0
Fine tuning took 0.040514 minutes
Epoch 0
Fine tuning took 0.043509 minutes
Epoch 0
Fine tuning took 0.041525 minutes
Epoch 0
Fine tuning took 0.043183 minutes
{'zero': {0: [0.2376847290640394, 0.14655172413793102, 0.19950738916256158, 0.16748768472906403, 0.16133004926108374, 0.1748768472906404, 0.11206896551724138, 0.18596059113300492, 0.093596059113300489, 0.12561576354679804, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.64532019704433496, 0.5, 0.64901477832512311, 0.60837438423645318, 0.65024630541871919, 0.70566502463054193, 0.6354679802955665, 0.72660098522167482, 0.67980295566502458, 0.58497536945812811], 5: [0.2105911330049261, 0.20812807881773399, 0.30049261083743845, 0.18349753694581281, 0.23029556650246305, 0.1748768472906404, 0.18226600985221675, 0.17857142857142858, 0.17980295566502463, 0.19458128078817735, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2376847290640394, 0.072660098522167482, 0.19950738916256158, 0.075123152709359611, 0.12807881773399016, 0.16871921182266009, 0.1145320197044335, 0.18226600985221675, 0.060344827586206899, 0.12438423645320197, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.63793103448275867, 0.5714285714285714, 0.74876847290640391, 0.58620689655172409, 0.70320197044334976, 0.7573891625615764, 0.67733990147783252, 0.8571428571428571, 0.77709359605911332, 0.61822660098522164], 5: [0.2105911330049261, 0.2894088669950739, 0.22906403940886699, 0.17610837438423646, 0.2857142857142857, 0.12807881773399016, 0.12807881773399016, 0.14039408866995073, 0.082512315270935957, 0.098522167487684734, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2376847290640394, 0.082512315270935957, 0.18472906403940886, 0.073891625615763554, 0.11206896551724138, 0.17733990147783252, 0.084975369458128072, 0.14778325123152711, 0.083743842364532015, 0.13669950738916256, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.65394088669950734, 0.60837438423645318, 0.78940886699507384, 0.6354679802955665, 0.64778325123152714, 0.78940886699507384, 0.7142857142857143, 0.83620689655172409, 0.77832512315270941, 0.6785714285714286], 5: [0.2105911330049261, 0.26354679802955666, 0.20689655172413793, 0.13669950738916256, 0.25246305418719212, 0.1748768472906404, 0.12561576354679804, 0.13793103448275862, 0.080049261083743842, 0.084975369458128072, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2376847290640394, 0.05295566502463054, 0.20073891625615764, 0.061576354679802957, 0.11699507389162561, 0.15517241379310345, 0.086206896551724144, 0.19334975369458129, 0.056650246305418719, 0.1145320197044335, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.68965517241379315, 0.56773399014778325, 0.7857142857142857, 0.6145320197044335, 0.70320197044334976, 0.78325123152709364, 0.66502463054187189, 0.85098522167487689, 0.77832512315270941, 0.57758620689655171], 5: [0.2105911330049261, 0.25738916256157635, 0.23152709359605911, 0.15270935960591134, 0.26847290640394089, 0.14162561576354679, 0.13054187192118227, 0.14162561576354679, 0.092364532019704432, 0.10714285714285714, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106783 minutes
Weight histogram
[ 222 1654 1768 1333 3458 5187 4938 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  134   161   215   300   467   744   756  1748  5492 16308] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.8814
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.73394
Epoch 1, cost is  3.68258
Epoch 2, cost is  3.66891
Epoch 3, cost is  3.63598
Epoch 4, cost is  3.62324
Training took 0.076155 minutes
Weight histogram
[7102 5719 4185 4034 2555  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2503 2397 2160 2186 2067 2158 2650 3085 3213 3906] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.80697
2.31678
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148758 minutes
Weight histogram
[  120   460  1804  4820  7207 10399  3402    92    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 14543   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.115008 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 2708 3283  630] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4857 1394 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.52934
Epoch 1, cost is  5.32136
Epoch 2, cost is  5.23866
Epoch 3, cost is  5.04192
Epoch 4, cost is  4.76396
Epoch 5, cost is  4.49958
Epoch 6, cost is  4.22547
Epoch 7, cost is  3.97396
Epoch 8, cost is  3.77536
Epoch 9, cost is  3.62181
Training took 0.249009 minutes
Weight histogram
[2387 6867 1280  682  397  230  143   85   49   30] [-0.01881931 -0.01695318 -0.01508705 -0.01322093 -0.0113548  -0.00948867
 -0.00762255 -0.00575642 -0.00389029 -0.00202416 -0.00015804]
[3192 1285  874  928  901  860  889  949 1088 1184] [-0.01881931 -0.01695318 -0.01508705 -0.01322093 -0.0113548  -0.00948867
 -0.00762255 -0.00575642 -0.00389029 -0.00202416 -0.00015804]
-0.276114
0.255027
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044485 minutes
Epoch 0
Fine tuning took 0.044744 minutes
Epoch 0
Fine tuning took 0.044718 minutes
Epoch 0
Fine tuning took 0.046252 minutes
Epoch 0
Fine tuning took 0.046761 minutes
Epoch 0
Fine tuning took 0.047621 minutes
Epoch 0
Fine tuning took 0.046393 minutes
Epoch 0
Fine tuning took 0.044978 minutes
Epoch 0
Fine tuning took 0.045316 minutes
Epoch 0
Fine tuning took 0.044596 minutes
{'zero': {0: [0.16009852216748768, 0.18226600985221675, 0.2229064039408867, 0.19458128078817735, 0.19581280788177341, 0.16009852216748768, 0.20689655172413793, 0.19950738916256158, 0.2229064039408867, 0.24014778325123154, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.55665024630541871, 0.52093596059113301, 0.63916256157635465, 0.61083743842364535, 0.63423645320197042, 0.60467980295566504, 0.6071428571428571, 0.54433497536945807, 0.58743842364532017, 0.55172413793103448], 5: [0.19950738916256158, 0.26108374384236455, 0.25615763546798032, 0.16625615763546797, 0.19334975369458129, 0.20566502463054187, 0.18842364532019704, 0.19334975369458129, 0.23275862068965517, 0.17241379310344829, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16009852216748768, 0.19704433497536947, 0.19581280788177341, 0.16133004926108374, 0.18103448275862069, 0.17118226600985223, 0.21428571428571427, 0.19581280788177341, 0.20812807881773399, 0.23275862068965517, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.56773399014778325, 0.54926108374384242, 0.65270935960591137, 0.62684729064039413, 0.64162561576354682, 0.57635467980295563, 0.61822660098522164, 0.54064039408866993, 0.59852216748768472, 0.58743842364532017], 5: [0.19950738916256158, 0.23522167487684728, 0.25492610837438423, 0.18596059113300492, 0.19211822660098521, 0.18719211822660098, 0.20935960591133004, 0.18596059113300492, 0.25123152709359609, 0.16871921182266009, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16009852216748768, 0.19704433497536947, 0.20320197044334976, 0.17733990147783252, 0.20073891625615764, 0.16748768472906403, 0.19950738916256158, 0.22536945812807882, 0.21182266009852216, 0.25862068965517243, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.5714285714285714, 0.57266009852216748, 0.64655172413793105, 0.62438423645320196, 0.6428571428571429, 0.58866995073891626, 0.57635467980295563, 0.54064039408866993, 0.5923645320197044, 0.59359605911330049], 5: [0.19950738916256158, 0.23152709359605911, 0.22413793103448276, 0.17610837438423646, 0.1748768472906404, 0.18965517241379309, 0.21182266009852216, 0.19827586206896552, 0.24753694581280788, 0.14901477832512317, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16009852216748768, 0.20566502463054187, 0.20320197044334976, 0.17857142857142858, 0.22536945812807882, 0.14655172413793102, 0.21305418719211822, 0.18349753694581281, 0.20320197044334976, 0.21798029556650247, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.5788177339901478, 0.5714285714285714, 0.67487684729064035, 0.61330049261083741, 0.66502463054187189, 0.58620689655172409, 0.59605911330049266, 0.55049261083743839, 0.63054187192118227, 0.57635467980295563], 5: [0.19950738916256158, 0.21551724137931033, 0.22536945812807882, 0.14655172413793102, 0.16133004926108374, 0.18842364532019704, 0.20073891625615764, 0.22044334975369459, 0.24630541871921183, 0.15147783251231528, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108673 minutes
Weight histogram
[ 222 1654 1768 1333 3458 5187 4938 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  134   161   215   300   467   744   756  1748  5492 16308] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.8814
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.73394
Epoch 1, cost is  3.68258
Epoch 2, cost is  3.66891
Epoch 3, cost is  3.63598
Epoch 4, cost is  3.62324
Training took 0.072670 minutes
Weight histogram
[7102 5719 4185 4034 2555  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2503 2397 2160 2186 2067 2158 2650 3085 3213 3906] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.80697
2.31678
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150363 minutes
Weight histogram
[  120   460  1804  4820  7207 10399  3402    92    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 14543   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.115367 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 2708 3283  630] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4857 1394 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.40683
Epoch 1, cost is  5.29093
Epoch 2, cost is  5.06718
Epoch 3, cost is  4.63981
Epoch 4, cost is  4.26043
Epoch 5, cost is  3.919
Epoch 6, cost is  3.65553
Epoch 7, cost is  3.46787
Epoch 8, cost is  3.30623
Epoch 9, cost is  3.15507
Training took 0.339059 minutes
Weight histogram
[2402 1939 1946 1825 3595  259   92   46   28   18] [-0.01131206 -0.0101929  -0.00907373 -0.00795457 -0.00683541 -0.00571624
 -0.00459708 -0.00347792 -0.00235875 -0.00123959 -0.00012043]
[3165  799  760  822  839  873 1000 1183 1328 1381] [-0.01131206 -0.0101929  -0.00907373 -0.00795457 -0.00683541 -0.00571624
 -0.00459708 -0.00347792 -0.00235875 -0.00123959 -0.00012043]
-0.235478
0.201586
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050914 minutes
Epoch 0
Fine tuning took 0.049228 minutes
Epoch 0
Fine tuning took 0.048755 minutes
Epoch 0
Fine tuning took 0.051211 minutes
Epoch 0
Fine tuning took 0.050198 minutes
Epoch 0
Fine tuning took 0.052128 minutes
Epoch 0
Fine tuning took 0.050244 minutes
Epoch 0
Fine tuning took 0.051844 minutes
Epoch 0
Fine tuning took 0.050229 minutes
Epoch 0
Fine tuning took 0.052534 minutes
{'zero': {0: [0.17241379310344829, 0.25, 0.2019704433497537, 0.20566502463054187, 0.19581280788177341, 0.18596059113300492, 0.20073891625615764, 0.19581280788177341, 0.22413793103448276, 0.24630541871921183, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.55172413793103448, 0.5714285714285714, 0.6145320197044335, 0.58004926108374388, 0.57019704433497542, 0.55295566502463056, 0.60591133004926112, 0.53325123152709364, 0.51354679802955661, 0.55665024630541871], 5: [0.23029556650246305, 0.19827586206896552, 0.22660098522167488, 0.17980295566502463, 0.22413793103448276, 0.24384236453201971, 0.24630541871921183, 0.19827586206896552, 0.24261083743842365, 0.24014778325123154, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17241379310344829, 0.24507389162561577, 0.19827586206896552, 0.22413793103448276, 0.19950738916256158, 0.17364532019704434, 0.21182266009852216, 0.2105911330049261, 0.23645320197044334, 0.25738916256157635, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.55295566502463056, 0.54926108374384242, 0.58743842364532017, 0.61083743842364535, 0.58990147783251234, 0.54802955665024633, 0.58866995073891626, 0.52463054187192115, 0.52463054187192115, 0.58743842364532017], 5: [0.23029556650246305, 0.2019704433497537, 0.25246305418719212, 0.18842364532019704, 0.18965517241379309, 0.23645320197044334, 0.24014778325123154, 0.20073891625615764, 0.23891625615763548, 0.21798029556650247, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17241379310344829, 0.20812807881773399, 0.19581280788177341, 0.2019704433497537, 0.19088669950738915, 0.19088669950738915, 0.20320197044334976, 0.21305418719211822, 0.23275862068965517, 0.26108374384236455, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.5714285714285714, 0.58251231527093594, 0.6071428571428571, 0.59482758620689657, 0.56034482758620685, 0.55049261083743839, 0.60837438423645318, 0.51970443349753692, 0.48522167487684731, 0.57512315270935965], 5: [0.23029556650246305, 0.22044334975369459, 0.22167487684729065, 0.19088669950738915, 0.21428571428571427, 0.24876847290640394, 0.24630541871921183, 0.17857142857142858, 0.24753694581280788, 0.2536945812807882, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17241379310344829, 0.21305418719211822, 0.21182266009852216, 0.18472906403940886, 0.21551724137931033, 0.18349753694581281, 0.18596059113300492, 0.20689655172413793, 0.21674876847290642, 0.25123152709359609, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.59852216748768472, 0.55541871921182262, 0.60098522167487689, 0.58497536945812811, 0.56403940886699511, 0.55665024630541871, 0.6071428571428571, 0.5357142857142857, 0.51108374384236455, 0.57266009852216748], 5: [0.23029556650246305, 0.18842364532019704, 0.23275862068965517, 0.21428571428571427, 0.19950738916256158, 0.25246305418719212, 0.25738916256157635, 0.18596059113300492, 0.24753694581280788, 0.2376847290640394, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.109872 minutes
Weight histogram
[ 222 1654 1768 1333 3458 5187 4938 4229 3067  469] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
[  134   161   215   300   467   744   756  1748  5492 16308] [ -1.90030341e-03  -1.43512865e-03  -9.69953882e-04  -5.04779117e-04
  -3.96043528e-05   4.25570412e-04   8.90745176e-04   1.35591994e-03
   1.82109470e-03   2.28626947e-03   2.75144423e-03]
-1.95951
1.8814
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.73394
Epoch 1, cost is  3.68258
Epoch 2, cost is  3.66891
Epoch 3, cost is  3.63598
Epoch 4, cost is  3.62324
Training took 0.073921 minutes
Weight histogram
[7102 5719 4185 4034 2555  384 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2503 2397 2160 2186 2067 2158 2650 3085 3213 3906] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.80697
2.31678
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150018 minutes
Weight histogram
[  120   460  1804  4820  7207 10399  3402    92    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 14543   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.117379 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 2708 3283  630] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4857 1394 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.39103
Epoch 1, cost is  5.23201
Epoch 2, cost is  4.81342
Epoch 3, cost is  4.29169
Epoch 4, cost is  3.85322
Epoch 5, cost is  3.5304
Epoch 6, cost is  3.29962
Epoch 7, cost is  3.10318
Epoch 8, cost is  2.91646
Epoch 9, cost is  2.7572
Training took 0.537200 minutes
Weight histogram
[2372 2205 1511 1439 1380 3127   70   24   12   10] [-0.00591584 -0.00533591 -0.00475598 -0.00417605 -0.00359612 -0.00301619
 -0.00243626 -0.00185633 -0.00127641 -0.00069648 -0.00011655]
[2841  779  777  806  839  970 1148 1283 1322 1385] [-0.00591584 -0.00533591 -0.00475598 -0.00417605 -0.00359612 -0.00301619
 -0.00243626 -0.00185633 -0.00127641 -0.00069648 -0.00011655]
-0.183692
0.174146
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.061396 minutes
Epoch 0
Fine tuning took 0.059995 minutes
Epoch 0
Fine tuning took 0.060762 minutes
Epoch 0
Fine tuning took 0.061355 minutes
Epoch 0
Fine tuning took 0.061946 minutes
Epoch 0
Fine tuning took 0.062297 minutes
Epoch 0
Fine tuning took 0.060074 minutes
Epoch 0
Fine tuning took 0.061959 minutes
Epoch 0
Fine tuning took 0.062290 minutes
Epoch 0
Fine tuning took 0.062380 minutes
{'zero': {0: [0.15763546798029557, 0.21428571428571427, 0.19211822660098521, 0.23891625615763548, 0.27093596059113301, 0.24876847290640394, 0.19088669950738915, 0.17733990147783252, 0.20443349753694581, 0.29064039408866993, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58866995073891626, 0.56157635467980294, 0.5788177339901478, 0.55665024630541871, 0.54926108374384242, 0.55541871921182262, 0.58004926108374388, 0.62438423645320196, 0.56280788177339902, 0.49137931034482757, 0.57389162561576357], 5: [0.2536945812807882, 0.22413793103448276, 0.22906403940886699, 0.20443349753694581, 0.17980295566502463, 0.19581280788177341, 0.22906403940886699, 0.19827586206896552, 0.23275862068965517, 0.21798029556650247, 0.25123152709359609], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.22044334975369459, 0.23522167487684728, 0.18965517241379309, 0.22536945812807882, 0.24753694581280788, 0.21551724137931033, 0.16379310344827586, 0.21798029556650247, 0.31034482758620691, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58866995073891626, 0.55295566502463056, 0.53448275862068961, 0.58620689655172409, 0.55295566502463056, 0.57758620689655171, 0.56157635467980294, 0.6280788177339901, 0.55665024630541871, 0.49014778325123154, 0.59359605911330049], 5: [0.2536945812807882, 0.22660098522167488, 0.23029556650246305, 0.22413793103448276, 0.22167487684729065, 0.1748768472906404, 0.2229064039408867, 0.20812807881773399, 0.22536945812807882, 0.19950738916256158, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.2229064039408867, 0.21305418719211822, 0.21551724137931033, 0.23891625615763548, 0.25, 0.20689655172413793, 0.16871921182266009, 0.20443349753694581, 0.29556650246305421, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58866995073891626, 0.56403940886699511, 0.56527093596059108, 0.58866995073891626, 0.54433497536945807, 0.56527093596059108, 0.55911330049261088, 0.61330049261083741, 0.58620689655172409, 0.49507389162561577, 0.53817733990147787], 5: [0.2536945812807882, 0.21305418719211822, 0.22167487684729065, 0.19581280788177341, 0.21674876847290642, 0.18472906403940886, 0.23399014778325122, 0.21798029556650247, 0.20935960591133004, 0.20935960591133004, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.21182266009852216, 0.20443349753694581, 0.23399014778325122, 0.23275862068965517, 0.24261083743842365, 0.19211822660098521, 0.16502463054187191, 0.23645320197044334, 0.27709359605911332, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58866995073891626, 0.59113300492610843, 0.55418719211822665, 0.55172413793103448, 0.57635467980295563, 0.56896551724137934, 0.56896551724137934, 0.62684729064039413, 0.53940886699507384, 0.5073891625615764, 0.62684729064039413], 5: [0.2536945812807882, 0.19704433497536947, 0.2413793103448276, 0.21428571428571427, 0.19088669950738915, 0.18842364532019704, 0.23891625615763548, 0.20812807881773399, 0.22413793103448276, 0.21551724137931033, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150102 minutes
Weight histogram
[ 247  311  560  871 2178 4687 6786 7308 3163  214] [-0.0001064   0.000141    0.00038839  0.00063579  0.00088319  0.00113059
  0.00137798  0.00162538  0.00187278  0.00212018  0.00236757]
[  136   165   223   329   517   863  1078  2009  6663 14342] [-0.0001064   0.000141    0.00038839  0.00063579  0.00088319  0.00113059
  0.00137798  0.00162538  0.00187278  0.00212018  0.00236757]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.33944
Epoch 1, cost is  2.29923
Epoch 2, cost is  2.27424
Epoch 3, cost is  2.24849
Epoch 4, cost is  2.22946
Training took 0.117472 minutes
Weight histogram
[4657 5229 3513 3166 2644 1910 2017 1433 1435  321] [ -5.92062585e-02  -5.32822677e-02  -4.73582770e-02  -4.14342862e-02
  -3.55102955e-02  -2.95863047e-02  -2.36623139e-02  -1.77383232e-02
  -1.18143324e-02  -5.89034166e-03   3.36491030e-05]
[2637 1400 1458 1924 2313 2621 2901 3365 3474 4232] [ -5.92062585e-02  -5.32822677e-02  -4.73582770e-02  -4.14342862e-02
  -3.55102955e-02  -2.95863047e-02  -2.36623139e-02  -1.77383232e-02
  -1.18143324e-02  -5.89034166e-03   3.36491030e-05]
-1.28662
1.62754
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149855 minutes
Weight histogram
[  65   95  492 1204 2907 4177 5557 7305 5336 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 17144] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.117649 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 2181 3796  644] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4859 1392 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.20279
Epoch 1, cost is  5.93682
Epoch 2, cost is  5.78261
Epoch 3, cost is  5.60253
Epoch 4, cost is  5.30516
Epoch 5, cost is  4.97932
Epoch 6, cost is  4.69536
Epoch 7, cost is  4.44762
Epoch 8, cost is  4.23365
Epoch 9, cost is  4.04761
Training took 0.207197 minutes
Weight histogram
[1421 1485 1408 1599 3427 1296 1051  343   81   39] [-0.02539939 -0.02287585 -0.0203523  -0.01782876 -0.01530522 -0.01278168
 -0.01025814 -0.00773459 -0.00521105 -0.00268751 -0.00016397]
[2371 2214  980  820  857  911  951  983 1033 1030] [-0.02539939 -0.02287585 -0.0203523  -0.01782876 -0.01530522 -0.01278168
 -0.01025814 -0.00773459 -0.00521105 -0.00268751 -0.00016397]
-0.421477
0.470927
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.054804 minutes
Epoch 0
Fine tuning took 0.053992 minutes
Epoch 0
Fine tuning took 0.054045 minutes
Epoch 0
Fine tuning took 0.054642 minutes
Epoch 0
Fine tuning took 0.053652 minutes
Epoch 0
Fine tuning took 0.053978 minutes
Epoch 0
Fine tuning took 0.052459 minutes
Epoch 0
Fine tuning took 0.055111 minutes
Epoch 0
Fine tuning took 0.053220 minutes
Epoch 0
Fine tuning took 0.053460 minutes
{'zero': {0: [0.23029556650246305, 0.14162561576354679, 0.18226600985221675, 0.27709359605911332, 0.080049261083743842, 0.10221674876847291, 0.12192118226600986, 0.13423645320197045, 0.15024630541871922, 0.12807881773399016, 0.080049261083743842], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7068965517241379, 0.69334975369458129, 0.61576354679802958, 0.60591133004926112, 0.82019704433497542, 0.76108374384236455, 0.76108374384236455, 0.70812807881773399, 0.68472906403940892, 0.74630541871921185, 0.73522167487684731], 5: [0.062807881773399021, 0.16502463054187191, 0.2019704433497537, 0.11699507389162561, 0.099753694581280791, 0.13669950738916256, 0.11699507389162561, 0.15763546798029557, 0.16502463054187191, 0.12561576354679804, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23029556650246305, 0.14039408866995073, 0.14532019704433496, 0.26970443349753692, 0.086206896551724144, 0.098522167487684734, 0.10714285714285714, 0.091133004926108374, 0.15640394088669951, 0.14162561576354679, 0.078817733990147784], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7068965517241379, 0.79187192118226601, 0.7142857142857143, 0.65270935960591137, 0.85221674876847286, 0.83743842364532017, 0.83497536945812811, 0.72290640394088668, 0.76847290640394084, 0.75615763546798032, 0.85467980295566504], 5: [0.062807881773399021, 0.067733990147783252, 0.14039408866995073, 0.077586206896551727, 0.061576354679802957, 0.064039408866995079, 0.057881773399014777, 0.18596059113300492, 0.075123152709359611, 0.10221674876847291, 0.066502463054187194], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23029556650246305, 0.16871921182266009, 0.14039408866995073, 0.30295566502463056, 0.073891625615763554, 0.099753694581280791, 0.1268472906403941, 0.11699507389162561, 0.11945812807881774, 0.16502463054187191, 0.080049261083743842], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7068965517241379, 0.75369458128078815, 0.70073891625615758, 0.60221674876847286, 0.8780788177339901, 0.85098522167487689, 0.80788177339901479, 0.70935960591133007, 0.80541871921182262, 0.73645320197044339, 0.83743842364532017], 5: [0.062807881773399021, 0.077586206896551727, 0.15886699507389163, 0.094827586206896547, 0.048029556650246302, 0.049261083743842367, 0.065270935960591137, 0.17364532019704434, 0.075123152709359611, 0.098522167487684734, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23029556650246305, 0.16748768472906403, 0.14532019704433496, 0.29064039408866993, 0.078817733990147784, 0.098522167487684734, 0.14039408866995073, 0.087438423645320201, 0.1268472906403941, 0.11206896551724138, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7068965517241379, 0.76354679802955661, 0.69827586206896552, 0.63177339901477836, 0.87315270935960587, 0.82512315270935965, 0.79926108374384242, 0.73029556650246308, 0.79926108374384242, 0.79064039408866993, 0.83128078817733986], 5: [0.062807881773399021, 0.068965517241379309, 0.15640394088669951, 0.077586206896551727, 0.048029556650246302, 0.076354679802955669, 0.060344827586206899, 0.18226600985221675, 0.073891625615763554, 0.097290640394088676, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149579 minutes
Weight histogram
[ 247  311  560  871 2178 4687 6786 7308 3163  214] [-0.0001064   0.000141    0.00038839  0.00063579  0.00088319  0.00113059
  0.00137798  0.00162538  0.00187278  0.00212018  0.00236757]
[  136   165   223   329   517   863  1078  2009  6663 14342] [-0.0001064   0.000141    0.00038839  0.00063579  0.00088319  0.00113059
  0.00137798  0.00162538  0.00187278  0.00212018  0.00236757]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.33944
Epoch 1, cost is  2.29923
Epoch 2, cost is  2.27424
Epoch 3, cost is  2.24849
Epoch 4, cost is  2.22946
Training took 0.114981 minutes
Weight histogram
[4657 5229 3513 3166 2644 1910 2017 1433 1435  321] [ -5.92062585e-02  -5.32822677e-02  -4.73582770e-02  -4.14342862e-02
  -3.55102955e-02  -2.95863047e-02  -2.36623139e-02  -1.77383232e-02
  -1.18143324e-02  -5.89034166e-03   3.36491030e-05]
[2637 1400 1458 1924 2313 2621 2901 3365 3474 4232] [ -5.92062585e-02  -5.32822677e-02  -4.73582770e-02  -4.14342862e-02
  -3.55102955e-02  -2.95863047e-02  -2.36623139e-02  -1.77383232e-02
  -1.18143324e-02  -5.89034166e-03   3.36491030e-05]
-1.28662
1.62754
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150134 minutes
Weight histogram
[  65   95  492 1204 2907 4177 5557 7305 5336 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 17144] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.117200 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 2181 3796  644] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4859 1392 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.58612
Epoch 1, cost is  5.35525
Epoch 2, cost is  5.22959
Epoch 3, cost is  4.96968
Epoch 4, cost is  4.57661
Epoch 5, cost is  4.2437
Epoch 6, cost is  3.9538
Epoch 7, cost is  3.68779
Epoch 8, cost is  3.47331
Epoch 9, cost is  3.30681
Training took 0.299705 minutes
Weight histogram
[2015 2074 1867 4689  841  356  158   80   43   27] [-0.01764149 -0.01589161 -0.01414173 -0.01239184 -0.01064196 -0.00889208
 -0.0071422  -0.00539231 -0.00364243 -0.00189255 -0.00014267]
[3671  970  742  802  870  909  944  999 1087 1156] [-0.01764149 -0.01589161 -0.01414173 -0.01239184 -0.01064196 -0.00889208
 -0.0071422  -0.00539231 -0.00364243 -0.00189255 -0.00014267]
-0.262348
0.331947
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.060424 minutes
Epoch 0
Fine tuning took 0.059251 minutes
Epoch 0
Fine tuning took 0.060027 minutes
Epoch 0
Fine tuning took 0.058882 minutes
Epoch 0
Fine tuning took 0.059584 minutes
Epoch 0
Fine tuning took 0.060042 minutes
Epoch 0
Fine tuning took 0.059161 minutes
Epoch 0
Fine tuning took 0.059679 minutes
Epoch 0
Fine tuning took 0.059344 minutes
Epoch 0
Fine tuning took 0.059963 minutes
{'zero': {0: [0.15147783251231528, 0.17118226600985223, 0.17118226600985223, 0.13423645320197045, 0.12192118226600986, 0.10098522167487685, 0.17980295566502463, 0.13793103448275862, 0.19334975369458129, 0.1539408866995074, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.61822660098522164, 0.65024630541871919, 0.66502463054187189, 0.65394088669950734, 0.75985221674876846, 0.67610837438423643, 0.64408866995073888, 0.54187192118226601, 0.68349753694581283, 0.63300492610837433], 5: [0.13669950738916256, 0.2105911330049261, 0.17857142857142858, 0.20073891625615764, 0.22413793103448276, 0.13916256157635468, 0.14408866995073891, 0.21798029556650247, 0.26477832512315269, 0.1625615763546798, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15147783251231528, 0.18719211822660098, 0.19334975369458129, 0.17241379310344829, 0.15517241379310345, 0.13669950738916256, 0.18596059113300492, 0.16871921182266009, 0.19334975369458129, 0.16625615763546797, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.63423645320197042, 0.66748768472906406, 0.68349753694581283, 0.66133004926108374, 0.73275862068965514, 0.65640394088669951, 0.63054187192118227, 0.60221674876847286, 0.70566502463054193, 0.66256157635467983], 5: [0.13669950738916256, 0.17857142857142858, 0.13916256157635468, 0.14408866995073891, 0.18349753694581281, 0.13054187192118227, 0.15763546798029557, 0.20073891625615764, 0.20443349753694581, 0.12807881773399016, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15147783251231528, 0.15147783251231528, 0.18349753694581281, 0.17364532019704434, 0.15024630541871922, 0.11822660098522167, 0.17364532019704434, 0.14408866995073891, 0.18349753694581281, 0.13793103448275862, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.66995073891625612, 0.66748768472906406, 0.68719211822660098, 0.69704433497536944, 0.72906403940886699, 0.66133004926108374, 0.65394088669950734, 0.60344827586206895, 0.72660098522167482, 0.69334975369458129], 5: [0.13669950738916256, 0.17857142857142858, 0.14901477832512317, 0.13916256157635468, 0.15270935960591134, 0.15270935960591134, 0.16502463054187191, 0.2019704433497537, 0.21305418719211822, 0.1354679802955665, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15147783251231528, 0.16871921182266009, 0.16502463054187191, 0.16502463054187191, 0.14285714285714285, 0.11945812807881774, 0.21305418719211822, 0.13793103448275862, 0.19211822660098521, 0.13669950738916256, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.65270935960591137, 0.67241379310344829, 0.70566502463054193, 0.68349753694581283, 0.71921182266009853, 0.62438423645320196, 0.67241379310344829, 0.58866995073891626, 0.70443349753694584, 0.6428571428571429], 5: [0.13669950738916256, 0.17857142857142858, 0.1625615763546798, 0.12931034482758622, 0.17364532019704434, 0.16133004926108374, 0.1625615763546798, 0.18965517241379309, 0.21921182266009853, 0.15886699507389163, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149508 minutes
Weight histogram
[ 247  311  560  871 2178 4687 6786 7308 3163  214] [-0.0001064   0.000141    0.00038839  0.00063579  0.00088319  0.00113059
  0.00137798  0.00162538  0.00187278  0.00212018  0.00236757]
[  136   165   223   329   517   863  1078  2009  6663 14342] [-0.0001064   0.000141    0.00038839  0.00063579  0.00088319  0.00113059
  0.00137798  0.00162538  0.00187278  0.00212018  0.00236757]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.33944
Epoch 1, cost is  2.29923
Epoch 2, cost is  2.27424
Epoch 3, cost is  2.24849
Epoch 4, cost is  2.22946
Training took 0.114888 minutes
Weight histogram
[4657 5229 3513 3166 2644 1910 2017 1433 1435  321] [ -5.92062585e-02  -5.32822677e-02  -4.73582770e-02  -4.14342862e-02
  -3.55102955e-02  -2.95863047e-02  -2.36623139e-02  -1.77383232e-02
  -1.18143324e-02  -5.89034166e-03   3.36491030e-05]
[2637 1400 1458 1924 2313 2621 2901 3365 3474 4232] [ -5.92062585e-02  -5.32822677e-02  -4.73582770e-02  -4.14342862e-02
  -3.55102955e-02  -2.95863047e-02  -2.36623139e-02  -1.77383232e-02
  -1.18143324e-02  -5.89034166e-03   3.36491030e-05]
-1.28662
1.62754
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149470 minutes
Weight histogram
[  65   95  492 1204 2907 4177 5557 7305 5336 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 17144] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.115028 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 2181 3796  644] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4859 1392 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.22415
Epoch 1, cost is  5.07792
Epoch 2, cost is  4.91372
Epoch 3, cost is  4.52415
Epoch 4, cost is  4.12187
Epoch 5, cost is  3.77749
Epoch 6, cost is  3.46386
Epoch 7, cost is  3.22162
Epoch 8, cost is  3.0366
Epoch 9, cost is  2.88504
Training took 0.414013 minutes
Weight histogram
[2547 2622 5652  691  306  155   84   48   27   18] [-0.01201195 -0.01082364 -0.00963533 -0.00844701 -0.0072587  -0.00607038
 -0.00488207 -0.00369376 -0.00250544 -0.00131713 -0.00012881]
[3420  819  749  842  861  910  969 1069 1205 1306] [-0.01201195 -0.01082364 -0.00963533 -0.00844701 -0.0072587  -0.00607038
 -0.00488207 -0.00369376 -0.00250544 -0.00131713 -0.00012881]
-0.222079
0.227248
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.065218 minutes
Epoch 0
Fine tuning took 0.065267 minutes
Epoch 0
Fine tuning took 0.065198 minutes
Epoch 0
Fine tuning took 0.063945 minutes
Epoch 0
Fine tuning took 0.064637 minutes
Epoch 0
Fine tuning took 0.065221 minutes
Epoch 0
Fine tuning took 0.063463 minutes
Epoch 0
Fine tuning took 0.063963 minutes
Epoch 0
Fine tuning took 0.063668 minutes
Epoch 0
Fine tuning took 0.064381 minutes
{'zero': {0: [0.15147783251231528, 0.24753694581280788, 0.28201970443349755, 0.19458128078817735, 0.23522167487684728, 0.22536945812807882, 0.19211822660098521, 0.19211822660098521, 0.25123152709359609, 0.21182266009852216, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.56034482758620685, 0.5357142857142857, 0.60960591133004927, 0.6219211822660099, 0.60960591133004927, 0.62684729064039413, 0.65394088669950734, 0.53078817733990147, 0.61699507389162567, 0.62931034482758619], 5: [0.21182266009852216, 0.19211822660098521, 0.18226600985221675, 0.19581280788177341, 0.14285714285714285, 0.16502463054187191, 0.18103448275862069, 0.1539408866995074, 0.21798029556650247, 0.17118226600985223, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15147783251231528, 0.26108374384236455, 0.25862068965517243, 0.18719211822660098, 0.21921182266009853, 0.24876847290640394, 0.19581280788177341, 0.18226600985221675, 0.2413793103448276, 0.18842364532019704, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.54802955665024633, 0.55295566502463056, 0.5923645320197044, 0.59605911330049266, 0.59359605911330049, 0.63177339901477836, 0.65886699507389157, 0.57389162561576357, 0.65517241379310343, 0.61083743842364535], 5: [0.21182266009852216, 0.19088669950738915, 0.18842364532019704, 0.22044334975369459, 0.18472906403940886, 0.15763546798029557, 0.17241379310344829, 0.15886699507389163, 0.18472906403940886, 0.15640394088669951, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15147783251231528, 0.22413793103448276, 0.25246305418719212, 0.22044334975369459, 0.20812807881773399, 0.19704433497536947, 0.21551724137931033, 0.19334975369458129, 0.23029556650246305, 0.20320197044334976, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.5788177339901478, 0.55295566502463056, 0.60098522167487689, 0.64901477832512311, 0.6071428571428571, 0.57389162561576357, 0.61206896551724133, 0.56896551724137934, 0.64408866995073888, 0.60837438423645318], 5: [0.21182266009852216, 0.19704433497536947, 0.19458128078817735, 0.17857142857142858, 0.14285714285714285, 0.19581280788177341, 0.2105911330049261, 0.19458128078817735, 0.20073891625615764, 0.15270935960591134, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15147783251231528, 0.22167487684729065, 0.25738916256157635, 0.19088669950738915, 0.24384236453201971, 0.22413793103448276, 0.23645320197044334, 0.18842364532019704, 0.22536945812807882, 0.19581280788177341, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.62068965517241381, 0.51477832512315269, 0.58866995073891626, 0.60591133004926112, 0.58374384236453203, 0.58620689655172409, 0.65394088669950734, 0.57635467980295563, 0.6280788177339901, 0.61330049261083741], 5: [0.21182266009852216, 0.15763546798029557, 0.22783251231527094, 0.22044334975369459, 0.15024630541871922, 0.19211822660098521, 0.17733990147783252, 0.15763546798029557, 0.19827586206896552, 0.17610837438423646, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149431 minutes
Weight histogram
[ 247  311  560  871 2178 4687 6786 7308 3163  214] [-0.0001064   0.000141    0.00038839  0.00063579  0.00088319  0.00113059
  0.00137798  0.00162538  0.00187278  0.00212018  0.00236757]
[  136   165   223   329   517   863  1078  2009  6663 14342] [-0.0001064   0.000141    0.00038839  0.00063579  0.00088319  0.00113059
  0.00137798  0.00162538  0.00187278  0.00212018  0.00236757]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.33944
Epoch 1, cost is  2.29923
Epoch 2, cost is  2.27424
Epoch 3, cost is  2.24849
Epoch 4, cost is  2.22946
Training took 0.114824 minutes
Weight histogram
[4657 5229 3513 3166 2644 1910 2017 1433 1435  321] [ -5.92062585e-02  -5.32822677e-02  -4.73582770e-02  -4.14342862e-02
  -3.55102955e-02  -2.95863047e-02  -2.36623139e-02  -1.77383232e-02
  -1.18143324e-02  -5.89034166e-03   3.36491030e-05]
[2637 1400 1458 1924 2313 2621 2901 3365 3474 4232] [ -5.92062585e-02  -5.32822677e-02  -4.73582770e-02  -4.14342862e-02
  -3.55102955e-02  -2.95863047e-02  -2.36623139e-02  -1.77383232e-02
  -1.18143324e-02  -5.89034166e-03   3.36491030e-05]
-1.28662
1.62754
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149471 minutes
Weight histogram
[  65   95  492 1204 2907 4177 5557 7305 5336 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  272   317   436   598   946  1331  1003  1863  4440 17144] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.117193 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 2181 3796  644] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4859 1392 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.18105
Epoch 1, cost is  5.00211
Epoch 2, cost is  4.52731
Epoch 3, cost is  3.96222
Epoch 4, cost is  3.4886
Epoch 5, cost is  3.13433
Epoch 6, cost is  2.89098
Epoch 7, cost is  2.69849
Epoch 8, cost is  2.53346
Epoch 9, cost is  2.38447
Training took 0.688602 minutes
Weight histogram
[2540 2087 1588 1577 1460 2794   56   24   12   12] [-0.00658948 -0.00594345 -0.00529742 -0.00465138 -0.00400535 -0.00335932
 -0.00271328 -0.00206725 -0.00142122 -0.00077518 -0.00012915]
[2811  762  764  802  838  928 1080 1272 1425 1468] [-0.00658948 -0.00594345 -0.00529742 -0.00465138 -0.00400535 -0.00335932
 -0.00271328 -0.00206725 -0.00142122 -0.00077518 -0.00012915]
-0.186761
0.197416
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077398 minutes
Epoch 0
Fine tuning took 0.078235 minutes
Epoch 0
Fine tuning took 0.077950 minutes
Epoch 0
Fine tuning took 0.078295 minutes
Epoch 0
Fine tuning took 0.077670 minutes
Epoch 0
Fine tuning took 0.077674 minutes
Epoch 0
Fine tuning took 0.078011 minutes
Epoch 0
Fine tuning took 0.078553 minutes
Epoch 0
Fine tuning took 0.077210 minutes
Epoch 0
Fine tuning took 0.077807 minutes
{'zero': {0: [0.16009852216748768, 0.21674876847290642, 0.19088669950738915, 0.22660098522167488, 0.21921182266009853, 0.20073891625615764, 0.19211822660098521, 0.16009852216748768, 0.2105911330049261, 0.22906403940886699, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.56650246305418717, 0.58374384236453203, 0.55788177339901479, 0.54556650246305416, 0.58128078817733986, 0.57019704433497542, 0.63916256157635465, 0.56527093596059108, 0.52339901477832518, 0.58128078817733986], 5: [0.23645320197044334, 0.21674876847290642, 0.22536945812807882, 0.21551724137931033, 0.23522167487684728, 0.21798029556650247, 0.2376847290640394, 0.20073891625615764, 0.22413793103448276, 0.24753694581280788, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16009852216748768, 0.21428571428571427, 0.17610837438423646, 0.22536945812807882, 0.22906403940886699, 0.21798029556650247, 0.18472906403940886, 0.20935960591133004, 0.20320197044334976, 0.23645320197044334, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.59359605911330049, 0.59852216748768472, 0.54556650246305416, 0.56527093596059108, 0.58990147783251234, 0.56403940886699511, 0.59359605911330049, 0.58374384236453203, 0.52832512315270941, 0.60098522167487689], 5: [0.23645320197044334, 0.19211822660098521, 0.22536945812807882, 0.22906403940886699, 0.20566502463054187, 0.19211822660098521, 0.25123152709359609, 0.19704433497536947, 0.21305418719211822, 0.23522167487684728, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16009852216748768, 0.21921182266009853, 0.22660098522167488, 0.22906403940886699, 0.23029556650246305, 0.19827586206896552, 0.20073891625615764, 0.21428571428571427, 0.20073891625615764, 0.21551724137931033, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.56157635467980294, 0.57019704433497542, 0.57019704433497542, 0.54433497536945807, 0.59482758620689657, 0.55911330049261088, 0.59605911330049266, 0.58743842364532017, 0.53940886699507384, 0.56650246305418717], 5: [0.23645320197044334, 0.21921182266009853, 0.20320197044334976, 0.20073891625615764, 0.22536945812807882, 0.20689655172413793, 0.24014778325123154, 0.18965517241379309, 0.21182266009852216, 0.24507389162561577, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16009852216748768, 0.23399014778325122, 0.16379310344827586, 0.21428571428571427, 0.23891625615763548, 0.19211822660098521, 0.18103448275862069, 0.20689655172413793, 0.21428571428571427, 0.2413793103448276, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.54802955665024633, 0.61945812807881773, 0.55788177339901479, 0.56157635467980294, 0.58497536945812811, 0.59482758620689657, 0.58866995073891626, 0.59729064039408863, 0.5357142857142857, 0.57758620689655171], 5: [0.23645320197044334, 0.21798029556650247, 0.21674876847290642, 0.22783251231527094, 0.19950738916256158, 0.2229064039408867, 0.22413793103448276, 0.20443349753694581, 0.18842364532019704, 0.2229064039408867, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236739 minutes
Weight histogram
[ 184  569  703  472  702 1653 4570 8932 7822  718] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 158  264  408  693  875 1177 2946 3878 6474 9452] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.52175
Epoch 1, cost is  1.49031
Epoch 2, cost is  1.46514
Epoch 3, cost is  1.45058
Epoch 4, cost is  1.43491
Training took 0.224251 minutes
Weight histogram
[5937 4111 3949 2775 2308 1958 1495 1382 1421  989] [ -5.95014878e-02  -5.35443561e-02  -4.75872245e-02  -4.16300929e-02
  -3.56729612e-02  -2.97158296e-02  -2.37586979e-02  -1.78015663e-02
  -1.18444347e-02  -5.88730303e-03   6.98286021e-05]
[2249 1324 1621 1876 2234 2413 3058 3210 3929 4411] [ -5.95014878e-02  -5.35443561e-02  -4.75872245e-02  -4.16300929e-02
  -3.56729612e-02  -2.97158296e-02  -2.37586979e-02  -1.78015663e-02
  -1.18444347e-02  -5.88730303e-03   6.98286021e-05]
-0.933111
1.70404
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149547 minutes
Weight histogram
[  65   94  749 1648 3108 3863 5148 7156 5307 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 17144] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.117347 minutes
Weight histogram
[4509 4395 3094 2827 3034 2018 1853 1923 3394 1303] [ -5.92875443e-02  -5.33518070e-02  -4.74160698e-02  -4.14803325e-02
  -3.55445952e-02  -2.96088579e-02  -2.36731206e-02  -1.77373833e-02
  -1.18016460e-02  -5.86590869e-03   6.98286021e-05]
[4859 1392 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33518070e-02  -4.74160698e-02  -4.14803325e-02
  -3.55445952e-02  -2.96088579e-02  -2.36731206e-02  -1.77373833e-02
  -1.18016460e-02  -5.86590869e-03   6.98286021e-05]
-1.18341
1.34484
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.31169
Epoch 1, cost is  6.02931
Epoch 2, cost is  5.7734
Epoch 3, cost is  5.4086
Epoch 4, cost is  5.03784
Epoch 5, cost is  4.73265
Epoch 6, cost is  4.48196
Epoch 7, cost is  4.28068
Epoch 8, cost is  4.11057
Epoch 9, cost is  3.96807
Training took 0.249854 minutes
Weight histogram
[1309 1312 1280 1214 1246 1386 2263 1438  648   54] [-0.02929136 -0.02637939 -0.02346741 -0.02055544 -0.01764347 -0.01473149
 -0.01181952 -0.00890754 -0.00599557 -0.00308359 -0.00017162]
[2391 1385  851  832  907  963 1068 1166 1272 1315] [-0.02929136 -0.02637939 -0.02346741 -0.02055544 -0.01764347 -0.01473149
 -0.01181952 -0.00890754 -0.00599557 -0.00308359 -0.00017162]
-0.474795
0.641182
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.075872 minutes
Epoch 0
Fine tuning took 0.076462 minutes
Epoch 0
Fine tuning took 0.076545 minutes
Epoch 0
Fine tuning took 0.078070 minutes
Epoch 0
Fine tuning took 0.077112 minutes
Epoch 0
Fine tuning took 0.077834 minutes
Epoch 0
Fine tuning took 0.076890 minutes
Epoch 0
Fine tuning took 0.077222 minutes
Epoch 0
Fine tuning took 0.075222 minutes
Epoch 0
Fine tuning took 0.076400 minutes
{'zero': {0: [0.18226600985221675, 0.10714285714285714, 0.26231527093596058, 0.16871921182266009, 0.18226600985221675, 0.16871921182266009, 0.097290640394088676, 0.21798029556650247, 0.15763546798029557, 0.076354679802955669, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.75492610837438423, 0.58128078817733986, 0.75862068965517238, 0.69088669950738912, 0.73522167487684731, 0.79802955665024633, 0.69458128078817738, 0.73891625615763545, 0.87561576354679804, 0.66009852216748766], 5: [0.054187192118226604, 0.13793103448275862, 0.15640394088669951, 0.072660098522167482, 0.1268472906403941, 0.096059113300492605, 0.10467980295566502, 0.087438423645320201, 0.10344827586206896, 0.048029556650246302, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18226600985221675, 0.13054187192118227, 0.22413793103448276, 0.10714285714285714, 0.20689655172413793, 0.14162561576354679, 0.050492610837438424, 0.20935960591133004, 0.18842364532019704, 0.068965517241379309, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.80911330049261088, 0.67364532019704437, 0.85098522167487689, 0.69827586206896552, 0.77339901477832518, 0.85344827586206895, 0.74876847290640391, 0.76724137931034486, 0.88793103448275867, 0.68596059113300489], 5: [0.054187192118226604, 0.060344827586206899, 0.10221674876847291, 0.041871921182266007, 0.094827586206896547, 0.084975369458128072, 0.096059113300492605, 0.041871921182266007, 0.044334975369458129, 0.043103448275862072, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18226600985221675, 0.12561576354679804, 0.22413793103448276, 0.10714285714285714, 0.21674876847290642, 0.14778325123152711, 0.065270935960591137, 0.21182266009852216, 0.19088669950738915, 0.057881773399014777, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.80172413793103448, 0.66133004926108374, 0.83866995073891626, 0.70935960591133007, 0.76970443349753692, 0.84605911330049266, 0.75123152709359609, 0.75369458128078815, 0.90517241379310343, 0.66379310344827591], 5: [0.054187192118226604, 0.072660098522167482, 0.1145320197044335, 0.054187192118226604, 0.073891625615763554, 0.082512315270935957, 0.088669950738916259, 0.036945812807881777, 0.055418719211822662, 0.036945812807881777, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18226600985221675, 0.14408866995073891, 0.21551724137931033, 0.10960591133004927, 0.19334975369458129, 0.16502463054187191, 0.064039408866995079, 0.22044334975369459, 0.16133004926108374, 0.071428571428571425, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.80665024630541871, 0.67733990147783252, 0.84852216748768472, 0.72660098522167482, 0.7573891625615764, 0.84852216748768472, 0.73152709359605916, 0.78694581280788178, 0.8928571428571429, 0.67487684729064035], 5: [0.054187192118226604, 0.049261083743842367, 0.10714285714285714, 0.041871921182266007, 0.080049261083743842, 0.077586206896551727, 0.087438423645320201, 0.048029556650246302, 0.051724137931034482, 0.035714285714285712, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237010 minutes
Weight histogram
[ 184  569  703  472  702 1653 4570 8932 7822  718] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 158  264  408  693  875 1177 2946 3878 6474 9452] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.52175
Epoch 1, cost is  1.49031
Epoch 2, cost is  1.46514
Epoch 3, cost is  1.45058
Epoch 4, cost is  1.43491
Training took 0.226343 minutes
Weight histogram
[5937 4111 3949 2775 2308 1958 1495 1382 1421  989] [ -5.95014878e-02  -5.35443561e-02  -4.75872245e-02  -4.16300929e-02
  -3.56729612e-02  -2.97158296e-02  -2.37586979e-02  -1.78015663e-02
  -1.18444347e-02  -5.88730303e-03   6.98286021e-05]
[2249 1324 1621 1876 2234 2413 3058 3210 3929 4411] [ -5.95014878e-02  -5.35443561e-02  -4.75872245e-02  -4.16300929e-02
  -3.56729612e-02  -2.97158296e-02  -2.37586979e-02  -1.78015663e-02
  -1.18444347e-02  -5.88730303e-03   6.98286021e-05]
-0.933111
1.70404
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149647 minutes
Weight histogram
[  65   94  749 1648 3108 3863 5148 7156 5307 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 17144] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.118808 minutes
Weight histogram
[4509 4395 3094 2827 3034 2018 1853 1923 3394 1303] [ -5.92875443e-02  -5.33518070e-02  -4.74160698e-02  -4.14803325e-02
  -3.55445952e-02  -2.96088579e-02  -2.36731206e-02  -1.77373833e-02
  -1.18016460e-02  -5.86590869e-03   6.98286021e-05]
[4859 1392 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33518070e-02  -4.74160698e-02  -4.14803325e-02
  -3.55445952e-02  -2.96088579e-02  -2.36731206e-02  -1.77373833e-02
  -1.18016460e-02  -5.86590869e-03   6.98286021e-05]
-1.18341
1.34484
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.69889
Epoch 1, cost is  5.41928
Epoch 2, cost is  5.08603
Epoch 3, cost is  4.60485
Epoch 4, cost is  4.19085
Epoch 5, cost is  3.89119
Epoch 6, cost is  3.65453
Epoch 7, cost is  3.44857
Epoch 8, cost is  3.26175
Epoch 9, cost is  3.10026
Training took 0.353432 minutes
Weight histogram
[2091 1842 1446 1181 1310 3045  939  206   63   27] [-0.01850437 -0.01666854 -0.01483271 -0.01299688 -0.01116105 -0.00932522
 -0.00748939 -0.00565356 -0.00381772 -0.00198189 -0.00014606]
[2953  839  767  810  933 1050 1148 1189 1232 1229] [-0.01850437 -0.01666854 -0.01483271 -0.01299688 -0.01116105 -0.00932522
 -0.00748939 -0.00565356 -0.00381772 -0.00198189 -0.00014606]
-0.272732
0.389321
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080654 minutes
Epoch 0
Fine tuning took 0.080917 minutes
Epoch 0
Fine tuning took 0.082551 minutes
Epoch 0
Fine tuning took 0.081922 minutes
Epoch 0
Fine tuning took 0.080568 minutes
Epoch 0
Fine tuning took 0.082131 minutes
Epoch 0
Fine tuning took 0.081712 minutes
Epoch 0
Fine tuning took 0.080749 minutes
Epoch 0
Fine tuning took 0.082446 minutes
Epoch 0
Fine tuning took 0.081598 minutes
{'zero': {0: [0.17980295566502463, 0.21674876847290642, 0.20073891625615764, 0.18842364532019704, 0.14655172413793102, 0.12931034482758622, 0.11576354679802955, 0.17364532019704434, 0.26231527093596058, 0.10344827586206896, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.61822660098522164, 0.61330049261083741, 0.69827586206896552, 0.69088669950738912, 0.73645320197044339, 0.66009852216748766, 0.61206896551724133, 0.58743842364532017, 0.72906403940886699, 0.78078817733990147], 5: [0.11206896551724138, 0.16502463054187191, 0.18596059113300492, 0.11330049261083744, 0.1625615763546798, 0.13423645320197045, 0.22413793103448276, 0.21428571428571427, 0.15024630541871922, 0.16748768472906403, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17980295566502463, 0.15024630541871922, 0.1625615763546798, 0.21182266009852216, 0.14408866995073891, 0.11699507389162561, 0.13054187192118227, 0.18103448275862069, 0.26231527093596058, 0.13669950738916256, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.75123152709359609, 0.73522167487684731, 0.70812807881773399, 0.76847290640394084, 0.79556650246305416, 0.66379310344827591, 0.67733990147783252, 0.6280788177339901, 0.7068965517241379, 0.78817733990147787], 5: [0.11206896551724138, 0.098522167487684734, 0.10221674876847291, 0.080049261083743842, 0.087438423645320201, 0.087438423645320201, 0.20566502463054187, 0.14162561576354679, 0.10960591133004927, 0.15640394088669951, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17980295566502463, 0.17118226600985223, 0.19950738916256158, 0.24384236453201971, 0.18226600985221675, 0.13669950738916256, 0.13300492610837439, 0.2229064039408867, 0.21798029556650247, 0.13423645320197045, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.72536945812807885, 0.67733990147783252, 0.65394088669950734, 0.69334975369458129, 0.74753694581280783, 0.65517241379310343, 0.62684729064039413, 0.66379310344827591, 0.72783251231527091, 0.76108374384236455], 5: [0.11206896551724138, 0.10344827586206896, 0.12315270935960591, 0.10221674876847291, 0.12438423645320197, 0.11576354679802955, 0.21182266009852216, 0.15024630541871922, 0.11822660098522167, 0.13793103448275862, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17980295566502463, 0.17610837438423646, 0.16625615763546797, 0.19211822660098521, 0.16625615763546797, 0.11822660098522167, 0.14285714285714285, 0.17610837438423646, 0.25985221674876846, 0.11330049261083744, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.73891625615763545, 0.70566502463054193, 0.71182266009852213, 0.71182266009852213, 0.78940886699507384, 0.64162561576354682, 0.68349753694581283, 0.65517241379310343, 0.75369458128078815, 0.79556650246305416], 5: [0.11206896551724138, 0.084975369458128072, 0.12807881773399016, 0.096059113300492605, 0.12192118226600986, 0.092364532019704432, 0.21551724137931033, 0.14039408866995073, 0.084975369458128072, 0.13300492610837439, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236782 minutes
Weight histogram
[ 184  569  703  472  702 1653 4570 8932 7822  718] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 158  264  408  693  875 1177 2946 3878 6474 9452] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.52175
Epoch 1, cost is  1.49031
Epoch 2, cost is  1.46514
Epoch 3, cost is  1.45058
Epoch 4, cost is  1.43491
Training took 0.226215 minutes
Weight histogram
[5937 4111 3949 2775 2308 1958 1495 1382 1421  989] [ -5.95014878e-02  -5.35443561e-02  -4.75872245e-02  -4.16300929e-02
  -3.56729612e-02  -2.97158296e-02  -2.37586979e-02  -1.78015663e-02
  -1.18444347e-02  -5.88730303e-03   6.98286021e-05]
[2249 1324 1621 1876 2234 2413 3058 3210 3929 4411] [ -5.95014878e-02  -5.35443561e-02  -4.75872245e-02  -4.16300929e-02
  -3.56729612e-02  -2.97158296e-02  -2.37586979e-02  -1.78015663e-02
  -1.18444347e-02  -5.88730303e-03   6.98286021e-05]
-0.933111
1.70404
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149963 minutes
Weight histogram
[  65   94  749 1648 3108 3863 5148 7156 5307 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 17144] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.115801 minutes
Weight histogram
[4509 4395 3094 2827 3034 2018 1853 1923 3394 1303] [ -5.92875443e-02  -5.33518070e-02  -4.74160698e-02  -4.14803325e-02
  -3.55445952e-02  -2.96088579e-02  -2.36731206e-02  -1.77373833e-02
  -1.18016460e-02  -5.86590869e-03   6.98286021e-05]
[4859 1392 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33518070e-02  -4.74160698e-02  -4.14803325e-02
  -3.55445952e-02  -2.96088579e-02  -2.36731206e-02  -1.77373833e-02
  -1.18016460e-02  -5.86590869e-03   6.98286021e-05]
-1.18341
1.34484
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.1113
Epoch 1, cost is  4.87255
Epoch 2, cost is  4.52715
Epoch 3, cost is  4.06694
Epoch 4, cost is  3.69526
Epoch 5, cost is  3.41391
Epoch 6, cost is  3.17002
Epoch 7, cost is  2.94896
Epoch 8, cost is  2.7643
Epoch 9, cost is  2.61512
Training took 0.544270 minutes
Weight histogram
[2618 2494 1720 4014  768  290  129   64   34   19] [-0.01303397 -0.01174368 -0.01045338 -0.00916308 -0.00787279 -0.00658249
 -0.00529219 -0.0040019  -0.0027116  -0.0014213  -0.00013101]
[2936  822  786  848  988 1068 1094 1131 1218 1259] [-0.01303397 -0.01174368 -0.01045338 -0.00916308 -0.00787279 -0.00658249
 -0.00529219 -0.0040019  -0.0027116  -0.0014213  -0.00013101]
-0.209239
0.249578
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090257 minutes
Epoch 0
Fine tuning took 0.090114 minutes
Epoch 0
Fine tuning took 0.090652 minutes
Epoch 0
Fine tuning took 0.091323 minutes
Epoch 0
Fine tuning took 0.091867 minutes
Epoch 0
Fine tuning took 0.091492 minutes
Epoch 0
Fine tuning took 0.089783 minutes
Epoch 0
Fine tuning took 0.091582 minutes
Epoch 0
Fine tuning took 0.090440 minutes
Epoch 0
Fine tuning took 0.091846 minutes
{'zero': {0: [0.15886699507389163, 0.19458128078817735, 0.18103448275862069, 0.17857142857142858, 0.2105911330049261, 0.17364532019704434, 0.16625615763546797, 0.1625615763546798, 0.20812807881773399, 0.18226600985221675, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.6071428571428571, 0.5923645320197044, 0.63054187192118227, 0.59852216748768472, 0.65024630541871919, 0.63916256157635465, 0.65024630541871919, 0.54433497536945807, 0.63423645320197042, 0.62438423645320196], 5: [0.13793103448275862, 0.19827586206896552, 0.22660098522167488, 0.19088669950738915, 0.19088669950738915, 0.17610837438423646, 0.19458128078817735, 0.18719211822660098, 0.24753694581280788, 0.18349753694581281, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15886699507389163, 0.21305418719211822, 0.19211822660098521, 0.18226600985221675, 0.20566502463054187, 0.17118226600985223, 0.19950738916256158, 0.17980295566502463, 0.19581280788177341, 0.19704433497536947, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.58004926108374388, 0.58374384236453203, 0.65394088669950734, 0.6428571428571429, 0.62561576354679804, 0.61330049261083741, 0.62315270935960587, 0.5923645320197044, 0.62315270935960587, 0.63793103448275867], 5: [0.13793103448275862, 0.20689655172413793, 0.22413793103448276, 0.16379310344827586, 0.15147783251231528, 0.20320197044334976, 0.18719211822660098, 0.19704433497536947, 0.21182266009852216, 0.17980295566502463, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15886699507389163, 0.19704433497536947, 0.18226600985221675, 0.19334975369458129, 0.20073891625615764, 0.16379310344827586, 0.2105911330049261, 0.1748768472906404, 0.19950738916256158, 0.19827586206896552, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.60960591133004927, 0.62438423645320196, 0.63054187192118227, 0.62931034482758619, 0.6711822660098522, 0.58251231527093594, 0.62684729064039413, 0.58743842364532017, 0.61822660098522164, 0.62931034482758619], 5: [0.13793103448275862, 0.19334975369458129, 0.19334975369458129, 0.17610837438423646, 0.16995073891625614, 0.16502463054187191, 0.20689655172413793, 0.19827586206896552, 0.21305418719211822, 0.18349753694581281, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15886699507389163, 0.21305418719211822, 0.18842364532019704, 0.18965517241379309, 0.23891625615763548, 0.1539408866995074, 0.22536945812807882, 0.15270935960591134, 0.20566502463054187, 0.18226600985221675, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.60221674876847286, 0.59482758620689657, 0.64408866995073888, 0.59113300492610843, 0.64162561576354682, 0.56527093596059108, 0.64039408866995073, 0.58128078817733986, 0.65270935960591137, 0.6280788177339901], 5: [0.13793103448275862, 0.18472906403940886, 0.21674876847290642, 0.16625615763546797, 0.16995073891625614, 0.20443349753694581, 0.20935960591133004, 0.20689655172413793, 0.21305418719211822, 0.16502463054187191, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235346 minutes
Weight histogram
[ 184  569  703  472  702 1653 4570 8932 7822  718] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 158  264  408  693  875 1177 2946 3878 6474 9452] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.29878
1.38407
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.52175
Epoch 1, cost is  1.49031
Epoch 2, cost is  1.46514
Epoch 3, cost is  1.45058
Epoch 4, cost is  1.43491
Training took 0.224632 minutes
Weight histogram
[5937 4111 3949 2775 2308 1958 1495 1382 1421  989] [ -5.95014878e-02  -5.35443561e-02  -4.75872245e-02  -4.16300929e-02
  -3.56729612e-02  -2.97158296e-02  -2.37586979e-02  -1.78015663e-02
  -1.18444347e-02  -5.88730303e-03   6.98286021e-05]
[2249 1324 1621 1876 2234 2413 3058 3210 3929 4411] [ -5.95014878e-02  -5.35443561e-02  -4.75872245e-02  -4.16300929e-02
  -3.56729612e-02  -2.97158296e-02  -2.37586979e-02  -1.78015663e-02
  -1.18444347e-02  -5.88730303e-03   6.98286021e-05]
-0.933111
1.70404
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150080 minutes
Weight histogram
[  65   94  749 1648 3108 3863 5148 7156 5307 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[  462   948  1099   273   412   706  1003  1863  4440 17144] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.119002 minutes
Weight histogram
[4509 4395 3094 2827 3034 2018 1853 1923 3394 1303] [ -5.92875443e-02  -5.33518070e-02  -4.74160698e-02  -4.14803325e-02
  -3.55445952e-02  -2.96088579e-02  -2.36731206e-02  -1.77373833e-02
  -1.18016460e-02  -5.86590869e-03   6.98286021e-05]
[4859 1392 1471 1836 2208 2488 2919 3368 3625 4184] [ -5.92875443e-02  -5.33518070e-02  -4.74160698e-02  -4.14803325e-02
  -3.55445952e-02  -2.96088579e-02  -2.36731206e-02  -1.77373833e-02
  -1.18016460e-02  -5.86590869e-03   6.98286021e-05]
-1.18341
1.34484
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.89458
Epoch 1, cost is  4.59246
Epoch 2, cost is  3.98911
Epoch 3, cost is  3.49267
Epoch 4, cost is  3.13262
Epoch 5, cost is  2.8306
Epoch 6, cost is  2.59434
Epoch 7, cost is  2.41983
Epoch 8, cost is  2.28268
Epoch 9, cost is  2.17357
Training took 0.946534 minutes
Weight histogram
[2650 2165 2137 1477 2813  751   88   39   18   12] [-0.0080684  -0.00727458 -0.00648076 -0.00568695 -0.00489313 -0.00409932
 -0.0033055  -0.00251168 -0.00171787 -0.00092405 -0.00013023]
[2454  754  789  896  970 1006 1097 1237 1421 1526] [-0.0080684  -0.00727458 -0.00648076 -0.00568695 -0.00489313 -0.00409932
 -0.0033055  -0.00251168 -0.00171787 -0.00092405 -0.00013023]
-0.174459
0.182722
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.109981 minutes
Epoch 0
Fine tuning took 0.109269 minutes
Epoch 0
Fine tuning took 0.110003 minutes
Epoch 0
Fine tuning took 0.109307 minutes
Epoch 0
Fine tuning took 0.110537 minutes
Epoch 0
Fine tuning took 0.110567 minutes
Epoch 0
Fine tuning took 0.110935 minutes
Epoch 0
Fine tuning took 0.110851 minutes
Epoch 0
Fine tuning took 0.108327 minutes
Epoch 0
Fine tuning took 0.109553 minutes
{'zero': {0: [0.15517241379310345, 0.19950738916256158, 0.15517241379310345, 0.2105911330049261, 0.21798029556650247, 0.14408866995073891, 0.20566502463054187, 0.18226600985221675, 0.23152709359605911, 0.22660098522167488, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.5431034482758621, 0.63669950738916259, 0.57635467980295563, 0.59482758620689657, 0.60098522167487689, 0.56280788177339902, 0.59482758620689657, 0.58990147783251234, 0.58374384236453203, 0.58004926108374388], 5: [0.19950738916256158, 0.25738916256157635, 0.20812807881773399, 0.21305418719211822, 0.18719211822660098, 0.25492610837438423, 0.23152709359605911, 0.2229064039408867, 0.17857142857142858, 0.18965517241379309, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15517241379310345, 0.19827586206896552, 0.16133004926108374, 0.18842364532019704, 0.2376847290640394, 0.17980295566502463, 0.21921182266009853, 0.1748768472906404, 0.20935960591133004, 0.24753694581280788, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.56773399014778325, 0.58743842364532017, 0.60837438423645318, 0.58743842364532017, 0.5714285714285714, 0.56157635467980294, 0.63177339901477836, 0.59852216748768472, 0.56896551724137934, 0.55541871921182262], 5: [0.19950738916256158, 0.23399014778325122, 0.25123152709359609, 0.20320197044334976, 0.1748768472906404, 0.24876847290640394, 0.21921182266009853, 0.19334975369458129, 0.19211822660098521, 0.18349753694581281, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15517241379310345, 0.1748768472906404, 0.19334975369458129, 0.19827586206896552, 0.19827586206896552, 0.16379310344827586, 0.19211822660098521, 0.21674876847290642, 0.2413793103448276, 0.22660098522167488, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.58128078817733986, 0.59729064039408863, 0.58004926108374388, 0.62561576354679804, 0.59729064039408863, 0.54802955665024633, 0.60098522167487689, 0.59359605911330049, 0.58004926108374388, 0.56896551724137934], 5: [0.19950738916256158, 0.24384236453201971, 0.20935960591133004, 0.22167487684729065, 0.17610837438423646, 0.23891625615763548, 0.25985221674876846, 0.18226600985221675, 0.16502463054187191, 0.19334975369458129, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15517241379310345, 0.20073891625615764, 0.15886699507389163, 0.18719211822660098, 0.19088669950738915, 0.18842364532019704, 0.22660098522167488, 0.18103448275862069, 0.22660098522167488, 0.19827586206896552, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.58128078817733986, 0.61083743842364535, 0.61330049261083741, 0.6219211822660099, 0.56403940886699511, 0.57019704433497542, 0.61945812807881773, 0.60344827586206895, 0.60221674876847286, 0.58004926108374388], 5: [0.19950738916256158, 0.21798029556650247, 0.23029556650246305, 0.19950738916256158, 0.18719211822660098, 0.24753694581280788, 0.20320197044334976, 0.19950738916256158, 0.16995073891625614, 0.19950738916256158, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422256 minutes
Weight histogram
[  231   514   610   458   949  7003 11330  3532  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1194  987  898 1255 1266 2796 3233 3400 4153 7143] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.977761
Epoch 1, cost is  0.958164
Epoch 2, cost is  0.945213
Epoch 3, cost is  0.934325
Epoch 4, cost is  0.925163
Training took 0.647404 minutes
Weight histogram
[6284 5034 4001 2696 2140 1690 1378 1071  982 1049] [ -4.33969498e-02  -3.90627811e-02  -3.47286124e-02  -3.03944437e-02
  -2.60602750e-02  -2.17261063e-02  -1.73919376e-02  -1.30577689e-02
  -8.72360017e-03  -4.38943147e-03  -5.52627716e-05]
[1947 1356 1464 1756 2150 2649 2976 3285 4021 4721] [ -4.33969498e-02  -3.90627811e-02  -3.47286124e-02  -3.03944437e-02
  -2.60602750e-02  -2.17261063e-02  -1.73919376e-02  -1.30577689e-02
  -8.72360017e-03  -4.38943147e-03  -5.52627716e-05]
-0.901249
1.82225
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149961 minutes
Weight histogram
[  65   75  931 1941 2832 3683 5148 7156 5307 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 17146] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.117483 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 1913 3014 1694] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4857 1393 1470 1837 2208 2488 2919 3367 3626 4185] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.44496
Epoch 1, cost is  6.07906
Epoch 2, cost is  5.61064
Epoch 3, cost is  5.21103
Epoch 4, cost is  4.90697
Epoch 5, cost is  4.66348
Epoch 6, cost is  4.46221
Epoch 7, cost is  4.29371
Epoch 8, cost is  4.14951
Epoch 9, cost is  4.02457
Training took 0.325051 minutes
Weight histogram
[1420 1489 1318 1219 1115 1104 1011 1222 1789  463] [-0.03341708 -0.03009224 -0.0267674  -0.02344256 -0.02011772 -0.01679288
 -0.01346805 -0.01014321 -0.00681837 -0.00349353 -0.00016869]
[1970  869  833  939  998 1092 1192 1315 1416 1526] [-0.03341708 -0.03009224 -0.0267674  -0.02344256 -0.02011772 -0.01679288
 -0.01346805 -0.01014321 -0.00681837 -0.00349353 -0.00016869]
-0.411392
0.50831
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.142121 minutes
Epoch 0
Fine tuning took 0.141678 minutes
Epoch 0
Fine tuning took 0.142758 minutes
Epoch 0
Fine tuning took 0.141579 minutes
Epoch 0
Fine tuning took 0.142824 minutes
Epoch 0
Fine tuning took 0.142805 minutes
Epoch 0
Fine tuning took 0.143263 minutes
Epoch 0
Fine tuning took 0.141936 minutes
Epoch 0
Fine tuning took 0.141050 minutes
Epoch 0
Fine tuning took 0.141330 minutes
{'zero': {0: [0.17980295566502463, 0.20689655172413793, 0.33251231527093594, 0.14655172413793102, 0.11945812807881774, 0.092364532019704432, 0.26847290640394089, 0.23399014778325122, 0.15517241379310345, 0.18472906403940886, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.45073891625615764, 0.45812807881773399, 0.79064039408866993, 0.75862068965517238, 0.83990147783251234, 0.63177339901477836, 0.55788177339901479, 0.64778325123152714, 0.73891625615763545, 0.71305418719211822], 5: [0.14285714285714285, 0.34236453201970446, 0.20935960591133004, 0.062807881773399021, 0.12192118226600986, 0.067733990147783252, 0.099753694581280791, 0.20812807881773399, 0.19704433497536947, 0.076354679802955669, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17980295566502463, 0.19088669950738915, 0.32142857142857145, 0.12931034482758622, 0.13423645320197045, 0.081280788177339899, 0.25862068965517243, 0.21182266009852216, 0.16133004926108374, 0.14532019704433496, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.47413793103448276, 0.51477832512315269, 0.82512315270935965, 0.74507389162561577, 0.88423645320197042, 0.68719211822660098, 0.61945812807881773, 0.71674876847290636, 0.80049261083743839, 0.76970443349753692], 5: [0.14285714285714285, 0.33497536945812806, 0.16379310344827586, 0.045566502463054187, 0.1206896551724138, 0.034482758620689655, 0.054187192118226604, 0.16871921182266009, 0.12192118226600986, 0.054187192118226604, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17980295566502463, 0.18965517241379309, 0.33743842364532017, 0.14285714285714285, 0.13177339901477833, 0.10837438423645321, 0.22906403940886699, 0.26847290640394089, 0.18719211822660098, 0.14039408866995073, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.47660098522167488, 0.46182266009852219, 0.80541871921182262, 0.75492610837438423, 0.85467980295566504, 0.71674876847290636, 0.59359605911330049, 0.68103448275862066, 0.81280788177339902, 0.77216748768472909], 5: [0.14285714285714285, 0.33374384236453203, 0.20073891625615764, 0.051724137931034482, 0.11330049261083744, 0.036945812807881777, 0.054187192118226604, 0.13793103448275862, 0.13177339901477833, 0.046798029556650245, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17980295566502463, 0.19211822660098521, 0.33743842364532017, 0.12561576354679804, 0.13423645320197045, 0.099753694581280791, 0.27339901477832512, 0.24630541871921183, 0.16009852216748768, 0.15024630541871922, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.49876847290640391, 0.49507389162561577, 0.80049261083743839, 0.76970443349753692, 0.84975369458128081, 0.66625615763546797, 0.61083743842364535, 0.7068965517241379, 0.79802955665024633, 0.79064039408866993], 5: [0.14285714285714285, 0.30911330049261082, 0.16748768472906403, 0.073891625615763554, 0.096059113300492605, 0.050492610837438424, 0.060344827586206899, 0.14285714285714285, 0.13300492610837439, 0.051724137931034482, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422506 minutes
Weight histogram
[  231   514   610   458   949  7003 11330  3532  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1194  987  898 1255 1266 2796 3233 3400 4153 7143] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.977761
Epoch 1, cost is  0.958164
Epoch 2, cost is  0.945213
Epoch 3, cost is  0.934325
Epoch 4, cost is  0.925163
Training took 0.647186 minutes
Weight histogram
[6284 5034 4001 2696 2140 1690 1378 1071  982 1049] [ -4.33969498e-02  -3.90627811e-02  -3.47286124e-02  -3.03944437e-02
  -2.60602750e-02  -2.17261063e-02  -1.73919376e-02  -1.30577689e-02
  -8.72360017e-03  -4.38943147e-03  -5.52627716e-05]
[1947 1356 1464 1756 2150 2649 2976 3285 4021 4721] [ -4.33969498e-02  -3.90627811e-02  -3.47286124e-02  -3.03944437e-02
  -2.60602750e-02  -2.17261063e-02  -1.73919376e-02  -1.30577689e-02
  -8.72360017e-03  -4.38943147e-03  -5.52627716e-05]
-0.901249
1.82225
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149131 minutes
Weight histogram
[  65   75  931 1941 2832 3683 5148 7156 5307 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 17146] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.116569 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 1913 3014 1694] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4857 1393 1470 1837 2208 2488 2919 3367 3626 4185] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.94975
Epoch 1, cost is  5.48141
Epoch 2, cost is  4.90197
Epoch 3, cost is  4.45384
Epoch 4, cost is  4.09983
Epoch 5, cost is  3.81343
Epoch 6, cost is  3.5852
Epoch 7, cost is  3.39737
Epoch 8, cost is  3.23861
Epoch 9, cost is  3.10732
Training took 0.501348 minutes
Weight histogram
[1758 1612 1478 1339 1184 1131 1029 1955  622   42] [-0.02047332 -0.01844112 -0.01640892 -0.01437671 -0.01234451 -0.01031231
 -0.0082801  -0.0062479  -0.0042157  -0.0021835  -0.00015129]
[2110  839  943  941 1007 1079 1164 1250 1355 1462] [-0.02047332 -0.01844112 -0.01640892 -0.01437671 -0.01234451 -0.01031231
 -0.0082801  -0.0062479  -0.0042157  -0.0021835  -0.00015129]
-0.387108
0.391565
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.151551 minutes
Epoch 0
Fine tuning took 0.150845 minutes
Epoch 0
Fine tuning took 0.150066 minutes
Epoch 0
Fine tuning took 0.150894 minutes
Epoch 0
Fine tuning took 0.149561 minutes
Epoch 0
Fine tuning took 0.148879 minutes
Epoch 0
Fine tuning took 0.149267 minutes
Epoch 0
Fine tuning took 0.149365 minutes
Epoch 0
Fine tuning took 0.150445 minutes
Epoch 0
Fine tuning took 0.151290 minutes
{'zero': {0: [0.18842364532019704, 0.25, 0.31527093596059114, 0.11330049261083744, 0.19211822660098521, 0.081280788177339899, 0.15640394088669951, 0.15517241379310345, 0.13177339901477833, 0.14285714285714285, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66995073891625612, 0.6219211822660099, 0.44704433497536944, 0.78325123152709364, 0.63669950738916259, 0.76724137931034486, 0.72783251231527091, 0.65147783251231528, 0.75862068965517238, 0.70566502463054193, 0.66502463054187189], 5: [0.14162561576354679, 0.12807881773399016, 0.2376847290640394, 0.10344827586206896, 0.17118226600985223, 0.15147783251231528, 0.11576354679802955, 0.19334975369458129, 0.10960591133004927, 0.15147783251231528, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18842364532019704, 0.25738916256157635, 0.27586206896551724, 0.14655172413793102, 0.19334975369458129, 0.11206896551724138, 0.20073891625615764, 0.13423645320197045, 0.14408866995073891, 0.16133004926108374, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66995073891625612, 0.64162561576354682, 0.57389162561576357, 0.7857142857142857, 0.68719211822660098, 0.79926108374384242, 0.67980295566502458, 0.68965517241379315, 0.76108374384236455, 0.70197044334975367, 0.70812807881773399], 5: [0.14162561576354679, 0.10098522167487685, 0.15024630541871922, 0.067733990147783252, 0.11945812807881774, 0.088669950738916259, 0.11945812807881774, 0.17610837438423646, 0.094827586206896547, 0.13669950738916256, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18842364532019704, 0.30049261083743845, 0.26600985221674878, 0.15147783251231528, 0.25246305418719212, 0.10714285714285714, 0.17610837438423646, 0.12315270935960591, 0.098522167487684734, 0.18226600985221675, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66995073891625612, 0.59605911330049266, 0.57635467980295563, 0.78940886699507384, 0.63054187192118227, 0.80541871921182262, 0.73399014778325122, 0.71798029556650245, 0.80911330049261088, 0.71182266009852213, 0.7142857142857143], 5: [0.14162561576354679, 0.10344827586206896, 0.15763546798029557, 0.059113300492610835, 0.11699507389162561, 0.087438423645320201, 0.089901477832512317, 0.15886699507389163, 0.092364532019704432, 0.10591133004926108, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18842364532019704, 0.26231527093596058, 0.28817733990147781, 0.12438423645320197, 0.19827586206896552, 0.11822660098522167, 0.22536945812807882, 0.1268472906403941, 0.11330049261083744, 0.17118226600985223, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66995073891625612, 0.63300492610837433, 0.53694581280788178, 0.80172413793103448, 0.64655172413793105, 0.80295566502463056, 0.63054187192118227, 0.70073891625615758, 0.80665024630541871, 0.69211822660098521, 0.70566502463054193], 5: [0.14162561576354679, 0.10467980295566502, 0.1748768472906404, 0.073891625615763554, 0.15517241379310345, 0.078817733990147784, 0.14408866995073891, 0.17241379310344829, 0.080049261083743842, 0.13669950738916256, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422038 minutes
Weight histogram
[  231   514   610   458   949  7003 11330  3532  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1194  987  898 1255 1266 2796 3233 3400 4153 7143] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.977761
Epoch 1, cost is  0.958164
Epoch 2, cost is  0.945213
Epoch 3, cost is  0.934325
Epoch 4, cost is  0.925163
Training took 0.647757 minutes
Weight histogram
[6284 5034 4001 2696 2140 1690 1378 1071  982 1049] [ -4.33969498e-02  -3.90627811e-02  -3.47286124e-02  -3.03944437e-02
  -2.60602750e-02  -2.17261063e-02  -1.73919376e-02  -1.30577689e-02
  -8.72360017e-03  -4.38943147e-03  -5.52627716e-05]
[1947 1356 1464 1756 2150 2649 2976 3285 4021 4721] [ -4.33969498e-02  -3.90627811e-02  -3.47286124e-02  -3.03944437e-02
  -2.60602750e-02  -2.17261063e-02  -1.73919376e-02  -1.30577689e-02
  -8.72360017e-03  -4.38943147e-03  -5.52627716e-05]
-0.901249
1.82225
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148144 minutes
Weight histogram
[  65   75  931 1941 2832 3683 5148 7156 5307 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 17146] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.118106 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 1913 3014 1694] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4857 1393 1470 1837 2208 2488 2919 3367 3626 4185] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.33303
Epoch 1, cost is  4.81899
Epoch 2, cost is  4.23576
Epoch 3, cost is  3.79644
Epoch 4, cost is  3.46269
Epoch 5, cost is  3.19804
Epoch 6, cost is  2.98918
Epoch 7, cost is  2.81769
Epoch 8, cost is  2.67542
Epoch 9, cost is  2.55472
Training took 0.816164 minutes
Weight histogram
[2227 1995 1675 1423 1275 1064 2109  301   60   21] [-0.01359871 -0.01225254 -0.01090637 -0.00956019 -0.00821402 -0.00686785
 -0.00552167 -0.0041755  -0.00282933 -0.00148316 -0.00013698]
[2132  871  921  949 1009 1078 1163 1244 1344 1439] [-0.01359871 -0.01225254 -0.01090637 -0.00956019 -0.00821402 -0.00686785
 -0.00552167 -0.0041755  -0.00282933 -0.00148316 -0.00013698]
-0.241803
0.324349
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.163442 minutes
Epoch 0
Fine tuning took 0.165114 minutes
Epoch 0
Fine tuning took 0.165672 minutes
Epoch 0
Fine tuning took 0.163904 minutes
Epoch 0
Fine tuning took 0.165442 minutes
Epoch 0
Fine tuning took 0.163690 minutes
Epoch 0
Fine tuning took 0.163399 minutes
Epoch 0
Fine tuning took 0.163777 minutes
Epoch 0
Fine tuning took 0.164105 minutes
Epoch 0
Fine tuning took 0.163843 minutes
{'zero': {0: [0.18842364532019704, 0.20935960591133004, 0.21921182266009853, 0.21798029556650247, 0.18596059113300492, 0.12192118226600986, 0.14655172413793102, 0.16133004926108374, 0.22906403940886699, 0.19704433497536947, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.61206896551724133, 0.58620689655172409, 0.63177339901477836, 0.65640394088669951, 0.72906403940886699, 0.66748768472906406, 0.63054187192118227, 0.60098522167487689, 0.6428571428571429, 0.69458128078817738], 5: [0.10960591133004927, 0.17857142857142858, 0.19458128078817735, 0.15024630541871922, 0.15763546798029557, 0.14901477832512317, 0.18596059113300492, 0.20812807881773399, 0.16995073891625614, 0.16009852216748768, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18842364532019704, 0.19704433497536947, 0.20320197044334976, 0.21674876847290642, 0.18472906403940886, 0.11083743842364532, 0.16009852216748768, 0.16995073891625614, 0.21182266009852216, 0.17241379310344829, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.64162561576354682, 0.60591133004926112, 0.63793103448275867, 0.66871921182266014, 0.74507389162561577, 0.68103448275862066, 0.60098522167487689, 0.59359605911330049, 0.68103448275862066, 0.63300492610837433], 5: [0.10960591133004927, 0.16133004926108374, 0.19088669950738915, 0.14532019704433496, 0.14655172413793102, 0.14408866995073891, 0.15886699507389163, 0.22906403940886699, 0.19458128078817735, 0.14655172413793102, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18842364532019704, 0.19704433497536947, 0.20443349753694581, 0.20812807881773399, 0.2229064039408867, 0.13300492610837439, 0.17610837438423646, 0.17733990147783252, 0.19334975369458129, 0.18349753694581281, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.62315270935960587, 0.60960591133004927, 0.63793103448275867, 0.62438423645320196, 0.73399014778325122, 0.6071428571428571, 0.60837438423645318, 0.65147783251231528, 0.67487684729064035, 0.64778325123152714], 5: [0.10960591133004927, 0.17980295566502463, 0.18596059113300492, 0.1539408866995074, 0.15270935960591134, 0.13300492610837439, 0.21674876847290642, 0.21428571428571427, 0.15517241379310345, 0.14162561576354679, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18842364532019704, 0.21305418719211822, 0.20073891625615764, 0.21551724137931033, 0.18349753694581281, 0.14655172413793102, 0.19581280788177341, 0.18103448275862069, 0.21182266009852216, 0.18226600985221675, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.64532019704433496, 0.62684729064039413, 0.64408866995073888, 0.66009852216748766, 0.69581280788177335, 0.6071428571428571, 0.60960591133004927, 0.59852216748768472, 0.66379310344827591, 0.6428571428571429], 5: [0.10960591133004927, 0.14162561576354679, 0.17241379310344829, 0.14039408866995073, 0.15640394088669951, 0.15763546798029557, 0.19704433497536947, 0.20935960591133004, 0.18965517241379309, 0.1539408866995074, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.425025 minutes
Weight histogram
[  231   514   610   458   949  7003 11330  3532  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1194  987  898 1255 1266 2796 3233 3400 4153 7143] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.977761
Epoch 1, cost is  0.958164
Epoch 2, cost is  0.945213
Epoch 3, cost is  0.934325
Epoch 4, cost is  0.925163
Training took 0.646075 minutes
Weight histogram
[6284 5034 4001 2696 2140 1690 1378 1071  982 1049] [ -4.33969498e-02  -3.90627811e-02  -3.47286124e-02  -3.03944437e-02
  -2.60602750e-02  -2.17261063e-02  -1.73919376e-02  -1.30577689e-02
  -8.72360017e-03  -4.38943147e-03  -5.52627716e-05]
[1947 1356 1464 1756 2150 2649 2976 3285 4021 4721] [ -4.33969498e-02  -3.90627811e-02  -3.47286124e-02  -3.03944437e-02
  -2.60602750e-02  -2.17261063e-02  -1.73919376e-02  -1.30577689e-02
  -8.72360017e-03  -4.38943147e-03  -5.52627716e-05]
-0.901249
1.82225
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151395 minutes
Weight histogram
[  65   75  931 1941 2832 3683 5148 7156 5307 1212] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 17146] [ -4.75582376e-04  -2.65876768e-04  -5.61711611e-05   1.53534446e-04
   3.63240053e-04   5.72945661e-04   7.82651268e-04   9.92356875e-04
   1.20206248e-03   1.41176809e-03   1.62147370e-03]
-1.12025
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.59756
Epoch 1, cost is  2.55142
Epoch 2, cost is  2.52074
Epoch 3, cost is  2.49505
Epoch 4, cost is  2.47219
Training took 0.116667 minutes
Weight histogram
[4506 4386 3104 2826 3021 2031 1855 1913 3014 1694] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
[4857 1393 1470 1837 2208 2488 2919 3367 3626 4185] [ -5.92875443e-02  -5.33549070e-02  -4.74222696e-02  -4.14896322e-02
  -3.55569948e-02  -2.96243574e-02  -2.36917201e-02  -1.77590827e-02
  -1.18264453e-02  -5.89380791e-03   3.88294720e-05]
-1.18341
1.34484
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.72512
Epoch 1, cost is  4.22675
Epoch 2, cost is  3.69686
Epoch 3, cost is  3.29292
Epoch 4, cost is  2.98726
Epoch 5, cost is  2.75311
Epoch 6, cost is  2.56717
Epoch 7, cost is  2.41833
Epoch 8, cost is  2.30113
Epoch 9, cost is  2.20851
Training took 1.470482 minutes
Weight histogram
[2849 2473 2025 1807 2396  362  136   60   27   15] [-0.00909592 -0.00820138 -0.00730684 -0.0064123  -0.00551776 -0.00462322
 -0.00372868 -0.00283414 -0.0019396  -0.00104506 -0.00015052]
[2089  880  888  915  981 1058 1151 1255 1406 1527] [-0.00909592 -0.00820138 -0.00730684 -0.0064123  -0.00551776 -0.00462322
 -0.00372868 -0.00283414 -0.0019396  -0.00104506 -0.00015052]
-0.19743
0.229257
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.195378 minutes
Epoch 0
Fine tuning took 0.193895 minutes
Epoch 0
Fine tuning took 0.193502 minutes
Epoch 0
Fine tuning took 0.194947 minutes
Epoch 0
Fine tuning took 0.195231 minutes
Epoch 0
Fine tuning took 0.193655 minutes
Epoch 0
Fine tuning took 0.192334 minutes
Epoch 0
Fine tuning took 0.194787 minutes
Epoch 0
Fine tuning took 0.192405 minutes
Epoch 0
Fine tuning took 0.194874 minutes
{'zero': {0: [0.15517241379310345, 0.20443349753694581, 0.1748768472906404, 0.22660098522167488, 0.22413793103448276, 0.25862068965517243, 0.18965517241379309, 0.17241379310344829, 0.21798029556650247, 0.19088669950738915, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.56650246305418717, 0.63669950738916259, 0.5431034482758621, 0.61206896551724133, 0.55665024630541871, 0.61576354679802958, 0.60591133004926112, 0.59852216748768472, 0.63300492610837433, 0.61699507389162567], 5: [0.16748768472906403, 0.22906403940886699, 0.18842364532019704, 0.23029556650246305, 0.16379310344827586, 0.18472906403940886, 0.19458128078817735, 0.22167487684729065, 0.18349753694581281, 0.17610837438423646, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15517241379310345, 0.2229064039408867, 0.16133004926108374, 0.21305418719211822, 0.24261083743842365, 0.20443349753694581, 0.18965517241379309, 0.17980295566502463, 0.24630541871921183, 0.17364532019704434, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.60221674876847286, 0.6219211822660099, 0.57758620689655171, 0.59359605911330049, 0.61822660098522164, 0.59113300492610843, 0.60837438423645318, 0.56403940886699511, 0.63916256157635465, 0.61576354679802958], 5: [0.16748768472906403, 0.1748768472906404, 0.21674876847290642, 0.20935960591133004, 0.16379310344827586, 0.17733990147783252, 0.21921182266009853, 0.21182266009852216, 0.18965517241379309, 0.18719211822660098, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15517241379310345, 0.21921182266009853, 0.18596059113300492, 0.23152709359605911, 0.23275862068965517, 0.21921182266009853, 0.19211822660098521, 0.1625615763546798, 0.2105911330049261, 0.19581280788177341, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.58128078817733986, 0.58251231527093594, 0.54187192118226601, 0.59482758620689657, 0.5923645320197044, 0.59359605911330049, 0.61945812807881773, 0.61822660098522164, 0.61083743842364535, 0.62931034482758619], 5: [0.16748768472906403, 0.19950738916256158, 0.23152709359605911, 0.22660098522167488, 0.17241379310344829, 0.18842364532019704, 0.21428571428571427, 0.21798029556650247, 0.17118226600985223, 0.19334975369458129, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15517241379310345, 0.19950738916256158, 0.18719211822660098, 0.19581280788177341, 0.22660098522167488, 0.25492610837438423, 0.2105911330049261, 0.16748768472906403, 0.20935960591133004, 0.19950738916256158, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.60098522167487689, 0.59113300492610843, 0.59113300492610843, 0.59975369458128081, 0.56773399014778325, 0.58620689655172409, 0.60098522167487689, 0.58128078817733986, 0.61699507389162567, 0.63669950738916259], 5: [0.16748768472906403, 0.19950738916256158, 0.22167487684729065, 0.21305418719211822, 0.17364532019704434, 0.17733990147783252, 0.20320197044334976, 0.23152709359605911, 0.20935960591133004, 0.18349753694581281, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106828 minutes
Weight histogram
[ 671 2134 2462 2428 1654 4606 6184 5114 4404  718] [ -2.71980138e-03  -2.17267682e-03  -1.62555226e-03  -1.07842770e-03
  -5.31303138e-04   1.58214243e-05   5.62945986e-04   1.11007055e-03
   1.65719511e-03   2.20431967e-03   2.75144423e-03]
[  136   165   215   308   486   758   797  1894  8536 17080] [ -2.71980138e-03  -2.17267682e-03  -1.62555226e-03  -1.07842770e-03
  -5.31303138e-04   1.58214243e-05   5.62945986e-04   1.11007055e-03
   1.65719511e-03   2.20431967e-03   2.75144423e-03]
-2.24522
2.07852
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.60762
Epoch 1, cost is  3.55563
Epoch 2, cost is  3.52488
Epoch 3, cost is  3.50878
Epoch 4, cost is  3.48939
Training took 0.076448 minutes
Weight histogram
[7102 5719 4185 4249 2366 2839 2871  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2656 2686 2240 2491 2147 2634 3331 3257 4168 4765] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-2.00121
2.42817
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149233 minutes
Weight histogram
[  120   460  1804  4877  7888 13104  3991   110    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7737 18348   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.33223
1.10388
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.49113
Epoch 1, cost is  2.44706
Epoch 2, cost is  2.41592
Epoch 3, cost is  2.39594
Epoch 4, cost is  2.3728
Training took 0.114746 minutes
Weight histogram
[6044 5013 3623 3157 3207 2398 2001 2060 4196  701] [ -6.28061742e-02  -5.65216738e-02  -5.02371734e-02  -4.39526731e-02
  -3.76681727e-02  -3.13836723e-02  -2.50991720e-02  -1.88146716e-02
  -1.25301713e-02  -6.24567089e-03   3.88294720e-05]
[5004 1522 1711 2168 2567 2970 3520 3984 4378 4576] [ -6.28061742e-02  -5.65216738e-02  -5.02371734e-02  -4.39526731e-02
  -3.76681727e-02  -3.13836723e-02  -2.50991720e-02  -1.88146716e-02
  -1.25301713e-02  -6.24567089e-03   3.88294720e-05]
-1.32049
1.45344
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.93802
Epoch 1, cost is  5.60283
Epoch 2, cost is  5.50824
Epoch 3, cost is  5.38701
Epoch 4, cost is  5.1715
Epoch 5, cost is  4.96033
Epoch 6, cost is  4.74982
Epoch 7, cost is  4.52931
Epoch 8, cost is  4.32633
Epoch 9, cost is  4.16194
Training took 0.177789 minutes
Weight histogram
[2998 3794 4829 1754 1046  729  572  294  120   64] [ -2.83447839e-02  -2.55199973e-02  -2.26952106e-02  -1.98704239e-02
  -1.70456372e-02  -1.42208505e-02  -1.13960639e-02  -8.57127720e-03
  -5.74649052e-03  -2.92170384e-03  -9.69171670e-05]
[1255 2531 2850 1584 1486 1425 1322 1318 1398 1031] [ -2.83447839e-02  -2.55199973e-02  -2.26952106e-02  -1.98704239e-02
  -1.70456372e-02  -1.42208505e-02  -1.13960639e-02  -8.57127720e-03
  -5.74649052e-03  -2.92170384e-03  -9.69171670e-05]
-0.359219
0.578597
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040861 minutes
Epoch 0
Fine tuning took 0.040967 minutes
Epoch 0
Fine tuning took 0.040828 minutes
Epoch 0
Fine tuning took 0.042126 minutes
Epoch 0
Fine tuning took 0.041301 minutes
Epoch 0
Fine tuning took 0.044289 minutes
Epoch 0
Fine tuning took 0.040402 minutes
Epoch 0
Fine tuning took 0.041178 minutes
Epoch 0
Fine tuning took 0.041924 minutes
Epoch 0
Fine tuning took 0.041794 minutes
{'zero': {0: [0.084975369458128072, 0.086206896551724144, 0.10837438423645321, 0.11822660098522167, 0.11699507389162561, 0.091133004926108374, 0.13054187192118227, 0.17118226600985223, 0.16871921182266009, 0.15763546798029557, 0.12807881773399016], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.81896551724137934, 0.76108374384236455, 0.68349753694581283, 0.67980295566502458, 0.71305418719211822, 0.73891625615763545, 0.69827586206896552, 0.65640394088669951, 0.70320197044334976, 0.69088669950738912, 0.72167487684729059], 5: [0.096059113300492605, 0.15270935960591134, 0.20812807881773399, 0.2019704433497537, 0.16995073891625614, 0.16995073891625614, 0.17118226600985223, 0.17241379310344829, 0.12807881773399016, 0.15147783251231528, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.084975369458128072, 0.091133004926108374, 0.093596059113300489, 0.087438423645320201, 0.070197044334975367, 0.086206896551724144, 0.1145320197044335, 0.17118226600985223, 0.13300492610837439, 0.12561576354679804, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.81896551724137934, 0.81034482758620685, 0.75615763546798032, 0.72290640394088668, 0.80788177339901479, 0.79679802955665024, 0.78448275862068961, 0.69458128078817738, 0.75862068965517238, 0.73768472906403937, 0.78940886699507384], 5: [0.096059113300492605, 0.098522167487684734, 0.15024630541871922, 0.18965517241379309, 0.12192118226600986, 0.11699507389162561, 0.10098522167487685, 0.13423645320197045, 0.10837438423645321, 0.13669950738916256, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.084975369458128072, 0.098522167487684734, 0.083743842364532015, 0.10591133004926108, 0.070197044334975367, 0.088669950738916259, 0.089901477832512317, 0.13177339901477833, 0.13300492610837439, 0.11822660098522167, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.81896551724137934, 0.75615763546798032, 0.77586206896551724, 0.68226600985221675, 0.77832512315270941, 0.78201970443349755, 0.79679802955665024, 0.74137931034482762, 0.78078817733990147, 0.74014778325123154, 0.79556650246305416], 5: [0.096059113300492605, 0.14532019704433496, 0.14039408866995073, 0.21182266009852216, 0.15147783251231528, 0.12931034482758622, 0.11330049261083744, 0.1268472906403941, 0.086206896551724144, 0.14162561576354679, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.084975369458128072, 0.082512315270935957, 0.077586206896551727, 0.1145320197044335, 0.086206896551724144, 0.092364532019704432, 0.10837438423645321, 0.16379310344827586, 0.16009852216748768, 0.13300492610837439, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.81896551724137934, 0.79679802955665024, 0.78201970443349755, 0.7142857142857143, 0.78817733990147787, 0.78201970443349755, 0.78694581280788178, 0.7142857142857143, 0.75985221674876846, 0.74137931034482762, 0.77709359605911332], 5: [0.096059113300492605, 0.1206896551724138, 0.14039408866995073, 0.17118226600985223, 0.12561576354679804, 0.12561576354679804, 0.10467980295566502, 0.12192118226600986, 0.080049261083743842, 0.12561576354679804, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108195 minutes
Weight histogram
[ 363 1570 2779 1493 2459 5118 5751 4527 3723  567] [-0.00228328 -0.0017798  -0.00127633 -0.00077286 -0.00026939  0.00023408
  0.00073756  0.00124103  0.0017445   0.00224797  0.00275144]
[  134   161   215   300   467   744   756  1748  5492 18333] [-0.00228328 -0.0017798  -0.00127633 -0.00077286 -0.00026939  0.00023408
  0.00073756  0.00124103  0.0017445   0.00224797  0.00275144]
-1.95951
1.8814
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.63369
Epoch 1, cost is  3.59304
Epoch 2, cost is  3.57014
Epoch 3, cost is  3.54337
Epoch 4, cost is  3.52073
Training took 0.074422 minutes
Weight histogram
[7102 5719 4185 4034 2567 2397 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2581 2572 2195 2364 2087 2399 3040 3118 3808 4186] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.92433
2.42407
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148921 minutes
Weight histogram
[  120   460  1804  4820  7307 11421  4270   127    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 16568   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.114863 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 2145 3980  661] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.53236
Epoch 1, cost is  5.33024
Epoch 2, cost is  5.23849
Epoch 3, cost is  5.01149
Epoch 4, cost is  4.73211
Epoch 5, cost is  4.47242
Epoch 6, cost is  4.20846
Epoch 7, cost is  3.97233
Epoch 8, cost is  3.78563
Epoch 9, cost is  3.63692
Training took 0.247002 minutes
Weight histogram
[2961 8454 2537  955  556  316  197  117   66   41] [-0.01881931 -0.01695315 -0.01508699 -0.01322083 -0.01135467 -0.00948852
 -0.00762236 -0.0057562  -0.00389004 -0.00202388 -0.00015772]
[4150 1753 1163 1247 1215 1162 1197 1281 1458 1574] [-0.01881931 -0.01695315 -0.01508699 -0.01322083 -0.01135467 -0.00948852
 -0.00762236 -0.0057562  -0.00389004 -0.00202388 -0.00015772]
-0.278106
0.259488
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046571 minutes
Epoch 0
Fine tuning took 0.046571 minutes
Epoch 0
Fine tuning took 0.046965 minutes
Epoch 0
Fine tuning took 0.048262 minutes
Epoch 0
Fine tuning took 0.044991 minutes
Epoch 0
Fine tuning took 0.045517 minutes
Epoch 0
Fine tuning took 0.044930 minutes
Epoch 0
Fine tuning took 0.046130 minutes
Epoch 0
Fine tuning took 0.046792 minutes
Epoch 0
Fine tuning took 0.047140 minutes
{'zero': {0: [0.13423645320197045, 0.19458128078817735, 0.19211822660098521, 0.19704433497536947, 0.22906403940886699, 0.16871921182266009, 0.21921182266009853, 0.2019704433497537, 0.20443349753694581, 0.19334975369458129, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.59975369458128081, 0.63300492610837433, 0.55541871921182262, 0.56034482758620685, 0.55788177339901479, 0.55049261083743839, 0.61083743842364535, 0.55788177339901479, 0.63300492610837433, 0.61945812807881773], 5: [0.23029556650246305, 0.20566502463054187, 0.1748768472906404, 0.24753694581280788, 0.2105911330049261, 0.27339901477832512, 0.23029556650246305, 0.18719211822660098, 0.2376847290640394, 0.17364532019704434, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13423645320197045, 0.20073891625615764, 0.17364532019704434, 0.16009852216748768, 0.2019704433497537, 0.18472906403940886, 0.2019704433497537, 0.16625615763546797, 0.19581280788177341, 0.18842364532019704, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.57758620689655171, 0.66871921182266014, 0.57389162561576357, 0.59482758620689657, 0.55172413793103448, 0.57758620689655171, 0.6280788177339901, 0.58497536945812811, 0.6354679802955665, 0.62561576354679804], 5: [0.23029556650246305, 0.22167487684729065, 0.15763546798029557, 0.26600985221674878, 0.20320197044334976, 0.26354679802955666, 0.22044334975369459, 0.20566502463054187, 0.21921182266009853, 0.17610837438423646, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13423645320197045, 0.16502463054187191, 0.18103448275862069, 0.16748768472906403, 0.19088669950738915, 0.1748768472906404, 0.20812807881773399, 0.17980295566502463, 0.17118226600985223, 0.1748768472906404, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.61083743842364535, 0.70320197044334976, 0.59482758620689657, 0.61945812807881773, 0.57635467980295563, 0.59852216748768472, 0.63916256157635465, 0.61330049261083741, 0.6785714285714286, 0.56650246305418717], 5: [0.23029556650246305, 0.22413793103448276, 0.11576354679802955, 0.2376847290640394, 0.18965517241379309, 0.24876847290640394, 0.19334975369458129, 0.18103448275862069, 0.21551724137931033, 0.14655172413793102, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13423645320197045, 0.18349753694581281, 0.18349753694581281, 0.18596059113300492, 0.2105911330049261, 0.17364532019704434, 0.20935960591133004, 0.16625615763546797, 0.19334975369458129, 0.18842364532019704, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.63669950738916259, 0.66009852216748766, 0.55911330049261088, 0.59729064039408863, 0.52586206896551724, 0.53694581280788178, 0.56034482758620685, 0.55788177339901479, 0.62068965517241381, 0.58990147783251234], 5: [0.23029556650246305, 0.17980295566502463, 0.15640394088669951, 0.25492610837438423, 0.19211822660098521, 0.30049261083743845, 0.2536945812807882, 0.27339901477832512, 0.24876847290640394, 0.19088669950738915, 0.26970443349753692], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107674 minutes
Weight histogram
[ 363 1570 2779 1493 2459 5118 5751 4527 3723  567] [-0.00228328 -0.0017798  -0.00127633 -0.00077286 -0.00026939  0.00023408
  0.00073756  0.00124103  0.0017445   0.00224797  0.00275144]
[  134   161   215   300   467   744   756  1748  5492 18333] [-0.00228328 -0.0017798  -0.00127633 -0.00077286 -0.00026939  0.00023408
  0.00073756  0.00124103  0.0017445   0.00224797  0.00275144]
-1.95951
1.8814
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.63369
Epoch 1, cost is  3.59304
Epoch 2, cost is  3.57014
Epoch 3, cost is  3.54337
Epoch 4, cost is  3.52073
Training took 0.074157 minutes
Weight histogram
[7102 5719 4185 4034 2567 2397 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2581 2572 2195 2364 2087 2399 3040 3118 3808 4186] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.92433
2.42407
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149213 minutes
Weight histogram
[  120   460  1804  4820  7307 11421  4270   127    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 16568   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.115004 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 2145 3980  661] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.42373
Epoch 1, cost is  5.30495
Epoch 2, cost is  5.05666
Epoch 3, cost is  4.62565
Epoch 4, cost is  4.26888
Epoch 5, cost is  3.9427
Epoch 6, cost is  3.68798
Epoch 7, cost is  3.49686
Epoch 8, cost is  3.33735
Epoch 9, cost is  3.18683
Training took 0.337215 minutes
Weight histogram
[2817 2708 2660 2471 4841  449  129   63   38   24] [-0.01131206 -0.0101929  -0.00907373 -0.00795457 -0.00683541 -0.00571624
 -0.00459708 -0.00347792 -0.00235875 -0.00123959 -0.00012043]
[4199 1067 1014 1111 1138 1176 1341 1577 1775 1802] [-0.01131206 -0.0101929  -0.00907373 -0.00795457 -0.00683541 -0.00571624
 -0.00459708 -0.00347792 -0.00235875 -0.00123959 -0.00012043]
-0.235478
0.20507
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051866 minutes
Epoch 0
Fine tuning took 0.049452 minutes
Epoch 0
Fine tuning took 0.050262 minutes
Epoch 0
Fine tuning took 0.052186 minutes
Epoch 0
Fine tuning took 0.050918 minutes
Epoch 0
Fine tuning took 0.051575 minutes
Epoch 0
Fine tuning took 0.050945 minutes
Epoch 0
Fine tuning took 0.050728 minutes
Epoch 0
Fine tuning took 0.049520 minutes
Epoch 0
Fine tuning took 0.050311 minutes
{'zero': {0: [0.12931034482758622, 0.16871921182266009, 0.25492610837438423, 0.25246305418719212, 0.17241379310344829, 0.19458128078817735, 0.17980295566502463, 0.21798029556650247, 0.23522167487684728, 0.17118226600985223, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.58620689655172409, 0.58004926108374388, 0.56034482758620685, 0.58251231527093594, 0.51600985221674878, 0.58743842364532017, 0.51724137931034486, 0.5073891625615764, 0.59113300492610843, 0.48152709359605911], 5: [0.23891625615763548, 0.24507389162561577, 0.16502463054187191, 0.18719211822660098, 0.24507389162561577, 0.2894088669950739, 0.23275862068965517, 0.26477832512315269, 0.25738916256157635, 0.2376847290640394, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12931034482758622, 0.16871921182266009, 0.20689655172413793, 0.20320197044334976, 0.19334975369458129, 0.18596059113300492, 0.17857142857142858, 0.2229064039408867, 0.23645320197044334, 0.18965517241379309, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.60098522167487689, 0.6280788177339901, 0.57635467980295563, 0.53694581280788178, 0.51231527093596063, 0.57512315270935965, 0.5357142857142857, 0.49507389162561577, 0.61083743842364535, 0.50123152709359609], 5: [0.23891625615763548, 0.23029556650246305, 0.16502463054187191, 0.22044334975369459, 0.26970443349753692, 0.30172413793103448, 0.24630541871921183, 0.2413793103448276, 0.26847290640394089, 0.19950738916256158, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12931034482758622, 0.16625615763546797, 0.23891625615763548, 0.20689655172413793, 0.18472906403940886, 0.2019704433497537, 0.19088669950738915, 0.23275862068965517, 0.23152709359605911, 0.19088669950738915, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.59852216748768472, 0.62315270935960587, 0.58866995073891626, 0.56034482758620685, 0.5431034482758621, 0.54802955665024633, 0.51724137931034486, 0.52832512315270941, 0.57019704433497542, 0.4963054187192118], 5: [0.23891625615763548, 0.23522167487684728, 0.13793103448275862, 0.20443349753694581, 0.25492610837438423, 0.25492610837438423, 0.26108374384236455, 0.25, 0.24014778325123154, 0.23891625615763548, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12931034482758622, 0.20812807881773399, 0.21921182266009853, 0.22413793103448276, 0.16995073891625614, 0.18349753694581281, 0.17857142857142858, 0.24261083743842365, 0.20320197044334976, 0.20566502463054187, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.59113300492610843, 0.58497536945812811, 0.55665024630541871, 0.61083743842364535, 0.53694581280788178, 0.53817733990147787, 0.50615763546798032, 0.53078817733990147, 0.58004926108374388, 0.47413793103448276], 5: [0.23891625615763548, 0.20073891625615764, 0.19581280788177341, 0.21921182266009853, 0.21921182266009853, 0.27955665024630544, 0.28325123152709358, 0.25123152709359609, 0.26600985221674878, 0.21428571428571427, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107823 minutes
Weight histogram
[ 363 1570 2779 1493 2459 5118 5751 4527 3723  567] [-0.00228328 -0.0017798  -0.00127633 -0.00077286 -0.00026939  0.00023408
  0.00073756  0.00124103  0.0017445   0.00224797  0.00275144]
[  134   161   215   300   467   744   756  1748  5492 18333] [-0.00228328 -0.0017798  -0.00127633 -0.00077286 -0.00026939  0.00023408
  0.00073756  0.00124103  0.0017445   0.00224797  0.00275144]
-1.95951
1.8814
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.63369
Epoch 1, cost is  3.59304
Epoch 2, cost is  3.57014
Epoch 3, cost is  3.54337
Epoch 4, cost is  3.52073
Training took 0.074082 minutes
Weight histogram
[7102 5719 4185 4034 2567 2397 1302  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2581 2572 2195 2364 2087 2399 3040 3118 3808 4186] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-1.92433
2.42407
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148750 minutes
Weight histogram
[  120   460  1804  4820  7307 11421  4270   127    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 16568   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.117096 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 2145 3980  661] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.40845
Epoch 1, cost is  5.24037
Epoch 2, cost is  4.80105
Epoch 3, cost is  4.30083
Epoch 4, cost is  3.88348
Epoch 5, cost is  3.56777
Epoch 6, cost is  3.33579
Epoch 7, cost is  3.1409
Epoch 8, cost is  2.95951
Epoch 9, cost is  2.80211
Training took 0.537294 minutes
Weight histogram
[2882 3007 2117 1954 1820 4243  114   33   16   14] [-0.00591584 -0.00533591 -0.00475598 -0.00417605 -0.00359612 -0.00301619
 -0.00243626 -0.00185633 -0.00127641 -0.00069648 -0.00011655]
[3778 1041 1047 1093 1130 1298 1534 1711 1771 1797] [-0.00591584 -0.00533591 -0.00475598 -0.00417605 -0.00359612 -0.00301619
 -0.00243626 -0.00185633 -0.00127641 -0.00069648 -0.00011655]
-0.183692
0.174146
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.059683 minutes
Epoch 0
Fine tuning took 0.061714 minutes
Epoch 0
Fine tuning took 0.060866 minutes
Epoch 0
Fine tuning took 0.061161 minutes
Epoch 0
Fine tuning took 0.061525 minutes
Epoch 0
Fine tuning took 0.061564 minutes
Epoch 0
Fine tuning took 0.062664 minutes
Epoch 0
Fine tuning took 0.060580 minutes
Epoch 0
Fine tuning took 0.060908 minutes
Epoch 0
Fine tuning took 0.061626 minutes
{'zero': {0: [0.16133004926108374, 0.17118226600985223, 0.24630541871921183, 0.24261083743842365, 0.17980295566502463, 0.20566502463054187, 0.20566502463054187, 0.25123152709359609, 0.18965517241379309, 0.26354679802955666, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.61083743842364535, 0.54433497536945807, 0.50246305418719217, 0.53817733990147787, 0.51231527093596063, 0.57758620689655171, 0.52709359605911332, 0.52339901477832518, 0.48768472906403942, 0.50246305418719217], 5: [0.2413793103448276, 0.21798029556650247, 0.20935960591133004, 0.25492610837438423, 0.28201970443349755, 0.28201970443349755, 0.21674876847290642, 0.22167487684729065, 0.28694581280788178, 0.24876847290640394, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.17364532019704434, 0.21305418719211822, 0.23645320197044334, 0.22044334975369459, 0.22413793103448276, 0.19581280788177341, 0.26354679802955666, 0.21551724137931033, 0.24876847290640394, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.61576354679802958, 0.59975369458128081, 0.51108374384236455, 0.52832512315270941, 0.51847290640394084, 0.56527093596059108, 0.50246305418719217, 0.49137931034482757, 0.51477832512315269, 0.5073891625615764], 5: [0.2413793103448276, 0.2105911330049261, 0.18719211822660098, 0.25246305418719212, 0.25123152709359609, 0.25738916256157635, 0.23891625615763548, 0.23399014778325122, 0.29310344827586204, 0.23645320197044334, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.17241379310344829, 0.23645320197044334, 0.24261083743842365, 0.21921182266009853, 0.20566502463054187, 0.19827586206896552, 0.25492610837438423, 0.19827586206896552, 0.29926108374384236, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.62068965517241381, 0.56403940886699511, 0.51477832512315269, 0.51847290640394084, 0.54556650246305416, 0.54556650246305416, 0.52216748768472909, 0.4963054187192118, 0.48399014778325122, 0.4963054187192118], 5: [0.2413793103448276, 0.20689655172413793, 0.19950738916256158, 0.24261083743842365, 0.26231527093596058, 0.24876847290640394, 0.25615763546798032, 0.2229064039408867, 0.30541871921182268, 0.21674876847290642, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.18349753694581281, 0.22044334975369459, 0.2413793103448276, 0.22906403940886699, 0.23029556650246305, 0.21182266009852216, 0.23029556650246305, 0.19827586206896552, 0.27093596059113301, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.60221674876847286, 0.57512315270935965, 0.51477832512315269, 0.51231527093596063, 0.51477832512315269, 0.54926108374384242, 0.53078817733990147, 0.51970443349753692, 0.48152709359605911, 0.47660098522167488], 5: [0.2413793103448276, 0.21428571428571427, 0.20443349753694581, 0.24384236453201971, 0.25862068965517243, 0.25492610837438423, 0.23891625615763548, 0.23891625615763548, 0.28201970443349755, 0.24753694581280788, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150117 minutes
Weight histogram
[ 307  395  805 1803 4851 8009 8101 2037 1161  881] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
[  136   165   223   329   517   863  1078  2009  6663 16367] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.27595
Epoch 1, cost is  2.24635
Epoch 2, cost is  2.22712
Epoch 3, cost is  2.20219
Epoch 4, cost is  2.18569
Training took 0.117441 minutes
Weight histogram
[5422 5458 3726 3347 2859 2115 2041 1583 1454  345] [ -6.11061864e-02  -5.49922028e-02  -4.88782193e-02  -4.27642357e-02
  -3.66502522e-02  -3.05362686e-02  -2.44222851e-02  -1.83083015e-02
  -1.21943180e-02  -6.08033444e-03   3.36491030e-05]
[2709 1451 1565 2090 2498 2823 3114 3771 3652 4677] [ -6.11061864e-02  -5.49922028e-02  -4.88782193e-02  -4.27642357e-02
  -3.66502522e-02  -3.05362686e-02  -2.44222851e-02  -1.83083015e-02
  -1.21943180e-02  -6.08033444e-03   3.36491030e-05]
-1.38387
1.65153
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148846 minutes
Weight histogram
[  69  122  553 1616 3530 4716 6712 8607 4086  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  272   317   436   598   946  1331  1003  1863  4440 19169] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.115824 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 2178 3925  683] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.21129
Epoch 1, cost is  5.94937
Epoch 2, cost is  5.79298
Epoch 3, cost is  5.60631
Epoch 4, cost is  5.29601
Epoch 5, cost is  4.9745
Epoch 6, cost is  4.70157
Epoch 7, cost is  4.46402
Epoch 8, cost is  4.25849
Epoch 9, cost is  4.07525
Training took 0.207398 minutes
Weight histogram
[1900 1993 1871 2122 4576 1726 1392  459  109   52] [-0.02539939 -0.0228756  -0.02035182 -0.01782803 -0.01530425 -0.01278047
 -0.01025668 -0.0077329  -0.00520911 -0.00268533 -0.00016155]
[3141 2959 1308 1096 1147 1226 1279 1319 1385 1340] [-0.02539939 -0.0228756  -0.02035182 -0.01782803 -0.01530425 -0.01278047
 -0.01025668 -0.0077329  -0.00520911 -0.00268533 -0.00016155]
-0.421477
0.470927
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.053659 minutes
Epoch 0
Fine tuning took 0.054859 minutes
Epoch 0
Fine tuning took 0.054880 minutes
Epoch 0
Fine tuning took 0.052130 minutes
Epoch 0
Fine tuning took 0.054201 minutes
Epoch 0
Fine tuning took 0.054512 minutes
Epoch 0
Fine tuning took 0.053547 minutes
Epoch 0
Fine tuning took 0.054425 minutes
Epoch 0
Fine tuning took 0.055088 minutes
Epoch 0
Fine tuning took 0.054345 minutes
{'zero': {0: [0.22044334975369459, 0.064039408866995079, 0.092364532019704432, 0.11206896551724138, 0.054187192118226604, 0.13793103448275862, 0.10467980295566502, 0.11206896551724138, 0.10467980295566502, 0.11822660098522167, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.8719211822660099, 0.73522167487684731, 0.77586206896551724, 0.82389162561576357, 0.7857142857142857, 0.76231527093596063, 0.8214285714285714, 0.80541871921182262, 0.76354679802955661, 0.69704433497536944], 5: [0.25862068965517243, 0.064039408866995079, 0.17241379310344829, 0.11206896551724138, 0.12192118226600986, 0.076354679802955669, 0.13300492610837439, 0.066502463054187194, 0.089901477832512317, 0.11822660098522167, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22044334975369459, 0.11699507389162561, 0.066502463054187194, 0.11206896551724138, 0.044334975369458129, 0.099753694581280791, 0.084975369458128072, 0.11576354679802955, 0.083743842364532015, 0.088669950738916259, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.80665024630541871, 0.75615763546798032, 0.76354679802955661, 0.83866995073891626, 0.77463054187192115, 0.80911330049261088, 0.79433497536945807, 0.86330049261083741, 0.74014778325123154, 0.71674876847290636], 5: [0.25862068965517243, 0.076354679802955669, 0.17733990147783252, 0.12438423645320197, 0.11699507389162561, 0.12561576354679804, 0.10591133004926108, 0.089901477832512317, 0.05295566502463054, 0.17118226600985223, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22044334975369459, 0.10591133004926108, 0.091133004926108374, 0.12438423645320197, 0.035714285714285712, 0.11330049261083744, 0.088669950738916259, 0.12192118226600986, 0.10837438423645321, 0.094827586206896547, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.8288177339901478, 0.73891625615763545, 0.75369458128078815, 0.85221674876847286, 0.80418719211822665, 0.81773399014778325, 0.78694581280788178, 0.85098522167487689, 0.78201970443349755, 0.73399014778325122], 5: [0.25862068965517243, 0.065270935960591137, 0.16995073891625614, 0.12192118226600986, 0.11206896551724138, 0.082512315270935957, 0.093596059113300489, 0.091133004926108374, 0.04064039408866995, 0.12315270935960591, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22044334975369459, 0.10714285714285714, 0.067733990147783252, 0.14285714285714285, 0.038177339901477834, 0.11822660098522167, 0.094827586206896547, 0.10467980295566502, 0.11330049261083744, 0.081280788177339899, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.81034482758620685, 0.73275862068965514, 0.72290640394088668, 0.8645320197044335, 0.77216748768472909, 0.78817733990147787, 0.80788177339901479, 0.83990147783251234, 0.74876847290640391, 0.70935960591133007], 5: [0.25862068965517243, 0.082512315270935957, 0.19950738916256158, 0.13423645320197045, 0.097290640394088676, 0.10960591133004927, 0.11699507389162561, 0.087438423645320201, 0.046798029556650245, 0.16995073891625614, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150443 minutes
Weight histogram
[ 307  395  805 1803 4851 8009 8101 2037 1161  881] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
[  136   165   223   329   517   863  1078  2009  6663 16367] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.27595
Epoch 1, cost is  2.24635
Epoch 2, cost is  2.22712
Epoch 3, cost is  2.20219
Epoch 4, cost is  2.18569
Training took 0.118022 minutes
Weight histogram
[5422 5458 3726 3347 2859 2115 2041 1583 1454  345] [ -6.11061864e-02  -5.49922028e-02  -4.88782193e-02  -4.27642357e-02
  -3.66502522e-02  -3.05362686e-02  -2.44222851e-02  -1.83083015e-02
  -1.21943180e-02  -6.08033444e-03   3.36491030e-05]
[2709 1451 1565 2090 2498 2823 3114 3771 3652 4677] [ -6.11061864e-02  -5.49922028e-02  -4.88782193e-02  -4.27642357e-02
  -3.66502522e-02  -3.05362686e-02  -2.44222851e-02  -1.83083015e-02
  -1.21943180e-02  -6.08033444e-03   3.36491030e-05]
-1.38387
1.65153
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150213 minutes
Weight histogram
[  69  122  553 1616 3530 4716 6712 8607 4086  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  272   317   436   598   946  1331  1003  1863  4440 19169] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.118862 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 2178 3925  683] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.60246
Epoch 1, cost is  5.37383
Epoch 2, cost is  5.24374
Epoch 3, cost is  4.96783
Epoch 4, cost is  4.57409
Epoch 5, cost is  4.2565
Epoch 6, cost is  3.97808
Epoch 7, cost is  3.71928
Epoch 8, cost is  3.50914
Epoch 9, cost is  3.33951
Training took 0.299929 minutes
Weight histogram
[2635 2826 2484 6238 1127  478  212  106   58   36] [-0.01764149 -0.01589161 -0.01414173 -0.01239184 -0.01064196 -0.00889208
 -0.0071422  -0.00539231 -0.00364243 -0.00189255 -0.00014267]
[4885 1294  991 1076 1170 1224 1263 1336 1451 1510] [-0.01764149 -0.01589161 -0.01414173 -0.01239184 -0.01064196 -0.00889208
 -0.0071422  -0.00539231 -0.00364243 -0.00189255 -0.00014267]
-0.262348
0.335099
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.058644 minutes
Epoch 0
Fine tuning took 0.058716 minutes
Epoch 0
Fine tuning took 0.060537 minutes
Epoch 0
Fine tuning took 0.059159 minutes
Epoch 0
Fine tuning took 0.060400 minutes
Epoch 0
Fine tuning took 0.059992 minutes
Epoch 0
Fine tuning took 0.060553 minutes
Epoch 0
Fine tuning took 0.058692 minutes
Epoch 0
Fine tuning took 0.058295 minutes
Epoch 0
Fine tuning took 0.060393 minutes
{'zero': {0: [0.1268472906403941, 0.16748768472906403, 0.16009852216748768, 0.089901477832512317, 0.2413793103448276, 0.11576354679802955, 0.19827586206896552, 0.17733990147783252, 0.14039408866995073, 0.12807881773399016, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.64655172413793105, 0.68965517241379315, 0.68719211822660098, 0.6280788177339901, 0.62931034482758619, 0.63177339901477836, 0.55172413793103448, 0.65640394088669951, 0.68349753694581283, 0.72413793103448276], 5: [0.23645320197044334, 0.18596059113300492, 0.15024630541871922, 0.2229064039408867, 0.13054187192118227, 0.25492610837438423, 0.16995073891625614, 0.27093596059113301, 0.20320197044334976, 0.18842364532019704, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1268472906403941, 0.14162561576354679, 0.13916256157635468, 0.10344827586206896, 0.16502463054187191, 0.12561576354679804, 0.12931034482758622, 0.1625615763546798, 0.15270935960591134, 0.11576354679802955, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.66995073891625612, 0.69334975369458129, 0.69334975369458129, 0.71551724137931039, 0.6711822660098522, 0.69950738916256161, 0.6219211822660099, 0.67364532019704437, 0.69950738916256161, 0.73522167487684731], 5: [0.23645320197044334, 0.18842364532019704, 0.16748768472906403, 0.20320197044334976, 0.11945812807881774, 0.20320197044334976, 0.17118226600985223, 0.21551724137931033, 0.17364532019704434, 0.18472906403940886, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1268472906403941, 0.1354679802955665, 0.13054187192118227, 0.11330049261083744, 0.19581280788177341, 0.12192118226600986, 0.13793103448275862, 0.17118226600985223, 0.16625615763546797, 0.1539408866995074, 0.10591133004926108], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.66871921182266014, 0.71551724137931039, 0.69704433497536944, 0.68596059113300489, 0.66748768472906406, 0.68103448275862066, 0.60591133004926112, 0.6711822660098522, 0.67980295566502458, 0.72906403940886699], 5: [0.23645320197044334, 0.19581280788177341, 0.1539408866995074, 0.18965517241379309, 0.11822660098522167, 0.2105911330049261, 0.18103448275862069, 0.2229064039408867, 0.1625615763546798, 0.16625615763546797, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1268472906403941, 0.15763546798029557, 0.13423645320197045, 0.10591133004926108, 0.1748768472906404, 0.14285714285714285, 0.13793103448275862, 0.19334975369458129, 0.16379310344827586, 0.10960591133004927, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.68226600985221675, 0.69950738916256161, 0.65517241379310343, 0.72290640394088668, 0.64532019704433496, 0.68965517241379315, 0.58374384236453203, 0.67487684729064035, 0.72044334975369462, 0.70443349753694584], 5: [0.23645320197044334, 0.16009852216748768, 0.16625615763546797, 0.23891625615763548, 0.10221674876847291, 0.21182266009852216, 0.17241379310344829, 0.2229064039408867, 0.16133004926108374, 0.16995073891625614, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149856 minutes
Weight histogram
[ 307  395  805 1803 4851 8009 8101 2037 1161  881] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
[  136   165   223   329   517   863  1078  2009  6663 16367] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.27595
Epoch 1, cost is  2.24635
Epoch 2, cost is  2.22712
Epoch 3, cost is  2.20219
Epoch 4, cost is  2.18569
Training took 0.115776 minutes
Weight histogram
[5422 5458 3726 3347 2859 2115 2041 1583 1454  345] [ -6.11061864e-02  -5.49922028e-02  -4.88782193e-02  -4.27642357e-02
  -3.66502522e-02  -3.05362686e-02  -2.44222851e-02  -1.83083015e-02
  -1.21943180e-02  -6.08033444e-03   3.36491030e-05]
[2709 1451 1565 2090 2498 2823 3114 3771 3652 4677] [ -6.11061864e-02  -5.49922028e-02  -4.88782193e-02  -4.27642357e-02
  -3.66502522e-02  -3.05362686e-02  -2.44222851e-02  -1.83083015e-02
  -1.21943180e-02  -6.08033444e-03   3.36491030e-05]
-1.38387
1.65153
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148802 minutes
Weight histogram
[  69  122  553 1616 3530 4716 6712 8607 4086  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  272   317   436   598   946  1331  1003  1863  4440 19169] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.115483 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 2178 3925  683] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.24544
Epoch 1, cost is  5.09849
Epoch 2, cost is  4.91964
Epoch 3, cost is  4.5163
Epoch 4, cost is  4.13109
Epoch 5, cost is  3.80098
Epoch 6, cost is  3.49793
Epoch 7, cost is  3.25659
Epoch 8, cost is  3.06848
Epoch 9, cost is  2.91052
Training took 0.419268 minutes
Weight histogram
[3293 3574 7544  929  415  208  113   64   36   24] [-0.01201195 -0.01082364 -0.00963533 -0.00844701 -0.0072587  -0.00607038
 -0.00488207 -0.00369376 -0.00250544 -0.00131713 -0.00012881]
[4552 1092 1005 1131 1158 1221 1296 1429 1607 1709] [-0.01201195 -0.01082364 -0.00963533 -0.00844701 -0.0072587  -0.00607038
 -0.00488207 -0.00369376 -0.00250544 -0.00131713 -0.00012881]
-0.222437
0.227248
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.064641 minutes
Epoch 0
Fine tuning took 0.064913 minutes
Epoch 0
Fine tuning took 0.064178 minutes
Epoch 0
Fine tuning took 0.063925 minutes
Epoch 0
Fine tuning took 0.062470 minutes
Epoch 0
Fine tuning took 0.064406 minutes
Epoch 0
Fine tuning took 0.065310 minutes
Epoch 0
Fine tuning took 0.063479 minutes
Epoch 0
Fine tuning took 0.063873 minutes
Epoch 0
Fine tuning took 0.064789 minutes
{'zero': {0: [0.12315270935960591, 0.1625615763546798, 0.20073891625615764, 0.17610837438423646, 0.13054187192118227, 0.16995073891625614, 0.17364532019704434, 0.15517241379310345, 0.13177339901477833, 0.22413793103448276, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.64778325123152714, 0.61822660098522164, 0.55788177339901479, 0.68349753694581283, 0.57512315270935965, 0.60591133004926112, 0.58990147783251234, 0.65886699507389157, 0.60221674876847286, 0.64532019704433496], 5: [0.2413793103448276, 0.18965517241379309, 0.18103448275862069, 0.26600985221674878, 0.18596059113300492, 0.25492610837438423, 0.22044334975369459, 0.25492610837438423, 0.20935960591133004, 0.17364532019704434, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12315270935960591, 0.18103448275862069, 0.19088669950738915, 0.19458128078817735, 0.13793103448275862, 0.16502463054187191, 0.18103448275862069, 0.1748768472906404, 0.1354679802955665, 0.19334975369458129, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.61330049261083741, 0.62684729064039413, 0.56527093596059108, 0.67364532019704437, 0.57266009852216748, 0.6071428571428571, 0.59852216748768472, 0.64901477832512311, 0.64039408866995073, 0.64039408866995073], 5: [0.2413793103448276, 0.20566502463054187, 0.18226600985221675, 0.24014778325123154, 0.18842364532019704, 0.26231527093596058, 0.21182266009852216, 0.22660098522167488, 0.21551724137931033, 0.16625615763546797, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12315270935960591, 0.15517241379310345, 0.19458128078817735, 0.18842364532019704, 0.14655172413793102, 0.17610837438423646, 0.17980295566502463, 0.19704433497536947, 0.12315270935960591, 0.2019704433497537, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.65394088669950734, 0.63177339901477836, 0.57389162561576357, 0.69581280788177335, 0.55911330049261088, 0.60098522167487689, 0.57019704433497542, 0.65640394088669951, 0.61330049261083741, 0.64532019704433496], 5: [0.2413793103448276, 0.19088669950738915, 0.17364532019704434, 0.2376847290640394, 0.15763546798029557, 0.26477832512315269, 0.21921182266009853, 0.23275862068965517, 0.22044334975369459, 0.18472906403940886, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12315270935960591, 0.1625615763546798, 0.2019704433497537, 0.17857142857142858, 0.13793103448275862, 0.17118226600985223, 0.19211822660098521, 0.18103448275862069, 0.14778325123152711, 0.18719211822660098, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.64162561576354682, 0.62684729064039413, 0.57635467980295563, 0.68103448275862066, 0.56403940886699511, 0.58866995073891626, 0.58251231527093594, 0.65024630541871919, 0.63177339901477836, 0.62438423645320196], 5: [0.2413793103448276, 0.19581280788177341, 0.17118226600985223, 0.24507389162561577, 0.18103448275862069, 0.26477832512315269, 0.21921182266009853, 0.23645320197044334, 0.2019704433497537, 0.18103448275862069, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149765 minutes
Weight histogram
[ 307  395  805 1803 4851 8009 8101 2037 1161  881] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
[  136   165   223   329   517   863  1078  2009  6663 16367] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
-1.36373
1.40722
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.27595
Epoch 1, cost is  2.24635
Epoch 2, cost is  2.22712
Epoch 3, cost is  2.20219
Epoch 4, cost is  2.18569
Training took 0.114927 minutes
Weight histogram
[5422 5458 3726 3347 2859 2115 2041 1583 1454  345] [ -6.11061864e-02  -5.49922028e-02  -4.88782193e-02  -4.27642357e-02
  -3.66502522e-02  -3.05362686e-02  -2.44222851e-02  -1.83083015e-02
  -1.21943180e-02  -6.08033444e-03   3.36491030e-05]
[2709 1451 1565 2090 2498 2823 3114 3771 3652 4677] [ -6.11061864e-02  -5.49922028e-02  -4.88782193e-02  -4.27642357e-02
  -3.66502522e-02  -3.05362686e-02  -2.44222851e-02  -1.83083015e-02
  -1.21943180e-02  -6.08033444e-03   3.36491030e-05]
-1.38387
1.65153
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148708 minutes
Weight histogram
[  69  122  553 1616 3530 4716 6712 8607 4086  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  272   317   436   598   946  1331  1003  1863  4440 19169] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.114711 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 2178 3925  683] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.20194
Epoch 1, cost is  5.01476
Epoch 2, cost is  4.52452
Epoch 3, cost is  3.97895
Epoch 4, cost is  3.51857
Epoch 5, cost is  3.16576
Epoch 6, cost is  2.91731
Epoch 7, cost is  2.71667
Epoch 8, cost is  2.54684
Epoch 9, cost is  2.39728
Training took 0.682592 minutes
Weight histogram
[3269 2799 2165 2153 1932 3743   74   33   16   16] [-0.00658948 -0.00594345 -0.00529742 -0.00465138 -0.00400535 -0.00335932
 -0.00271328 -0.00206725 -0.00142122 -0.00077518 -0.00012915]
[3744 1021 1027 1077 1122 1240 1439 1688 1888 1954] [-0.00658948 -0.00594345 -0.00529742 -0.00465138 -0.00400535 -0.00335932
 -0.00271328 -0.00206725 -0.00142122 -0.00077518 -0.00012915]
-0.190585
0.200444
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076182 minutes
Epoch 0
Fine tuning took 0.077214 minutes
Epoch 0
Fine tuning took 0.078100 minutes
Epoch 0
Fine tuning took 0.077686 minutes
Epoch 0
Fine tuning took 0.077166 minutes
Epoch 0
Fine tuning took 0.077739 minutes
Epoch 0
Fine tuning took 0.078635 minutes
Epoch 0
Fine tuning took 0.078042 minutes
Epoch 0
Fine tuning took 0.077481 minutes
Epoch 0
Fine tuning took 0.077517 minutes
{'zero': {0: [0.1625615763546798, 0.23891625615763548, 0.23399014778325122, 0.23275862068965517, 0.22783251231527094, 0.18719211822660098, 0.1748768472906404, 0.25123152709359609, 0.25123152709359609, 0.18226600985221675, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58128078817733986, 0.5357142857142857, 0.5923645320197044, 0.54926108374384242, 0.53694581280788178, 0.52463054187192115, 0.55911330049261088, 0.53694581280788178, 0.51354679802955661, 0.61576354679802958, 0.52093596059113301], 5: [0.25615763546798032, 0.22536945812807882, 0.17364532019704434, 0.21798029556650247, 0.23522167487684728, 0.28817733990147781, 0.26600985221674878, 0.21182266009852216, 0.23522167487684728, 0.2019704433497537, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1625615763546798, 0.20320197044334976, 0.22783251231527094, 0.2413793103448276, 0.23152709359605911, 0.20073891625615764, 0.22167487684729065, 0.21921182266009853, 0.21798029556650247, 0.19950738916256158, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58128078817733986, 0.5714285714285714, 0.6354679802955665, 0.53940886699507384, 0.54802955665024633, 0.48029556650246308, 0.55665024630541871, 0.58004926108374388, 0.54433497536945807, 0.57266009852216748, 0.5431034482758621], 5: [0.25615763546798032, 0.22536945812807882, 0.13669950738916256, 0.21921182266009853, 0.22044334975369459, 0.31896551724137934, 0.22167487684729065, 0.20073891625615764, 0.2376847290640394, 0.22783251231527094, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1625615763546798, 0.20812807881773399, 0.23029556650246305, 0.18842364532019704, 0.23152709359605911, 0.19211822660098521, 0.19704433497536947, 0.23029556650246305, 0.20689655172413793, 0.19827586206896552, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58128078817733986, 0.55049261083743839, 0.63054187192118227, 0.57266009852216748, 0.53325123152709364, 0.50369458128078815, 0.54064039408866993, 0.50862068965517238, 0.54802955665024633, 0.60098522167487689, 0.55049261083743839], 5: [0.25615763546798032, 0.2413793103448276, 0.13916256157635468, 0.23891625615763548, 0.23522167487684728, 0.30418719211822659, 0.26231527093596058, 0.26108374384236455, 0.24507389162561577, 0.20073891625615764, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1625615763546798, 0.19704433497536947, 0.25246305418719212, 0.23399014778325122, 0.22660098522167488, 0.18472906403940886, 0.21182266009852216, 0.23645320197044334, 0.26231527093596058, 0.18596059113300492, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58128078817733986, 0.57758620689655171, 0.57512315270935965, 0.5357142857142857, 0.53325123152709364, 0.52463054187192115, 0.55295566502463056, 0.54064039408866993, 0.51354679802955661, 0.61083743842364535, 0.54926108374384242], 5: [0.25615763546798032, 0.22536945812807882, 0.17241379310344829, 0.23029556650246305, 0.24014778325123154, 0.29064039408866993, 0.23522167487684728, 0.2229064039408867, 0.22413793103448276, 0.20320197044334976, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237095 minutes
Weight histogram
[ 184  569  703  472  702 1653 4570 9823 8936  738] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[  158   264   408   693   876  1176  2947  3883  6479 11466] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.33497
1.41546
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.45641
Epoch 1, cost is  1.42927
Epoch 2, cost is  1.41135
Epoch 3, cost is  1.39417
Epoch 4, cost is  1.38238
Training took 0.224226 minutes
Weight histogram
[6335 4870 4105 2855 2512 2118 1590 1467 1432 1066] [ -6.16571121e-02  -5.54844180e-02  -4.93117240e-02  -4.31390299e-02
  -3.69663358e-02  -3.07936417e-02  -2.46209477e-02  -1.84482536e-02
  -1.22755595e-02  -6.10286547e-03   6.98286021e-05]
[2305 1386 1727 1991 2409 2663 3150 3620 4389 4710] [ -6.16571121e-02  -5.54844180e-02  -4.93117240e-02  -4.31390299e-02
  -3.69663358e-02  -3.07936417e-02  -2.46209477e-02  -1.84482536e-02
  -1.22755595e-02  -6.10286547e-03   6.98286021e-05]
-0.980924
1.75477
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151820 minutes
Weight histogram
[  69  117  849 2125 3572 4312 6378 8514 4075  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  462   948  1099   273   412   706  1003  1863  4440 19169] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.114806 minutes
Weight histogram
[5704 4319 3449 2842 3230 2173 1873 1950 3451 1384] [ -6.09070361e-02  -5.48093496e-02  -4.87116631e-02  -4.26139767e-02
  -3.65162902e-02  -3.04186037e-02  -2.43209173e-02  -1.82232308e-02
  -1.21255443e-02  -6.02785786e-03   6.98286021e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48093496e-02  -4.87116631e-02  -4.26139767e-02
  -3.65162902e-02  -3.04186037e-02  -2.43209173e-02  -1.82232308e-02
  -1.21255443e-02  -6.02785786e-03   6.98286021e-05]
-1.21737
1.44563
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.3112
Epoch 1, cost is  6.02801
Epoch 2, cost is  5.76882
Epoch 3, cost is  5.39997
Epoch 4, cost is  5.03004
Epoch 5, cost is  4.72705
Epoch 6, cost is  4.48279
Epoch 7, cost is  4.28558
Epoch 8, cost is  4.1206
Epoch 9, cost is  3.9776
Training took 0.250912 minutes
Weight histogram
[1743 1753 1702 1621 1660 1842 3024 1915  868   72] [-0.02929136 -0.02637939 -0.02346741 -0.02055544 -0.01764347 -0.01473149
 -0.01181952 -0.00890754 -0.00599557 -0.00308359 -0.00017162]
[3182 1846 1135 1112 1208 1286 1420 1556 1698 1757] [-0.02929136 -0.02637939 -0.02346741 -0.02055544 -0.01764347 -0.01473149
 -0.01181952 -0.00890754 -0.00599557 -0.00308359 -0.00017162]
-0.474795
0.641182
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076457 minutes
Epoch 0
Fine tuning took 0.075150 minutes
Epoch 0
Fine tuning took 0.077469 minutes
Epoch 0
Fine tuning took 0.077193 minutes
Epoch 0
Fine tuning took 0.075988 minutes
Epoch 0
Fine tuning took 0.075029 minutes
Epoch 0
Fine tuning took 0.076354 minutes
Epoch 0
Fine tuning took 0.075848 minutes
Epoch 0
Fine tuning took 0.078098 minutes
Epoch 0
Fine tuning took 0.077128 minutes
{'zero': {0: [0.25985221674876846, 0.16625615763546797, 0.13916256157635468, 0.092364532019704432, 0.21305418719211822, 0.099753694581280791, 0.083743842364532015, 0.16748768472906403, 0.17733990147783252, 0.29679802955665024, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.70812807881773399, 0.67364532019704437, 0.82266009852216748, 0.66625615763546797, 0.75, 0.65024630541871919, 0.72783251231527091, 0.75862068965517238, 0.60837438423645318, 0.55049261083743839], 5: [0.31157635467980294, 0.12561576354679804, 0.18719211822660098, 0.084975369458128072, 0.1206896551724138, 0.15024630541871922, 0.26600985221674878, 0.10467980295566502, 0.064039408866995079, 0.094827586206896547, 0.33128078817733991], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25985221674876846, 0.23891625615763548, 0.19211822660098521, 0.17857142857142858, 0.28817733990147781, 0.13793103448275862, 0.088669950738916259, 0.21798029556650247, 0.16502463054187191, 0.37192118226600984, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.60344827586206895, 0.62561576354679804, 0.7857142857142857, 0.61576354679802958, 0.72783251231527091, 0.65886699507389157, 0.69581280788177335, 0.78325123152709364, 0.54187192118226601, 0.54433497536945807], 5: [0.31157635467980294, 0.15763546798029557, 0.18226600985221675, 0.035714285714285712, 0.096059113300492605, 0.13423645320197045, 0.25246305418719212, 0.086206896551724144, 0.051724137931034482, 0.086206896551724144, 0.31157635467980294], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25985221674876846, 0.25, 0.18226600985221675, 0.16625615763546797, 0.27093596059113301, 0.14532019704433496, 0.086206896551724144, 0.24384236453201971, 0.18226600985221675, 0.37684729064039407, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.59975369458128081, 0.62684729064039413, 0.80418719211822665, 0.6428571428571429, 0.7142857142857143, 0.6428571428571429, 0.64039408866995073, 0.74753694581280783, 0.55172413793103448, 0.53448275862068961], 5: [0.31157635467980294, 0.15024630541871922, 0.19088669950738915, 0.029556650246305417, 0.086206896551724144, 0.14039408866995073, 0.27093596059113301, 0.11576354679802955, 0.070197044334975367, 0.071428571428571425, 0.30418719211822659], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25985221674876846, 0.25615763546798032, 0.18719211822660098, 0.19581280788177341, 0.28201970443349755, 0.15024630541871922, 0.094827586206896547, 0.20320197044334976, 0.15763546798029557, 0.3891625615763547, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.58004926108374388, 0.63916256157635465, 0.7573891625615764, 0.64655172413793105, 0.71182266009852213, 0.65517241379310343, 0.70197044334975367, 0.77586206896551724, 0.54064039408866993, 0.51600985221674878], 5: [0.31157635467980294, 0.16379310344827586, 0.17364532019704434, 0.046798029556650245, 0.071428571428571425, 0.13793103448275862, 0.25, 0.094827586206896547, 0.066502463054187194, 0.070197044334975367, 0.33620689655172414], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235995 minutes
Weight histogram
[ 184  569  703  472  702 1653 4570 9823 8936  738] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[  158   264   408   693   876  1176  2947  3883  6479 11466] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.33497
1.41546
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.45641
Epoch 1, cost is  1.42927
Epoch 2, cost is  1.41135
Epoch 3, cost is  1.39417
Epoch 4, cost is  1.38238
Training took 0.226746 minutes
Weight histogram
[6335 4870 4105 2855 2512 2118 1590 1467 1432 1066] [ -6.16571121e-02  -5.54844180e-02  -4.93117240e-02  -4.31390299e-02
  -3.69663358e-02  -3.07936417e-02  -2.46209477e-02  -1.84482536e-02
  -1.22755595e-02  -6.10286547e-03   6.98286021e-05]
[2305 1386 1727 1991 2409 2663 3150 3620 4389 4710] [ -6.16571121e-02  -5.54844180e-02  -4.93117240e-02  -4.31390299e-02
  -3.69663358e-02  -3.07936417e-02  -2.46209477e-02  -1.84482536e-02
  -1.22755595e-02  -6.10286547e-03   6.98286021e-05]
-0.980924
1.75477
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150478 minutes
Weight histogram
[  69  117  849 2125 3572 4312 6378 8514 4075  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  462   948  1099   273   412   706  1003  1863  4440 19169] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.117586 minutes
Weight histogram
[5704 4319 3449 2842 3230 2173 1873 1950 3451 1384] [ -6.09070361e-02  -5.48093496e-02  -4.87116631e-02  -4.26139767e-02
  -3.65162902e-02  -3.04186037e-02  -2.43209173e-02  -1.82232308e-02
  -1.21255443e-02  -6.02785786e-03   6.98286021e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48093496e-02  -4.87116631e-02  -4.26139767e-02
  -3.65162902e-02  -3.04186037e-02  -2.43209173e-02  -1.82232308e-02
  -1.21255443e-02  -6.02785786e-03   6.98286021e-05]
-1.21737
1.44563
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.70377
Epoch 1, cost is  5.42271
Epoch 2, cost is  5.08496
Epoch 3, cost is  4.60683
Epoch 4, cost is  4.19758
Epoch 5, cost is  3.90172
Epoch 6, cost is  3.66937
Epoch 7, cost is  3.46297
Epoch 8, cost is  3.27942
Epoch 9, cost is  3.11738
Training took 0.354051 minutes
Weight histogram
[2737 2482 1935 1572 1759 4058 1262  275   83   37] [-0.01850437 -0.01666827 -0.01483217 -0.01299607 -0.01115997 -0.00932387
 -0.00748777 -0.00565167 -0.00381557 -0.00197947 -0.00014337]
[3931 1122 1027 1078 1243 1400 1529 1586 1640 1644] [-0.01850437 -0.01666827 -0.01483217 -0.01299607 -0.01115997 -0.00932387
 -0.00748777 -0.00565167 -0.00381557 -0.00197947 -0.00014337]
-0.272732
0.389321
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082191 minutes
Epoch 0
Fine tuning took 0.081735 minutes
Epoch 0
Fine tuning took 0.081215 minutes
Epoch 0
Fine tuning took 0.081987 minutes
Epoch 0
Fine tuning took 0.082234 minutes
Epoch 0
Fine tuning took 0.081690 minutes
Epoch 0
Fine tuning took 0.081622 minutes
Epoch 0
Fine tuning took 0.082004 minutes
Epoch 0
Fine tuning took 0.082568 minutes
Epoch 0
Fine tuning took 0.081646 minutes
{'zero': {0: [0.19827586206896552, 0.17118226600985223, 0.1354679802955665, 0.11083743842364532, 0.15024630541871922, 0.16379310344827586, 0.1206896551724138, 0.16748768472906403, 0.11576354679802955, 0.16502463054187191, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54802955665024633, 0.70443349753694584, 0.62315270935960587, 0.73645320197044339, 0.72783251231527091, 0.60960591133004927, 0.70443349753694584, 0.6071428571428571, 0.69581280788177335, 0.70197044334975367, 0.7068965517241379], 5: [0.2536945812807882, 0.12438423645320197, 0.2413793103448276, 0.15270935960591134, 0.12192118226600986, 0.22660098522167488, 0.1748768472906404, 0.22536945812807882, 0.18842364532019704, 0.13300492610837439, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19827586206896552, 0.17610837438423646, 0.10221674876847291, 0.10591133004926108, 0.11699507389162561, 0.16133004926108374, 0.086206896551724144, 0.16009852216748768, 0.089901477832512317, 0.15270935960591134, 0.1145320197044335], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54802955665024633, 0.69950738916256161, 0.69334975369458129, 0.73768472906403937, 0.78078817733990147, 0.6711822660098522, 0.75369458128078815, 0.62561576354679804, 0.71182266009852213, 0.71674876847290636, 0.71921182266009853], 5: [0.2536945812807882, 0.12438423645320197, 0.20443349753694581, 0.15640394088669951, 0.10221674876847291, 0.16748768472906403, 0.16009852216748768, 0.21428571428571427, 0.19827586206896552, 0.13054187192118227, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19827586206896552, 0.1748768472906404, 0.10467980295566502, 0.11206896551724138, 0.13177339901477833, 0.17733990147783252, 0.092364532019704432, 0.13916256157635468, 0.096059113300492605, 0.15147783251231528, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54802955665024633, 0.71059113300492616, 0.68103448275862066, 0.71182266009852213, 0.76724137931034486, 0.66256157635467983, 0.75369458128078815, 0.65394088669950734, 0.74507389162561577, 0.72167487684729059, 0.73768472906403937], 5: [0.2536945812807882, 0.1145320197044335, 0.21428571428571427, 0.17610837438423646, 0.10098522167487685, 0.16009852216748768, 0.1539408866995074, 0.20689655172413793, 0.15886699507389163, 0.1268472906403941, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19827586206896552, 0.17610837438423646, 0.11206896551724138, 0.12931034482758622, 0.096059113300492605, 0.16995073891625614, 0.094827586206896547, 0.15886699507389163, 0.084975369458128072, 0.19458128078817735, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54802955665024633, 0.69827586206896552, 0.67610837438423643, 0.70443349753694584, 0.79679802955665024, 0.64901477832512311, 0.75369458128078815, 0.65640394088669951, 0.73891625615763545, 0.69458128078817738, 0.7142857142857143], 5: [0.2536945812807882, 0.12561576354679804, 0.21182266009852216, 0.16625615763546797, 0.10714285714285714, 0.18103448275862069, 0.15147783251231528, 0.18472906403940886, 0.17610837438423646, 0.11083743842364532, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236679 minutes
Weight histogram
[ 184  569  703  472  702 1653 4570 9823 8936  738] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[  158   264   408   693   876  1176  2947  3883  6479 11466] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.33497
1.41546
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.45641
Epoch 1, cost is  1.42927
Epoch 2, cost is  1.41135
Epoch 3, cost is  1.39417
Epoch 4, cost is  1.38238
Training took 0.223485 minutes
Weight histogram
[6335 4870 4105 2855 2512 2118 1590 1467 1432 1066] [ -6.16571121e-02  -5.54844180e-02  -4.93117240e-02  -4.31390299e-02
  -3.69663358e-02  -3.07936417e-02  -2.46209477e-02  -1.84482536e-02
  -1.22755595e-02  -6.10286547e-03   6.98286021e-05]
[2305 1386 1727 1991 2409 2663 3150 3620 4389 4710] [ -6.16571121e-02  -5.54844180e-02  -4.93117240e-02  -4.31390299e-02
  -3.69663358e-02  -3.07936417e-02  -2.46209477e-02  -1.84482536e-02
  -1.22755595e-02  -6.10286547e-03   6.98286021e-05]
-0.980924
1.75477
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150041 minutes
Weight histogram
[  69  117  849 2125 3572 4312 6378 8514 4075  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  462   948  1099   273   412   706  1003  1863  4440 19169] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.114727 minutes
Weight histogram
[5704 4319 3449 2842 3230 2173 1873 1950 3451 1384] [ -6.09070361e-02  -5.48093496e-02  -4.87116631e-02  -4.26139767e-02
  -3.65162902e-02  -3.04186037e-02  -2.43209173e-02  -1.82232308e-02
  -1.21255443e-02  -6.02785786e-03   6.98286021e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48093496e-02  -4.87116631e-02  -4.26139767e-02
  -3.65162902e-02  -3.04186037e-02  -2.43209173e-02  -1.82232308e-02
  -1.21255443e-02  -6.02785786e-03   6.98286021e-05]
-1.21737
1.44563
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.12108
Epoch 1, cost is  4.88152
Epoch 2, cost is  4.53299
Epoch 3, cost is  4.07237
Epoch 4, cost is  3.70452
Epoch 5, cost is  3.42734
Epoch 6, cost is  3.18415
Epoch 7, cost is  2.96587
Epoch 8, cost is  2.78063
Epoch 9, cost is  2.62898
Training took 0.548049 minutes
Weight histogram
[3416 3347 2318 5365 1036  389  172   86   45   26] [-0.01303397 -0.01174368 -0.01045338 -0.00916308 -0.00787279 -0.00658249
 -0.00529219 -0.0040019  -0.0027116  -0.0014213  -0.00013101]
[3911 1100 1048 1129 1318 1423 1459 1509 1616 1687] [-0.01303397 -0.01174368 -0.01045338 -0.00916308 -0.00787279 -0.00658249
 -0.00529219 -0.0040019  -0.0027116  -0.0014213  -0.00013101]
-0.212425
0.252594
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090929 minutes
Epoch 0
Fine tuning took 0.090888 minutes
Epoch 0
Fine tuning took 0.091038 minutes
Epoch 0
Fine tuning took 0.092191 minutes
Epoch 0
Fine tuning took 0.092071 minutes
Epoch 0
Fine tuning took 0.090244 minutes
Epoch 0
Fine tuning took 0.091480 minutes
Epoch 0
Fine tuning took 0.090746 minutes
Epoch 0
Fine tuning took 0.090621 minutes
Epoch 0
Fine tuning took 0.091277 minutes
{'zero': {0: [0.14285714285714285, 0.20689655172413793, 0.18349753694581281, 0.18596059113300492, 0.2105911330049261, 0.14532019704433496, 0.21798029556650247, 0.16871921182266009, 0.14039408866995073, 0.11206896551724138, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.59482758620689657, 0.66871921182266014, 0.57266009852216748, 0.56773399014778325, 0.56157635467980294, 0.57019704433497542, 0.54679802955665024, 0.68103448275862066, 0.72660098522167482, 0.61083743842364535], 5: [0.25123152709359609, 0.19827586206896552, 0.14778325123152711, 0.2413793103448276, 0.22167487684729065, 0.29310344827586204, 0.21182266009852216, 0.28448275862068967, 0.17857142857142858, 0.16133004926108374, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14285714285714285, 0.21674876847290642, 0.1625615763546798, 0.16995073891625614, 0.2019704433497537, 0.15024630541871922, 0.21428571428571427, 0.14408866995073891, 0.15640394088669951, 0.12315270935960591, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.6071428571428571, 0.67980295566502458, 0.58743842364532017, 0.5923645320197044, 0.57019704433497542, 0.59113300492610843, 0.62684729064039413, 0.66625615763546797, 0.70197044334975367, 0.5714285714285714], 5: [0.25123152709359609, 0.17610837438423646, 0.15763546798029557, 0.24261083743842365, 0.20566502463054187, 0.27955665024630544, 0.19458128078817735, 0.22906403940886699, 0.17733990147783252, 0.1748768472906404, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14285714285714285, 0.18842364532019704, 0.14039408866995073, 0.16871921182266009, 0.1748768472906404, 0.14285714285714285, 0.22783251231527094, 0.13793103448275862, 0.16502463054187191, 0.13300492610837439, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.61083743842364535, 0.72290640394088668, 0.58866995073891626, 0.61699507389162567, 0.59482758620689657, 0.55049261083743839, 0.63916256157635465, 0.68103448275862066, 0.70812807881773399, 0.60098522167487689], 5: [0.25123152709359609, 0.20073891625615764, 0.13669950738916256, 0.24261083743842365, 0.20812807881773399, 0.26231527093596058, 0.22167487684729065, 0.2229064039408867, 0.1539408866995074, 0.15886699507389163, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14285714285714285, 0.2105911330049261, 0.17733990147783252, 0.1625615763546798, 0.19458128078817735, 0.15024630541871922, 0.19458128078817735, 0.16379310344827586, 0.17118226600985223, 0.14039408866995073, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.61083743842364535, 0.67241379310344829, 0.61576354679802958, 0.60960591133004927, 0.58497536945812811, 0.57635467980295563, 0.61206896551724133, 0.67241379310344829, 0.70073891625615758, 0.5788177339901478], 5: [0.25123152709359609, 0.17857142857142858, 0.15024630541871922, 0.22167487684729065, 0.19581280788177341, 0.26477832512315269, 0.22906403940886699, 0.22413793103448276, 0.15640394088669951, 0.15886699507389163, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239254 minutes
Weight histogram
[ 184  569  703  472  702 1653 4570 9823 8936  738] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[  158   264   408   693   876  1176  2947  3883  6479 11466] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.33497
1.41546
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.45641
Epoch 1, cost is  1.42927
Epoch 2, cost is  1.41135
Epoch 3, cost is  1.39417
Epoch 4, cost is  1.38238
Training took 0.224089 minutes
Weight histogram
[6335 4870 4105 2855 2512 2118 1590 1467 1432 1066] [ -6.16571121e-02  -5.54844180e-02  -4.93117240e-02  -4.31390299e-02
  -3.69663358e-02  -3.07936417e-02  -2.46209477e-02  -1.84482536e-02
  -1.22755595e-02  -6.10286547e-03   6.98286021e-05]
[2305 1386 1727 1991 2409 2663 3150 3620 4389 4710] [ -6.16571121e-02  -5.54844180e-02  -4.93117240e-02  -4.31390299e-02
  -3.69663358e-02  -3.07936417e-02  -2.46209477e-02  -1.84482536e-02
  -1.22755595e-02  -6.10286547e-03   6.98286021e-05]
-0.980924
1.75477
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147580 minutes
Weight histogram
[  69  117  849 2125 3572 4312 6378 8514 4075  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  462   948  1099   273   412   706  1003  1863  4440 19169] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.114660 minutes
Weight histogram
[5704 4319 3449 2842 3230 2173 1873 1950 3451 1384] [ -6.09070361e-02  -5.48093496e-02  -4.87116631e-02  -4.26139767e-02
  -3.65162902e-02  -3.04186037e-02  -2.43209173e-02  -1.82232308e-02
  -1.21255443e-02  -6.02785786e-03   6.98286021e-05]
[4933 1448 1588 1981 2403 2724 3128 3783 3914 4473] [ -6.09070361e-02  -5.48093496e-02  -4.87116631e-02  -4.26139767e-02
  -3.65162902e-02  -3.04186037e-02  -2.43209173e-02  -1.82232308e-02
  -1.21255443e-02  -6.02785786e-03   6.98286021e-05]
-1.21737
1.44563
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.90612
Epoch 1, cost is  4.59909
Epoch 2, cost is  3.99428
Epoch 3, cost is  3.50506
Epoch 4, cost is  3.14987
Epoch 5, cost is  2.84868
Epoch 6, cost is  2.60973
Epoch 7, cost is  2.43081
Epoch 8, cost is  2.29167
Epoch 9, cost is  2.18243
Training took 0.944789 minutes
Weight histogram
[3427 2938 2857 1994 3576 1197  118   53   24   16] [-0.0080684  -0.00727458 -0.00648076 -0.00568695 -0.00489313 -0.00409932
 -0.0033055  -0.00251168 -0.00171787 -0.00092405 -0.00013023]
[3270 1010 1054 1196 1298 1340 1459 1645 1885 2043] [-0.0080684  -0.00727458 -0.00648076 -0.00568695 -0.00489313 -0.00409932
 -0.0033055  -0.00251168 -0.00171787 -0.00092405 -0.00013023]
-0.174459
0.191502
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.109357 minutes
Epoch 0
Fine tuning took 0.110405 minutes
Epoch 0
Fine tuning took 0.109246 minutes
Epoch 0
Fine tuning took 0.108444 minutes
Epoch 0
Fine tuning took 0.110656 minutes
Epoch 0
Fine tuning took 0.109916 minutes
Epoch 0
Fine tuning took 0.109983 minutes
Epoch 0
Fine tuning took 0.110870 minutes
Epoch 0
Fine tuning took 0.110258 minutes
Epoch 0
Fine tuning took 0.110300 minutes
{'zero': {0: [0.14901477832512317, 0.15517241379310345, 0.24630541871921183, 0.21921182266009853, 0.20935960591133004, 0.19334975369458129, 0.17364532019704434, 0.20320197044334976, 0.16748768472906403, 0.19088669950738915, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.62315270935960587, 0.57512315270935965, 0.5788177339901478, 0.56157635467980294, 0.51847290640394084, 0.58128078817733986, 0.51724137931034486, 0.6071428571428571, 0.63177339901477836, 0.56896551724137934], 5: [0.2413793103448276, 0.22167487684729065, 0.17857142857142858, 0.2019704433497537, 0.22906403940886699, 0.28817733990147781, 0.24507389162561577, 0.27955665024630544, 0.22536945812807882, 0.17733990147783252, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14901477832512317, 0.15024630541871922, 0.22660098522167488, 0.20073891625615764, 0.18103448275862069, 0.21674876847290642, 0.15024630541871922, 0.17364532019704434, 0.21182266009852216, 0.20812807881773399, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.60591133004926112, 0.61083743842364535, 0.5788177339901478, 0.57512315270935965, 0.51354679802955661, 0.56527093596059108, 0.52093596059113301, 0.56896551724137934, 0.60344827586206895, 0.56034482758620685], 5: [0.2413793103448276, 0.24384236453201971, 0.1625615763546798, 0.22044334975369459, 0.24384236453201971, 0.26970443349753692, 0.28448275862068967, 0.30541871921182268, 0.21921182266009853, 0.18842364532019704, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14901477832512317, 0.16625615763546797, 0.26231527093596058, 0.20935960591133004, 0.18965517241379309, 0.21921182266009853, 0.15024630541871922, 0.19334975369458129, 0.18472906403940886, 0.18349753694581281, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.61822660098522164, 0.58620689655172409, 0.56773399014778325, 0.56650246305418717, 0.51477832512315269, 0.5923645320197044, 0.54802955665024633, 0.56896551724137934, 0.64039408866995073, 0.56896551724137934], 5: [0.2413793103448276, 0.21551724137931033, 0.15147783251231528, 0.2229064039408867, 0.24384236453201971, 0.26600985221674878, 0.25738916256157635, 0.25862068965517243, 0.24630541871921183, 0.17610837438423646, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14901477832512317, 0.17118226600985223, 0.24507389162561577, 0.18472906403940886, 0.18349753694581281, 0.21428571428571427, 0.18103448275862069, 0.17980295566502463, 0.18103448275862069, 0.18103448275862069, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.60221674876847286, 0.58374384236453203, 0.58251231527093594, 0.57512315270935965, 0.50615763546798032, 0.55788177339901479, 0.53694581280788178, 0.60960591133004927, 0.6219211822660099, 0.56896551724137934], 5: [0.2413793103448276, 0.22660098522167488, 0.17118226600985223, 0.23275862068965517, 0.2413793103448276, 0.27955665024630544, 0.26108374384236455, 0.28325123152709358, 0.20935960591133004, 0.19704433497536947, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422111 minutes
Weight histogram
[  231   514   610   496  2101  7817 11351  3532  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1216 1017  977 1344 2039 2221 3711 3716 6258 5851] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.9193
Epoch 1, cost is  0.901166
Epoch 2, cost is  0.890621
Epoch 3, cost is  0.881956
Epoch 4, cost is  0.87234
Training took 0.647853 minutes
Weight histogram
[7115 5145 4336 3091 2330 1704 1409 1159  985 1076] [ -4.45248149e-02  -4.00778597e-02  -3.56309045e-02  -3.11839493e-02
  -2.67369941e-02  -2.22900389e-02  -1.78430836e-02  -1.33961284e-02
  -8.94917320e-03  -4.50221799e-03  -5.52627716e-05]
[2005 1417 1547 1907 2292 2913 3235 3556 4509 4969] [ -4.45248149e-02  -4.00778597e-02  -3.56309045e-02  -3.11839493e-02
  -2.67369941e-02  -2.22900389e-02  -1.78430836e-02  -1.33961284e-02
  -8.94917320e-03  -4.50221799e-03  -5.52627716e-05]
-0.913175
1.87536
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149741 minutes
Weight histogram
[  69  108 1124 2306 3236 4201 6378 8514 4075  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 19171] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.116953 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 1947 3086 1753] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4931 1448 1588 1981 2403 2724 3129 3783 3915 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.44656
Epoch 1, cost is  6.07672
Epoch 2, cost is  5.61224
Epoch 3, cost is  5.2151
Epoch 4, cost is  4.91003
Epoch 5, cost is  4.6649
Epoch 6, cost is  4.46636
Epoch 7, cost is  4.29929
Epoch 8, cost is  4.15878
Epoch 9, cost is  4.03508
Training took 0.323974 minutes
Weight histogram
[1905 1992 1756 1625 1475 1471 1349 1641 2365  621] [-0.03341708 -0.03009224 -0.0267674  -0.02344256 -0.02011772 -0.01679288
 -0.01346805 -0.01014321 -0.00681837 -0.00349353 -0.00016869]
[2629 1163 1121 1259 1339 1468 1605 1769 1914 1933] [-0.03341708 -0.03009224 -0.0267674  -0.02344256 -0.02011772 -0.01679288
 -0.01346805 -0.01014321 -0.00681837 -0.00349353 -0.00016869]
-0.411392
0.522755
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.143288 minutes
Epoch 0
Fine tuning took 0.142251 minutes
Epoch 0
Fine tuning took 0.142754 minutes
Epoch 0
Fine tuning took 0.142160 minutes
Epoch 0
Fine tuning took 0.141471 minutes
Epoch 0
Fine tuning took 0.143666 minutes
Epoch 0
Fine tuning took 0.142783 minutes
Epoch 0
Fine tuning took 0.142671 minutes
Epoch 0
Fine tuning took 0.143040 minutes
Epoch 0
Fine tuning took 0.142873 minutes
{'zero': {0: [0.12315270935960591, 0.10221674876847291, 0.049261083743842367, 0.19704433497536947, 0.23399014778325122, 0.096059113300492605, 0.062807881773399021, 0.099753694581280791, 0.15024630541871922, 0.20689655172413793, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.79433497536945807, 0.85591133004926112, 0.79064039408866993, 0.59975369458128081, 0.79064039408866993, 0.8719211822660099, 0.8288177339901478, 0.80911330049261088, 0.75, 0.69458128078817738], 5: [0.24014778325123154, 0.10344827586206896, 0.094827586206896547, 0.012315270935960592, 0.16625615763546797, 0.11330049261083744, 0.065270935960591137, 0.071428571428571425, 0.04064039408866995, 0.043103448275862072, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12315270935960591, 0.10591133004926108, 0.044334975369458129, 0.21182266009852216, 0.27463054187192121, 0.11699507389162561, 0.059113300492610835, 0.096059113300492605, 0.16379310344827586, 0.22413793103448276, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.78078817733990147, 0.86945812807881773, 0.76600985221674878, 0.58251231527093594, 0.78201970443349755, 0.85467980295566504, 0.84113300492610843, 0.7857142857142857, 0.73152709359605916, 0.68349753694581283], 5: [0.24014778325123154, 0.11330049261083744, 0.086206896551724144, 0.022167487684729065, 0.14285714285714285, 0.10098522167487685, 0.086206896551724144, 0.062807881773399021, 0.050492610837438424, 0.044334975369458129, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12315270935960591, 0.073891625615763554, 0.041871921182266007, 0.2105911330049261, 0.20812807881773399, 0.10837438423645321, 0.055418719211822662, 0.092364532019704432, 0.15640394088669951, 0.2376847290640394, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.79556650246305416, 0.88423645320197042, 0.76600985221674878, 0.63793103448275867, 0.77463054187192115, 0.84852216748768472, 0.85467980295566504, 0.78201970443349755, 0.71921182266009853, 0.67364532019704437], 5: [0.24014778325123154, 0.13054187192118227, 0.073891625615763554, 0.023399014778325122, 0.1539408866995074, 0.11699507389162561, 0.096059113300492605, 0.05295566502463054, 0.061576354679802957, 0.043103448275862072, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12315270935960591, 0.073891625615763554, 0.038177339901477834, 0.20320197044334976, 0.23645320197044334, 0.12192118226600986, 0.062807881773399021, 0.11945812807881774, 0.15270935960591134, 0.17118226600985223, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.63669950738916259, 0.81773399014778325, 0.8854679802955665, 0.78448275862068961, 0.58743842364532017, 0.76847290640394084, 0.86206896551724133, 0.81896551724137934, 0.78694581280788178, 0.7931034482758621, 0.67733990147783252], 5: [0.24014778325123154, 0.10837438423645321, 0.076354679802955669, 0.012315270935960592, 0.17610837438423646, 0.10960591133004927, 0.075123152709359611, 0.061576354679802957, 0.060344827586206899, 0.035714285714285712, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422188 minutes
Weight histogram
[  231   514   610   496  2101  7817 11351  3532  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1216 1017  977 1344 2039 2221 3711 3716 6258 5851] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.9193
Epoch 1, cost is  0.901166
Epoch 2, cost is  0.890621
Epoch 3, cost is  0.881956
Epoch 4, cost is  0.87234
Training took 0.647813 minutes
Weight histogram
[7115 5145 4336 3091 2330 1704 1409 1159  985 1076] [ -4.45248149e-02  -4.00778597e-02  -3.56309045e-02  -3.11839493e-02
  -2.67369941e-02  -2.22900389e-02  -1.78430836e-02  -1.33961284e-02
  -8.94917320e-03  -4.50221799e-03  -5.52627716e-05]
[2005 1417 1547 1907 2292 2913 3235 3556 4509 4969] [ -4.45248149e-02  -4.00778597e-02  -3.56309045e-02  -3.11839493e-02
  -2.67369941e-02  -2.22900389e-02  -1.78430836e-02  -1.33961284e-02
  -8.94917320e-03  -4.50221799e-03  -5.52627716e-05]
-0.913175
1.87536
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149186 minutes
Weight histogram
[  69  108 1124 2306 3236 4201 6378 8514 4075  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 19171] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.114869 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 1947 3086 1753] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4931 1448 1588 1981 2403 2724 3129 3783 3915 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.95912
Epoch 1, cost is  5.48972
Epoch 2, cost is  4.90308
Epoch 3, cost is  4.4489
Epoch 4, cost is  4.09779
Epoch 5, cost is  3.81484
Epoch 6, cost is  3.59111
Epoch 7, cost is  3.40571
Epoch 8, cost is  3.2492
Epoch 9, cost is  3.1195
Training took 0.500688 minutes
Weight histogram
[2355 2146 1957 1789 1578 1507 1373 2602  837   56] [-0.02048593 -0.01845247 -0.01641901 -0.01438554 -0.01235208 -0.01031861
 -0.00828515 -0.00625169 -0.00421822 -0.00218476 -0.00015129]
[2813 1120 1258 1261 1345 1448 1561 1679 1819 1896] [-0.02048593 -0.01845247 -0.01641901 -0.01438554 -0.01235208 -0.01031861
 -0.00828515 -0.00625169 -0.00421822 -0.00218476 -0.00015129]
-0.387108
0.405674
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.149112 minutes
Epoch 0
Fine tuning took 0.150341 minutes
Epoch 0
Fine tuning took 0.151096 minutes
Epoch 0
Fine tuning took 0.151152 minutes
Epoch 0
Fine tuning took 0.149958 minutes
Epoch 0
Fine tuning took 0.150025 minutes
Epoch 0
Fine tuning took 0.151232 minutes
Epoch 0
Fine tuning took 0.150736 minutes
Epoch 0
Fine tuning took 0.149196 minutes
Epoch 0
Fine tuning took 0.150456 minutes
{'zero': {0: [0.20566502463054187, 0.12315270935960591, 0.21674876847290642, 0.14901477832512317, 0.22167487684729065, 0.18965517241379309, 0.19334975369458129, 0.13300492610837439, 0.13916256157635468, 0.14408866995073891, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.68103448275862066, 0.58990147783251234, 0.72906403940886699, 0.55172413793103448, 0.72906403940886699, 0.65886699507389157, 0.71182266009852213, 0.74630541871921185, 0.7068965517241379, 0.57389162561576357], 5: [0.25615763546798032, 0.19581280788177341, 0.19334975369458129, 0.12192118226600986, 0.22660098522167488, 0.081280788177339899, 0.14778325123152711, 0.15517241379310345, 0.1145320197044335, 0.14901477832512317, 0.20320197044334976], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20566502463054187, 0.23275862068965517, 0.17364532019704434, 0.1625615763546798, 0.18965517241379309, 0.17610837438423646, 0.12438423645320197, 0.15640394088669951, 0.1268472906403941, 0.15024630541871922, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.60344827586206895, 0.66502463054187189, 0.69950738916256161, 0.61330049261083741, 0.72660098522167482, 0.73029556650246308, 0.69704433497536944, 0.77216748768472909, 0.69581280788177335, 0.60837438423645318], 5: [0.25615763546798032, 0.16379310344827586, 0.16133004926108374, 0.13793103448275862, 0.19704433497536947, 0.097290640394088676, 0.14532019704433496, 0.14655172413793102, 0.10098522167487685, 0.1539408866995074, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20566502463054187, 0.26354679802955666, 0.17857142857142858, 0.2019704433497537, 0.18103448275862069, 0.1625615763546798, 0.14778325123152711, 0.18349753694581281, 0.13669950738916256, 0.18842364532019704, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.52955665024630538, 0.65517241379310343, 0.67610837438423643, 0.59359605911330049, 0.74137931034482762, 0.70443349753694584, 0.68472906403940892, 0.75123152709359609, 0.66379310344827591, 0.66379310344827591], 5: [0.25615763546798032, 0.20689655172413793, 0.16625615763546797, 0.12192118226600986, 0.22536945812807882, 0.096059113300492605, 0.14778325123152711, 0.13177339901477833, 0.11206896551724138, 0.14778325123152711, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20566502463054187, 0.22167487684729065, 0.12807881773399016, 0.14901477832512317, 0.1539408866995074, 0.16748768472906403, 0.1206896551724138, 0.12807881773399016, 0.10714285714285714, 0.16502463054187191, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.61083743842364535, 0.68965517241379315, 0.70443349753694584, 0.6280788177339901, 0.72783251231527091, 0.74753694581280783, 0.73029556650246308, 0.79433497536945807, 0.68596059113300489, 0.64039408866995073], 5: [0.25615763546798032, 0.16748768472906403, 0.18226600985221675, 0.14655172413793102, 0.21798029556650247, 0.10467980295566502, 0.13177339901477833, 0.14162561576354679, 0.098522167487684734, 0.14901477832512317, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422492 minutes
Weight histogram
[  231   514   610   496  2101  7817 11351  3532  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1216 1017  977 1344 2039 2221 3711 3716 6258 5851] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.9193
Epoch 1, cost is  0.901166
Epoch 2, cost is  0.890621
Epoch 3, cost is  0.881956
Epoch 4, cost is  0.87234
Training took 0.647318 minutes
Weight histogram
[7115 5145 4336 3091 2330 1704 1409 1159  985 1076] [ -4.45248149e-02  -4.00778597e-02  -3.56309045e-02  -3.11839493e-02
  -2.67369941e-02  -2.22900389e-02  -1.78430836e-02  -1.33961284e-02
  -8.94917320e-03  -4.50221799e-03  -5.52627716e-05]
[2005 1417 1547 1907 2292 2913 3235 3556 4509 4969] [ -4.45248149e-02  -4.00778597e-02  -3.56309045e-02  -3.11839493e-02
  -2.67369941e-02  -2.22900389e-02  -1.78430836e-02  -1.33961284e-02
  -8.94917320e-03  -4.50221799e-03  -5.52627716e-05]
-0.913175
1.87536
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149111 minutes
Weight histogram
[  69  108 1124 2306 3236 4201 6378 8514 4075  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 19171] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.114613 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 1947 3086 1753] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4931 1448 1588 1981 2403 2724 3129 3783 3915 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.35233
Epoch 1, cost is  4.8351
Epoch 2, cost is  4.23745
Epoch 3, cost is  3.79599
Epoch 4, cost is  3.46668
Epoch 5, cost is  3.20894
Epoch 6, cost is  3.00352
Epoch 7, cost is  2.8334
Epoch 8, cost is  2.69219
Epoch 9, cost is  2.57167
Training took 0.818713 minutes
Weight histogram
[2952 2671 2235 1898 1696 1423 2811  406   80   28] [-0.01359871 -0.01225213 -0.01090556 -0.00955898 -0.0082124  -0.00686582
 -0.00551925 -0.00417267 -0.00282609 -0.00147951 -0.00013294]
[2839 1162 1227 1271 1349 1450 1555 1666 1801 1880] [-0.01359871 -0.01225213 -0.01090556 -0.00955898 -0.0082124  -0.00686582
 -0.00551925 -0.00417267 -0.00282609 -0.00147951 -0.00013294]
-0.244611
0.324349
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164781 minutes
Epoch 0
Fine tuning took 0.165777 minutes
Epoch 0
Fine tuning took 0.165094 minutes
Epoch 0
Fine tuning took 0.165607 minutes
Epoch 0
Fine tuning took 0.165281 minutes
Epoch 0
Fine tuning took 0.164572 minutes
Epoch 0
Fine tuning took 0.165451 minutes
Epoch 0
Fine tuning took 0.163990 minutes
Epoch 0
Fine tuning took 0.165204 minutes
Epoch 0
Fine tuning took 0.163205 minutes
{'zero': {0: [0.15517241379310345, 0.17241379310344829, 0.21674876847290642, 0.18103448275862069, 0.14901477832512317, 0.15517241379310345, 0.2019704433497537, 0.17364532019704434, 0.21674876847290642, 0.14408866995073891, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60221674876847286, 0.64778325123152714, 0.61083743842364535, 0.58743842364532017, 0.68226600985221675, 0.59482758620689657, 0.58004926108374388, 0.60221674876847286, 0.61822660098522164, 0.69827586206896552, 0.60960591133004927], 5: [0.24261083743842365, 0.17980295566502463, 0.17241379310344829, 0.23152709359605911, 0.16871921182266009, 0.25, 0.21798029556650247, 0.22413793103448276, 0.16502463054187191, 0.15763546798029557, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15517241379310345, 0.16379310344827586, 0.17857142857142858, 0.14901477832512317, 0.14408866995073891, 0.14655172413793102, 0.16748768472906403, 0.19334975369458129, 0.19950738916256158, 0.14162561576354679, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60221674876847286, 0.67364532019704437, 0.64655172413793105, 0.61576354679802958, 0.6785714285714286, 0.58620689655172409, 0.6145320197044335, 0.61083743842364535, 0.60960591133004927, 0.70566502463054193, 0.6145320197044335], 5: [0.24261083743842365, 0.1625615763546798, 0.1748768472906404, 0.23522167487684728, 0.17733990147783252, 0.26724137931034481, 0.21798029556650247, 0.19581280788177341, 0.19088669950738915, 0.15270935960591134, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15517241379310345, 0.14532019704433496, 0.17610837438423646, 0.18349753694581281, 0.16379310344827586, 0.1539408866995074, 0.15024630541871922, 0.16748768472906403, 0.18472906403940886, 0.13423645320197045, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60221674876847286, 0.68596059113300489, 0.63177339901477836, 0.58497536945812811, 0.65270935960591137, 0.59482758620689657, 0.59975369458128081, 0.60467980295566504, 0.61206896551724133, 0.70197044334975367, 0.62561576354679804], 5: [0.24261083743842365, 0.16871921182266009, 0.19211822660098521, 0.23152709359605911, 0.18349753694581281, 0.25123152709359609, 0.25, 0.22783251231527094, 0.20320197044334976, 0.16379310344827586, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15517241379310345, 0.16871921182266009, 0.17857142857142858, 0.18103448275862069, 0.16133004926108374, 0.16009852216748768, 0.16625615763546797, 0.16748768472906403, 0.20073891625615764, 0.12807881773399016, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60221674876847286, 0.65640394088669951, 0.63669950738916259, 0.58743842364532017, 0.69211822660098521, 0.56403940886699511, 0.58743842364532017, 0.61945812807881773, 0.62315270935960587, 0.70443349753694584, 0.63669950738916259], 5: [0.24261083743842365, 0.1748768472906404, 0.18472906403940886, 0.23152709359605911, 0.14655172413793102, 0.27586206896551724, 0.24630541871921183, 0.21305418719211822, 0.17610837438423646, 0.16748768472906403, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421973 minutes
Weight histogram
[  231   514   610   496  2101  7817 11351  3532  1487   211] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
[1216 1017  977 1344 2039 2221 3711 3716 6258 5851] [ -6.90304223e-05   4.71075029e-05   1.63245428e-04   2.79383353e-04
   3.95521279e-04   5.11659204e-04   6.27797129e-04   7.43935054e-04
   8.60072979e-04   9.76210905e-04   1.09234883e-03]
-1.08243
1.30753
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.9193
Epoch 1, cost is  0.901166
Epoch 2, cost is  0.890621
Epoch 3, cost is  0.881956
Epoch 4, cost is  0.87234
Training took 0.647848 minutes
Weight histogram
[7115 5145 4336 3091 2330 1704 1409 1159  985 1076] [ -4.45248149e-02  -4.00778597e-02  -3.56309045e-02  -3.11839493e-02
  -2.67369941e-02  -2.22900389e-02  -1.78430836e-02  -1.33961284e-02
  -8.94917320e-03  -4.50221799e-03  -5.52627716e-05]
[2005 1417 1547 1907 2292 2913 3235 3556 4509 4969] [ -4.45248149e-02  -4.00778597e-02  -3.56309045e-02  -3.11839493e-02
  -2.67369941e-02  -2.22900389e-02  -1.78430836e-02  -1.33961284e-02
  -8.94917320e-03  -4.50221799e-03  -5.52627716e-05]
-0.913175
1.87536
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149342 minutes
Weight histogram
[  69  108 1124 2306 3236 4201 6378 8514 4075  364] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[ 2160   148   201   272   411   707  1002  1864  4439 19171] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.08098
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46874
Epoch 1, cost is  2.4281
Epoch 2, cost is  2.40035
Epoch 3, cost is  2.37753
Epoch 4, cost is  2.35284
Training took 0.115049 minutes
Weight histogram
[5696 4325 3446 2839 3234 2172 1877 1947 3086 1753] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
[4931 1448 1588 1981 2403 2724 3129 3783 3915 4473] [ -6.09070361e-02  -5.48124495e-02  -4.87178630e-02  -4.26232764e-02
  -3.65286899e-02  -3.04341033e-02  -2.43395167e-02  -1.82449302e-02
  -1.21503436e-02  -6.05575708e-03   3.88294720e-05]
-1.21737
1.44563
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.75265
Epoch 1, cost is  4.24574
Epoch 2, cost is  3.6982
Epoch 3, cost is  3.29387
Epoch 4, cost is  2.99431
Epoch 5, cost is  2.7628
Epoch 6, cost is  2.57884
Epoch 7, cost is  2.43018
Epoch 8, cost is  2.31064
Epoch 9, cost is  2.21587
Training took 1.469998 minutes
Weight histogram
[3739 3325 2693 2414 3221  488  183   81   36   20] [-0.00909592 -0.00820138 -0.00730684 -0.0064123  -0.00551776 -0.00462322
 -0.00372868 -0.00283414 -0.0019396  -0.00104506 -0.00015052]
[2785 1171 1188 1226 1319 1420 1543 1684 1883 1981] [-0.00909592 -0.00820138 -0.00730684 -0.0064123  -0.00551776 -0.00462322
 -0.00372868 -0.00283414 -0.0019396  -0.00104506 -0.00015052]
-0.19743
0.229257
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.192918 minutes
Epoch 0
Fine tuning took 0.194193 minutes
Epoch 0
Fine tuning took 0.192764 minutes
Epoch 0
Fine tuning took 0.191555 minutes
Epoch 0
Fine tuning took 0.195349 minutes
Epoch 0
Fine tuning took 0.192969 minutes
Epoch 0
Fine tuning took 0.194770 minutes
Epoch 0
Fine tuning took 0.192166 minutes
Epoch 0
Fine tuning took 0.195344 minutes
Epoch 0
Fine tuning took 0.192661 minutes
{'zero': {0: [0.13669950738916256, 0.1625615763546798, 0.23152709359605911, 0.2019704433497537, 0.1354679802955665, 0.16379310344827586, 0.18596059113300492, 0.16379310344827586, 0.12561576354679804, 0.20073891625615764, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.63916256157635465, 0.58374384236453203, 0.55172413793103448, 0.64901477832512311, 0.55911330049261088, 0.58128078817733986, 0.53201970443349755, 0.66009852216748766, 0.61945812807881773, 0.56403940886699511], 5: [0.25615763546798032, 0.19827586206896552, 0.18472906403940886, 0.24630541871921183, 0.21551724137931033, 0.27709359605911332, 0.23275862068965517, 0.30418719211822659, 0.21428571428571427, 0.17980295566502463, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13669950738916256, 0.17118226600985223, 0.23275862068965517, 0.20320197044334976, 0.15147783251231528, 0.13669950738916256, 0.19458128078817735, 0.15886699507389163, 0.14655172413793102, 0.20566502463054187, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.60344827586206895, 0.57019704433497542, 0.55788177339901479, 0.60591133004926112, 0.57635467980295563, 0.55295566502463056, 0.56280788177339902, 0.66625615763546797, 0.59729064039408863, 0.54187192118226601], 5: [0.25615763546798032, 0.22536945812807882, 0.19704433497536947, 0.23891625615763548, 0.24261083743842365, 0.28694581280788178, 0.25246305418719212, 0.27832512315270935, 0.18719211822660098, 0.19704433497536947, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13669950738916256, 0.16871921182266009, 0.19458128078817735, 0.18472906403940886, 0.11945812807881774, 0.12561576354679804, 0.18472906403940886, 0.15886699507389163, 0.13423645320197045, 0.22906403940886699, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.63423645320197042, 0.59113300492610843, 0.58004926108374388, 0.65024630541871919, 0.59729064039408863, 0.59359605911330049, 0.54802955665024633, 0.65147783251231528, 0.59482758620689657, 0.54926108374384242], 5: [0.25615763546798032, 0.19704433497536947, 0.21428571428571427, 0.23522167487684728, 0.23029556650246305, 0.27709359605911332, 0.22167487684729065, 0.29310344827586204, 0.21428571428571427, 0.17610837438423646, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13669950738916256, 0.16995073891625614, 0.19088669950738915, 0.21182266009852216, 0.1268472906403941, 0.13300492610837439, 0.19211822660098521, 0.18349753694581281, 0.13916256157635468, 0.18596059113300492, 0.22783251231527094], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.62068965517241381, 0.58004926108374388, 0.52339901477832518, 0.63054187192118227, 0.57635467980295563, 0.58866995073891626, 0.53078817733990147, 0.6576354679802956, 0.63423645320197042, 0.52339901477832518], 5: [0.25615763546798032, 0.20935960591133004, 0.22906403940886699, 0.26477832512315269, 0.24261083743842365, 0.29064039408866993, 0.21921182266009853, 0.2857142857142857, 0.20320197044334976, 0.17980295566502463, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.109637 minutes
Weight histogram
[ 696 2452 3874 2698 1654 4606 6184 5114 4404  718] [ -2.71980138e-03  -2.17267682e-03  -1.62555226e-03  -1.07842770e-03
  -5.31303138e-04   1.58214243e-05   5.62945986e-04   1.11007055e-03
   1.65719511e-03   2.20431967e-03   2.75144423e-03]
[  136   165   215   308   486   758   797  1894  8536 19105] [ -2.71980138e-03  -2.17267682e-03  -1.62555226e-03  -1.07842770e-03
  -5.31303138e-04   1.58214243e-05   5.62945986e-04   1.11007055e-03
   1.65719511e-03   2.20431967e-03   2.75144423e-03]
-2.24522
2.13415
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.59895
Epoch 1, cost is  3.56891
Epoch 2, cost is  3.54839
Epoch 3, cost is  3.52166
Epoch 4, cost is  3.50151
Training took 0.072788 minutes
Weight histogram
[7102 5719 4185 4249 2366 2839 4896  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2751 2838 2283 2587 2332 2895 3496 3792 4575 4851] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-2.0325
2.57773
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149076 minutes
Weight histogram
[  120   460  1804  4877  7888 13201  5136   814   114    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7737 20373   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.33223
1.10388
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.37252
Epoch 1, cost is  2.32984
Epoch 2, cost is  2.30615
Epoch 3, cost is  2.28439
Epoch 4, cost is  2.26474
Training took 0.118159 minutes
Weight histogram
[7231 4639 4114 3314 3284 2603 2086 2096 4311  747] [ -6.48319349e-02  -5.83448584e-02  -5.18577820e-02  -4.53707056e-02
  -3.88836291e-02  -3.23965527e-02  -2.59094763e-02  -1.94223998e-02
  -1.29353234e-02  -6.44824696e-03   3.88294720e-05]
[5072 1589 1838 2333 2740 3254 3829 4184 4822 4764] [ -6.48319349e-02  -5.83448584e-02  -5.18577820e-02  -4.53707056e-02
  -3.88836291e-02  -3.23965527e-02  -2.59094763e-02  -1.94223998e-02
  -1.29353234e-02  -6.44824696e-03   3.88294720e-05]
-1.38947
1.49538
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.92373
Epoch 1, cost is  5.58777
Epoch 2, cost is  5.49646
Epoch 3, cost is  5.38118
Epoch 4, cost is  5.17127
Epoch 5, cost is  4.95493
Epoch 6, cost is  4.73768
Epoch 7, cost is  4.51736
Epoch 8, cost is  4.32159
Epoch 9, cost is  4.16388
Training took 0.179769 minutes
Weight histogram
[3750 4742 6079 2183 1291  891  701  378  154   81] [ -2.83447839e-02  -2.55199973e-02  -2.26952106e-02  -1.98704239e-02
  -1.70456372e-02  -1.42208505e-02  -1.13960639e-02  -8.57127720e-03
  -5.74649052e-03  -2.92170384e-03  -9.69171670e-05]
[1492 3033 3661 1993 1857 1789 1660 1648 1747 1370] [ -2.83447839e-02  -2.55199973e-02  -2.26952106e-02  -1.98704239e-02
  -1.70456372e-02  -1.42208505e-02  -1.13960639e-02  -8.57127720e-03
  -5.74649052e-03  -2.92170384e-03  -9.69171670e-05]
-0.364743
0.578597
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041663 minutes
Epoch 0
Fine tuning took 0.044137 minutes
Epoch 0
Fine tuning took 0.044260 minutes
Epoch 0
Fine tuning took 0.043346 minutes
Epoch 0
Fine tuning took 0.041675 minutes
Epoch 0
Fine tuning took 0.042818 minutes
Epoch 0
Fine tuning took 0.042357 minutes
Epoch 0
Fine tuning took 0.041202 minutes
Epoch 0
Fine tuning took 0.041832 minutes
Epoch 0
Fine tuning took 0.043144 minutes
{'zero': {0: [0.14901477832512317, 0.2105911330049261, 0.14778325123152711, 0.19458128078817735, 0.1539408866995074, 0.2105911330049261, 0.22167487684729065, 0.16871921182266009, 0.21798029556650247, 0.20566502463054187, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59605911330049266, 0.6219211822660099, 0.6354679802955665, 0.60221674876847286, 0.68349753694581283, 0.54679802955665024, 0.55049261083743839, 0.59729064039408863, 0.51477832512315269, 0.56773399014778325, 0.57019704433497542], 5: [0.25492610837438423, 0.16748768472906403, 0.21674876847290642, 0.20320197044334976, 0.1625615763546798, 0.24261083743842365, 0.22783251231527094, 0.23399014778325122, 0.26724137931034481, 0.22660098522167488, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14901477832512317, 0.14901477832512317, 0.15270935960591134, 0.12315270935960591, 0.10714285714285714, 0.17857142857142858, 0.2105911330049261, 0.14778325123152711, 0.12807881773399016, 0.16625615763546797, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59605911330049266, 0.72906403940886699, 0.58128078817733986, 0.69088669950738912, 0.78078817733990147, 0.65517241379310343, 0.57266009852216748, 0.64901477832512311, 0.64408866995073888, 0.63054187192118227, 0.77463054187192115], 5: [0.25492610837438423, 0.12192118226600986, 0.26600985221674878, 0.18596059113300492, 0.11206896551724138, 0.16625615763546797, 0.21674876847290642, 0.20320197044334976, 0.22783251231527094, 0.20320197044334976, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14901477832512317, 0.15024630541871922, 0.17610837438423646, 0.14901477832512317, 0.13669950738916256, 0.16625615763546797, 0.22906403940886699, 0.17118226600985223, 0.13300492610837439, 0.13669950738916256, 0.10467980295566502], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59605911330049266, 0.73029556650246308, 0.54556650246305416, 0.64655172413793105, 0.76724137931034486, 0.66625615763546797, 0.56527093596059108, 0.65394088669950734, 0.6711822660098522, 0.64532019704433496, 0.76847290640394084], 5: [0.25492610837438423, 0.11945812807881774, 0.27832512315270935, 0.20443349753694581, 0.096059113300492605, 0.16748768472906403, 0.20566502463054187, 0.1748768472906404, 0.19581280788177341, 0.21798029556650247, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14901477832512317, 0.14778325123152711, 0.14901477832512317, 0.15640394088669951, 0.094827586206896547, 0.1748768472906404, 0.23645320197044334, 0.13300492610837439, 0.1354679802955665, 0.16871921182266009, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59605911330049266, 0.72290640394088668, 0.58128078817733986, 0.66133004926108374, 0.7931034482758621, 0.61330049261083741, 0.56650246305418717, 0.68103448275862066, 0.62931034482758619, 0.63916256157635465, 0.71305418719211822], 5: [0.25492610837438423, 0.12931034482758622, 0.26970443349753692, 0.18226600985221675, 0.11206896551724138, 0.21182266009852216, 0.19704433497536947, 0.18596059113300492, 0.23522167487684728, 0.19211822660098521, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106591 minutes
Weight histogram
[ 462 1583 2773 2682 1640 4429 6392 5183 4486  745] [ -2.77616130e-03  -2.22340075e-03  -1.67064019e-03  -1.11787964e-03
  -5.65119088e-04  -1.23585341e-05   5.40402019e-04   1.09316257e-03
   1.64592313e-03   2.19868368e-03   2.75144423e-03]
[  136   165   216   308   493   752   801  1893  8792 16819] [ -2.77616130e-03  -2.22340075e-03  -1.67064019e-03  -1.11787964e-03
  -5.65119088e-04  -1.23585341e-05   5.40402019e-04   1.09316257e-03
   1.64592313e-03   2.19868368e-03   2.75144423e-03]
-2.31375
2.10155
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.6044
Epoch 1, cost is  3.55198
Epoch 2, cost is  3.52496
Epoch 3, cost is  3.50907
Epoch 4, cost is  3.48132
Training took 0.072463 minutes
Weight histogram
[7102 5719 4185 4034 2567 3213 2511  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2660 2696 2238 2503 2151 2649 3344 3269 4175 4690] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-2.00137
2.43667
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151011 minutes
Weight histogram
[  120   460  1804  4820  7528 12540  4896   186    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 18593   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.117259 minutes
Weight histogram
[6048 5083 3557 3159 3209 2388 2000 2061 4194  701] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
-1.2603
1.44932
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.55884
Epoch 1, cost is  5.36348
Epoch 2, cost is  5.2677
Epoch 3, cost is  5.03005
Epoch 4, cost is  4.75997
Epoch 5, cost is  4.50435
Epoch 6, cost is  4.23264
Epoch 7, cost is  3.9869
Epoch 8, cost is  3.7923
Epoch 9, cost is  3.64274
Training took 0.246273 minutes
Weight histogram
[3323 9766 4213 1272  719  414  256  151   84   52] [-0.01881931 -0.01695267 -0.01508603 -0.01321939 -0.01135275 -0.00948611
 -0.00761947 -0.00575283 -0.00388619 -0.00201955 -0.00015291]
[5138 2209 1479 1598 1547 1483 1523 1640 1874 1759] [-0.01881931 -0.01695267 -0.01508603 -0.01321939 -0.01135275 -0.00948611
 -0.00761947 -0.00575283 -0.00388619 -0.00201955 -0.00015291]
-0.297847
0.260292
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046057 minutes
Epoch 0
Fine tuning took 0.045056 minutes
Epoch 0
Fine tuning took 0.044876 minutes
Epoch 0
Fine tuning took 0.045531 minutes
Epoch 0
Fine tuning took 0.046231 minutes
Epoch 0
Fine tuning took 0.045517 minutes
Epoch 0
Fine tuning took 0.044892 minutes
Epoch 0
Fine tuning took 0.046867 minutes
Epoch 0
Fine tuning took 0.046683 minutes
Epoch 0
Fine tuning took 0.045580 minutes
{'zero': {0: [0.17364532019704434, 0.14162561576354679, 0.2536945812807882, 0.20320197044334976, 0.19211822660098521, 0.15763546798029557, 0.18472906403940886, 0.16625615763546797, 0.1539408866995074, 0.18719211822660098, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.64408866995073888, 0.50123152709359609, 0.56403940886699511, 0.61206896551724133, 0.61083743842364535, 0.61330049261083741, 0.6219211822660099, 0.56280788177339902, 0.65270935960591137, 0.60467980295566504], 5: [0.17610837438423646, 0.21428571428571427, 0.24507389162561577, 0.23275862068965517, 0.19581280788177341, 0.23152709359605911, 0.2019704433497537, 0.21182266009852216, 0.28325123152709358, 0.16009852216748768, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17364532019704434, 0.13793103448275862, 0.23029556650246305, 0.19458128078817735, 0.19950738916256158, 0.14039408866995073, 0.18965517241379309, 0.15886699507389163, 0.18103448275862069, 0.16871921182266009, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.64655172413793105, 0.53078817733990147, 0.57512315270935965, 0.60960591133004927, 0.67364532019704437, 0.57019704433497542, 0.63300492610837433, 0.55788177339901479, 0.64532019704433496, 0.60221674876847286], 5: [0.17610837438423646, 0.21551724137931033, 0.23891625615763548, 0.23029556650246305, 0.19088669950738915, 0.18596059113300492, 0.24014778325123154, 0.20812807881773399, 0.26108374384236455, 0.18596059113300492, 0.27586206896551724], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17364532019704434, 0.15147783251231528, 0.24384236453201971, 0.19088669950738915, 0.18842364532019704, 0.17241379310344829, 0.17610837438423646, 0.14655172413793102, 0.15024630541871922, 0.17241379310344829, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.64532019704433496, 0.53201970443349755, 0.59113300492610843, 0.63916256157635465, 0.62931034482758619, 0.62438423645320196, 0.65147783251231528, 0.61822660098522164, 0.6711822660098522, 0.57635467980295563], 5: [0.17610837438423646, 0.20320197044334976, 0.22413793103448276, 0.21798029556650247, 0.17241379310344829, 0.19827586206896552, 0.19950738916256158, 0.2019704433497537, 0.23152709359605911, 0.15640394088669951, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17364532019704434, 0.12807881773399016, 0.22906403940886699, 0.21182266009852216, 0.19827586206896552, 0.14778325123152711, 0.20320197044334976, 0.16625615763546797, 0.15024630541871922, 0.19088669950738915, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.68103448275862066, 0.53201970443349755, 0.55049261083743839, 0.60960591133004927, 0.65640394088669951, 0.5788177339901478, 0.6280788177339901, 0.5714285714285714, 0.65640394088669951, 0.60098522167487689], 5: [0.17610837438423646, 0.19088669950738915, 0.23891625615763548, 0.2376847290640394, 0.19211822660098521, 0.19581280788177341, 0.21798029556650247, 0.20566502463054187, 0.27832512315270935, 0.15270935960591134, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108994 minutes
Weight histogram
[ 462 1583 2773 2682 1640 4429 6392 5183 4486  745] [ -2.77616130e-03  -2.22340075e-03  -1.67064019e-03  -1.11787964e-03
  -5.65119088e-04  -1.23585341e-05   5.40402019e-04   1.09316257e-03
   1.64592313e-03   2.19868368e-03   2.75144423e-03]
[  136   165   216   308   493   752   801  1893  8792 16819] [ -2.77616130e-03  -2.22340075e-03  -1.67064019e-03  -1.11787964e-03
  -5.65119088e-04  -1.23585341e-05   5.40402019e-04   1.09316257e-03
   1.64592313e-03   2.19868368e-03   2.75144423e-03]
-2.31375
2.10155
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.6044
Epoch 1, cost is  3.55198
Epoch 2, cost is  3.52496
Epoch 3, cost is  3.50907
Epoch 4, cost is  3.48132
Training took 0.076120 minutes
Weight histogram
[7102 5719 4185 4034 2567 3213 2511  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2660 2696 2238 2503 2151 2649 3344 3269 4175 4690] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-2.00137
2.43667
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148471 minutes
Weight histogram
[  120   460  1804  4820  7528 12540  4896   186    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 18593   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.114404 minutes
Weight histogram
[6048 5083 3557 3159 3209 2388 2000 2061 4194  701] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
-1.2603
1.44932
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.46384
Epoch 1, cost is  5.34446
Epoch 2, cost is  5.08218
Epoch 3, cost is  4.65461
Epoch 4, cost is  4.29967
Epoch 5, cost is  3.96323
Epoch 6, cost is  3.70282
Epoch 7, cost is  3.51163
Epoch 8, cost is  3.34948
Epoch 9, cost is  3.19809
Training took 0.334778 minutes
Weight histogram
[3054 3569 3333 3166 5548 1250  171   80   48   31] [-0.01131206 -0.0101925  -0.00907293 -0.00795337 -0.0068338  -0.00571424
 -0.00459468 -0.00347511 -0.00235555 -0.00123598 -0.00011642]
[5216 1336 1277 1401 1432 1470 1672 1966 2215 2265] [-0.01131206 -0.0101925  -0.00907293 -0.00795337 -0.0068338  -0.00571424
 -0.00459468 -0.00347511 -0.00235555 -0.00123598 -0.00011642]
-0.235478
0.210966
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048781 minutes
Epoch 0
Fine tuning took 0.050391 minutes
Epoch 0
Fine tuning took 0.051911 minutes
Epoch 0
Fine tuning took 0.051500 minutes
Epoch 0
Fine tuning took 0.049109 minutes
Epoch 0
Fine tuning took 0.051547 minutes
Epoch 0
Fine tuning took 0.050474 minutes
Epoch 0
Fine tuning took 0.049518 minutes
Epoch 0
Fine tuning took 0.049436 minutes
Epoch 0
Fine tuning took 0.051333 minutes
{'zero': {0: [0.20073891625615764, 0.2536945812807882, 0.24507389162561577, 0.1625615763546798, 0.21798029556650247, 0.18103448275862069, 0.18349753694581281, 0.18226600985221675, 0.2413793103448276, 0.18226600985221675, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.57019704433497542, 0.58743842364532017, 0.62438423645320196, 0.58497536945812811, 0.65517241379310343, 0.64655172413793105, 0.58251231527093594, 0.57758620689655171, 0.58004926108374388, 0.56650246305418717], 5: [0.18965517241379309, 0.17610837438423646, 0.16748768472906403, 0.21305418719211822, 0.19704433497536947, 0.16379310344827586, 0.16995073891625614, 0.23522167487684728, 0.18103448275862069, 0.2376847290640394, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20073891625615764, 0.22783251231527094, 0.27586206896551724, 0.18472906403940886, 0.23891625615763548, 0.18226600985221675, 0.21182266009852216, 0.17364532019704434, 0.2229064039408867, 0.19458128078817735, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.58374384236453203, 0.56403940886699511, 0.61330049261083741, 0.57019704433497542, 0.63423645320197042, 0.61699507389162567, 0.56280788177339902, 0.61576354679802958, 0.57635467980295563, 0.58004926108374388], 5: [0.18965517241379309, 0.18842364532019704, 0.16009852216748768, 0.2019704433497537, 0.19088669950738915, 0.18349753694581281, 0.17118226600985223, 0.26354679802955666, 0.16133004926108374, 0.22906403940886699, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20073891625615764, 0.23029556650246305, 0.26970443349753692, 0.17241379310344829, 0.21428571428571427, 0.17980295566502463, 0.19827586206896552, 0.17118226600985223, 0.21921182266009853, 0.20443349753694581, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.56896551724137934, 0.56403940886699511, 0.60837438423645318, 0.59605911330049266, 0.66871921182266014, 0.60960591133004927, 0.59729064039408863, 0.62684729064039413, 0.56280788177339902, 0.54926108374384242], 5: [0.18965517241379309, 0.20073891625615764, 0.16625615763546797, 0.21921182266009853, 0.18965517241379309, 0.15147783251231528, 0.19211822660098521, 0.23152709359605911, 0.1539408866995074, 0.23275862068965517, 0.2857142857142857], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20073891625615764, 0.2413793103448276, 0.24507389162561577, 0.16995073891625614, 0.22906403940886699, 0.18103448275862069, 0.19211822660098521, 0.18965517241379309, 0.20443349753694581, 0.19950738916256158, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.57389162561576357, 0.60837438423645318, 0.61699507389162567, 0.5714285714285714, 0.64532019704433496, 0.60591133004926112, 0.57266009852216748, 0.62561576354679804, 0.55911330049261088, 0.55911330049261088], 5: [0.18965517241379309, 0.18472906403940886, 0.14655172413793102, 0.21305418719211822, 0.19950738916256158, 0.17364532019704434, 0.2019704433497537, 0.2376847290640394, 0.16995073891625614, 0.2413793103448276, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107101 minutes
Weight histogram
[ 462 1583 2773 2682 1640 4429 6392 5183 4486  745] [ -2.77616130e-03  -2.22340075e-03  -1.67064019e-03  -1.11787964e-03
  -5.65119088e-04  -1.23585341e-05   5.40402019e-04   1.09316257e-03
   1.64592313e-03   2.19868368e-03   2.75144423e-03]
[  136   165   216   308   493   752   801  1893  8792 16819] [ -2.77616130e-03  -2.22340075e-03  -1.67064019e-03  -1.11787964e-03
  -5.65119088e-04  -1.23585341e-05   5.40402019e-04   1.09316257e-03
   1.64592313e-03   2.19868368e-03   2.75144423e-03]
-2.31375
2.10155
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.6044
Epoch 1, cost is  3.55198
Epoch 2, cost is  3.52496
Epoch 3, cost is  3.50907
Epoch 4, cost is  3.48132
Training took 0.075564 minutes
Weight histogram
[7102 5719 4185 4034 2567 3213 2511  580  281  183] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
[2660 2696 2238 2503 2151 2649 3344 3269 4175 4690] [-0.03755646 -0.03382776 -0.03009905 -0.02637035 -0.02264164 -0.01891293
 -0.01518423 -0.01145552 -0.00772682 -0.00399811 -0.00026941]
-2.00137
2.43667
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148862 minutes
Weight histogram
[  120   460  1804  4820  7528 12540  4896   186    35    11] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
[  232   278   353   493   857  1267  2342  7492 18593   493] [-0.00047558 -0.00017932  0.00011695  0.00041321  0.00070948  0.00100574
  0.001302    0.00159827  0.00189453  0.0021908   0.00248706]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.114938 minutes
Weight histogram
[6048 5083 3557 3159 3209 2388 2000 2061 4194  701] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
-1.2603
1.44932
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.44954
Epoch 1, cost is  5.27412
Epoch 2, cost is  4.8239
Epoch 3, cost is  4.32948
Epoch 4, cost is  3.90548
Epoch 5, cost is  3.58166
Epoch 6, cost is  3.35141
Epoch 7, cost is  3.1518
Epoch 8, cost is  2.96487
Epoch 9, cost is  2.80349
Training took 0.535865 minutes
Weight histogram
[3317 3817 2776 2456 2289 5223  290   44   20   18] [-0.00591584 -0.00533536 -0.00475489 -0.00417442 -0.00359395 -0.00301348
 -0.00243301 -0.00185254 -0.00127207 -0.0006916  -0.00011113]
[4705 1304 1319 1377 1417 1617 1915 2131 2207 2258] [-0.00591584 -0.00533536 -0.00475489 -0.00417442 -0.00359395 -0.00301348
 -0.00243301 -0.00185254 -0.00127207 -0.0006916  -0.00011113]
-0.183692
0.174146
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.062445 minutes
Epoch 0
Fine tuning took 0.061611 minutes
Epoch 0
Fine tuning took 0.060311 minutes
Epoch 0
Fine tuning took 0.060960 minutes
Epoch 0
Fine tuning took 0.061506 minutes
Epoch 0
Fine tuning took 0.061080 minutes
Epoch 0
Fine tuning took 0.061415 minutes
Epoch 0
Fine tuning took 0.061585 minutes
Epoch 0
Fine tuning took 0.062394 minutes
Epoch 0
Fine tuning took 0.062375 minutes
{'zero': {0: [0.20812807881773399, 0.19088669950738915, 0.21798029556650247, 0.18965517241379309, 0.22167487684729065, 0.26477832512315269, 0.2229064039408867, 0.2229064039408867, 0.24014778325123154, 0.2413793103448276, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.61576354679802958, 0.54679802955665024, 0.60221674876847286, 0.54802955665024633, 0.56650246305418717, 0.56034482758620685, 0.53078817733990147, 0.60591133004926112, 0.54433497536945807, 0.54802955665024633], 5: [0.18842364532019704, 0.19334975369458129, 0.23522167487684728, 0.20812807881773399, 0.23029556650246305, 0.16871921182266009, 0.21674876847290642, 0.24630541871921183, 0.1539408866995074, 0.21428571428571427, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20812807881773399, 0.20566502463054187, 0.22906403940886699, 0.16502463054187191, 0.22906403940886699, 0.23645320197044334, 0.21182266009852216, 0.2019704433497537, 0.24753694581280788, 0.23152709359605911, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.63669950738916259, 0.58374384236453203, 0.63793103448275867, 0.54926108374384242, 0.60098522167487689, 0.56773399014778325, 0.57389162561576357, 0.57266009852216748, 0.54187192118226601, 0.53694581280788178], 5: [0.18842364532019704, 0.15763546798029557, 0.18719211822660098, 0.19704433497536947, 0.22167487684729065, 0.1625615763546798, 0.22044334975369459, 0.22413793103448276, 0.17980295566502463, 0.22660098522167488, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20812807881773399, 0.19334975369458129, 0.20320197044334976, 0.18596059113300492, 0.23645320197044334, 0.23891625615763548, 0.19950738916256158, 0.20812807881773399, 0.22906403940886699, 0.20935960591133004, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.6428571428571429, 0.5923645320197044, 0.6219211822660099, 0.55541871921182262, 0.57758620689655171, 0.55541871921182262, 0.54556650246305416, 0.60344827586206895, 0.60098522167487689, 0.5431034482758621], 5: [0.18842364532019704, 0.16379310344827586, 0.20443349753694581, 0.19211822660098521, 0.20812807881773399, 0.18349753694581281, 0.24507389162561577, 0.24630541871921183, 0.16748768472906403, 0.18965517241379309, 0.26724137931034481], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20812807881773399, 0.20320197044334976, 0.21921182266009853, 0.18103448275862069, 0.2536945812807882, 0.21551724137931033, 0.23029556650246305, 0.18842364532019704, 0.26231527093596058, 0.18965517241379309, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.63793103448275867, 0.56280788177339902, 0.62438423645320196, 0.51724137931034486, 0.59852216748768472, 0.56157635467980294, 0.55049261083743839, 0.57266009852216748, 0.59113300492610843, 0.5357142857142857], 5: [0.18842364532019704, 0.15886699507389163, 0.21798029556650247, 0.19458128078817735, 0.22906403940886699, 0.18596059113300492, 0.20812807881773399, 0.26108374384236455, 0.16502463054187191, 0.21921182266009853, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150240 minutes
Weight histogram
[ 307  395  805 1803 5073 8996 8887 2058 1166  885] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
[  137   168   237   327   534   923  1088  2159  7243 17559] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
-1.36373
1.5225
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.28319
Epoch 1, cost is  2.25073
Epoch 2, cost is  2.22769
Epoch 3, cost is  2.21001
Epoch 4, cost is  2.18883
Training took 0.114818 minutes
Weight histogram
[5901 5762 4222 3420 3044 2321 2053 1774 1506  372] [ -6.31762594e-02  -5.68552685e-02  -5.05342777e-02  -4.42132868e-02
  -3.78922960e-02  -3.15713051e-02  -2.52503143e-02  -1.89293234e-02
  -1.26083326e-02  -6.28734175e-03   3.36491030e-05]
[2787 1504 1689 2292 2689 3012 3478 3875 4379 4670] [ -6.31762594e-02  -5.68552685e-02  -5.05342777e-02  -4.42132868e-02
  -3.78922960e-02  -3.15713051e-02  -2.52503143e-02  -1.89293234e-02
  -1.26083326e-02  -6.28734175e-03   3.36491030e-05]
-1.47715
1.7085
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149799 minutes
Weight histogram
[  69  122  553 1616 3530 4771 7032 9539 4624  544] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  274   318   443   615   955  1319  1037  1919  5183 20337] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.114701 minutes
Weight histogram
[6048 5083 3557 3159 3209 2388 2000 2173 4051  732] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
-1.2603
1.44932
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.2277
Epoch 1, cost is  5.9731
Epoch 2, cost is  5.81653
Epoch 3, cost is  5.6134
Epoch 4, cost is  5.29224
Epoch 5, cost is  4.984
Epoch 6, cost is  4.71622
Epoch 7, cost is  4.48577
Epoch 8, cost is  4.28208
Epoch 9, cost is  4.10296
Training took 0.206967 minutes
Weight histogram
[2388 2510 2349 2612 5667 2183 1755  584  137   65] [-0.02539939 -0.02287559 -0.02035179 -0.01782798 -0.01530418 -0.01278038
 -0.01025658 -0.00773278 -0.00520898 -0.00268518 -0.00016138]
[3916 3663 1629 1374 1444 1549 1608 1657 1741 1669] [-0.02539939 -0.02287559 -0.02035179 -0.01782798 -0.01530418 -0.01278038
 -0.01025658 -0.00773278 -0.00520898 -0.00268518 -0.00016138]
-0.421477
0.474467
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052761 minutes
Epoch 0
Fine tuning took 0.051985 minutes
Epoch 0
Fine tuning took 0.052600 minutes
Epoch 0
Fine tuning took 0.054113 minutes
Epoch 0
Fine tuning took 0.053451 minutes
Epoch 0
Fine tuning took 0.053103 minutes
Epoch 0
Fine tuning took 0.053567 minutes
Epoch 0
Fine tuning took 0.053806 minutes
Epoch 0
Fine tuning took 0.053624 minutes
Epoch 0
Fine tuning took 0.053377 minutes
{'zero': {0: [0.088669950738916259, 0.25492610837438423, 0.060344827586206899, 0.16502463054187191, 0.11945812807881774, 0.071428571428571425, 0.10467980295566502, 0.065270935960591137, 0.084975369458128072, 0.12931034482758622, 0.1145320197044335], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.75, 0.4963054187192118, 0.73275862068965514, 0.61822660098522164, 0.72660098522167482, 0.88423645320197042, 0.76847290640394084, 0.79064039408866993, 0.82019704433497542, 0.74876847290640391, 0.78448275862068961], 5: [0.16133004926108374, 0.24876847290640394, 0.20689655172413793, 0.21674876847290642, 0.1539408866995074, 0.044334975369458129, 0.1268472906403941, 0.14408866995073891, 0.094827586206896547, 0.12192118226600986, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.088669950738916259, 0.18472906403940886, 0.046798029556650245, 0.13054187192118227, 0.075123152709359611, 0.076354679802955669, 0.057881773399014777, 0.049261083743842367, 0.11699507389162561, 0.1206896551724138, 0.1145320197044335], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.75, 0.66379310344827591, 0.76724137931034486, 0.70566502463054193, 0.80418719211822665, 0.90270935960591137, 0.87931034482758619, 0.84605911330049266, 0.8423645320197044, 0.7573891625615764, 0.80172413793103448], 5: [0.16133004926108374, 0.15147783251231528, 0.18596059113300492, 0.16379310344827586, 0.1206896551724138, 0.020935960591133004, 0.062807881773399021, 0.10467980295566502, 0.04064039408866995, 0.12192118226600986, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.088669950738916259, 0.18965517241379309, 0.036945812807881777, 0.1268472906403941, 0.070197044334975367, 0.080049261083743842, 0.029556650246305417, 0.043103448275862072, 0.11576354679802955, 0.10221674876847291, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.75, 0.63793103448275867, 0.77955665024630538, 0.70443349753694584, 0.80911330049261088, 0.88054187192118227, 0.88916256157635465, 0.85344827586206895, 0.83497536945812811, 0.78448275862068961, 0.76108374384236455], 5: [0.16133004926108374, 0.17241379310344829, 0.18349753694581281, 0.16871921182266009, 0.1206896551724138, 0.039408866995073892, 0.081280788177339899, 0.10344827586206896, 0.049261083743842367, 0.11330049261083744, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.088669950738916259, 0.19458128078817735, 0.04064039408866995, 0.13054187192118227, 0.087438423645320201, 0.075123152709359611, 0.036945812807881777, 0.046798029556650245, 0.10344827586206896, 0.10714285714285714, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.75, 0.62315270935960587, 0.79926108374384242, 0.66995073891625612, 0.82389162561576357, 0.90270935960591137, 0.90270935960591137, 0.86699507389162567, 0.8571428571428571, 0.75862068965517238, 0.80049261083743839], 5: [0.16133004926108374, 0.18226600985221675, 0.16009852216748768, 0.19950738916256158, 0.088669950738916259, 0.022167487684729065, 0.060344827586206899, 0.086206896551724144, 0.039408866995073892, 0.13423645320197045, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150887 minutes
Weight histogram
[ 307  395  805 1803 5073 8996 8887 2058 1166  885] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
[  137   168   237   327   534   923  1088  2159  7243 17559] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
-1.36373
1.5225
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.28319
Epoch 1, cost is  2.25073
Epoch 2, cost is  2.22769
Epoch 3, cost is  2.21001
Epoch 4, cost is  2.18883
Training took 0.116862 minutes
Weight histogram
[5901 5762 4222 3420 3044 2321 2053 1774 1506  372] [ -6.31762594e-02  -5.68552685e-02  -5.05342777e-02  -4.42132868e-02
  -3.78922960e-02  -3.15713051e-02  -2.52503143e-02  -1.89293234e-02
  -1.26083326e-02  -6.28734175e-03   3.36491030e-05]
[2787 1504 1689 2292 2689 3012 3478 3875 4379 4670] [ -6.31762594e-02  -5.68552685e-02  -5.05342777e-02  -4.42132868e-02
  -3.78922960e-02  -3.15713051e-02  -2.52503143e-02  -1.89293234e-02
  -1.26083326e-02  -6.28734175e-03   3.36491030e-05]
-1.47715
1.7085
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149304 minutes
Weight histogram
[  69  122  553 1616 3530 4771 7032 9539 4624  544] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  274   318   443   615   955  1319  1037  1919  5183 20337] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.117768 minutes
Weight histogram
[6048 5083 3557 3159 3209 2388 2000 2173 4051  732] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
-1.2603
1.44932
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.63301
Epoch 1, cost is  5.40894
Epoch 2, cost is  5.27324
Epoch 3, cost is  4.97933
Epoch 4, cost is  4.58873
Epoch 5, cost is  4.28069
Epoch 6, cost is  3.99749
Epoch 7, cost is  3.73699
Epoch 8, cost is  3.52516
Epoch 9, cost is  3.35197
Training took 0.300583 minutes
Weight histogram
[3216 3579 3137 7756 1439  604  268  133   73   45] [-0.01764149 -0.01589161 -0.01414173 -0.01239184 -0.01064196 -0.00889208
 -0.0071422  -0.00539231 -0.00364243 -0.00189255 -0.00014267]
[6076 1612 1242 1359 1473 1537 1581 1668 1810 1892] [-0.01764149 -0.01589161 -0.01414173 -0.01239184 -0.01064196 -0.00889208
 -0.0071422  -0.00539231 -0.00364243 -0.00189255 -0.00014267]
-0.262348
0.335099
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.058040 minutes
Epoch 0
Fine tuning took 0.060601 minutes
Epoch 0
Fine tuning took 0.059943 minutes
Epoch 0
Fine tuning took 0.059250 minutes
Epoch 0
Fine tuning took 0.059539 minutes
Epoch 0
Fine tuning took 0.059888 minutes
Epoch 0
Fine tuning took 0.059925 minutes
Epoch 0
Fine tuning took 0.059722 minutes
Epoch 0
Fine tuning took 0.058286 minutes
Epoch 0
Fine tuning took 0.060381 minutes
{'zero': {0: [0.14532019704433496, 0.14039408866995073, 0.20689655172413793, 0.17241379310344829, 0.10960591133004927, 0.11083743842364532, 0.27709359605911332, 0.10467980295566502, 0.089901477832512317, 0.15270935960591134, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7068965517241379, 0.60467980295566504, 0.53694581280788178, 0.66625615763546797, 0.68472906403940892, 0.73522167487684731, 0.56650246305418717, 0.6219211822660099, 0.72167487684729059, 0.70812807881773399, 0.64532019704433496], 5: [0.14778325123152711, 0.25492610837438423, 0.25615763546798032, 0.16133004926108374, 0.20566502463054187, 0.1539408866995074, 0.15640394088669951, 0.27339901477832512, 0.18842364532019704, 0.13916256157635468, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14532019704433496, 0.14655172413793102, 0.20320197044334976, 0.15024630541871922, 0.10960591133004927, 0.098522167487684734, 0.24753694581280788, 0.13669950738916256, 0.11699507389162561, 0.15147783251231528, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7068965517241379, 0.6280788177339901, 0.6428571428571429, 0.71059113300492616, 0.72783251231527091, 0.75123152709359609, 0.61576354679802958, 0.65270935960591137, 0.71059113300492616, 0.68103448275862066, 0.65270935960591137], 5: [0.14778325123152711, 0.22536945812807882, 0.1539408866995074, 0.13916256157635468, 0.1625615763546798, 0.15024630541871922, 0.13669950738916256, 0.2105911330049261, 0.17241379310344829, 0.16748768472906403, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14532019704433496, 0.16871921182266009, 0.21921182266009853, 0.16009852216748768, 0.099753694581280791, 0.091133004926108374, 0.25123152709359609, 0.13300492610837439, 0.11822660098522167, 0.16625615763546797, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7068965517241379, 0.6428571428571429, 0.6354679802955665, 0.68965517241379315, 0.75492610837438423, 0.74876847290640391, 0.60344827586206895, 0.67733990147783252, 0.69950738916256161, 0.68472906403940892, 0.67980295566502458], 5: [0.14778325123152711, 0.18842364532019704, 0.14532019704433496, 0.15024630541871922, 0.14532019704433496, 0.16009852216748768, 0.14532019704433496, 0.18965517241379309, 0.18226600985221675, 0.14901477832512317, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14532019704433496, 0.15024630541871922, 0.19334975369458129, 0.16748768472906403, 0.10960591133004927, 0.10098522167487685, 0.26354679802955666, 0.13054187192118227, 0.10344827586206896, 0.14901477832512317, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7068965517241379, 0.63423645320197042, 0.62931034482758619, 0.67364532019704437, 0.73275862068965514, 0.74507389162561577, 0.60221674876847286, 0.65886699507389157, 0.71674876847290636, 0.71798029556650245, 0.65394088669950734], 5: [0.14778325123152711, 0.21551724137931033, 0.17733990147783252, 0.15886699507389163, 0.15763546798029557, 0.1539408866995074, 0.13423645320197045, 0.2105911330049261, 0.17980295566502463, 0.13300492610837439, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.152184 minutes
Weight histogram
[ 307  395  805 1803 5073 8996 8887 2058 1166  885] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
[  137   168   237   327   534   923  1088  2159  7243 17559] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
-1.36373
1.5225
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.28319
Epoch 1, cost is  2.25073
Epoch 2, cost is  2.22769
Epoch 3, cost is  2.21001
Epoch 4, cost is  2.18883
Training took 0.116950 minutes
Weight histogram
[5901 5762 4222 3420 3044 2321 2053 1774 1506  372] [ -6.31762594e-02  -5.68552685e-02  -5.05342777e-02  -4.42132868e-02
  -3.78922960e-02  -3.15713051e-02  -2.52503143e-02  -1.89293234e-02
  -1.26083326e-02  -6.28734175e-03   3.36491030e-05]
[2787 1504 1689 2292 2689 3012 3478 3875 4379 4670] [ -6.31762594e-02  -5.68552685e-02  -5.05342777e-02  -4.42132868e-02
  -3.78922960e-02  -3.15713051e-02  -2.52503143e-02  -1.89293234e-02
  -1.26083326e-02  -6.28734175e-03   3.36491030e-05]
-1.47715
1.7085
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149508 minutes
Weight histogram
[  69  122  553 1616 3530 4771 7032 9539 4624  544] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  274   318   443   615   955  1319  1037  1919  5183 20337] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.117197 minutes
Weight histogram
[6048 5083 3557 3159 3209 2388 2000 2173 4051  732] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
-1.2603
1.44932
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.28475
Epoch 1, cost is  5.13603
Epoch 2, cost is  4.93749
Epoch 3, cost is  4.52268
Epoch 4, cost is  4.14883
Epoch 5, cost is  3.81607
Epoch 6, cost is  3.5071
Epoch 7, cost is  3.26427
Epoch 8, cost is  3.07321
Epoch 9, cost is  2.91522
Training took 0.414180 minutes
Weight histogram
[3978 4495 9494 1190  531  263  143   80   46   30] [-0.01201195 -0.01082336 -0.00963477 -0.00844619 -0.0072576  -0.00606901
 -0.00488042 -0.00369183 -0.00250324 -0.00131465 -0.00012606]
[5660 1364 1264 1429 1455 1528 1619 1780 2002 2149] [-0.01201195 -0.01082336 -0.00963477 -0.00844619 -0.0072576  -0.00606901
 -0.00488042 -0.00369183 -0.00250324 -0.00131465 -0.00012606]
-0.222437
0.227248
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.062377 minutes
Epoch 0
Fine tuning took 0.065012 minutes
Epoch 0
Fine tuning took 0.064166 minutes
Epoch 0
Fine tuning took 0.063928 minutes
Epoch 0
Fine tuning took 0.064176 minutes
Epoch 0
Fine tuning took 0.062734 minutes
Epoch 0
Fine tuning took 0.065123 minutes
Epoch 0
Fine tuning took 0.064668 minutes
Epoch 0
Fine tuning took 0.062720 minutes
Epoch 0
Fine tuning took 0.063638 minutes
{'zero': {0: [0.15763546798029557, 0.15886699507389163, 0.28325123152709358, 0.16995073891625614, 0.17241379310344829, 0.16748768472906403, 0.17241379310344829, 0.21798029556650247, 0.18103448275862069, 0.12192118226600986, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.60837438423645318, 0.5, 0.63300492610837433, 0.6428571428571429, 0.6711822660098522, 0.63054187192118227, 0.55911330049261088, 0.62438423645320196, 0.67364532019704437, 0.72660098522167482], 5: [0.16995073891625614, 0.23275862068965517, 0.21674876847290642, 0.19704433497536947, 0.18472906403940886, 0.16133004926108374, 0.19704433497536947, 0.2229064039408867, 0.19458128078817735, 0.20443349753694581, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.1539408866995074, 0.20812807881773399, 0.15886699507389163, 0.18719211822660098, 0.14162561576354679, 0.19704433497536947, 0.16379310344827586, 0.17733990147783252, 0.13916256157635468, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.61822660098522164, 0.57635467980295563, 0.59852216748768472, 0.6219211822660099, 0.68842364532019706, 0.60837438423645318, 0.59113300492610843, 0.62561576354679804, 0.68472906403940892, 0.74384236453201968], 5: [0.16995073891625614, 0.22783251231527094, 0.21551724137931033, 0.24261083743842365, 0.19088669950738915, 0.16995073891625614, 0.19458128078817735, 0.24507389162561577, 0.19704433497536947, 0.17610837438423646, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.16009852216748768, 0.24876847290640394, 0.14408866995073891, 0.15147783251231528, 0.13916256157635468, 0.19334975369458129, 0.18965517241379309, 0.15640394088669951, 0.10344827586206896, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.63177339901477836, 0.53694581280788178, 0.6428571428571429, 0.68349753694581283, 0.65886699507389157, 0.62068965517241381, 0.58620689655172409, 0.64778325123152714, 0.69458128078817738, 0.66502463054187189], 5: [0.16995073891625614, 0.20812807881773399, 0.21428571428571427, 0.21305418719211822, 0.16502463054187191, 0.2019704433497537, 0.18596059113300492, 0.22413793103448276, 0.19581280788177341, 0.2019704433497537, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.17241379310344829, 0.21305418719211822, 0.18226600985221675, 0.19704433497536947, 0.13177339901477833, 0.21674876847290642, 0.17364532019704434, 0.1539408866995074, 0.12561576354679804, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.61822660098522164, 0.57512315270935965, 0.58374384236453203, 0.60467980295566504, 0.6785714285714286, 0.59113300492610843, 0.55911330049261088, 0.65147783251231528, 0.68349753694581283, 0.73275862068965514], 5: [0.16995073891625614, 0.20935960591133004, 0.21182266009852216, 0.23399014778325122, 0.19827586206896552, 0.18965517241379309, 0.19211822660098521, 0.26724137931034481, 0.19458128078817735, 0.19088669950738915, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150350 minutes
Weight histogram
[ 307  395  805 1803 5073 8996 8887 2058 1166  885] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
[  137   168   237   327   534   923  1088  2159  7243 17559] [-0.0001064   0.00018515  0.0004767   0.00076825  0.0010598   0.00135135
  0.0016429   0.00193445  0.002226    0.00251755  0.0028091 ]
-1.36373
1.5225
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.28319
Epoch 1, cost is  2.25073
Epoch 2, cost is  2.22769
Epoch 3, cost is  2.21001
Epoch 4, cost is  2.18883
Training took 0.116572 minutes
Weight histogram
[5901 5762 4222 3420 3044 2321 2053 1774 1506  372] [ -6.31762594e-02  -5.68552685e-02  -5.05342777e-02  -4.42132868e-02
  -3.78922960e-02  -3.15713051e-02  -2.52503143e-02  -1.89293234e-02
  -1.26083326e-02  -6.28734175e-03   3.36491030e-05]
[2787 1504 1689 2292 2689 3012 3478 3875 4379 4670] [ -6.31762594e-02  -5.68552685e-02  -5.05342777e-02  -4.42132868e-02
  -3.78922960e-02  -3.15713051e-02  -2.52503143e-02  -1.89293234e-02
  -1.26083326e-02  -6.28734175e-03   3.36491030e-05]
-1.47715
1.7085
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150178 minutes
Weight histogram
[  69  122  553 1616 3530 4771 7032 9539 4624  544] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  274   318   443   615   955  1319  1037  1919  5183 20337] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.118397 minutes
Weight histogram
[6048 5083 3557 3159 3209 2388 2000 2173 4051  732] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64868721e-02  -5.02062386e-02  -4.39256051e-02
  -3.76449716e-02  -3.13643381e-02  -2.50837046e-02  -1.88030711e-02
  -1.25224376e-02  -6.24180404e-03   3.88294720e-05]
-1.2603
1.44932
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.24288
Epoch 1, cost is  5.04265
Epoch 2, cost is  4.52775
Epoch 3, cost is  3.99199
Epoch 4, cost is  3.52738
Epoch 5, cost is  3.17205
Epoch 6, cost is  2.92039
Epoch 7, cost is  2.7182
Epoch 8, cost is  2.54627
Epoch 9, cost is  2.39373
Training took 0.683879 minutes
Weight histogram
[3981 3520 2743 2741 2415 4675   93   42   20   20] [-0.00658948 -0.00594289 -0.0052963  -0.00464971 -0.00400313 -0.00335654
 -0.00270995 -0.00206336 -0.00141677 -0.00077018 -0.00012359]
[4663 1289 1303 1362 1412 1565 1810 2125 2360 2361] [-0.00658948 -0.00594289 -0.0052963  -0.00464971 -0.00400313 -0.00335654
 -0.00270995 -0.00206336 -0.00141677 -0.00077018 -0.00012359]
-0.190585
0.200444
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077931 minutes
Epoch 0
Fine tuning took 0.077211 minutes
Epoch 0
Fine tuning took 0.077570 minutes
Epoch 0
Fine tuning took 0.078849 minutes
Epoch 0
Fine tuning took 0.078556 minutes
Epoch 0
Fine tuning took 0.078423 minutes
Epoch 0
Fine tuning took 0.079030 minutes
Epoch 0
Fine tuning took 0.077031 minutes
Epoch 0
Fine tuning took 0.077376 minutes
Epoch 0
Fine tuning took 0.076669 minutes
{'zero': {0: [0.2019704433497537, 0.18842364532019704, 0.25492610837438423, 0.19581280788177341, 0.21551724137931033, 0.17857142857142858, 0.17733990147783252, 0.2019704433497537, 0.1748768472906404, 0.18226600985221675, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.55911330049261088, 0.54679802955665024, 0.56034482758620685, 0.56034482758620685, 0.65394088669950734, 0.59975369458128081, 0.58374384236453203, 0.66133004926108374, 0.56280788177339902, 0.60591133004926112], 5: [0.18103448275862069, 0.25246305418719212, 0.19827586206896552, 0.24384236453201971, 0.22413793103448276, 0.16748768472906403, 0.2229064039408867, 0.21428571428571427, 0.16379310344827586, 0.25492610837438423, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2019704433497537, 0.18472906403940886, 0.21182266009852216, 0.20073891625615764, 0.19827586206896552, 0.17610837438423646, 0.17857142857142858, 0.21798029556650247, 0.14532019704433496, 0.16625615763546797, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.58990147783251234, 0.56527093596059108, 0.58497536945812811, 0.60591133004926112, 0.64778325123152714, 0.58990147783251234, 0.54556650246305416, 0.7068965517241379, 0.6071428571428571, 0.58128078817733986], 5: [0.18103448275862069, 0.22536945812807882, 0.2229064039408867, 0.21428571428571427, 0.19581280788177341, 0.17610837438423646, 0.23152709359605911, 0.23645320197044334, 0.14778325123152711, 0.22660098522167488, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2019704433497537, 0.17364532019704434, 0.2376847290640394, 0.22660098522167488, 0.23029556650246305, 0.17118226600985223, 0.18719211822660098, 0.19950738916256158, 0.15886699507389163, 0.16625615763546797, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.58251231527093594, 0.56773399014778325, 0.58004926108374388, 0.56650246305418717, 0.62684729064039413, 0.57389162561576357, 0.59113300492610843, 0.67487684729064035, 0.58374384236453203, 0.56527093596059108], 5: [0.18103448275862069, 0.24384236453201971, 0.19458128078817735, 0.19334975369458129, 0.20320197044334976, 0.2019704433497537, 0.23891625615763548, 0.20935960591133004, 0.16625615763546797, 0.25, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2019704433497537, 0.18842364532019704, 0.25123152709359609, 0.20689655172413793, 0.23275862068965517, 0.16748768472906403, 0.19334975369458129, 0.19581280788177341, 0.17364532019704434, 0.17857142857142858, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.58497536945812811, 0.5357142857142857, 0.54802955665024633, 0.56034482758620685, 0.64901477832512311, 0.5714285714285714, 0.58497536945812811, 0.66625615763546797, 0.59852216748768472, 0.58743842364532017], 5: [0.18103448275862069, 0.22660098522167488, 0.21305418719211822, 0.24507389162561577, 0.20689655172413793, 0.18349753694581281, 0.23522167487684728, 0.21921182266009853, 0.16009852216748768, 0.2229064039408867, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239413 minutes
Weight histogram
[  184   569   703   472   702  1653  4597 10568 10177   750] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 164  277  440  729  929 1561 2842 5647 9042 8744] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.33497
1.63478
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.37425
Epoch 1, cost is  1.35111
Epoch 2, cost is  1.3339
Epoch 3, cost is  1.32089
Epoch 4, cost is  1.31118
Training took 0.225407 minutes
Weight histogram
[7099 5189 4281 3227 2566 2239 1698 1507 1458 1111] [ -6.32371828e-02  -5.69064817e-02  -5.05757805e-02  -4.42450794e-02
  -3.79143782e-02  -3.15836771e-02  -2.52529760e-02  -1.89222748e-02
  -1.25915737e-02  -6.26087254e-03   6.98286021e-05]
[2360 1461 1831 2114 2571 2994 3279 3967 4776 5022] [ -6.32371828e-02  -5.69064817e-02  -5.05757805e-02  -4.42450794e-02
  -3.79143782e-02  -3.15836771e-02  -2.52529760e-02  -1.89222748e-02
  -1.25915737e-02  -6.26087254e-03   6.98286021e-05]
-1.0271
1.80165
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150274 minutes
Weight histogram
[  69  117  849 2125 3572 4367 6698 9446 4613  544] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  465   965  1083   277   424   710  1037  1919  5183 20337] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.116219 minutes
Weight histogram
[6049 5103 3542 3157 3212 2392 1993 2012 3481 1459] [ -6.27675056e-02  -5.64837722e-02  -5.02000388e-02  -4.39163054e-02
  -3.76325719e-02  -3.13488385e-02  -2.50651051e-02  -1.87813717e-02
  -1.24976382e-02  -6.21390482e-03   6.98286021e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64837722e-02  -5.02000388e-02  -4.39163054e-02
  -3.76325719e-02  -3.13488385e-02  -2.50651051e-02  -1.87813717e-02
  -1.24976382e-02  -6.21390482e-03   6.98286021e-05]
-1.2603
1.44932
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.31796
Epoch 1, cost is  6.03729
Epoch 2, cost is  5.77598
Epoch 3, cost is  5.40635
Epoch 4, cost is  5.03822
Epoch 5, cost is  4.7382
Epoch 6, cost is  4.49693
Epoch 7, cost is  4.29785
Epoch 8, cost is  4.13097
Epoch 9, cost is  3.98841
Training took 0.250240 minutes
Weight histogram
[2154 2200 2139 2027 2078 2293 3789 2387 1092   91] [-0.02929136 -0.02637923 -0.02346711 -0.02055498 -0.01764285 -0.01473073
 -0.0118186  -0.00890647 -0.00599435 -0.00308222 -0.00017009]
[3980 2302 1421 1398 1516 1614 1781 1959 2132 2147] [-0.02929136 -0.02637923 -0.02346711 -0.02055498 -0.01764285 -0.01473073
 -0.0118186  -0.00890647 -0.00599435 -0.00308222 -0.00017009]
-0.474795
0.641182
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076423 minutes
Epoch 0
Fine tuning took 0.076423 minutes
Epoch 0
Fine tuning took 0.078125 minutes
Epoch 0
Fine tuning took 0.076603 minutes
Epoch 0
Fine tuning took 0.076765 minutes
Epoch 0
Fine tuning took 0.077468 minutes
Epoch 0
Fine tuning took 0.075071 minutes
Epoch 0
Fine tuning took 0.078350 minutes
Epoch 0
Fine tuning took 0.077578 minutes
Epoch 0
Fine tuning took 0.077103 minutes
{'zero': {0: [0.11945812807881774, 0.13423645320197045, 0.067733990147783252, 0.060344827586206899, 0.12561576354679804, 0.065270935960591137, 0.029556650246305417, 0.057881773399014777, 0.054187192118226604, 0.061576354679802957, 0.065270935960591137], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.61206896551724133, 0.74014778325123154, 0.79802955665024633, 0.65270935960591137, 0.87068965517241381, 0.84359605911330049, 0.7931034482758621, 0.90024630541871919, 0.80788177339901479, 0.86330049261083741], 5: [0.1539408866995074, 0.2536945812807882, 0.19211822660098521, 0.14162561576354679, 0.22167487684729065, 0.064039408866995079, 0.1268472906403941, 0.14901477832512317, 0.045566502463054187, 0.13054187192118227, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11945812807881774, 0.13793103448275862, 0.04064039408866995, 0.043103448275862072, 0.1625615763546798, 0.041871921182266007, 0.0036945812807881772, 0.076354679802955669, 0.066502463054187194, 0.034482758620689655, 0.059113300492610835], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.72660098522167482, 0.82389162561576357, 0.89162561576354682, 0.59359605911330049, 0.87931034482758619, 0.92980295566502458, 0.77709359605911332, 0.90024630541871919, 0.86206896551724133, 0.90640394088669951], 5: [0.1539408866995074, 0.1354679802955665, 0.1354679802955665, 0.065270935960591137, 0.24384236453201971, 0.078817733990147784, 0.066502463054187194, 0.14655172413793102, 0.033251231527093597, 0.10344827586206896, 0.034482758620689655], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11945812807881774, 0.10714285714285714, 0.032019704433497539, 0.065270935960591137, 0.1268472906403941, 0.048029556650246302, 0.017241379310344827, 0.080049261083743842, 0.050492610837438424, 0.036945812807881777, 0.065270935960591137], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.74384236453201968, 0.82512315270935965, 0.85960591133004927, 0.63300492610837433, 0.89532019704433496, 0.9211822660098522, 0.78940886699507384, 0.91256157635467983, 0.8928571428571429, 0.89778325123152714], 5: [0.1539408866995074, 0.14901477832512317, 0.14285714285714285, 0.075123152709359611, 0.24014778325123154, 0.056650246305418719, 0.061576354679802957, 0.13054187192118227, 0.036945812807881777, 0.070197044334975367, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11945812807881774, 0.12807881773399016, 0.02832512315270936, 0.064039408866995079, 0.15270935960591134, 0.057881773399014777, 0.0073891625615763543, 0.066502463054187194, 0.050492610837438424, 0.039408866995073892, 0.064039408866995079], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.74630541871921185, 0.83866995073891626, 0.86576354679802958, 0.6219211822660099, 0.90270935960591137, 0.93965517241379315, 0.81034482758620685, 0.92364532019704437, 0.87438423645320196, 0.89655172413793105], 5: [0.1539408866995074, 0.12561576354679804, 0.13300492610837439, 0.070197044334975367, 0.22536945812807882, 0.039408866995073892, 0.05295566502463054, 0.12315270935960591, 0.025862068965517241, 0.086206896551724144, 0.039408866995073892], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235389 minutes
Weight histogram
[  184   569   703   472   702  1653  4597 10568 10177   750] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 164  277  440  729  929 1561 2842 5647 9042 8744] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.33497
1.63478
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.37425
Epoch 1, cost is  1.35111
Epoch 2, cost is  1.3339
Epoch 3, cost is  1.32089
Epoch 4, cost is  1.31118
Training took 0.222998 minutes
Weight histogram
[7099 5189 4281 3227 2566 2239 1698 1507 1458 1111] [ -6.32371828e-02  -5.69064817e-02  -5.05757805e-02  -4.42450794e-02
  -3.79143782e-02  -3.15836771e-02  -2.52529760e-02  -1.89222748e-02
  -1.25915737e-02  -6.26087254e-03   6.98286021e-05]
[2360 1461 1831 2114 2571 2994 3279 3967 4776 5022] [ -6.32371828e-02  -5.69064817e-02  -5.05757805e-02  -4.42450794e-02
  -3.79143782e-02  -3.15836771e-02  -2.52529760e-02  -1.89222748e-02
  -1.25915737e-02  -6.26087254e-03   6.98286021e-05]
-1.0271
1.80165
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151619 minutes
Weight histogram
[  69  117  849 2125 3572 4367 6698 9446 4613  544] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  465   965  1083   277   424   710  1037  1919  5183 20337] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.116465 minutes
Weight histogram
[6049 5103 3542 3157 3212 2392 1993 2012 3481 1459] [ -6.27675056e-02  -5.64837722e-02  -5.02000388e-02  -4.39163054e-02
  -3.76325719e-02  -3.13488385e-02  -2.50651051e-02  -1.87813717e-02
  -1.24976382e-02  -6.21390482e-03   6.98286021e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64837722e-02  -5.02000388e-02  -4.39163054e-02
  -3.76325719e-02  -3.13488385e-02  -2.50651051e-02  -1.87813717e-02
  -1.24976382e-02  -6.21390482e-03   6.98286021e-05]
-1.2603
1.44932
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.71828
Epoch 1, cost is  5.4394
Epoch 2, cost is  5.09765
Epoch 3, cost is  4.61477
Epoch 4, cost is  4.20708
Epoch 5, cost is  3.91202
Epoch 6, cost is  3.67807
Epoch 7, cost is  3.47391
Epoch 8, cost is  3.28912
Epoch 9, cost is  3.12298
Training took 0.354474 minutes
Weight histogram
[3366 3111 2437 1978 2199 5070 1590  348  104   47] [-0.01850437 -0.01666827 -0.01483217 -0.01299607 -0.01115997 -0.00932387
 -0.00748777 -0.00565167 -0.00381557 -0.00197947 -0.00014337]
[4912 1406 1288 1355 1562 1760 1919 1995 2061 1992] [-0.01850437 -0.01666827 -0.01483217 -0.01299607 -0.01115997 -0.00932387
 -0.00748777 -0.00565167 -0.00381557 -0.00197947 -0.00014337]
-0.272732
0.389321
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081702 minutes
Epoch 0
Fine tuning took 0.082150 minutes
Epoch 0
Fine tuning took 0.081805 minutes
Epoch 0
Fine tuning took 0.082617 minutes
Epoch 0
Fine tuning took 0.081538 minutes
Epoch 0
Fine tuning took 0.081237 minutes
Epoch 0
Fine tuning took 0.080874 minutes
Epoch 0
Fine tuning took 0.081047 minutes
Epoch 0
Fine tuning took 0.081790 minutes
Epoch 0
Fine tuning took 0.081334 minutes
{'zero': {0: [0.15147783251231528, 0.19827586206896552, 0.10221674876847291, 0.1539408866995074, 0.13669950738916256, 0.11083743842364532, 0.17118226600985223, 0.082512315270935957, 0.14162561576354679, 0.12192118226600986, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.57635467980295563, 0.73275862068965514, 0.68842364532019706, 0.71059113300492616, 0.71798029556650245, 0.6219211822660099, 0.73768472906403937, 0.67364532019704437, 0.80418719211822665, 0.72167487684729059], 5: [0.14655172413793102, 0.22536945812807882, 0.16502463054187191, 0.15763546798029557, 0.15270935960591134, 0.17118226600985223, 0.20689655172413793, 0.17980295566502463, 0.18472906403940886, 0.073891625615763554, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15147783251231528, 0.19334975369458129, 0.13054187192118227, 0.11330049261083744, 0.098522167487684734, 0.11576354679802955, 0.18226600985221675, 0.076354679802955669, 0.12315270935960591, 0.11330049261083744, 0.094827586206896547], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.65394088669950734, 0.71674876847290636, 0.78078817733990147, 0.79187192118226601, 0.74384236453201968, 0.66995073891625612, 0.78325123152709364, 0.72783251231527091, 0.77832512315270941, 0.78448275862068961], 5: [0.14655172413793102, 0.15270935960591134, 0.15270935960591134, 0.10591133004926108, 0.10960591133004927, 0.14039408866995073, 0.14778325123152711, 0.14039408866995073, 0.14901477832512317, 0.10837438423645321, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15147783251231528, 0.18349753694581281, 0.13423645320197045, 0.13423645320197045, 0.087438423645320201, 0.1145320197044335, 0.1539408866995074, 0.078817733990147784, 0.12561576354679804, 0.13423645320197045, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.66995073891625612, 0.73891625615763545, 0.76354679802955661, 0.77586206896551724, 0.76108374384236455, 0.68103448275862066, 0.76600985221674878, 0.71921182266009853, 0.73768472906403937, 0.81157635467980294], 5: [0.14655172413793102, 0.14655172413793102, 0.1268472906403941, 0.10221674876847291, 0.13669950738916256, 0.12438423645320197, 0.16502463054187191, 0.15517241379310345, 0.15517241379310345, 0.12807881773399016, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15147783251231528, 0.19211822660098521, 0.12315270935960591, 0.11083743842364532, 0.097290640394088676, 0.14655172413793102, 0.17733990147783252, 0.084975369458128072, 0.1539408866995074, 0.11576354679802955, 0.084975369458128072], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.64039408866995073, 0.74014778325123154, 0.7857142857142857, 0.76970443349753692, 0.73152709359605916, 0.67241379310344829, 0.78817733990147787, 0.72167487684729059, 0.77955665024630538, 0.7857142857142857], 5: [0.14655172413793102, 0.16748768472906403, 0.13669950738916256, 0.10344827586206896, 0.13300492610837439, 0.12192118226600986, 0.15024630541871922, 0.1268472906403941, 0.12438423645320197, 0.10467980295566502, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237327 minutes
Weight histogram
[  184   569   703   472   702  1653  4597 10568 10177   750] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
[ 164  277  440  729  929 1561 2842 5647 9042 8744] [ -1.39271215e-04   4.93317071e-05   2.37934629e-04   4.26537551e-04
   6.15140473e-04   8.03743394e-04   9.92346316e-04   1.18094924e-03
   1.36955216e-03   1.55815508e-03   1.74675800e-03]
-1.33497
1.63478
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.37425
Epoch 1, cost is  1.35111
Epoch 2, cost is  1.3339
Epoch 3, cost is  1.32089
Epoch 4, cost is  1.31118
Training took 0.224370 minutes
Weight histogram
[7099 5189 4281 3227 2566 2239 1698 1507 1458 1111] [ -6.32371828e-02  -5.69064817e-02  -5.05757805e-02  -4.42450794e-02
  -3.79143782e-02  -3.15836771e-02  -2.52529760e-02  -1.89222748e-02
  -1.25915737e-02  -6.26087254e-03   6.98286021e-05]
[2360 1461 1831 2114 2571 2994 3279 3967 4776 5022] [ -6.32371828e-02  -5.69064817e-02  -5.05757805e-02  -4.42450794e-02
  -3.79143782e-02  -3.15836771e-02  -2.52529760e-02  -1.89222748e-02
  -1.25915737e-02  -6.26087254e-03   6.98286021e-05]
-1.0271
1.80165
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148746 minutes
Weight histogram
[  69  117  849 2125 3572 4367 6698 9446 4613  544] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
[  465   965  1083   277   424   710  1037  1919  5183 20337] [ -4.75582376e-04  -2.54547870e-04  -3.35133635e-05   1.87521143e-04
   4.08555649e-04   6.29590155e-04   8.50624661e-04   1.07165917e-03
   1.29269367e-03   1.51372818e-03   1.73476269e-03]
-1.36364
1.17757
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48412
Epoch 1, cost is  2.44268
Epoch 2, cost is  2.41517
Epoch 3, cost is  2.39351
Epoch 4, cost is  2.36975
Training took 0.114418 minutes
Weight histogram
[6049 5103 3542 3157 3212 2392 1993 2012 3481 1459] [ -6.27675056e-02  -5.64837722e-02  -5.02000388e-02  -4.39163054e-02
  -3.76325719e-02  -3.13488385e-02  -2.50651051e-02  -1.87813717e-02
  -1.24976382e-02  -6.21390482e-03   6.98286021e-05]
[5004 1521 1711 2167 2567 2968 3519 3982 4473 4488] [ -6.27675056e-02  -5.64837722e-02  -5.02000388e-02  -4.39163054e-02
  -3.76325719e-02  -3.13488385e-02  -2.50651051e-02  -1.87813717e-02
  -1.24976382e-02  -6.21390482e-03   6.98286021e-05]
-1.2603
1.44932
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.14337
Epoch 1, cost is  4.9052
Epoch 2, cost is  4.55119
Epoch 3, cost is  4.0846
Epoch 4, cost is  3.71378
Epoch 5, cost is  3.43283
Epoch 6, cost is  3.18896
Epoch 7, cost is  2.96641
Epoch 8, cost is  2.77807
Epoch 9, cost is  2.62264
Training took 0.544574 minutes
Weight histogram
[4167 4200 2937 6720 1320  492  217  108   56   33] [-0.01303397 -0.01174368 -0.01045338 -0.00916308 -0.00787279 -0.00658249
 -0.00529219 -0.0040019  -0.0027116  -0.0014213  -0.00013101]
[4894 1386 1320 1426 1666 1796 1837 1906 2041 1978] [-0.01303397 -0.01174368 -0.01045338 -0.00916308 -0.00787279 -0.00658249
 -0.00529219 -0.0040019  -0.0027116  -0.0014213  -0.00013101]
-0.212425
0.257095
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090972 minutes
Epoch 0
Fine tuning took 0.090644 minutes
Epoch 0
Fine tuning took 0.090657 minutes
Epoch 0
Fine tuning took 0.089898 minutes
Epoch 0
Fine tuning took 0.091659 minutes
Epoch 0
Fine tuning took 0.091914 minutes
Epoch 0
Fine tuning took 0.091515 minutes
Epoch 0
Fine tuning took 0.091056 minutes
Epoch 0
Fine tuning took 0.092048 minutes
Epoch 0
Fine tuning took 0.089875 minutes
{'zero': {0: [0.19458128078817735, 0.12807881773399016, 0.26354679802955666, 0.19581280788177341, 0.15640394088669951, 0.13054187192118227, 0.14901477832512317, 0.13669950738916256, 0.15640394088669951, 0.18596059113300492, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.62068965517241381, 0.49507389162561577, 0.60591133004926112, 0.61206896551724133, 0.65886699507389157, 0.6428571428571429, 0.63793103448275867, 0.60098522167487689, 0.63177339901477836, 0.71305418719211822], 5: [0.13177339901477833, 0.25123152709359609, 0.2413793103448276, 0.19827586206896552, 0.23152709359605911, 0.2105911330049261, 0.20812807881773399, 0.22536945812807882, 0.24261083743842365, 0.18226600985221675, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19458128078817735, 0.1539408866995074, 0.2857142857142857, 0.17118226600985223, 0.1268472906403941, 0.1354679802955665, 0.16748768472906403, 0.14901477832512317, 0.17241379310344829, 0.2413793103448276, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.61822660098522164, 0.50862068965517238, 0.6280788177339901, 0.6280788177339901, 0.70197044334975367, 0.62684729064039413, 0.65640394088669951, 0.61699507389162567, 0.58866995073891626, 0.68965517241379315], 5: [0.13177339901477833, 0.22783251231527094, 0.20566502463054187, 0.200738916