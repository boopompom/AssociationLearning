Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
Ambivalent
... data manager created. project_root: ExperimentADBN_ambi
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106852 minutes
Weight histogram
[1402  788 3584 5802 5192 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  134   146   224   296   463   707   739  1337  3532 14697] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-1.92963
2.02283
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.99386
Epoch 1, cost is  3.95869
Epoch 2, cost is  3.92736
Epoch 3, cost is  3.90256
Epoch 4, cost is  3.8655
Training took 0.076188 minutes
Weight histogram
[7044 5020 3596 3350  523  418 1508  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2473 1842 1873 1786 2079 1841 2093 2358 2846 3084] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.3767
2.05991
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149217 minutes
Weight histogram
[  48  106  545 1430 1634 7400 8759 4084  291    3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 10620   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116266 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 2859 2202  490] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.05097
Epoch 1, cost is  5.73882
Epoch 2, cost is  5.60944
Epoch 3, cost is  5.44454
Epoch 4, cost is  5.20096
Epoch 5, cost is  4.98451
Epoch 6, cost is  4.77828
Epoch 7, cost is  4.58458
Epoch 8, cost is  4.40715
Epoch 9, cost is  4.24974
Training took 0.180412 minutes
Weight histogram
[759 805 932 690 332 236 174  74  32  16] [ -2.92806122e-02  -2.63623666e-02  -2.34441209e-02  -2.05258753e-02
  -1.76076296e-02  -1.46893840e-02  -1.17711383e-02  -8.85289264e-03
  -5.93464699e-03  -3.01640133e-03  -9.81556732e-05]
[366 671 547 334 349 350 349 344 356 384] [ -2.92806122e-02  -2.63623666e-02  -2.34441209e-02  -2.05258753e-02
  -1.76076296e-02  -1.46893840e-02  -1.17711383e-02  -8.85289264e-03
  -5.93464699e-03  -3.01640133e-03  -9.81556732e-05]
-0.363524
0.348245
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041514 minutes
Epoch 0
Fine tuning took 0.043063 minutes
Epoch 0
Fine tuning took 0.040410 minutes
Epoch 0
Fine tuning took 0.042915 minutes
Epoch 0
Fine tuning took 0.042139 minutes
Epoch 0
Fine tuning took 0.043231 minutes
Epoch 0
Fine tuning took 0.041815 minutes
Epoch 0
Fine tuning took 0.041737 minutes
Epoch 0
Fine tuning took 0.041861 minutes
Epoch 0
Fine tuning took 0.044380 minutes
{'zero': {0: [0.29802955665024633, 0.12931034482758622, 0.16748768472906403, 0.11822660098522167, 0.094827586206896547, 0.099753694581280791, 0.092364532019704432, 0.14039408866995073, 0.16748768472906403, 0.16009852216748768, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53078817733990147, 0.70812807881773399, 0.6354679802955665, 0.67487684729064035, 0.72536945812807885, 0.61945812807881773, 0.66871921182266014, 0.57635467980295563, 0.65024630541871919, 0.55911330049261088, 0.6280788177339901], 5: [0.17118226600985223, 0.1625615763546798, 0.19704433497536947, 0.20689655172413793, 0.17980295566502463, 0.28078817733990147, 0.23891625615763548, 0.28325123152709358, 0.18226600985221675, 0.28078817733990147, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29802955665024633, 0.17610837438423646, 0.19211822660098521, 0.11945812807881774, 0.084975369458128072, 0.087438423645320201, 0.13669950738916256, 0.15270935960591134, 0.1354679802955665, 0.19088669950738915, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53078817733990147, 0.47413793103448276, 0.60221674876847286, 0.63669950738916259, 0.6785714285714286, 0.47536945812807879, 0.66133004926108374, 0.58990147783251234, 0.68965517241379315, 0.6145320197044335, 0.59113300492610843], 5: [0.17118226600985223, 0.34975369458128081, 0.20566502463054187, 0.24384236453201971, 0.23645320197044334, 0.43719211822660098, 0.2019704433497537, 0.25738916256157635, 0.1748768472906404, 0.19458128078817735, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29802955665024633, 0.17610837438423646, 0.17733990147783252, 0.13793103448275862, 0.11083743842364532, 0.10344827586206896, 0.13177339901477833, 0.15024630541871922, 0.1206896551724138, 0.18719211822660098, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53078817733990147, 0.51600985221674878, 0.60467980295566504, 0.66871921182266014, 0.6576354679802956, 0.49384236453201968, 0.66502463054187189, 0.61699507389162567, 0.70320197044334976, 0.58251231527093594, 0.56280788177339902], 5: [0.17118226600985223, 0.30788177339901479, 0.21798029556650247, 0.19334975369458129, 0.23152709359605911, 0.40270935960591131, 0.20320197044334976, 0.23275862068965517, 0.17610837438423646, 0.23029556650246305, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29802955665024633, 0.16748768472906403, 0.1748768472906404, 0.10467980295566502, 0.067733990147783252, 0.082512315270935957, 0.10960591133004927, 0.12561576354679804, 0.098522167487684734, 0.14162561576354679, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53078817733990147, 0.46674876847290642, 0.6145320197044335, 0.64039408866995073, 0.66502463054187189, 0.42610837438423643, 0.67487684729064035, 0.58743842364532017, 0.76724137931034486, 0.63793103448275867, 0.59605911330049266], 5: [0.17118226600985223, 0.36576354679802958, 0.2105911330049261, 0.25492610837438423, 0.26724137931034481, 0.49137931034482757, 0.21551724137931033, 0.28694581280788178, 0.13423645320197045, 0.22044334975369459, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107356 minutes
Weight histogram
[1402  788 3584 5802 5192 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  134   146   224   296   463   707   739  1337  3532 14697] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-1.92963
2.02283
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.99386
Epoch 1, cost is  3.95869
Epoch 2, cost is  3.92736
Epoch 3, cost is  3.90256
Epoch 4, cost is  3.8655
Training took 0.074599 minutes
Weight histogram
[7044 5020 3596 3350  523  418 1508  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2473 1842 1873 1786 2079 1841 2093 2358 2846 3084] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.3767
2.05991
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147707 minutes
Weight histogram
[  48  106  545 1430 1634 7400 8759 4084  291    3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 10620   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116009 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 2859 2202  490] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.57083
Epoch 1, cost is  5.35914
Epoch 2, cost is  5.2593
Epoch 3, cost is  5.02267
Epoch 4, cost is  4.73346
Epoch 5, cost is  4.46936
Epoch 6, cost is  4.23158
Epoch 7, cost is  4.03452
Epoch 8, cost is  3.85716
Epoch 9, cost is  3.6995
Training took 0.244611 minutes
Weight histogram
[1027 2034  441  230  132   80   50   29   16   11] [-0.0187807  -0.01691845 -0.0150562  -0.01319395 -0.0113317  -0.00946945
 -0.0076072  -0.00574495 -0.0038827  -0.00202045 -0.0001582 ]
[1026  402  261  295  296  304  327  357  377  405] [-0.0187807  -0.01691845 -0.0150562  -0.01319395 -0.0113317  -0.00946945
 -0.0076072  -0.00574495 -0.0038827  -0.00202045 -0.0001582 ]
-0.330819
0.270692
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044501 minutes
Epoch 0
Fine tuning took 0.045037 minutes
Epoch 0
Fine tuning took 0.046446 minutes
Epoch 0
Fine tuning took 0.045950 minutes
Epoch 0
Fine tuning took 0.045116 minutes
Epoch 0
Fine tuning took 0.044559 minutes
Epoch 0
Fine tuning took 0.047086 minutes
Epoch 0
Fine tuning took 0.044627 minutes
Epoch 0
Fine tuning took 0.044925 minutes
Epoch 0
Fine tuning took 0.045681 minutes
{'zero': {0: [0.24384236453201971, 0.2019704433497537, 0.16009852216748768, 0.17241379310344829, 0.16995073891625614, 0.18226600985221675, 0.18103448275862069, 0.18226600985221675, 0.20073891625615764, 0.18226600985221675, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.5788177339901478, 0.56280788177339902, 0.52955665024630538, 0.55911330049261088, 0.53448275862068961, 0.52586206896551724, 0.49507389162561577, 0.48152709359605911, 0.55172413793103448, 0.44581280788177341], 5: [0.20935960591133004, 0.21921182266009853, 0.27709359605911332, 0.29802955665024633, 0.27093596059113301, 0.28325123152709358, 0.29310344827586204, 0.32266009852216748, 0.31773399014778325, 0.26600985221674878, 0.30541871921182268], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24384236453201971, 0.18472906403940886, 0.17118226600985223, 0.16133004926108374, 0.19088669950738915, 0.20073891625615764, 0.18965517241379309, 0.18103448275862069, 0.21798029556650247, 0.18842364532019704, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.58620689655172409, 0.57512315270935965, 0.58620689655172409, 0.53448275862068961, 0.52339901477832518, 0.55295566502463056, 0.52955665024630538, 0.51354679802955661, 0.53694581280788178, 0.47044334975369456], 5: [0.20935960591133004, 0.22906403940886699, 0.2536945812807882, 0.25246305418719212, 0.27463054187192121, 0.27586206896551724, 0.25738916256157635, 0.2894088669950739, 0.26847290640394089, 0.27463054187192121, 0.28078817733990147], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24384236453201971, 0.19088669950738915, 0.16009852216748768, 0.1748768472906404, 0.16133004926108374, 0.18472906403940886, 0.20566502463054187, 0.16995073891625614, 0.20566502463054187, 0.19088669950738915, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.59359605911330049, 0.5714285714285714, 0.58497536945812811, 0.54679802955665024, 0.53817733990147787, 0.50492610837438423, 0.53940886699507384, 0.47660098522167488, 0.53694581280788178, 0.50246305418719217], 5: [0.20935960591133004, 0.21551724137931033, 0.26847290640394089, 0.24014778325123154, 0.29187192118226601, 0.27709359605911332, 0.2894088669950739, 0.29064039408866993, 0.31773399014778325, 0.27216748768472904, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24384236453201971, 0.19704433497536947, 0.16995073891625614, 0.19088669950738915, 0.15886699507389163, 0.20073891625615764, 0.18226600985221675, 0.16625615763546797, 0.19458128078817735, 0.2105911330049261, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.60960591133004927, 0.57389162561576357, 0.58128078817733986, 0.57389162561576357, 0.50985221674876846, 0.52586206896551724, 0.56650246305418717, 0.48768472906403942, 0.54064039408866993, 0.45197044334975367], 5: [0.20935960591133004, 0.19334975369458129, 0.25615763546798032, 0.22783251231527094, 0.26724137931034481, 0.2894088669950739, 0.29187192118226601, 0.26724137931034481, 0.31773399014778325, 0.24876847290640394, 0.2894088669950739], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106945 minutes
Weight histogram
[1402  788 3584 5802 5192 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  134   146   224   296   463   707   739  1337  3532 14697] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-1.92963
2.02283
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.99386
Epoch 1, cost is  3.95869
Epoch 2, cost is  3.92736
Epoch 3, cost is  3.90256
Epoch 4, cost is  3.8655
Training took 0.074143 minutes
Weight histogram
[7044 5020 3596 3350  523  418 1508  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2473 1842 1873 1786 2079 1841 2093 2358 2846 3084] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.3767
2.05991
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149298 minutes
Weight histogram
[  48  106  545 1430 1634 7400 8759 4084  291    3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 10620   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116032 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 2859 2202  490] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.44708
Epoch 1, cost is  5.32213
Epoch 2, cost is  5.05498
Epoch 3, cost is  4.62725
Epoch 4, cost is  4.25967
Epoch 5, cost is  3.96746
Epoch 6, cost is  3.7233
Epoch 7, cost is  3.51515
Epoch 8, cost is  3.33839
Epoch 9, cost is  3.18304
Training took 0.335390 minutes
Weight histogram
[ 871  807  617  591 1034   73   29   13    9    6] [-0.01086076 -0.00978651 -0.00871225 -0.007638   -0.00656375 -0.0054895
 -0.00441524 -0.00334099 -0.00226674 -0.00119249 -0.00011823]
[1024  257  264  275  291  334  360  387  410  448] [-0.01086076 -0.00978651 -0.00871225 -0.007638   -0.00656375 -0.0054895
 -0.00441524 -0.00334099 -0.00226674 -0.00119249 -0.00011823]
-0.204724
0.227813
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050801 minutes
Epoch 0
Fine tuning took 0.049958 minutes
Epoch 0
Fine tuning took 0.048891 minutes
Epoch 0
Fine tuning took 0.049017 minutes
Epoch 0
Fine tuning took 0.052134 minutes
Epoch 0
Fine tuning took 0.049076 minutes
Epoch 0
Fine tuning took 0.050764 minutes
Epoch 0
Fine tuning took 0.050144 minutes
Epoch 0
Fine tuning took 0.052062 minutes
Epoch 0
Fine tuning took 0.050858 minutes
{'zero': {0: [0.25, 0.20566502463054187, 0.26724137931034481, 0.2229064039408867, 0.21798029556650247, 0.19704433497536947, 0.24876847290640394, 0.21551724137931033, 0.24384236453201971, 0.29064039408866993, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.48645320197044334, 0.44704433497536944, 0.48029556650246308, 0.45073891625615764, 0.45443349753694579, 0.42241379310344829, 0.47413793103448276, 0.40270935960591131, 0.45689655172413796, 0.37561576354679804], 5: [0.23645320197044334, 0.30788177339901479, 0.2857142857142857, 0.29679802955665024, 0.33128078817733991, 0.34852216748768472, 0.3288177339901478, 0.31034482758620691, 0.35344827586206895, 0.25246305418719212, 0.34236453201970446], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25, 0.22783251231527094, 0.31157635467980294, 0.19088669950738915, 0.22660098522167488, 0.19950738916256158, 0.25, 0.24261083743842365, 0.28078817733990147, 0.2894088669950739, 0.27709359605911332], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.50246305418719217, 0.4211822660098522, 0.51847290640394084, 0.46798029556650245, 0.44458128078817732, 0.43965517241379309, 0.48522167487684731, 0.40517241379310343, 0.45812807881773399, 0.38300492610837439], 5: [0.23645320197044334, 0.26970443349753692, 0.26724137931034481, 0.29064039408866993, 0.30541871921182268, 0.35591133004926107, 0.31034482758620691, 0.27216748768472904, 0.31403940886699505, 0.25246305418719212, 0.33990147783251229], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25, 0.19950738916256158, 0.27216748768472904, 0.21921182266009853, 0.21798029556650247, 0.18719211822660098, 0.24630541871921183, 0.21182266009852216, 0.26724137931034481, 0.31157635467980294, 0.29187192118226601], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.50615763546798032, 0.44704433497536944, 0.48275862068965519, 0.41133004926108374, 0.45320197044334976, 0.45935960591133007, 0.49014778325123154, 0.39901477832512317, 0.44088669950738918, 0.3854679802955665], 5: [0.23645320197044334, 0.29433497536945813, 0.28078817733990147, 0.29802955665024633, 0.37068965517241381, 0.35960591133004927, 0.29433497536945813, 0.29802955665024633, 0.33374384236453203, 0.24753694581280788, 0.32266009852216748], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25, 0.22413793103448276, 0.27832512315270935, 0.2376847290640394, 0.24753694581280788, 0.20566502463054187, 0.2413793103448276, 0.20566502463054187, 0.25492610837438423, 0.27093596059113301, 0.30172413793103448], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.4963054187192118, 0.42980295566502463, 0.44704433497536944, 0.43349753694581283, 0.39778325123152708, 0.44704433497536944, 0.48768472906403942, 0.41133004926108374, 0.46674876847290642, 0.3817733990147783], 5: [0.23645320197044334, 0.27955665024630544, 0.29187192118226601, 0.31527093596059114, 0.31896551724137934, 0.39655172413793105, 0.31157635467980294, 0.30665024630541871, 0.33374384236453203, 0.26231527093596058, 0.31650246305418717], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105486 minutes
Weight histogram
[1402  788 3584 5802 5192 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  134   146   224   296   463   707   739  1337  3532 14697] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-1.92963
2.02283
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.99386
Epoch 1, cost is  3.95869
Epoch 2, cost is  3.92736
Epoch 3, cost is  3.90256
Epoch 4, cost is  3.8655
Training took 0.072026 minutes
Weight histogram
[7044 5020 3596 3350  523  418 1508  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2473 1842 1873 1786 2079 1841 2093 2358 2846 3084] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.3767
2.05991
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149313 minutes
Weight histogram
[  48  106  545 1430 1634 7400 8759 4084  291    3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 10620   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.115050 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 2859 2202  490] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.43441
Epoch 1, cost is  5.25369
Epoch 2, cost is  4.78689
Epoch 3, cost is  4.28702
Epoch 4, cost is  3.88717
Epoch 5, cost is  3.58108
Epoch 6, cost is  3.33037
Epoch 7, cost is  3.13124
Epoch 8, cost is  2.9563
Epoch 9, cost is  2.81012
Training took 0.535756 minutes
Weight histogram
[823 750 556 497 424 966  19   7   4   4] [-0.00572379 -0.00516195 -0.00460012 -0.00403828 -0.00347644 -0.0029146
 -0.00235276 -0.00179092 -0.00122909 -0.00066725 -0.00010541]
[912 265 267 278 304 333 364 400 437 490] [-0.00572379 -0.00516195 -0.00460012 -0.00403828 -0.00347644 -0.0029146
 -0.00235276 -0.00179092 -0.00122909 -0.00066725 -0.00010541]
-0.179335
0.195032
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.062488 minutes
Epoch 0
Fine tuning took 0.059943 minutes
Epoch 0
Fine tuning took 0.060776 minutes
Epoch 0
Fine tuning took 0.061446 minutes
Epoch 0
Fine tuning took 0.060790 minutes
Epoch 0
Fine tuning took 0.060569 minutes
Epoch 0
Fine tuning took 0.060033 minutes
Epoch 0
Fine tuning took 0.061812 minutes
Epoch 0
Fine tuning took 0.062619 minutes
Epoch 0
Fine tuning took 0.062527 minutes
{'zero': {0: [0.2536945812807882, 0.27339901477832512, 0.2536945812807882, 0.28817733990147781, 0.23399014778325122, 0.29187192118226601, 0.30541871921182268, 0.24261083743842365, 0.26724137931034481, 0.24384236453201971, 0.31896551724137934], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45689655172413796, 0.41748768472906406, 0.46674876847290642, 0.38054187192118227, 0.42364532019704432, 0.36699507389162561, 0.40640394088669951, 0.40763546798029554, 0.35098522167487683, 0.43719211822660098, 0.35467980295566504], 5: [0.2894088669950739, 0.30911330049261082, 0.27955665024630544, 0.33128078817733991, 0.34236453201970446, 0.34113300492610837, 0.28817733990147781, 0.34975369458128081, 0.3817733990147783, 0.31896551724137934, 0.32635467980295568], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2536945812807882, 0.23645320197044334, 0.23891625615763548, 0.25, 0.2376847290640394, 0.27093596059113301, 0.27093596059113301, 0.27463054187192121, 0.23645320197044334, 0.27586206896551724, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45689655172413796, 0.43965517241379309, 0.48399014778325122, 0.4248768472906404, 0.40024630541871919, 0.37684729064039407, 0.44334975369458129, 0.42610837438423643, 0.37807881773399016, 0.44704433497536944, 0.33990147783251229], 5: [0.2894088669950739, 0.32389162561576357, 0.27709359605911332, 0.3251231527093596, 0.36206896551724138, 0.35221674876847292, 0.2857142857142857, 0.29926108374384236, 0.3854679802955665, 0.27709359605911332, 0.33374384236453203], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2536945812807882, 0.24507389162561577, 0.23891625615763548, 0.26600985221674878, 0.26108374384236455, 0.28448275862068967, 0.2857142857142857, 0.21798029556650247, 0.24384236453201971, 0.2857142857142857, 0.32142857142857145], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45689655172413796, 0.43842364532019706, 0.46551724137931033, 0.40640394088669951, 0.39285714285714285, 0.39532019704433496, 0.44211822660098521, 0.42980295566502463, 0.37438423645320196, 0.44704433497536944, 0.33990147783251229], 5: [0.2894088669950739, 0.31650246305418717, 0.29556650246305421, 0.32758620689655171, 0.3460591133004926, 0.32019704433497537, 0.27216748768472904, 0.35221674876847292, 0.3817733990147783, 0.26724137931034481, 0.33866995073891626], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2536945812807882, 0.24753694581280788, 0.24630541871921183, 0.25, 0.2536945812807882, 0.28201970443349755, 0.28201970443349755, 0.25492610837438423, 0.24014778325123154, 0.26600985221674878, 0.33004926108374383], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.45689655172413796, 0.44827586206896552, 0.43842364532019706, 0.42733990147783252, 0.43226600985221675, 0.3682266009852217, 0.4039408866995074, 0.45320197044334976, 0.3891625615763547, 0.44827586206896552, 0.35467980295566504], 5: [0.2894088669950739, 0.30418719211822659, 0.31527093596059114, 0.32266009852216748, 0.31403940886699505, 0.34975369458128081, 0.31403940886699505, 0.29187192118226601, 0.37068965517241381, 0.2857142857142857, 0.31527093596059114], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148673 minutes
Weight histogram
[ 168  443  678  641 1495 4511 4303 7208 2433  395] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
[  134   154   231   324   476   854   999  1981  6517 10605] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
-1.05152
1.08348
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.63043
Epoch 1, cost is  2.59119
Epoch 2, cost is  2.55994
Epoch 3, cost is  2.53863
Epoch 4, cost is  2.51084
Training took 0.115637 minutes
Weight histogram
[5259 3238 2785 2566 1920 1714 2003 1104 1424  262] [ -5.38926944e-02  -4.85003980e-02  -4.31081015e-02  -3.77158050e-02
  -3.23235086e-02  -2.69312121e-02  -2.15389157e-02  -1.61466192e-02
  -1.07543227e-02  -5.36202628e-03   3.02701774e-05]
[2535 1358 1264 1511 1821 2158 2488 2708 3201 3231] [ -5.38926944e-02  -4.85003980e-02  -4.31081015e-02  -3.77158050e-02
  -3.23235086e-02  -2.69312121e-02  -2.15389157e-02  -1.61466192e-02
  -1.07543227e-02  -5.36202628e-03   3.02701774e-05]
-1.13894
1.50519
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148731 minutes
Weight histogram
[ 223  554 1069 1208 2505 6212 6499 4349 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  273   309   458   634   960  1451   845  1963  4387 13020] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.115457 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 2013 3060  478] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.22997
Epoch 1, cost is  5.98599
Epoch 2, cost is  5.82885
Epoch 3, cost is  5.60062
Epoch 4, cost is  5.2413
Epoch 5, cost is  4.90184
Epoch 6, cost is  4.63348
Epoch 7, cost is  4.41737
Epoch 8, cost is  4.23941
Epoch 9, cost is  4.08621
Training took 0.206484 minutes
Weight histogram
[484 464 442 410 855 765 430 154  32  14] [-0.02656188 -0.0239213  -0.02128071 -0.01864013 -0.01599955 -0.01335896
 -0.01071838 -0.00807779 -0.00543721 -0.00279663 -0.00015604]
[895 602 279 245 268 297 324 351 381 408] [-0.02656188 -0.0239213  -0.02128071 -0.01864013 -0.01599955 -0.01335896
 -0.01071838 -0.00807779 -0.00543721 -0.00279663 -0.00015604]
-0.348383
0.437969
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052833 minutes
Epoch 0
Fine tuning took 0.054837 minutes
Epoch 0
Fine tuning took 0.053654 minutes
Epoch 0
Fine tuning took 0.052690 minutes
Epoch 0
Fine tuning took 0.054382 minutes
Epoch 0
Fine tuning took 0.053082 minutes
Epoch 0
Fine tuning took 0.052907 minutes
Epoch 0
Fine tuning took 0.052862 minutes
Epoch 0
Fine tuning took 0.052977 minutes
Epoch 0
Fine tuning took 0.053281 minutes
{'zero': {0: [0.35837438423645318, 0.14408866995073891, 0.15270935960591134, 0.11576354679802955, 0.16009852216748768, 0.10714285714285714, 0.13916256157635468, 0.059113300492610835, 0.11822660098522167, 0.13423645320197045, 0.093596059113300489], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44950738916256155, 0.57266009852216748, 0.67733990147783252, 0.64408866995073888, 0.68472906403940892, 0.5073891625615764, 0.61330049261083741, 0.64901477832512311, 0.56280788177339902, 0.55172413793103448, 0.66871921182266014], 5: [0.19211822660098521, 0.28325123152709358, 0.16995073891625614, 0.24014778325123154, 0.15517241379310345, 0.3854679802955665, 0.24753694581280788, 0.29187192118226601, 0.31896551724137934, 0.31403940886699505, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.35837438423645318, 0.22660098522167488, 0.16379310344827586, 0.19704433497536947, 0.25862068965517243, 0.16502463054187191, 0.32266009852216748, 0.1625615763546798, 0.21305418719211822, 0.21798029556650247, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44950738916256155, 0.47044334975369456, 0.70443349753694584, 0.61699507389162567, 0.61083743842364535, 0.45073891625615764, 0.49261083743842365, 0.65640394088669951, 0.53325123152709364, 0.57758620689655171, 0.67241379310344829], 5: [0.19211822660098521, 0.30295566502463056, 0.13177339901477833, 0.18596059113300492, 0.13054187192118227, 0.38423645320197042, 0.18472906403940886, 0.18103448275862069, 0.2536945812807882, 0.20443349753694581, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.35837438423645318, 0.22660098522167488, 0.11083743842364532, 0.18596059113300492, 0.26724137931034481, 0.15763546798029557, 0.30788177339901479, 0.13669950738916256, 0.22167487684729065, 0.24507389162561577, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44950738916256155, 0.46674876847290642, 0.73645320197044339, 0.64532019704433496, 0.61083743842364535, 0.44827586206896552, 0.49753694581280788, 0.65024630541871919, 0.54064039408866993, 0.53325123152709364, 0.69827586206896552], 5: [0.19211822660098521, 0.30665024630541871, 0.15270935960591134, 0.16871921182266009, 0.12192118226600986, 0.39408866995073893, 0.19458128078817735, 0.21305418719211822, 0.2376847290640394, 0.22167487684729065, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.35837438423645318, 0.25985221674876846, 0.16748768472906403, 0.20689655172413793, 0.25862068965517243, 0.15886699507389163, 0.30049261083743845, 0.16379310344827586, 0.21674876847290642, 0.22660098522167488, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44950738916256155, 0.42980295566502463, 0.70073891625615758, 0.60591133004926112, 0.63669950738916259, 0.44827586206896552, 0.48152709359605911, 0.64408866995073888, 0.55541871921182262, 0.53325123152709364, 0.70073891625615758], 5: [0.19211822660098521, 0.31034482758620691, 0.13177339901477833, 0.18719211822660098, 0.10467980295566502, 0.39285714285714285, 0.21798029556650247, 0.19211822660098521, 0.22783251231527094, 0.24014778325123154, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150576 minutes
Weight histogram
[ 168  443  678  641 1495 4511 4303 7208 2433  395] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
[  134   154   231   324   476   854   999  1981  6517 10605] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
-1.05152
1.08348
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.63043
Epoch 1, cost is  2.59119
Epoch 2, cost is  2.55994
Epoch 3, cost is  2.53863
Epoch 4, cost is  2.51084
Training took 0.115235 minutes
Weight histogram
[5259 3238 2785 2566 1920 1714 2003 1104 1424  262] [ -5.38926944e-02  -4.85003980e-02  -4.31081015e-02  -3.77158050e-02
  -3.23235086e-02  -2.69312121e-02  -2.15389157e-02  -1.61466192e-02
  -1.07543227e-02  -5.36202628e-03   3.02701774e-05]
[2535 1358 1264 1511 1821 2158 2488 2708 3201 3231] [ -5.38926944e-02  -4.85003980e-02  -4.31081015e-02  -3.77158050e-02
  -3.23235086e-02  -2.69312121e-02  -2.15389157e-02  -1.61466192e-02
  -1.07543227e-02  -5.36202628e-03   3.02701774e-05]
-1.13894
1.50519
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150747 minutes
Weight histogram
[ 223  554 1069 1208 2505 6212 6499 4349 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  273   309   458   634   960  1451   845  1963  4387 13020] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116515 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 2013 3060  478] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.62072
Epoch 1, cost is  5.39663
Epoch 2, cost is  5.25207
Epoch 3, cost is  4.94203
Epoch 4, cost is  4.534
Epoch 5, cost is  4.21938
Epoch 6, cost is  3.96346
Epoch 7, cost is  3.7378
Epoch 8, cost is  3.5403
Epoch 9, cost is  3.37442
Training took 0.297436 minutes
Weight histogram
[ 797  637  568 1450  365  122   59   28   15    9] [-0.01784189 -0.0160722  -0.01430251 -0.01253282 -0.01076313 -0.00899344
 -0.00722375 -0.00545406 -0.00368437 -0.00191468 -0.00014499]
[1188  305  227  259  288  314  334  358  374  403] [-0.01784189 -0.0160722  -0.01430251 -0.01253282 -0.01076313 -0.00899344
 -0.00722375 -0.00545406 -0.00368437 -0.00191468 -0.00014499]
-0.276674
0.336729
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.060462 minutes
Epoch 0
Fine tuning took 0.058613 minutes
Epoch 0
Fine tuning took 0.059162 minutes
Epoch 0
Fine tuning took 0.058254 minutes
Epoch 0
Fine tuning took 0.059655 minutes
Epoch 0
Fine tuning took 0.058289 minutes
Epoch 0
Fine tuning took 0.060022 minutes
Epoch 0
Fine tuning took 0.059574 minutes
Epoch 0
Fine tuning took 0.058760 minutes
Epoch 0
Fine tuning took 0.060038 minutes
{'zero': {0: [0.24876847290640394, 0.18472906403940886, 0.18965517241379309, 0.16502463054187191, 0.1539408866995074, 0.19088669950738915, 0.16502463054187191, 0.1625615763546798, 0.18349753694581281, 0.19458128078817735, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.58497536945812811, 0.59482758620689657, 0.58004926108374388, 0.59975369458128081, 0.50246305418719217, 0.57512315270935965, 0.56527093596059108, 0.50985221674876846, 0.5073891625615764, 0.51231527093596063], 5: [0.18719211822660098, 0.23029556650246305, 0.21551724137931033, 0.25492610837438423, 0.24630541871921183, 0.30665024630541871, 0.25985221674876846, 0.27216748768472904, 0.30665024630541871, 0.29802955665024633, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24876847290640394, 0.18472906403940886, 0.19334975369458129, 0.17364532019704434, 0.14778325123152711, 0.21551724137931033, 0.18226600985221675, 0.16502463054187191, 0.17733990147783252, 0.20566502463054187, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.57019704433497542, 0.59605911330049266, 0.6428571428571429, 0.59113300492610843, 0.51724137931034486, 0.57635467980295563, 0.60098522167487689, 0.56896551724137934, 0.55911330049261088, 0.49876847290640391], 5: [0.18719211822660098, 0.24507389162561577, 0.2105911330049261, 0.18349753694581281, 0.26108374384236455, 0.26724137931034481, 0.2413793103448276, 0.23399014778325122, 0.2536945812807882, 0.23522167487684728, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24876847290640394, 0.21182266009852216, 0.19088669950738915, 0.16995073891625614, 0.17364532019704434, 0.20566502463054187, 0.17241379310344829, 0.15886699507389163, 0.17118226600985223, 0.24261083743842365, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.56650246305418717, 0.56896551724137934, 0.65517241379310343, 0.59605911330049266, 0.52832512315270941, 0.6071428571428571, 0.58374384236453203, 0.53940886699507384, 0.52463054187192115, 0.54064039408866993], 5: [0.18719211822660098, 0.22167487684729065, 0.24014778325123154, 0.1748768472906404, 0.23029556650246305, 0.26600985221674878, 0.22044334975369459, 0.25738916256157635, 0.2894088669950739, 0.23275862068965517, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24876847290640394, 0.20073891625615764, 0.17118226600985223, 0.16133004926108374, 0.15270935960591134, 0.19581280788177341, 0.16748768472906403, 0.13916256157635468, 0.16625615763546797, 0.21551724137931033, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.56157635467980294, 0.61945812807881773, 0.6428571428571429, 0.59482758620689657, 0.53694581280788178, 0.58743842364532017, 0.61330049261083741, 0.59359605911330049, 0.5431034482758621, 0.54926108374384242], 5: [0.18719211822660098, 0.2376847290640394, 0.20935960591133004, 0.19581280788177341, 0.25246305418719212, 0.26724137931034481, 0.24507389162561577, 0.24753694581280788, 0.24014778325123154, 0.2413793103448276, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148580 minutes
Weight histogram
[ 168  443  678  641 1495 4511 4303 7208 2433  395] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
[  134   154   231   324   476   854   999  1981  6517 10605] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
-1.05152
1.08348
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.63043
Epoch 1, cost is  2.59119
Epoch 2, cost is  2.55994
Epoch 3, cost is  2.53863
Epoch 4, cost is  2.51084
Training took 0.113727 minutes
Weight histogram
[5259 3238 2785 2566 1920 1714 2003 1104 1424  262] [ -5.38926944e-02  -4.85003980e-02  -4.31081015e-02  -3.77158050e-02
  -3.23235086e-02  -2.69312121e-02  -2.15389157e-02  -1.61466192e-02
  -1.07543227e-02  -5.36202628e-03   3.02701774e-05]
[2535 1358 1264 1511 1821 2158 2488 2708 3201 3231] [ -5.38926944e-02  -4.85003980e-02  -4.31081015e-02  -3.77158050e-02
  -3.23235086e-02  -2.69312121e-02  -2.15389157e-02  -1.61466192e-02
  -1.07543227e-02  -5.36202628e-03   3.02701774e-05]
-1.13894
1.50519
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150024 minutes
Weight histogram
[ 223  554 1069 1208 2505 6212 6499 4349 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  273   309   458   634   960  1451   845  1963  4387 13020] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116352 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 2013 3060  478] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.26069
Epoch 1, cost is  5.11198
Epoch 2, cost is  4.91613
Epoch 3, cost is  4.4776
Epoch 4, cost is  4.09515
Epoch 5, cost is  3.80007
Epoch 6, cost is  3.54525
Epoch 7, cost is  3.3104
Epoch 8, cost is  3.11478
Epoch 9, cost is  2.9548
Training took 0.413676 minutes
Weight histogram
[1007  905 1691  231  101   54   30   16    9    6] [-0.01199183 -0.01080513 -0.00961844 -0.00843174 -0.00724504 -0.00605834
 -0.00487164 -0.00368495 -0.00249825 -0.00131155 -0.00012485]
[1109  254  237  277  304  333  346  363  395  432] [-0.01199183 -0.01080513 -0.00961844 -0.00843174 -0.00724504 -0.00605834
 -0.00487164 -0.00368495 -0.00249825 -0.00131155 -0.00012485]
-0.213143
0.260389
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.064568 minutes
Epoch 0
Fine tuning took 0.063665 minutes
Epoch 0
Fine tuning took 0.063578 minutes
Epoch 0
Fine tuning took 0.065239 minutes
Epoch 0
Fine tuning took 0.063401 minutes
Epoch 0
Fine tuning took 0.065086 minutes
Epoch 0
Fine tuning took 0.064788 minutes
Epoch 0
Fine tuning took 0.063790 minutes
Epoch 0
Fine tuning took 0.064243 minutes
Epoch 0
Fine tuning took 0.064713 minutes
{'zero': {0: [0.21921182266009853, 0.18103448275862069, 0.18719211822660098, 0.19827586206896552, 0.20689655172413793, 0.19211822660098521, 0.25123152709359609, 0.19211822660098521, 0.23399014778325122, 0.22167487684729065, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55788177339901479, 0.56773399014778325, 0.55172413793103448, 0.50615763546798032, 0.51108374384236455, 0.4642857142857143, 0.50862068965517238, 0.53078817733990147, 0.47660098522167488, 0.50862068965517238, 0.50246305418719217], 5: [0.2229064039408867, 0.25123152709359609, 0.26108374384236455, 0.29556650246305421, 0.28201970443349755, 0.34359605911330049, 0.24014778325123154, 0.27709359605911332, 0.2894088669950739, 0.26970443349753692, 0.29556650246305421], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21921182266009853, 0.17980295566502463, 0.18965517241379309, 0.18103448275862069, 0.18472906403940886, 0.18842364532019704, 0.23891625615763548, 0.20073891625615764, 0.22413793103448276, 0.23522167487684728, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55788177339901479, 0.58866995073891626, 0.53325123152709364, 0.53694581280788178, 0.49753694581280788, 0.49507389162561577, 0.50369458128078815, 0.50862068965517238, 0.45073891625615764, 0.5, 0.49384236453201968], 5: [0.2229064039408867, 0.23152709359605911, 0.27709359605911332, 0.28201970443349755, 0.31773399014778325, 0.31650246305418717, 0.25738916256157635, 0.29064039408866993, 0.3251231527093596, 0.26477832512315269, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21921182266009853, 0.19581280788177341, 0.2105911330049261, 0.16995073891625614, 0.1748768472906404, 0.18965517241379309, 0.2376847290640394, 0.17857142857142858, 0.2413793103448276, 0.23029556650246305, 0.25], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55788177339901479, 0.55541871921182262, 0.51231527093596063, 0.54433497536945807, 0.50492610837438423, 0.50246305418719217, 0.51108374384236455, 0.53940886699507384, 0.47167487684729065, 0.5, 0.47783251231527096], 5: [0.2229064039408867, 0.24876847290640394, 0.27709359605911332, 0.2857142857142857, 0.32019704433497537, 0.30788177339901479, 0.25123152709359609, 0.28201970443349755, 0.28694581280788178, 0.26970443349753692, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21921182266009853, 0.18842364532019704, 0.20073891625615764, 0.18842364532019704, 0.19950738916256158, 0.18842364532019704, 0.22413793103448276, 0.18842364532019704, 0.24507389162561577, 0.24507389162561577, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55788177339901479, 0.56527093596059108, 0.50862068965517238, 0.49014778325123154, 0.49753694581280788, 0.50123152709359609, 0.47413793103448276, 0.5431034482758621, 0.45935960591133007, 0.50123152709359609, 0.44211822660098521], 5: [0.2229064039408867, 0.24630541871921183, 0.29064039408866993, 0.32142857142857145, 0.30295566502463056, 0.31034482758620691, 0.30172413793103448, 0.26847290640394089, 0.29556650246305421, 0.2536945812807882, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150552 minutes
Weight histogram
[ 168  443  678  641 1495 4511 4303 7208 2433  395] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
[  134   154   231   324   476   854   999  1981  6517 10605] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
-1.05152
1.08348
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.63043
Epoch 1, cost is  2.59119
Epoch 2, cost is  2.55994
Epoch 3, cost is  2.53863
Epoch 4, cost is  2.51084
Training took 0.115519 minutes
Weight histogram
[5259 3238 2785 2566 1920 1714 2003 1104 1424  262] [ -5.38926944e-02  -4.85003980e-02  -4.31081015e-02  -3.77158050e-02
  -3.23235086e-02  -2.69312121e-02  -2.15389157e-02  -1.61466192e-02
  -1.07543227e-02  -5.36202628e-03   3.02701774e-05]
[2535 1358 1264 1511 1821 2158 2488 2708 3201 3231] [ -5.38926944e-02  -4.85003980e-02  -4.31081015e-02  -3.77158050e-02
  -3.23235086e-02  -2.69312121e-02  -2.15389157e-02  -1.61466192e-02
  -1.07543227e-02  -5.36202628e-03   3.02701774e-05]
-1.13894
1.50519
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149380 minutes
Weight histogram
[ 223  554 1069 1208 2505 6212 6499 4349 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  273   309   458   634   960  1451   845  1963  4387 13020] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116742 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 2013 3060  478] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.22039
Epoch 1, cost is  5.01234
Epoch 2, cost is  4.47882
Epoch 3, cost is  3.94575
Epoch 4, cost is  3.53107
Epoch 5, cost is  3.19993
Epoch 6, cost is  2.94684
Epoch 7, cost is  2.75255
Epoch 8, cost is  2.60124
Epoch 9, cost is  2.47055
Training took 0.687175 minutes
Weight histogram
[861 734 607 455 430 926  20   9   4   4] [-0.0065721  -0.00592732 -0.00528254 -0.00463776 -0.00399298 -0.00334821
 -0.00270343 -0.00205865 -0.00141387 -0.00076909 -0.00012431]
[896 250 260 280 302 323 354 401 461 523] [-0.0065721  -0.00592732 -0.00528254 -0.00463776 -0.00399298 -0.00334821
 -0.00270343 -0.00205865 -0.00141387 -0.00076909 -0.00012431]
-0.179131
0.186021
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077875 minutes
Epoch 0
Fine tuning took 0.078133 minutes
Epoch 0
Fine tuning took 0.078300 minutes
Epoch 0
Fine tuning took 0.077580 minutes
Epoch 0
Fine tuning took 0.076834 minutes
Epoch 0
Fine tuning took 0.076974 minutes
Epoch 0
Fine tuning took 0.078130 minutes
Epoch 0
Fine tuning took 0.078602 minutes
Epoch 0
Fine tuning took 0.076599 minutes
Epoch 0
Fine tuning took 0.078495 minutes
{'zero': {0: [0.24630541871921183, 0.21551724137931033, 0.25985221674876846, 0.25985221674876846, 0.21798029556650247, 0.24384236453201971, 0.26724137931034481, 0.20320197044334976, 0.26108374384236455, 0.27955665024630544, 0.30911330049261082], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.4963054187192118, 0.45689655172413796, 0.43349753694581283, 0.45197044334975367, 0.41256157635467983, 0.41871921182266009, 0.49014778325123154, 0.3817733990147783, 0.42364532019704432, 0.39901477832512317], 5: [0.30541871921182268, 0.28817733990147781, 0.28325123152709358, 0.30665024630541871, 0.33004926108374383, 0.34359605911330049, 0.31403940886699505, 0.30665024630541871, 0.35714285714285715, 0.29679802955665024, 0.29187192118226601], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24630541871921183, 0.23152709359605911, 0.25, 0.2536945812807882, 0.21428571428571427, 0.26970443349753692, 0.29310344827586204, 0.20566502463054187, 0.25862068965517243, 0.28325123152709358, 0.35098522167487683], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.46182266009852219, 0.49384236453201968, 0.43842364532019706, 0.41995073891625617, 0.39285714285714285, 0.41502463054187194, 0.5073891625615764, 0.37561576354679804, 0.4211822660098522, 0.35837438423645318], 5: [0.30541871921182268, 0.30665024630541871, 0.25615763546798032, 0.30788177339901479, 0.36576354679802958, 0.33743842364532017, 0.29187192118226601, 0.28694581280788178, 0.36576354679802958, 0.29556650246305421, 0.29064039408866993], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24630541871921183, 0.21551724137931033, 0.25246305418719212, 0.22044334975369459, 0.20689655172413793, 0.26724137931034481, 0.24876847290640394, 0.19704433497536947, 0.27339901477832512, 0.29187192118226601, 0.30911330049261082], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.48399014778325122, 0.48768472906403942, 0.44827586206896552, 0.41871921182266009, 0.41256157635467983, 0.41871921182266009, 0.49014778325123154, 0.40147783251231528, 0.43596059113300495, 0.3682266009852217], 5: [0.30541871921182268, 0.30049261083743845, 0.25985221674876846, 0.33128078817733991, 0.37438423645320196, 0.32019704433497537, 0.33251231527093594, 0.31280788177339902, 0.3251231527093596, 0.27216748768472904, 0.32266009852216748], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24630541871921183, 0.20320197044334976, 0.23645320197044334, 0.22413793103448276, 0.22413793103448276, 0.24014778325123154, 0.25738916256157635, 0.20935960591133004, 0.24876847290640394, 0.30295566502463056, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.47660098522167488, 0.47044334975369456, 0.43226600985221675, 0.46305418719211822, 0.39901477832512317, 0.42364532019704432, 0.50123152709359609, 0.41133004926108374, 0.42241379310344829, 0.3645320197044335], 5: [0.30541871921182268, 0.32019704433497537, 0.29310344827586204, 0.34359605911330049, 0.31280788177339902, 0.3608374384236453, 0.31896551724137934, 0.2894088669950739, 0.33990147783251229, 0.27463054187192121, 0.30911330049261082], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234758 minutes
Weight histogram
[ 161  475  719  520  561 1635 2537 7837 7327  503] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
[ 144  236  359  611  855  948 2042 3239 5797 8044] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
-1.16234
1.21375
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.70864
Epoch 1, cost is  1.66631
Epoch 2, cost is  1.63792
Epoch 3, cost is  1.61334
Epoch 4, cost is  1.59328
Training took 0.223721 minutes
Weight histogram
[4558 3902 2766 2451 1990 1751 1376 1250 1615  616] [ -5.48574366e-02  -4.93649294e-02  -4.38724222e-02  -3.83799150e-02
  -3.28874078e-02  -2.73949006e-02  -2.19023934e-02  -1.64098862e-02
  -1.09173790e-02  -5.42487185e-03   6.76353375e-05]
[2169 1206 1428 1629 1886 2161 2513 2655 3033 3595] [ -5.48574366e-02  -4.93649294e-02  -4.38724222e-02  -3.83799150e-02
  -3.28874078e-02  -2.73949006e-02  -2.19023934e-02  -1.64098862e-02
  -1.09173790e-02  -5.42487185e-03   6.76353375e-05]
-0.867193
1.60573
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149244 minutes
Weight histogram
[ 356 1117 1465  803 2030 6056 6452 4340 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  466   917  1150   306   449   796   846  1963  4387 13020] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116641 minutes
Weight histogram
[4065 3305 2761 2709 2214 1793 1924 1574 3360  595] [ -5.07407673e-02  -4.56599270e-02  -4.05790867e-02  -3.54982465e-02
  -3.04174062e-02  -2.53365660e-02  -2.02557257e-02  -1.51748854e-02
  -1.00940452e-02  -5.01320492e-03   6.76353375e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56599270e-02  -4.05790867e-02  -3.54982465e-02
  -3.04174062e-02  -2.53365660e-02  -2.02557257e-02  -1.51748854e-02
  -1.00940452e-02  -5.01320492e-03   6.76353375e-05]
-1.18275
1.42023
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.30426
Epoch 1, cost is  6.02241
Epoch 2, cost is  5.76504
Epoch 3, cost is  5.39769
Epoch 4, cost is  5.02861
Epoch 5, cost is  4.73475
Epoch 6, cost is  4.49848
Epoch 7, cost is  4.30373
Epoch 8, cost is  4.13162
Epoch 9, cost is  3.97955
Training took 0.249280 minutes
Weight histogram
[500 460 428 379 378 493 751 465 179  17] [-0.02840349 -0.02558004 -0.0227566  -0.01993315 -0.01710971 -0.01428626
 -0.01146282 -0.00863937 -0.00581593 -0.00299249 -0.00016904]
[792 451 279 281 293 329 367 396 421 441] [-0.02840349 -0.02558004 -0.0227566  -0.01993315 -0.01710971 -0.01428626
 -0.01146282 -0.00863937 -0.00581593 -0.00299249 -0.00016904]
-0.36644
0.599255
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077395 minutes
Epoch 0
Fine tuning took 0.076443 minutes
Epoch 0
Fine tuning took 0.077506 minutes
Epoch 0
Fine tuning took 0.077548 minutes
Epoch 0
Fine tuning took 0.075498 minutes
Epoch 0
Fine tuning took 0.076749 minutes
Epoch 0
Fine tuning took 0.076654 minutes
Epoch 0
Fine tuning took 0.076186 minutes
Epoch 0
Fine tuning took 0.075818 minutes
Epoch 0
Fine tuning took 0.075801 minutes
{'zero': {0: [0.23275862068965517, 0.14778325123152711, 0.11206896551724138, 0.11699507389162561, 0.15886699507389163, 0.13423645320197045, 0.11945812807881774, 0.098522167487684734, 0.3288177339901478, 0.22906403940886699, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.57512315270935965, 0.69704433497536944, 0.71921182266009853, 0.62561576354679804, 0.5714285714285714, 0.6711822660098522, 0.64162561576354682, 0.38669950738916259, 0.56527093596059108, 0.5923645320197044], 5: [0.21551724137931033, 0.27709359605911332, 0.19088669950738915, 0.16379310344827586, 0.21551724137931033, 0.29433497536945813, 0.20935960591133004, 0.25985221674876846, 0.28448275862068967, 0.20566502463054187, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23275862068965517, 0.10221674876847291, 0.11822660098522167, 0.11822660098522167, 0.18103448275862069, 0.15886699507389163, 0.1539408866995074, 0.1268472906403941, 0.39901477832512317, 0.21798029556650247, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.59852216748768472, 0.68965517241379315, 0.71798029556650245, 0.60467980295566504, 0.51354679802955661, 0.71921182266009853, 0.62561576354679804, 0.32758620689655171, 0.64532019704433496, 0.63177339901477836], 5: [0.21551724137931033, 0.29926108374384236, 0.19211822660098521, 0.16379310344827586, 0.21428571428571427, 0.32758620689655171, 0.1268472906403941, 0.24753694581280788, 0.27339901477832512, 0.13669950738916256, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23275862068965517, 0.11330049261083744, 0.11083743842364532, 0.12438423645320197, 0.19827586206896552, 0.14285714285714285, 0.14655172413793102, 0.11576354679802955, 0.3854679802955665, 0.25, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.5923645320197044, 0.68719211822660098, 0.72783251231527091, 0.60960591133004927, 0.56157635467980294, 0.73891625615763545, 0.63669950738916259, 0.38054187192118227, 0.64162561576354682, 0.63300492610837433], 5: [0.21551724137931033, 0.29433497536945813, 0.2019704433497537, 0.14778325123152711, 0.19211822660098521, 0.29556650246305421, 0.1145320197044335, 0.24753694581280788, 0.23399014778325122, 0.10837438423645321, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23275862068965517, 0.11083743842364532, 0.13423645320197045, 0.12315270935960591, 0.20443349753694581, 0.1354679802955665, 0.15147783251231528, 0.11699507389162561, 0.38300492610837439, 0.2413793103448276, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55172413793103448, 0.58990147783251234, 0.64901477832512311, 0.73275862068965514, 0.59605911330049266, 0.54926108374384242, 0.70443349753694584, 0.62931034482758619, 0.34236453201970446, 0.62931034482758619, 0.66995073891625612], 5: [0.21551724137931033, 0.29926108374384236, 0.21674876847290642, 0.14408866995073891, 0.19950738916256158, 0.31527093596059114, 0.14408866995073891, 0.2536945812807882, 0.27463054187192121, 0.12931034482758622, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235466 minutes
Weight histogram
[ 161  475  719  520  561 1635 2537 7837 7327  503] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
[ 144  236  359  611  855  948 2042 3239 5797 8044] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
-1.16234
1.21375
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.70864
Epoch 1, cost is  1.66631
Epoch 2, cost is  1.63792
Epoch 3, cost is  1.61334
Epoch 4, cost is  1.59328
Training took 0.223408 minutes
Weight histogram
[4558 3902 2766 2451 1990 1751 1376 1250 1615  616] [ -5.48574366e-02  -4.93649294e-02  -4.38724222e-02  -3.83799150e-02
  -3.28874078e-02  -2.73949006e-02  -2.19023934e-02  -1.64098862e-02
  -1.09173790e-02  -5.42487185e-03   6.76353375e-05]
[2169 1206 1428 1629 1886 2161 2513 2655 3033 3595] [ -5.48574366e-02  -4.93649294e-02  -4.38724222e-02  -3.83799150e-02
  -3.28874078e-02  -2.73949006e-02  -2.19023934e-02  -1.64098862e-02
  -1.09173790e-02  -5.42487185e-03   6.76353375e-05]
-0.867193
1.60573
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148741 minutes
Weight histogram
[ 356 1117 1465  803 2030 6056 6452 4340 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  466   917  1150   306   449   796   846  1963  4387 13020] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.115397 minutes
Weight histogram
[4065 3305 2761 2709 2214 1793 1924 1574 3360  595] [ -5.07407673e-02  -4.56599270e-02  -4.05790867e-02  -3.54982465e-02
  -3.04174062e-02  -2.53365660e-02  -2.02557257e-02  -1.51748854e-02
  -1.00940452e-02  -5.01320492e-03   6.76353375e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56599270e-02  -4.05790867e-02  -3.54982465e-02
  -3.04174062e-02  -2.53365660e-02  -2.02557257e-02  -1.51748854e-02
  -1.00940452e-02  -5.01320492e-03   6.76353375e-05]
-1.18275
1.42023
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.69012
Epoch 1, cost is  5.41225
Epoch 2, cost is  5.091
Epoch 3, cost is  4.61335
Epoch 4, cost is  4.2234
Epoch 5, cost is  3.92958
Epoch 6, cost is  3.69589
Epoch 7, cost is  3.49388
Epoch 8, cost is  3.32046
Epoch 9, cost is  3.17038
Training took 0.354194 minutes
Weight histogram
[ 706  595  518  406  419 1018  292   67   19   10] [-0.01835062 -0.0165294  -0.01470819 -0.01288698 -0.01106576 -0.00924455
 -0.00742333 -0.00560212 -0.00378091 -0.00195969 -0.00013848]
[993 271 250 277 312 351 368 387 410 431] [-0.01835062 -0.0165294  -0.01470819 -0.01288698 -0.01106576 -0.00924455
 -0.00742333 -0.00560212 -0.00378091 -0.00195969 -0.00013848]
-0.259161
0.341686
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080928 minutes
Epoch 0
Fine tuning took 0.081705 minutes
Epoch 0
Fine tuning took 0.081201 minutes
Epoch 0
Fine tuning took 0.081592 minutes
Epoch 0
Fine tuning took 0.080321 minutes
Epoch 0
Fine tuning took 0.081130 minutes
Epoch 0
Fine tuning took 0.081291 minutes
Epoch 0
Fine tuning took 0.082467 minutes
Epoch 0
Fine tuning took 0.080737 minutes
Epoch 0
Fine tuning took 0.082059 minutes
{'zero': {0: [0.27832512315270935, 0.17733990147783252, 0.23152709359605911, 0.16379310344827586, 0.19827586206896552, 0.14162561576354679, 0.14039408866995073, 0.096059113300492605, 0.17118226600985223, 0.16009852216748768, 0.21182266009852216], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.6145320197044335, 0.57635467980295563, 0.61822660098522164, 0.60344827586206895, 0.58990147783251234, 0.55541871921182262, 0.61822660098522164, 0.59852216748768472, 0.58620689655172409, 0.54679802955665024], 5: [0.19581280788177341, 0.20812807881773399, 0.19211822660098521, 0.21798029556650247, 0.19827586206896552, 0.26847290640394089, 0.30418719211822659, 0.2857142857142857, 0.23029556650246305, 0.2536945812807882, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27832512315270935, 0.22413793103448276, 0.24261083743842365, 0.22044334975369459, 0.16871921182266009, 0.1625615763546798, 0.19581280788177341, 0.12561576354679804, 0.16625615763546797, 0.16995073891625614, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.5431034482758621, 0.54926108374384242, 0.60221674876847286, 0.64655172413793105, 0.58374384236453203, 0.52093596059113301, 0.63669950738916259, 0.59729064039408863, 0.6576354679802956, 0.60098522167487689], 5: [0.19581280788177341, 0.23275862068965517, 0.20812807881773399, 0.17733990147783252, 0.18472906403940886, 0.2536945812807882, 0.28325123152709358, 0.2376847290640394, 0.23645320197044334, 0.17241379310344829, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27832512315270935, 0.22536945812807882, 0.21182266009852216, 0.2105911330049261, 0.15147783251231528, 0.16625615763546797, 0.21798029556650247, 0.13793103448275862, 0.20320197044334976, 0.17980295566502463, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.54556650246305416, 0.55541871921182262, 0.6354679802955665, 0.64408866995073888, 0.56650246305418717, 0.54802955665024633, 0.60221674876847286, 0.57266009852216748, 0.61576354679802958, 0.60591133004926112], 5: [0.19581280788177341, 0.22906403940886699, 0.23275862068965517, 0.1539408866995074, 0.20443349753694581, 0.26724137931034481, 0.23399014778325122, 0.25985221674876846, 0.22413793103448276, 0.20443349753694581, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27832512315270935, 0.24014778325123154, 0.25492610837438423, 0.24384236453201971, 0.17118226600985223, 0.13916256157635468, 0.20689655172413793, 0.11576354679802955, 0.17980295566502463, 0.16625615763546797, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.53078817733990147, 0.5357142857142857, 0.58251231527093594, 0.62561576354679804, 0.55295566502463056, 0.54187192118226601, 0.61330049261083741, 0.60467980295566504, 0.66009852216748766, 0.58620689655172409], 5: [0.19581280788177341, 0.22906403940886699, 0.20935960591133004, 0.17364532019704434, 0.20320197044334976, 0.30788177339901479, 0.25123152709359609, 0.27093596059113301, 0.21551724137931033, 0.17364532019704434, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235244 minutes
Weight histogram
[ 161  475  719  520  561 1635 2537 7837 7327  503] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
[ 144  236  359  611  855  948 2042 3239 5797 8044] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
-1.16234
1.21375
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.70864
Epoch 1, cost is  1.66631
Epoch 2, cost is  1.63792
Epoch 3, cost is  1.61334
Epoch 4, cost is  1.59328
Training took 0.224419 minutes
Weight histogram
[4558 3902 2766 2451 1990 1751 1376 1250 1615  616] [ -5.48574366e-02  -4.93649294e-02  -4.38724222e-02  -3.83799150e-02
  -3.28874078e-02  -2.73949006e-02  -2.19023934e-02  -1.64098862e-02
  -1.09173790e-02  -5.42487185e-03   6.76353375e-05]
[2169 1206 1428 1629 1886 2161 2513 2655 3033 3595] [ -5.48574366e-02  -4.93649294e-02  -4.38724222e-02  -3.83799150e-02
  -3.28874078e-02  -2.73949006e-02  -2.19023934e-02  -1.64098862e-02
  -1.09173790e-02  -5.42487185e-03   6.76353375e-05]
-0.867193
1.60573
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149533 minutes
Weight histogram
[ 356 1117 1465  803 2030 6056 6452 4340 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  466   917  1150   306   449   796   846  1963  4387 13020] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116140 minutes
Weight histogram
[4065 3305 2761 2709 2214 1793 1924 1574 3360  595] [ -5.07407673e-02  -4.56599270e-02  -4.05790867e-02  -3.54982465e-02
  -3.04174062e-02  -2.53365660e-02  -2.02557257e-02  -1.51748854e-02
  -1.00940452e-02  -5.01320492e-03   6.76353375e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56599270e-02  -4.05790867e-02  -3.54982465e-02
  -3.04174062e-02  -2.53365660e-02  -2.02557257e-02  -1.51748854e-02
  -1.00940452e-02  -5.01320492e-03   6.76353375e-05]
-1.18275
1.42023
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.10344
Epoch 1, cost is  4.87217
Epoch 2, cost is  4.54981
Epoch 3, cost is  4.09721
Epoch 4, cost is  3.72867
Epoch 5, cost is  3.44575
Epoch 6, cost is  3.20587
Epoch 7, cost is  3.00109
Epoch 8, cost is  2.82977
Epoch 9, cost is  2.68738
Training took 0.546223 minutes
Weight histogram
[ 889  794  628 1342  223   92   43   22   10    7] [-0.01290335 -0.01162679 -0.01035023 -0.00907368 -0.00779712 -0.00652056
 -0.00524401 -0.00396745 -0.00269089 -0.00141433 -0.00013778]
[997 271 260 284 315 338 357 378 405 445] [-0.01290335 -0.01162679 -0.01035023 -0.00907368 -0.00779712 -0.00652056
 -0.00524401 -0.00396745 -0.00269089 -0.00141433 -0.00013778]
-0.219073
0.255452
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090301 minutes
Epoch 0
Fine tuning took 0.090852 minutes
Epoch 0
Fine tuning took 0.091182 minutes
Epoch 0
Fine tuning took 0.090006 minutes
Epoch 0
Fine tuning took 0.090026 minutes
Epoch 0
Fine tuning took 0.090333 minutes
Epoch 0
Fine tuning took 0.091795 minutes
Epoch 0
Fine tuning took 0.090955 minutes
Epoch 0
Fine tuning took 0.090592 minutes
Epoch 0
Fine tuning took 0.089997 minutes
{'zero': {0: [0.24384236453201971, 0.1748768472906404, 0.19334975369458129, 0.2019704433497537, 0.12192118226600986, 0.15640394088669951, 0.17364532019704434, 0.17980295566502463, 0.19088669950738915, 0.2229064039408867, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54433497536945807, 0.60221674876847286, 0.58128078817733986, 0.52463054187192115, 0.6071428571428571, 0.54802955665024633, 0.56527093596059108, 0.50246305418719217, 0.49261083743842365, 0.53694581280788178, 0.46305418719211822], 5: [0.21182266009852216, 0.2229064039408867, 0.22536945812807882, 0.27339901477832512, 0.27093596059113301, 0.29556650246305421, 0.26108374384236455, 0.31773399014778325, 0.31650246305418717, 0.24014778325123154, 0.2857142857142857], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24384236453201971, 0.15517241379310345, 0.19458128078817735, 0.18965517241379309, 0.13177339901477833, 0.18226600985221675, 0.18472906403940886, 0.15270935960591134, 0.16625615763546797, 0.19950738916256158, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54433497536945807, 0.60467980295566504, 0.58251231527093594, 0.52955665024630538, 0.57389162561576357, 0.5357142857142857, 0.54556650246305416, 0.57512315270935965, 0.52339901477832518, 0.56403940886699511, 0.47413793103448276], 5: [0.21182266009852216, 0.24014778325123154, 0.2229064039408867, 0.28078817733990147, 0.29433497536945813, 0.28201970443349755, 0.26970443349753692, 0.27216748768472904, 0.31034482758620691, 0.23645320197044334, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24384236453201971, 0.20320197044334976, 0.16995073891625614, 0.20320197044334976, 0.15640394088669951, 0.19088669950738915, 0.19581280788177341, 0.15763546798029557, 0.17980295566502463, 0.20566502463054187, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54433497536945807, 0.55665024630541871, 0.58128078817733986, 0.54556650246305416, 0.57019704433497542, 0.51970443349753692, 0.53817733990147787, 0.54433497536945807, 0.51970443349753692, 0.54433497536945807, 0.49507389162561577], 5: [0.21182266009852216, 0.24014778325123154, 0.24876847290640394, 0.25123152709359609, 0.27339901477832512, 0.2894088669950739, 0.26600985221674878, 0.29802955665024633, 0.30049261083743845, 0.25, 0.27093596059113301], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24384236453201971, 0.17857142857142858, 0.19334975369458129, 0.21674876847290642, 0.15270935960591134, 0.18103448275862069, 0.1748768472906404, 0.19581280788177341, 0.15763546798029557, 0.21674876847290642, 0.30172413793103448], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54433497536945807, 0.60221674876847286, 0.56773399014778325, 0.5357142857142857, 0.59852216748768472, 0.54064039408866993, 0.58497536945812811, 0.53078817733990147, 0.52832512315270941, 0.55541871921182262, 0.45566502463054187], 5: [0.21182266009852216, 0.21921182266009853, 0.23891625615763548, 0.24753694581280788, 0.24876847290640394, 0.27832512315270935, 0.24014778325123154, 0.27339901477832512, 0.31403940886699505, 0.22783251231527094, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234484 minutes
Weight histogram
[ 161  475  719  520  561 1635 2537 7837 7327  503] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
[ 144  236  359  611  855  948 2042 3239 5797 8044] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
-1.16234
1.21375
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.70864
Epoch 1, cost is  1.66631
Epoch 2, cost is  1.63792
Epoch 3, cost is  1.61334
Epoch 4, cost is  1.59328
Training took 0.224591 minutes
Weight histogram
[4558 3902 2766 2451 1990 1751 1376 1250 1615  616] [ -5.48574366e-02  -4.93649294e-02  -4.38724222e-02  -3.83799150e-02
  -3.28874078e-02  -2.73949006e-02  -2.19023934e-02  -1.64098862e-02
  -1.09173790e-02  -5.42487185e-03   6.76353375e-05]
[2169 1206 1428 1629 1886 2161 2513 2655 3033 3595] [ -5.48574366e-02  -4.93649294e-02  -4.38724222e-02  -3.83799150e-02
  -3.28874078e-02  -2.73949006e-02  -2.19023934e-02  -1.64098862e-02
  -1.09173790e-02  -5.42487185e-03   6.76353375e-05]
-0.867193
1.60573
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148724 minutes
Weight histogram
[ 356 1117 1465  803 2030 6056 6452 4340 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  466   917  1150   306   449   796   846  1963  4387 13020] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.114255 minutes
Weight histogram
[4065 3305 2761 2709 2214 1793 1924 1574 3360  595] [ -5.07407673e-02  -4.56599270e-02  -4.05790867e-02  -3.54982465e-02
  -3.04174062e-02  -2.53365660e-02  -2.02557257e-02  -1.51748854e-02
  -1.00940452e-02  -5.01320492e-03   6.76353375e-05]
[4708 1275 1308 1673 1939 2151 2445 2653 3023 3125] [ -5.07407673e-02  -4.56599270e-02  -4.05790867e-02  -3.54982465e-02
  -3.04174062e-02  -2.53365660e-02  -2.02557257e-02  -1.51748854e-02
  -1.00940452e-02  -5.01320492e-03   6.76353375e-05]
-1.18275
1.42023
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.88983
Epoch 1, cost is  4.6008
Epoch 2, cost is  4.00331
Epoch 3, cost is  3.52868
Epoch 4, cost is  3.18211
Epoch 5, cost is  2.90389
Epoch 6, cost is  2.68071
Epoch 7, cost is  2.50484
Epoch 8, cost is  2.36936
Epoch 9, cost is  2.26055
Training took 0.942863 minutes
Weight histogram
[899 779 709 495 971 146  29  12   6   4] [-0.00793658 -0.00715525 -0.00637391 -0.00559258 -0.00481125 -0.00402992
 -0.00324858 -0.00246725 -0.00168592 -0.00090459 -0.00012325]
[820 247 259 300 317 342 372 408 462 523] [-0.00793658 -0.00715525 -0.00637391 -0.00559258 -0.00481125 -0.00402992
 -0.00324858 -0.00246725 -0.00168592 -0.00090459 -0.00012325]
-0.175686
0.188538
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.108928 minutes
Epoch 0
Fine tuning took 0.109585 minutes
Epoch 0
Fine tuning took 0.110315 minutes
Epoch 0
Fine tuning took 0.108965 minutes
Epoch 0
Fine tuning took 0.110376 minutes
Epoch 0
Fine tuning took 0.109198 minutes
Epoch 0
Fine tuning took 0.108492 minutes
Epoch 0
Fine tuning took 0.109187 minutes
Epoch 0
Fine tuning took 0.109126 minutes
Epoch 0
Fine tuning took 0.109471 minutes
{'zero': {0: [0.25738916256157635, 0.22783251231527094, 0.27586206896551724, 0.22660098522167488, 0.20320197044334976, 0.19088669950738915, 0.24507389162561577, 0.17980295566502463, 0.25862068965517243, 0.25738916256157635, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.47783251231527096, 0.4605911330049261, 0.51231527093596063, 0.47536945812807879, 0.44704433497536944, 0.45935960591133007, 0.52216748768472909, 0.41502463054187194, 0.48891625615763545, 0.42610837438423643], 5: [0.26477832512315269, 0.29433497536945813, 0.26354679802955666, 0.26108374384236455, 0.32142857142857145, 0.36206896551724138, 0.29556650246305421, 0.29802955665024633, 0.32635467980295568, 0.2536945812807882, 0.31034482758620691], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25738916256157635, 0.23152709359605911, 0.27339901477832512, 0.21921182266009853, 0.23029556650246305, 0.21551724137931033, 0.25492610837438423, 0.21798029556650247, 0.25246305418719212, 0.27709359605911332, 0.27955665024630544], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.49014778325123154, 0.49876847290640391, 0.5, 0.47783251231527096, 0.45935960591133007, 0.45812807881773399, 0.51477832512315269, 0.41009852216748771, 0.47906403940886699, 0.41379310344827586], 5: [0.26477832512315269, 0.27832512315270935, 0.22783251231527094, 0.28078817733990147, 0.29187192118226601, 0.3251231527093596, 0.28694581280788178, 0.26724137931034481, 0.33743842364532017, 0.24384236453201971, 0.30665024630541871], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25738916256157635, 0.20320197044334976, 0.28694581280788178, 0.20443349753694581, 0.2105911330049261, 0.17980295566502463, 0.25, 0.18965517241379309, 0.22413793103448276, 0.27955665024630544, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.52709359605911332, 0.45812807881773399, 0.49261083743842365, 0.47044334975369456, 0.47536945812807879, 0.45935960591133007, 0.49507389162561577, 0.43842364532019706, 0.4605911330049261, 0.39532019704433496], 5: [0.26477832512315269, 0.26970443349753692, 0.25492610837438423, 0.30295566502463056, 0.31896551724137934, 0.34482758620689657, 0.29064039408866993, 0.31527093596059114, 0.33743842364532017, 0.25985221674876846, 0.34236453201970446], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25738916256157635, 0.23152709359605911, 0.29802955665024633, 0.22413793103448276, 0.19211822660098521, 0.19827586206896552, 0.26231527093596058, 0.21305418719211822, 0.25738916256157635, 0.29433497536945813, 0.27955665024630544], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.47167487684729065, 0.45443349753694579, 0.48891625615763545, 0.48029556650246308, 0.46551724137931033, 0.46551724137931033, 0.51231527093596063, 0.37931034482758619, 0.44458128078817732, 0.41995073891625617], 5: [0.26477832512315269, 0.29679802955665024, 0.24753694581280788, 0.28694581280788178, 0.32758620689655171, 0.33620689655172414, 0.27216748768472904, 0.27463054187192121, 0.36330049261083741, 0.26108374384236455, 0.30049261083743845], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.424960 minutes
Weight histogram
[ 151  494  667  562 1275 6315 8234 3013 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1093  995  765 1035 1387 2663 1457 3917 3598 5365] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.04654
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.14894
Epoch 1, cost is  1.12211
Epoch 2, cost is  1.10358
Epoch 3, cost is  1.08838
Epoch 4, cost is  1.07583
Training took 0.644270 minutes
Weight histogram
[5255 4098 2986 2520 1697 1502 1286  978  957  996] [ -4.06963155e-02  -3.66323734e-02  -3.25684313e-02  -2.85044893e-02
  -2.44405472e-02  -2.03766052e-02  -1.63126631e-02  -1.22487210e-02
  -8.18477896e-03  -4.12083690e-03  -5.68948381e-05]
[1869 1226 1286 1463 1905 2072 2756 2932 3011 3755] [ -4.06963155e-02  -3.66323734e-02  -3.25684313e-02  -2.85044893e-02
  -2.44405472e-02  -2.03766052e-02  -1.63126631e-02  -1.22487210e-02
  -8.18477896e-03  -4.12083690e-03  -5.68948381e-05]
-0.815083
1.54858
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150385 minutes
Weight histogram
[ 538 1619  977  608 2029 6056 6452 4340 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2162   153   217   306   449   796   846  1962  4387 13022] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.115195 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 1590 2477 1484] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4706 1276 1307 1673 1939 2152 2445 2653 3024 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.45078
Epoch 1, cost is  6.09539
Epoch 2, cost is  5.62674
Epoch 3, cost is  5.21808
Epoch 4, cost is  4.89981
Epoch 5, cost is  4.64372
Epoch 6, cost is  4.42823
Epoch 7, cost is  4.24136
Epoch 8, cost is  4.08454
Epoch 9, cost is  3.94906
Training took 0.322963 minutes
Weight histogram
[505 484 437 403 381 341 336 411 674  78] [-0.03172233 -0.02856721 -0.02541209 -0.02225697 -0.01910185 -0.01594673
 -0.01279161 -0.00963649 -0.00648137 -0.00332625 -0.00017113]
[681 298 290 311 340 365 395 422 456 492] [-0.03172233 -0.02856721 -0.02541209 -0.02225697 -0.01910185 -0.01594673
 -0.01279161 -0.00963649 -0.00648137 -0.00332625 -0.00017113]
-0.484167
0.639102
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.141934 minutes
Epoch 0
Fine tuning took 0.142000 minutes
Epoch 0
Fine tuning took 0.141936 minutes
Epoch 0
Fine tuning took 0.142306 minutes
Epoch 0
Fine tuning took 0.142564 minutes
Epoch 0
Fine tuning took 0.141776 minutes
Epoch 0
Fine tuning took 0.143127 minutes
Epoch 0
Fine tuning took 0.142724 minutes
Epoch 0
Fine tuning took 0.142063 minutes
Epoch 0
Fine tuning took 0.142733 minutes
{'zero': {0: [0.26108374384236455, 0.13793103448275862, 0.065270935960591137, 0.089901477832512317, 0.012315270935960592, 0.029556650246305417, 0.15147783251231528, 0.05295566502463054, 0.096059113300492605, 0.092364532019704432, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.62684729064039413, 0.83497536945812811, 0.56773399014778325, 0.86699507389162567, 0.66133004926108374, 0.6576354679802956, 0.7068965517241379, 0.8288177339901478, 0.66009852216748766, 0.5431034482758621], 5: [0.18349753694581281, 0.23522167487684728, 0.099753694581280791, 0.34236453201970446, 0.1206896551724138, 0.30911330049261082, 0.19088669950738915, 0.24014778325123154, 0.075123152709359611, 0.24753694581280788, 0.26970443349753692], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26108374384236455, 0.13300492610837439, 0.064039408866995079, 0.092364532019704432, 0.012315270935960592, 0.02832512315270936, 0.17364532019704434, 0.051724137931034482, 0.10591133004926108, 0.049261083743842367, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.61576354679802958, 0.83251231527093594, 0.56280788177339902, 0.84975369458128081, 0.64901477832512311, 0.62684729064039413, 0.72906403940886699, 0.79433497536945807, 0.65394088669950734, 0.55788177339901479], 5: [0.18349753694581281, 0.25123152709359609, 0.10344827586206896, 0.34482758620689657, 0.13793103448275862, 0.32266009852216748, 0.19950738916256158, 0.21921182266009853, 0.099753694581280791, 0.29679802955665024, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26108374384236455, 0.12561576354679804, 0.091133004926108374, 0.10837438423645321, 0.014778325123152709, 0.029556650246305417, 0.19211822660098521, 0.036945812807881777, 0.1206896551724138, 0.067733990147783252, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.62561576354679804, 0.80788177339901479, 0.53078817733990147, 0.83866995073891626, 0.66871921182266014, 0.60098522167487689, 0.73645320197044339, 0.79433497536945807, 0.66871921182266014, 0.56403940886699511], 5: [0.18349753694581281, 0.24876847290640394, 0.10098522167487685, 0.3608374384236453, 0.14655172413793102, 0.30172413793103448, 0.20689655172413793, 0.22660098522167488, 0.084975369458128072, 0.26354679802955666, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26108374384236455, 0.14039408866995073, 0.097290640394088676, 0.070197044334975367, 0.017241379310344827, 0.030788177339901478, 0.18349753694581281, 0.050492610837438424, 0.10960591133004927, 0.051724137931034482, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.59359605911330049, 0.82635467980295563, 0.56034482758620685, 0.85221674876847286, 0.67610837438423643, 0.60467980295566504, 0.75369458128078815, 0.79187192118226601, 0.71798029556650245, 0.5357142857142857], 5: [0.18349753694581281, 0.26600985221674878, 0.076354679802955669, 0.36945812807881773, 0.13054187192118227, 0.29310344827586204, 0.21182266009852216, 0.19581280788177341, 0.098522167487684734, 0.23029556650246305, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.423903 minutes
Weight histogram
[ 151  494  667  562 1275 6315 8234 3013 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1093  995  765 1035 1387 2663 1457 3917 3598 5365] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.04654
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.14894
Epoch 1, cost is  1.12211
Epoch 2, cost is  1.10358
Epoch 3, cost is  1.08838
Epoch 4, cost is  1.07583
Training took 0.645731 minutes
Weight histogram
[5255 4098 2986 2520 1697 1502 1286  978  957  996] [ -4.06963155e-02  -3.66323734e-02  -3.25684313e-02  -2.85044893e-02
  -2.44405472e-02  -2.03766052e-02  -1.63126631e-02  -1.22487210e-02
  -8.18477896e-03  -4.12083690e-03  -5.68948381e-05]
[1869 1226 1286 1463 1905 2072 2756 2932 3011 3755] [ -4.06963155e-02  -3.66323734e-02  -3.25684313e-02  -2.85044893e-02
  -2.44405472e-02  -2.03766052e-02  -1.63126631e-02  -1.22487210e-02
  -8.18477896e-03  -4.12083690e-03  -5.68948381e-05]
-0.815083
1.54858
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148781 minutes
Weight histogram
[ 538 1619  977  608 2029 6056 6452 4340 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2162   153   217   306   449   796   846  1962  4387 13022] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.114245 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 1590 2477 1484] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4706 1276 1307 1673 1939 2152 2445 2653 3024 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.95294
Epoch 1, cost is  5.49196
Epoch 2, cost is  4.90355
Epoch 3, cost is  4.4487
Epoch 4, cost is  4.08025
Epoch 5, cost is  3.78216
Epoch 6, cost is  3.54953
Epoch 7, cost is  3.36043
Epoch 8, cost is  3.20677
Epoch 9, cost is  3.07445
Training took 0.499633 minutes
Weight histogram
[593 510 472 456 419 377 339 667 203  14] [-0.02028304 -0.01826994 -0.01625684 -0.01424374 -0.01223064 -0.01021754
 -0.00820444 -0.00619134 -0.00417824 -0.00216514 -0.00015205]
[711 290 305 324 333 345 376 414 455 497] [-0.02028304 -0.01826994 -0.01625684 -0.01424374 -0.01223064 -0.01021754
 -0.00820444 -0.00619134 -0.00417824 -0.00216514 -0.00015205]
-0.348139
0.355715
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.150064 minutes
Epoch 0
Fine tuning took 0.149928 minutes
Epoch 0
Fine tuning took 0.149709 minutes
Epoch 0
Fine tuning took 0.150165 minutes
Epoch 0
Fine tuning took 0.150754 minutes
Epoch 0
Fine tuning took 0.149745 minutes
Epoch 0
Fine tuning took 0.149266 minutes
Epoch 0
Fine tuning took 0.149726 minutes
Epoch 0
Fine tuning took 0.150158 minutes
Epoch 0
Fine tuning took 0.150351 minutes
{'zero': {0: [0.27216748768472904, 0.13300492610837439, 0.15147783251231528, 0.14778325123152711, 0.17733990147783252, 0.12315270935960591, 0.11206896551724138, 0.11945812807881774, 0.22783251231527094, 0.11945812807881774, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.75492610837438423, 0.68965517241379315, 0.65147783251231528, 0.67733990147783252, 0.62315270935960587, 0.68226600985221675, 0.6576354679802956, 0.47906403940886699, 0.61083743842364535, 0.55418719211822665], 5: [0.20689655172413793, 0.11206896551724138, 0.15886699507389163, 0.20073891625615764, 0.14532019704433496, 0.2536945812807882, 0.20566502463054187, 0.2229064039408867, 0.29310344827586204, 0.26970443349753692, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27216748768472904, 0.14778325123152711, 0.12192118226600986, 0.23152709359605911, 0.22783251231527094, 0.15640394088669951, 0.15640394088669951, 0.1268472906403941, 0.22906403940886699, 0.1354679802955665, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.73029556650246308, 0.66133004926108374, 0.56896551724137934, 0.59852216748768472, 0.58620689655172409, 0.63177339901477836, 0.6280788177339901, 0.44581280788177341, 0.60591133004926112, 0.58620689655172409], 5: [0.20689655172413793, 0.12192118226600986, 0.21674876847290642, 0.19950738916256158, 0.17364532019704434, 0.25738916256157635, 0.21182266009852216, 0.24507389162561577, 0.3251231527093596, 0.25862068965517243, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27216748768472904, 0.17118226600985223, 0.15640394088669951, 0.22536945812807882, 0.17980295566502463, 0.15763546798029557, 0.14162561576354679, 0.14532019704433496, 0.25123152709359609, 0.14778325123152711, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.71182266009852213, 0.60467980295566504, 0.55295566502463056, 0.61206896551724133, 0.58251231527093594, 0.63669950738916259, 0.63054187192118227, 0.47290640394088668, 0.64162561576354682, 0.6071428571428571], 5: [0.20689655172413793, 0.11699507389162561, 0.23891625615763548, 0.22167487684729065, 0.20812807881773399, 0.25985221674876846, 0.22167487684729065, 0.22413793103448276, 0.27586206896551724, 0.2105911330049261, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27216748768472904, 0.15517241379310345, 0.14162561576354679, 0.23029556650246305, 0.23891625615763548, 0.14039408866995073, 0.12438423645320197, 0.15517241379310345, 0.25, 0.14408866995073891, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.72167487684729059, 0.62561576354679804, 0.54187192118226601, 0.54926108374384242, 0.57389162561576357, 0.65394088669950734, 0.62438423645320196, 0.47536945812807879, 0.61206896551724133, 0.56896551724137934], 5: [0.20689655172413793, 0.12315270935960591, 0.23275862068965517, 0.22783251231527094, 0.21182266009852216, 0.2857142857142857, 0.22167487684729065, 0.22044334975369459, 0.27463054187192121, 0.24384236453201971, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421573 minutes
Weight histogram
[ 151  494  667  562 1275 6315 8234 3013 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1093  995  765 1035 1387 2663 1457 3917 3598 5365] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.04654
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.14894
Epoch 1, cost is  1.12211
Epoch 2, cost is  1.10358
Epoch 3, cost is  1.08838
Epoch 4, cost is  1.07583
Training took 0.645438 minutes
Weight histogram
[5255 4098 2986 2520 1697 1502 1286  978  957  996] [ -4.06963155e-02  -3.66323734e-02  -3.25684313e-02  -2.85044893e-02
  -2.44405472e-02  -2.03766052e-02  -1.63126631e-02  -1.22487210e-02
  -8.18477896e-03  -4.12083690e-03  -5.68948381e-05]
[1869 1226 1286 1463 1905 2072 2756 2932 3011 3755] [ -4.06963155e-02  -3.66323734e-02  -3.25684313e-02  -2.85044893e-02
  -2.44405472e-02  -2.03766052e-02  -1.63126631e-02  -1.22487210e-02
  -8.18477896e-03  -4.12083690e-03  -5.68948381e-05]
-0.815083
1.54858
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150768 minutes
Weight histogram
[ 538 1619  977  608 2029 6056 6452 4340 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2162   153   217   306   449   796   846  1962  4387 13022] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.116265 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 1590 2477 1484] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4706 1276 1307 1673 1939 2152 2445 2653 3024 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.33302
Epoch 1, cost is  4.8294
Epoch 2, cost is  4.23009
Epoch 3, cost is  3.77536
Epoch 4, cost is  3.42472
Epoch 5, cost is  3.16365
Epoch 6, cost is  2.96103
Epoch 7, cost is  2.79483
Epoch 8, cost is  2.66242
Epoch 9, cost is  2.55426
Training took 0.817880 minutes
Weight histogram
[745 649 537 493 433 379 688 100  19   7] [-0.0134927  -0.01215713 -0.01082157 -0.00948601 -0.00815044 -0.00681488
 -0.00547931 -0.00414375 -0.00280819 -0.00147262 -0.00013706]
[717 288 299 308 321 350 387 417 456 507] [-0.0134927  -0.01215713 -0.01082157 -0.00948601 -0.00815044 -0.00681488
 -0.00547931 -0.00414375 -0.00280819 -0.00147262 -0.00013706]
-0.196785
0.304396
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164398 minutes
Epoch 0
Fine tuning took 0.165178 minutes
Epoch 0
Fine tuning took 0.164338 minutes
Epoch 0
Fine tuning took 0.164149 minutes
Epoch 0
Fine tuning took 0.165229 minutes
Epoch 0
Fine tuning took 0.164378 minutes
Epoch 0
Fine tuning took 0.164584 minutes
Epoch 0
Fine tuning took 0.165416 minutes
Epoch 0
Fine tuning took 0.164803 minutes
Epoch 0
Fine tuning took 0.164908 minutes
{'zero': {0: [0.30049261083743845, 0.19581280788177341, 0.22660098522167488, 0.23029556650246305, 0.14039408866995073, 0.23891625615763548, 0.16625615763546797, 0.17857142857142858, 0.24876847290640394, 0.21428571428571427, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.58004926108374388, 0.54187192118226601, 0.50492610837438423, 0.55788177339901479, 0.47413793103448276, 0.58004926108374388, 0.52709359605911332, 0.51354679802955661, 0.5, 0.49261083743842365], 5: [0.20073891625615764, 0.22413793103448276, 0.23152709359605911, 0.26477832512315269, 0.30172413793103448, 0.28694581280788178, 0.2536945812807882, 0.29433497536945813, 0.2376847290640394, 0.2857142857142857, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30049261083743845, 0.23275862068965517, 0.18842364532019704, 0.21921182266009853, 0.17364532019704434, 0.21674876847290642, 0.16009852216748768, 0.16009852216748768, 0.19827586206896552, 0.19458128078817735, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.54679802955665024, 0.57758620689655171, 0.49876847290640391, 0.56650246305418717, 0.49137931034482757, 0.60344827586206895, 0.57512315270935965, 0.54064039408866993, 0.54926108374384242, 0.49384236453201968], 5: [0.20073891625615764, 0.22044334975369459, 0.23399014778325122, 0.28201970443349755, 0.25985221674876846, 0.29187192118226601, 0.23645320197044334, 0.26477832512315269, 0.26108374384236455, 0.25615763546798032, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30049261083743845, 0.20320197044334976, 0.17118226600985223, 0.20689655172413793, 0.14285714285714285, 0.25123152709359609, 0.1625615763546798, 0.17610837438423646, 0.19088669950738915, 0.2105911330049261, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.57512315270935965, 0.57635467980295563, 0.52832512315270941, 0.59359605911330049, 0.47783251231527096, 0.5923645320197044, 0.51970443349753692, 0.5431034482758621, 0.5431034482758621, 0.50492610837438423], 5: [0.20073891625615764, 0.22167487684729065, 0.25246305418719212, 0.26477832512315269, 0.26354679802955666, 0.27093596059113301, 0.24507389162561577, 0.30418719211822659, 0.26600985221674878, 0.24630541871921183, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30049261083743845, 0.22167487684729065, 0.19334975369458129, 0.21182266009852216, 0.15517241379310345, 0.23152709359605911, 0.16995073891625614, 0.14285714285714285, 0.21551724137931033, 0.18472906403940886, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.55788177339901479, 0.57389162561576357, 0.53940886699507384, 0.58866995073891626, 0.48645320197044334, 0.5714285714285714, 0.55665024630541871, 0.51477832512315269, 0.57266009852216748, 0.48399014778325122], 5: [0.20073891625615764, 0.22044334975369459, 0.23275862068965517, 0.24876847290640394, 0.25615763546798032, 0.28201970443349755, 0.25862068965517243, 0.30049261083743845, 0.26970443349753692, 0.24261083743842365, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421580 minutes
Weight histogram
[ 151  494  667  562 1275 6315 8234 3013 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1093  995  765 1035 1387 2663 1457 3917 3598 5365] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.04654
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.14894
Epoch 1, cost is  1.12211
Epoch 2, cost is  1.10358
Epoch 3, cost is  1.08838
Epoch 4, cost is  1.07583
Training took 0.645621 minutes
Weight histogram
[5255 4098 2986 2520 1697 1502 1286  978  957  996] [ -4.06963155e-02  -3.66323734e-02  -3.25684313e-02  -2.85044893e-02
  -2.44405472e-02  -2.03766052e-02  -1.63126631e-02  -1.22487210e-02
  -8.18477896e-03  -4.12083690e-03  -5.68948381e-05]
[1869 1226 1286 1463 1905 2072 2756 2932 3011 3755] [ -4.06963155e-02  -3.66323734e-02  -3.25684313e-02  -2.85044893e-02
  -2.44405472e-02  -2.03766052e-02  -1.63126631e-02  -1.22487210e-02
  -8.18477896e-03  -4.12083690e-03  -5.68948381e-05]
-0.815083
1.54858
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150718 minutes
Weight histogram
[ 538 1619  977  608 2029 6056 6452 4340 1545  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2162   153   217   306   449   796   846  1962  4387 13022] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.8235
Epoch 1, cost is  2.77795
Epoch 2, cost is  2.74152
Epoch 3, cost is  2.71407
Epoch 4, cost is  2.68656
Training took 0.115301 minutes
Weight histogram
[4064 3305 2754 2706 2215 1799 1906 1590 2477 1484] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
[4706 1276 1307 1673 1939 2152 2445 2653 3024 3125] [ -5.07407673e-02  -4.56635026e-02  -4.05862378e-02  -3.55089731e-02
  -3.04317084e-02  -2.53544437e-02  -2.02771790e-02  -1.51999143e-02
  -1.01226496e-02  -5.04538487e-03   3.18798448e-05]
-1.18275
1.42023
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.72125
Epoch 1, cost is  4.23879
Epoch 2, cost is  3.68935
Epoch 3, cost is  3.2659
Epoch 4, cost is  2.95527
Epoch 5, cost is  2.7276
Epoch 6, cost is  2.55545
Epoch 7, cost is  2.42113
Epoch 8, cost is  2.31407
Epoch 9, cost is  2.22468
Training took 1.465598 minutes
Weight histogram
[954 794 641 630 825 122  49  21   9   5] [-0.00915086 -0.00825114 -0.00735143 -0.00645172 -0.00555201 -0.0046523
 -0.00375259 -0.00285288 -0.00195317 -0.00105346 -0.00015375]
[703 286 284 287 313 345 384 425 483 540] [-0.00915086 -0.00825114 -0.00735143 -0.00645172 -0.00555201 -0.0046523
 -0.00375259 -0.00285288 -0.00195317 -0.00105346 -0.00015375]
-0.185236
0.218415
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.192698 minutes
Epoch 0
Fine tuning took 0.194366 minutes
Epoch 0
Fine tuning took 0.193928 minutes
Epoch 0
Fine tuning took 0.193556 minutes
Epoch 0
Fine tuning took 0.194053 minutes
Epoch 0
Fine tuning took 0.193695 minutes
Epoch 0
Fine tuning took 0.194052 minutes
Epoch 0
Fine tuning took 0.194834 minutes
Epoch 0
Fine tuning took 0.193702 minutes
Epoch 0
Fine tuning took 0.194398 minutes
{'zero': {0: [0.24384236453201971, 0.16871921182266009, 0.22167487684729065, 0.20566502463054187, 0.19211822660098521, 0.18719211822660098, 0.23522167487684728, 0.2019704433497537, 0.26600985221674878, 0.24507389162561577, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.54556650246305416, 0.49384236453201968, 0.47536945812807879, 0.50246305418719217, 0.49753694581280788, 0.45197044334975367, 0.50369458128078815, 0.44211822660098521, 0.48768472906403942, 0.45443349753694579], 5: [0.21551724137931033, 0.2857142857142857, 0.28448275862068967, 0.31896551724137934, 0.30541871921182268, 0.31527093596059114, 0.31280788177339902, 0.29433497536945813, 0.29187192118226601, 0.26724137931034481, 0.25246305418719212], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24384236453201971, 0.19950738916256158, 0.25862068965517243, 0.18842364532019704, 0.17241379310344829, 0.2019704433497537, 0.22413793103448276, 0.18226600985221675, 0.26354679802955666, 0.27216748768472904, 0.25], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.55295566502463056, 0.47660098522167488, 0.51724137931034486, 0.48029556650246308, 0.46182266009852219, 0.48152709359605911, 0.5, 0.44704433497536944, 0.48522167487684731, 0.44088669950738918], 5: [0.21551724137931033, 0.24753694581280788, 0.26477832512315269, 0.29433497536945813, 0.34729064039408869, 0.33620689655172414, 0.29433497536945813, 0.31773399014778325, 0.2894088669950739, 0.24261083743842365, 0.30911330049261082], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24384236453201971, 0.2105911330049261, 0.23275862068965517, 0.19950738916256158, 0.18349753694581281, 0.20935960591133004, 0.20812807881773399, 0.22536945812807882, 0.28325123152709358, 0.24753694581280788, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.52586206896551724, 0.49014778325123154, 0.51354679802955661, 0.49014778325123154, 0.48152709359605911, 0.48029556650246308, 0.49261083743842365, 0.4248768472906404, 0.53448275862068961, 0.44950738916256155], 5: [0.21551724137931033, 0.26354679802955666, 0.27709359605911332, 0.28694581280788178, 0.32635467980295568, 0.30911330049261082, 0.31157635467980294, 0.28201970443349755, 0.29187192118226601, 0.21798029556650247, 0.29433497536945813], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24384236453201971, 0.17118226600985223, 0.19088669950738915, 0.19088669950738915, 0.14285714285714285, 0.20689655172413793, 0.21182266009852216, 0.2105911330049261, 0.26477832512315269, 0.25985221674876846, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.53817733990147787, 0.50985221674876846, 0.50862068965517238, 0.52709359605911332, 0.47044334975369456, 0.46305418719211822, 0.49384236453201968, 0.45073891625615764, 0.48522167487684731, 0.44950738916256155], 5: [0.21551724137931033, 0.29064039408866993, 0.29926108374384236, 0.30049261083743845, 0.33004926108374383, 0.32266009852216748, 0.3251231527093596, 0.29556650246305421, 0.28448275862068967, 0.25492610837438423, 0.27709359605911332], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105917 minutes
Weight histogram
[1629 2068 4831 7039 5251 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  134   146   224   296   463   707   739  1337  3532 18747] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-1.92963
2.02283
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.80398
Epoch 1, cost is  3.77604
Epoch 2, cost is  3.75025
Epoch 3, cost is  3.72983
Epoch 4, cost is  3.70226
Training took 0.071925 minutes
Weight histogram
[7044 5020 3596 3489 4195  657 1508  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2632 2109 2006 2225 2057 2176 2518 3162 3286 4154] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.48038
2.25842
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148039 minutes
Weight histogram
[   48   106   545  1430  1634  7642 10629  6014   299     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 14670   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.115330 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 3162 2508  575] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.03225
Epoch 1, cost is  5.71774
Epoch 2, cost is  5.6079
Epoch 3, cost is  5.47174
Epoch 4, cost is  5.2472
Epoch 5, cost is  5.02519
Epoch 6, cost is  4.80866
Epoch 7, cost is  4.61594
Epoch 8, cost is  4.44521
Epoch 9, cost is  4.30065
Training took 0.177934 minutes
Weight histogram
[1376 1623 2129 1286  642  449  339  157   66   33] [ -2.92806122e-02  -2.63619816e-02  -2.34433509e-02  -2.05247203e-02
  -1.76060896e-02  -1.46874590e-02  -1.17688283e-02  -8.85019770e-03
  -5.93156705e-03  -3.01293640e-03  -9.43057457e-05]
[ 669 1297 1212  691  690  684  687  690  720  760] [ -2.92806122e-02  -2.63619816e-02  -2.34433509e-02  -2.05247203e-02
  -1.76060896e-02  -1.46874590e-02  -1.17688283e-02  -8.85019770e-03
  -5.93156705e-03  -3.01293640e-03  -9.43057457e-05]
-0.366673
0.362733
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040450 minutes
Epoch 0
Fine tuning took 0.043117 minutes
Epoch 0
Fine tuning took 0.042430 minutes
Epoch 0
Fine tuning took 0.043399 minutes
Epoch 0
Fine tuning took 0.043401 minutes
Epoch 0
Fine tuning took 0.040720 minutes
Epoch 0
Fine tuning took 0.042687 minutes
Epoch 0
Fine tuning took 0.042450 minutes
Epoch 0
Fine tuning took 0.043518 minutes
Epoch 0
Fine tuning took 0.042919 minutes
{'zero': {0: [0.10221674876847291, 0.10467980295566502, 0.10344827586206896, 0.10960591133004927, 0.12561576354679804, 0.11083743842364532, 0.19827586206896552, 0.12807881773399016, 0.12315270935960591, 0.10837438423645321, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.70935960591133007, 0.72536945812807885, 0.69458128078817738, 0.72044334975369462, 0.62561576354679804, 0.6354679802955665, 0.75615763546798032, 0.62561576354679804, 0.72783251231527091, 0.61206896551724133], 5: [0.13423645320197045, 0.18596059113300492, 0.17118226600985223, 0.19581280788177341, 0.1539408866995074, 0.26354679802955666, 0.16625615763546797, 0.11576354679802955, 0.25123152709359609, 0.16379310344827586, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10221674876847291, 0.23275862068965517, 0.22044334975369459, 0.14039408866995073, 0.2105911330049261, 0.27586206896551724, 0.14778325123152711, 0.1748768472906404, 0.31280788177339902, 0.26231527093596058, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.57019704433497542, 0.67610837438423643, 0.67980295566502458, 0.73645320197044339, 0.53201970443349755, 0.76477832512315269, 0.73768472906403937, 0.51354679802955661, 0.56527093596059108, 0.59975369458128081], 5: [0.13423645320197045, 0.19704433497536947, 0.10344827586206896, 0.17980295566502463, 0.05295566502463054, 0.19211822660098521, 0.087438423645320201, 0.087438423645320201, 0.17364532019704434, 0.17241379310344829, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10221674876847291, 0.19581280788177341, 0.20320197044334976, 0.15886699507389163, 0.22906403940886699, 0.25862068965517243, 0.16379310344827586, 0.18596059113300492, 0.25492610837438423, 0.23152709359605911, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.59729064039408863, 0.69088669950738912, 0.66748768472906406, 0.66871921182266014, 0.53325123152709364, 0.6785714285714286, 0.69704433497536944, 0.53694581280788178, 0.54679802955665024, 0.57019704433497542], 5: [0.13423645320197045, 0.20689655172413793, 0.10591133004926108, 0.17364532019704434, 0.10221674876847291, 0.20812807881773399, 0.15763546798029557, 0.11699507389162561, 0.20812807881773399, 0.22167487684729065, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10221674876847291, 0.25862068965517243, 0.20935960591133004, 0.10837438423645321, 0.21305418719211822, 0.29433497536945813, 0.13054187192118227, 0.17241379310344829, 0.34359605911330049, 0.25, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.52709359605911332, 0.72906403940886699, 0.73891625615763545, 0.74753694581280783, 0.56896551724137934, 0.75985221674876846, 0.74876847290640391, 0.48399014778325122, 0.58128078817733986, 0.59605911330049266], 5: [0.13423645320197045, 0.21428571428571427, 0.061576354679802957, 0.15270935960591134, 0.039408866995073892, 0.13669950738916256, 0.10960591133004927, 0.078817733990147784, 0.17241379310344829, 0.16871921182266009, 0.26477832512315269], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105284 minutes
Weight histogram
[1629 2068 4831 7039 5251 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  134   146   224   296   463   707   739  1337  3532 18747] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-1.92963
2.02283
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.80398
Epoch 1, cost is  3.77604
Epoch 2, cost is  3.75025
Epoch 3, cost is  3.72983
Epoch 4, cost is  3.70226
Training took 0.072200 minutes
Weight histogram
[7044 5020 3596 3489 4195  657 1508  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2632 2109 2006 2225 2057 2176 2518 3162 3286 4154] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.48038
2.25842
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147939 minutes
Weight histogram
[   48   106   545  1430  1634  7642 10629  6014   299     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 14670   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.117448 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 3162 2508  575] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.60366
Epoch 1, cost is  5.40403
Epoch 2, cost is  5.31391
Epoch 3, cost is  5.09574
Epoch 4, cost is  4.78597
Epoch 5, cost is  4.50629
Epoch 6, cost is  4.27249
Epoch 7, cost is  4.08754
Epoch 8, cost is  3.91338
Epoch 9, cost is  3.76038
Training took 0.244276 minutes
Weight histogram
[1752 3659 1537  485  280  167  104   61   33   22] [-0.0187807  -0.01691822 -0.01505575 -0.01319328 -0.0113308  -0.00946833
 -0.00760586 -0.00574338 -0.00388091 -0.00201844 -0.00015596]
[2032  840  524  566  572  606  659  726  766  809] [-0.0187807  -0.01691822 -0.01505575 -0.01319328 -0.0113308  -0.00946833
 -0.00760586 -0.00574338 -0.00388091 -0.00201844 -0.00015596]
-0.330819
0.270692
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046358 minutes
Epoch 0
Fine tuning took 0.048145 minutes
Epoch 0
Fine tuning took 0.046341 minutes
Epoch 0
Fine tuning took 0.048007 minutes
Epoch 0
Fine tuning took 0.047765 minutes
Epoch 0
Fine tuning took 0.046396 minutes
Epoch 0
Fine tuning took 0.046317 minutes
Epoch 0
Fine tuning took 0.045079 minutes
Epoch 0
Fine tuning took 0.045801 minutes
Epoch 0
Fine tuning took 0.047001 minutes
{'zero': {0: [0.12438423645320197, 0.14532019704433496, 0.17610837438423646, 0.19581280788177341, 0.23645320197044334, 0.18596059113300492, 0.24014778325123154, 0.21798029556650247, 0.20443349753694581, 0.24384236453201971, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64408866995073888, 0.58497536945812811, 0.61330049261083741, 0.5357142857142857, 0.5431034482758621, 0.57512315270935965, 0.48891625615763545, 0.51847290640394084, 0.4963054187192118, 0.47783251231527096, 0.54187192118226601], 5: [0.23152709359605911, 0.26970443349753692, 0.2105911330049261, 0.26847290640394089, 0.22044334975369459, 0.23891625615763548, 0.27093596059113301, 0.26354679802955666, 0.29926108374384236, 0.27832512315270935, 0.27093596059113301], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12438423645320197, 0.19950738916256158, 0.20073891625615764, 0.19950738916256158, 0.22413793103448276, 0.18965517241379309, 0.22536945812807882, 0.22660098522167488, 0.22167487684729065, 0.21921182266009853, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64408866995073888, 0.55911330049261088, 0.56157635467980294, 0.58866995073891626, 0.51724137931034486, 0.59359605911330049, 0.50246305418719217, 0.52463054187192115, 0.49384236453201968, 0.46305418719211822, 0.52339901477832518], 5: [0.23152709359605911, 0.2413793103448276, 0.2376847290640394, 0.21182266009852216, 0.25862068965517243, 0.21674876847290642, 0.27216748768472904, 0.24876847290640394, 0.28448275862068967, 0.31773399014778325, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12438423645320197, 0.18842364532019704, 0.18226600985221675, 0.19458128078817735, 0.22044334975369459, 0.20812807881773399, 0.24384236453201971, 0.22167487684729065, 0.21305418719211822, 0.24876847290640394, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64408866995073888, 0.57266009852216748, 0.56527093596059108, 0.59482758620689657, 0.53078817733990147, 0.5714285714285714, 0.49261083743842365, 0.52586206896551724, 0.48891625615763545, 0.4642857142857143, 0.50246305418719217], 5: [0.23152709359605911, 0.23891625615763548, 0.25246305418719212, 0.2105911330049261, 0.24876847290640394, 0.22044334975369459, 0.26354679802955666, 0.25246305418719212, 0.29802955665024633, 0.28694581280788178, 0.2894088669950739], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12438423645320197, 0.19211822660098521, 0.18472906403940886, 0.17733990147783252, 0.2229064039408867, 0.20689655172413793, 0.23891625615763548, 0.20320197044334976, 0.19088669950738915, 0.27586206896551724, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64408866995073888, 0.58990147783251234, 0.55541871921182262, 0.58251231527093594, 0.53201970443349755, 0.56034482758620685, 0.5073891625615764, 0.54433497536945807, 0.49507389162561577, 0.45443349753694579, 0.50862068965517238], 5: [0.23152709359605911, 0.21798029556650247, 0.25985221674876846, 0.24014778325123154, 0.24507389162561577, 0.23275862068965517, 0.2536945812807882, 0.25246305418719212, 0.31403940886699505, 0.26970443349753692, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106912 minutes
Weight histogram
[1629 2068 4831 7039 5251 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  134   146   224   296   463   707   739  1337  3532 18747] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-1.92963
2.02283
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.80398
Epoch 1, cost is  3.77604
Epoch 2, cost is  3.75025
Epoch 3, cost is  3.72983
Epoch 4, cost is  3.70226
Training took 0.073703 minutes
Weight histogram
[7044 5020 3596 3489 4195  657 1508  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2632 2109 2006 2225 2057 2176 2518 3162 3286 4154] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.48038
2.25842
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149983 minutes
Weight histogram
[   48   106   545  1430  1634  7642 10629  6014   299     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 14670   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.114511 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 3162 2508  575] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.49916
Epoch 1, cost is  5.37917
Epoch 2, cost is  5.12502
Epoch 3, cost is  4.67277
Epoch 4, cost is  4.299
Epoch 5, cost is  4.02657
Epoch 6, cost is  3.79538
Epoch 7, cost is  3.58979
Epoch 8, cost is  3.412
Epoch 9, cost is  3.25442
Training took 0.335212 minutes
Weight histogram
[1469 1698 1254 1150 2221  188   61   29   17   13] [-0.01086076 -0.0097863  -0.00871184 -0.00763738 -0.00656292 -0.00548846
 -0.004414   -0.00333955 -0.00226509 -0.00119063 -0.00011617]
[2058  512  509  536  590  686  733  783  826  867] [-0.01086076 -0.0097863  -0.00871184 -0.00763738 -0.00656292 -0.00548846
 -0.004414   -0.00333955 -0.00226509 -0.00119063 -0.00011617]
-0.204724
0.227813
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051033 minutes
Epoch 0
Fine tuning took 0.051273 minutes
Epoch 0
Fine tuning took 0.049265 minutes
Epoch 0
Fine tuning took 0.049849 minutes
Epoch 0
Fine tuning took 0.051073 minutes
Epoch 0
Fine tuning took 0.052003 minutes
Epoch 0
Fine tuning took 0.050176 minutes
Epoch 0
Fine tuning took 0.050736 minutes
Epoch 0
Fine tuning took 0.051375 minutes
Epoch 0
Fine tuning took 0.049608 minutes
{'zero': {0: [0.14285714285714285, 0.28078817733990147, 0.22660098522167488, 0.21182266009852216, 0.25123152709359609, 0.25246305418719212, 0.25, 0.25985221674876846, 0.25985221674876846, 0.25738916256157635, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.41379310344827586, 0.47783251231527096, 0.48891625615763545, 0.45689655172413796, 0.45073891625615764, 0.37438423645320196, 0.43226600985221675, 0.4039408866995074, 0.42857142857142855, 0.41502463054187194], 5: [0.30295566502463056, 0.30541871921182268, 0.29556650246305421, 0.29926108374384236, 0.29187192118226601, 0.29679802955665024, 0.37561576354679804, 0.30788177339901479, 0.33620689655172414, 0.31403940886699505, 0.31896551724137934], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14285714285714285, 0.27832512315270935, 0.2229064039408867, 0.24261083743842365, 0.24630541871921183, 0.26600985221674878, 0.23522167487684728, 0.28694581280788178, 0.30418719211822659, 0.24507389162561577, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.43842364532019706, 0.4963054187192118, 0.45443349753694579, 0.43472906403940886, 0.42364532019704432, 0.4039408866995074, 0.4248768472906404, 0.37438423645320196, 0.41995073891625617, 0.41502463054187194], 5: [0.30295566502463056, 0.28325123152709358, 0.28078817733990147, 0.30295566502463056, 0.31896551724137934, 0.31034482758620691, 0.3608374384236453, 0.28817733990147781, 0.32142857142857145, 0.33497536945812806, 0.31157635467980294], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14285714285714285, 0.27709359605911332, 0.22906403940886699, 0.2413793103448276, 0.26600985221674878, 0.25738916256157635, 0.24876847290640394, 0.25492610837438423, 0.29064039408866993, 0.2894088669950739, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.41995073891625617, 0.45197044334975367, 0.44704433497536944, 0.43472906403940886, 0.45689655172413796, 0.41256157635467983, 0.43349753694581283, 0.35467980295566504, 0.41502463054187194, 0.43472906403940886], 5: [0.30295566502463056, 0.30295566502463056, 0.31896551724137934, 0.31157635467980294, 0.29926108374384236, 0.2857142857142857, 0.33866995073891626, 0.31157635467980294, 0.35467980295566504, 0.29556650246305421, 0.31157635467980294], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14285714285714285, 0.28448275862068967, 0.22783251231527094, 0.24261083743842365, 0.26600985221674878, 0.25862068965517243, 0.26477832512315269, 0.25615763546798032, 0.28325123152709358, 0.25123152709359609, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.43842364532019706, 0.48275862068965519, 0.46551724137931033, 0.45073891625615764, 0.42610837438423643, 0.42610837438423643, 0.45812807881773399, 0.37807881773399016, 0.42241379310344829, 0.44334975369458129], 5: [0.30295566502463056, 0.27709359605911332, 0.2894088669950739, 0.29187192118226601, 0.28325123152709358, 0.31527093596059114, 0.30911330049261082, 0.2857142857142857, 0.33866995073891626, 0.32635467980295568, 0.33497536945812806], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105324 minutes
Weight histogram
[1629 2068 4831 7039 5251 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  134   146   224   296   463   707   739  1337  3532 18747] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-1.92963
2.02283
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.80398
Epoch 1, cost is  3.77604
Epoch 2, cost is  3.75025
Epoch 3, cost is  3.72983
Epoch 4, cost is  3.70226
Training took 0.071922 minutes
Weight histogram
[7044 5020 3596 3489 4195  657 1508  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2632 2109 2006 2225 2057 2176 2518 3162 3286 4154] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.48038
2.25842
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149273 minutes
Weight histogram
[   48   106   545  1430  1634  7642 10629  6014   299     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 14670   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.115998 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 3162 2508  575] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.48763
Epoch 1, cost is  5.31448
Epoch 2, cost is  4.85352
Epoch 3, cost is  4.3305
Epoch 4, cost is  3.95112
Epoch 5, cost is  3.65667
Epoch 6, cost is  3.40844
Epoch 7, cost is  3.20261
Epoch 8, cost is  3.02415
Epoch 9, cost is  2.87548
Training took 0.536182 minutes
Weight histogram
[1372 1579 1262  955  820 2033   48   15    8    8] [-0.00572379 -0.00516195 -0.00460012 -0.00403828 -0.00347644 -0.0029146
 -0.00235276 -0.00179092 -0.00122909 -0.00066725 -0.00010541]
[1838  519  519  560  621  680  737  802  867  957] [-0.00572379 -0.00516195 -0.00460012 -0.00403828 -0.00347644 -0.0029146
 -0.00235276 -0.00179092 -0.00122909 -0.00066725 -0.00010541]
-0.181317
0.195561
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.060927 minutes
Epoch 0
Fine tuning took 0.062257 minutes
Epoch 0
Fine tuning took 0.060674 minutes
Epoch 0
Fine tuning took 0.061518 minutes
Epoch 0
Fine tuning took 0.061880 minutes
Epoch 0
Fine tuning took 0.061702 minutes
Epoch 0
Fine tuning took 0.060376 minutes
Epoch 0
Fine tuning took 0.062039 minutes
Epoch 0
Fine tuning took 0.060651 minutes
Epoch 0
Fine tuning took 0.061276 minutes
{'zero': {0: [0.13916256157635468, 0.29187192118226601, 0.29310344827586204, 0.24753694581280788, 0.20812807881773399, 0.24507389162561577, 0.27463054187192121, 0.24753694581280788, 0.2376847290640394, 0.26231527093596058, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.40270935960591131, 0.40517241379310343, 0.40270935960591131, 0.49384236453201968, 0.44211822660098521, 0.39285714285714285, 0.45073891625615764, 0.39408866995073893, 0.38423645320197042, 0.39901477832512317], 5: [0.32389162561576357, 0.30541871921182268, 0.30172413793103448, 0.34975369458128081, 0.29802955665024633, 0.31280788177339902, 0.33251231527093594, 0.30172413793103448, 0.3682266009852217, 0.35344827586206895, 0.34482758620689657], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13916256157635468, 0.26477832512315269, 0.28325123152709358, 0.25246305418719212, 0.2229064039408867, 0.25985221674876846, 0.21921182266009853, 0.26108374384236455, 0.25985221674876846, 0.26477832512315269, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.39162561576354682, 0.43472906403940886, 0.43349753694581283, 0.45935960591133007, 0.39901477832512317, 0.43349753694581283, 0.41502463054187194, 0.4211822660098522, 0.37931034482758619, 0.38423645320197042], 5: [0.32389162561576357, 0.34359605911330049, 0.28201970443349755, 0.31403940886699505, 0.31773399014778325, 0.34113300492610837, 0.34729064039408869, 0.32389162561576357, 0.31896551724137934, 0.35591133004926107, 0.37315270935960593], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13916256157635468, 0.31896551724137934, 0.28078817733990147, 0.22536945812807882, 0.18719211822660098, 0.24876847290640394, 0.26600985221674878, 0.25985221674876846, 0.25123152709359609, 0.29310344827586204, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.40517241379310343, 0.43596059113300495, 0.43842364532019706, 0.49137931034482757, 0.43103448275862066, 0.37807881773399016, 0.4248768472906404, 0.3817733990147783, 0.36699507389162561, 0.36330049261083741], 5: [0.32389162561576357, 0.27586206896551724, 0.28325123152709358, 0.33620689655172414, 0.32142857142857145, 0.32019704433497537, 0.35591133004926107, 0.31527093596059114, 0.36699507389162561, 0.33990147783251229, 0.3645320197044335], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13916256157635468, 0.30418719211822659, 0.27093596059113301, 0.26600985221674878, 0.21798029556650247, 0.24384236453201971, 0.25492610837438423, 0.26231527093596058, 0.21674876847290642, 0.25862068965517243, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.37438423645320196, 0.40763546798029554, 0.39039408866995073, 0.46551724137931033, 0.42980295566502463, 0.40886699507389163, 0.42610837438423643, 0.42857142857142855, 0.38054187192118227, 0.37192118226600984], 5: [0.32389162561576357, 0.32142857142857145, 0.32142857142857145, 0.34359605911330049, 0.31650246305418717, 0.32635467980295568, 0.33620689655172414, 0.31157635467980294, 0.35467980295566504, 0.3608374384236453, 0.3645320197044335], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151486 minutes
Weight histogram
[ 168  443  678  641 1495 4524 4715 9268 3913  480] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
[  134   154   231   324   476   854   999  1981  6517 14655] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
-1.31143
1.09692
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.45273
Epoch 1, cost is  2.40671
Epoch 2, cost is  2.37998
Epoch 3, cost is  2.35415
Epoch 4, cost is  2.337
Training took 0.115621 minutes
Weight histogram
[4563 5367 3591 2818 2539 1977 2173 1446 1533  318] [ -5.92379123e-02  -5.33110940e-02  -4.73842758e-02  -4.14574576e-02
  -3.55306393e-02  -2.96038211e-02  -2.36770028e-02  -1.77501846e-02
  -1.18233663e-02  -5.89654807e-03   3.02701774e-05]
[2687 1467 1444 1797 2179 2586 2969 3356 3631 4209] [ -5.92379123e-02  -5.33110940e-02  -4.73842758e-02  -4.14574576e-02
  -3.55306393e-02  -2.96038211e-02  -2.36770028e-02  -1.77501846e-02
  -1.18233663e-02  -5.89654807e-03   3.02701774e-05]
-1.20483
1.59556
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148656 minutes
Weight histogram
[ 223  554 1069 1208 2505 6480 7643 6481 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  273   310   461   634   970  1444   843  1996  4392 17027] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.117849 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 2368 3303  574] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.22828
Epoch 1, cost is  5.98445
Epoch 2, cost is  5.83254
Epoch 3, cost is  5.62583
Epoch 4, cost is  5.28775
Epoch 5, cost is  4.94635
Epoch 6, cost is  4.66979
Epoch 7, cost is  4.44705
Epoch 8, cost is  4.26918
Epoch 9, cost is  4.1127
Training took 0.204998 minutes
Weight histogram
[ 888  918  875  840 1802 1521  858  307   63   28] [-0.02656188 -0.0239213  -0.02128071 -0.01864013 -0.01599955 -0.01335896
 -0.01071838 -0.00807779 -0.00543721 -0.00279663 -0.00015604]
[1804 1231  568  493  535  591  645  701  766  766] [-0.02656188 -0.0239213  -0.02128071 -0.01864013 -0.01599955 -0.01335896
 -0.01071838 -0.00807779 -0.00543721 -0.00279663 -0.00015604]
-0.348383
0.437969
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052691 minutes
Epoch 0
Fine tuning took 0.054050 minutes
Epoch 0
Fine tuning took 0.055211 minutes
Epoch 0
Fine tuning took 0.052257 minutes
Epoch 0
Fine tuning took 0.054352 minutes
Epoch 0
Fine tuning took 0.054319 minutes
Epoch 0
Fine tuning took 0.053307 minutes
Epoch 0
Fine tuning took 0.052246 minutes
Epoch 0
Fine tuning took 0.052182 minutes
Epoch 0
Fine tuning took 0.052805 minutes
{'zero': {0: [0.18226600985221675, 0.15147783251231528, 0.098522167487684734, 0.23152709359605911, 0.15640394088669951, 0.22413793103448276, 0.15147783251231528, 0.17733990147783252, 0.20935960591133004, 0.18472906403940886, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.73399014778325122, 0.83620689655172409, 0.61945812807881773, 0.69581280788177335, 0.5714285714285714, 0.72906403940886699, 0.66748768472906406, 0.6354679802955665, 0.61083743842364535, 0.54556650246305416], 5: [0.11945812807881774, 0.1145320197044335, 0.065270935960591137, 0.14901477832512317, 0.14778325123152711, 0.20443349753694581, 0.11945812807881774, 0.15517241379310345, 0.15517241379310345, 0.20443349753694581, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18226600985221675, 0.25246305418719212, 0.19458128078817735, 0.44334975369458129, 0.2376847290640394, 0.31280788177339902, 0.34482758620689657, 0.37068965517241381, 0.3682266009852217, 0.3460591133004926, 0.30418719211822659], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.63916256157635465, 0.73399014778325122, 0.39162561576354682, 0.67610837438423643, 0.55541871921182262, 0.5714285714285714, 0.51477832512315269, 0.49384236453201968, 0.48522167487684731, 0.49014778325123154], 5: [0.11945812807881774, 0.10837438423645321, 0.071428571428571425, 0.16502463054187191, 0.086206896551724144, 0.13177339901477833, 0.083743842364532015, 0.1145320197044335, 0.13793103448275862, 0.16871921182266009, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18226600985221675, 0.23029556650246305, 0.2105911330049261, 0.43226600985221675, 0.21305418719211822, 0.31527093596059114, 0.33004926108374383, 0.34975369458128081, 0.3288177339901478, 0.30788177339901479, 0.31034482758620691], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.65024630541871919, 0.71798029556650245, 0.42241379310344829, 0.67487684729064035, 0.55788177339901479, 0.56773399014778325, 0.52832512315270941, 0.51354679802955661, 0.54433497536945807, 0.49014778325123154], 5: [0.11945812807881774, 0.11945812807881774, 0.071428571428571425, 0.14532019704433496, 0.11206896551724138, 0.1268472906403941, 0.10221674876847291, 0.12192118226600986, 0.15763546798029557, 0.14778325123152711, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18226600985221675, 0.26847290640394089, 0.21674876847290642, 0.43103448275862066, 0.20566502463054187, 0.35098522167487683, 0.38300492610837439, 0.35591133004926107, 0.36330049261083741, 0.32758620689655171, 0.34113300492610837], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.59852216748768472, 0.73522167487684731, 0.42364532019704432, 0.69458128078817738, 0.52586206896551724, 0.53325123152709364, 0.53817733990147787, 0.51724137931034486, 0.51477832512315269, 0.48645320197044334], 5: [0.11945812807881774, 0.13300492610837439, 0.048029556650246302, 0.14532019704433496, 0.099753694581280791, 0.12315270935960591, 0.083743842364532015, 0.10591133004926108, 0.11945812807881774, 0.15763546798029557, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147979 minutes
Weight histogram
[ 168  443  678  641 1495 4524 4715 9268 3913  480] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
[  134   154   231   324   476   854   999  1981  6517 14655] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
-1.31143
1.09692
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.45273
Epoch 1, cost is  2.40671
Epoch 2, cost is  2.37998
Epoch 3, cost is  2.35415
Epoch 4, cost is  2.337
Training took 0.115154 minutes
Weight histogram
[4563 5367 3591 2818 2539 1977 2173 1446 1533  318] [ -5.92379123e-02  -5.33110940e-02  -4.73842758e-02  -4.14574576e-02
  -3.55306393e-02  -2.96038211e-02  -2.36770028e-02  -1.77501846e-02
  -1.18233663e-02  -5.89654807e-03   3.02701774e-05]
[2687 1467 1444 1797 2179 2586 2969 3356 3631 4209] [ -5.92379123e-02  -5.33110940e-02  -4.73842758e-02  -4.14574576e-02
  -3.55306393e-02  -2.96038211e-02  -2.36770028e-02  -1.77501846e-02
  -1.18233663e-02  -5.89654807e-03   3.02701774e-05]
-1.20483
1.59556
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148323 minutes
Weight histogram
[ 223  554 1069 1208 2505 6480 7643 6481 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  273   310   461   634   970  1444   843  1996  4392 17027] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.115977 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 2368 3303  574] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.61834
Epoch 1, cost is  5.39503
Epoch 2, cost is  5.26285
Epoch 3, cost is  4.98074
Epoch 4, cost is  4.57354
Epoch 5, cost is  4.24726
Epoch 6, cost is  3.98918
Epoch 7, cost is  3.76644
Epoch 8, cost is  3.57216
Epoch 9, cost is  3.40741
Training took 0.298836 minutes
Weight histogram
[1513 1289 1108 2998  725  246  117   56   30   18] [-0.01784189 -0.0160722  -0.01430251 -0.01253282 -0.01076313 -0.00899344
 -0.00722375 -0.00545406 -0.00368437 -0.00191468 -0.00014499]
[2409  617  457  510  571  622  670  719  754  771] [-0.01784189 -0.0160722  -0.01430251 -0.01253282 -0.01076313 -0.00899344
 -0.00722375 -0.00545406 -0.00368437 -0.00191468 -0.00014499]
-0.283091
0.336729
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.058671 minutes
Epoch 0
Fine tuning took 0.060083 minutes
Epoch 0
Fine tuning took 0.058232 minutes
Epoch 0
Fine tuning took 0.059244 minutes
Epoch 0
Fine tuning took 0.059964 minutes
Epoch 0
Fine tuning took 0.059134 minutes
Epoch 0
Fine tuning took 0.059506 minutes
Epoch 0
Fine tuning took 0.057818 minutes
Epoch 0
Fine tuning took 0.058690 minutes
Epoch 0
Fine tuning took 0.057994 minutes
{'zero': {0: [0.12931034482758622, 0.1354679802955665, 0.17980295566502463, 0.12807881773399016, 0.20073891625615764, 0.14901477832512317, 0.20935960591133004, 0.17364532019704434, 0.24261083743842365, 0.23522167487684728, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.69211822660098521, 0.6711822660098522, 0.63916256157635465, 0.59359605911330049, 0.62931034482758619, 0.57266009852216748, 0.67241379310344829, 0.52093596059113301, 0.55418719211822665, 0.58620689655172409], 5: [0.19088669950738915, 0.17241379310344829, 0.14901477832512317, 0.23275862068965517, 0.20566502463054187, 0.22167487684729065, 0.21798029556650247, 0.1539408866995074, 0.23645320197044334, 0.2105911330049261, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12931034482758622, 0.21798029556650247, 0.22413793103448276, 0.16009852216748768, 0.26108374384236455, 0.21551724137931033, 0.16748768472906403, 0.16009852216748768, 0.25985221674876846, 0.24753694581280788, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.60467980295566504, 0.54556650246305416, 0.69950738916256161, 0.53940886699507384, 0.56896551724137934, 0.58743842364532017, 0.67487684729064035, 0.54926108374384242, 0.49876847290640391, 0.55665024630541871], 5: [0.19088669950738915, 0.17733990147783252, 0.23029556650246305, 0.14039408866995073, 0.19950738916256158, 0.21551724137931033, 0.24507389162561577, 0.16502463054187191, 0.19088669950738915, 0.2536945812807882, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12931034482758622, 0.21428571428571427, 0.22660098522167488, 0.14162561576354679, 0.22167487684729065, 0.18842364532019704, 0.20935960591133004, 0.1748768472906404, 0.26477832512315269, 0.25123152709359609, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.6219211822660099, 0.55172413793103448, 0.68472906403940892, 0.56403940886699511, 0.60591133004926112, 0.52955665024630538, 0.6354679802955665, 0.53817733990147787, 0.49261083743842365, 0.54679802955665024], 5: [0.19088669950738915, 0.16379310344827586, 0.22167487684729065, 0.17364532019704434, 0.21428571428571427, 0.20566502463054187, 0.26108374384236455, 0.18965517241379309, 0.19704433497536947, 0.25615763546798032, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12931034482758622, 0.28325123152709358, 0.26847290640394089, 0.13669950738916256, 0.24014778325123154, 0.19950738916256158, 0.22413793103448276, 0.17241379310344829, 0.24507389162561577, 0.25246305418719212, 0.27955665024630544], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.55172413793103448, 0.52709359605911332, 0.70812807881773399, 0.55665024630541871, 0.59852216748768472, 0.55295566502463056, 0.64162561576354682, 0.51847290640394084, 0.49014778325123154, 0.53817733990147787], 5: [0.19088669950738915, 0.16502463054187191, 0.20443349753694581, 0.15517241379310345, 0.20320197044334976, 0.2019704433497537, 0.2229064039408867, 0.18596059113300492, 0.23645320197044334, 0.25738916256157635, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149018 minutes
Weight histogram
[ 168  443  678  641 1495 4524 4715 9268 3913  480] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
[  134   154   231   324   476   854   999  1981  6517 14655] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
-1.31143
1.09692
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.45273
Epoch 1, cost is  2.40671
Epoch 2, cost is  2.37998
Epoch 3, cost is  2.35415
Epoch 4, cost is  2.337
Training took 0.115956 minutes
Weight histogram
[4563 5367 3591 2818 2539 1977 2173 1446 1533  318] [ -5.92379123e-02  -5.33110940e-02  -4.73842758e-02  -4.14574576e-02
  -3.55306393e-02  -2.96038211e-02  -2.36770028e-02  -1.77501846e-02
  -1.18233663e-02  -5.89654807e-03   3.02701774e-05]
[2687 1467 1444 1797 2179 2586 2969 3356 3631 4209] [ -5.92379123e-02  -5.33110940e-02  -4.73842758e-02  -4.14574576e-02
  -3.55306393e-02  -2.96038211e-02  -2.36770028e-02  -1.77501846e-02
  -1.18233663e-02  -5.89654807e-03   3.02701774e-05]
-1.20483
1.59556
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150401 minutes
Weight histogram
[ 223  554 1069 1208 2505 6480 7643 6481 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  273   310   461   634   970  1444   843  1996  4392 17027] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.115800 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 2368 3303  574] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.25733
Epoch 1, cost is  5.11289
Epoch 2, cost is  4.94252
Epoch 3, cost is  4.52789
Epoch 4, cost is  4.13072
Epoch 5, cost is  3.83168
Epoch 6, cost is  3.57758
Epoch 7, cost is  3.34436
Epoch 8, cost is  3.15133
Epoch 9, cost is  2.98861
Training took 0.413539 minutes
Weight histogram
[1894 1839 3471  465  202  108   59   32   18   12] [-0.01199183 -0.01080513 -0.00961844 -0.00843174 -0.00724504 -0.00605834
 -0.00487164 -0.00368495 -0.00249825 -0.00131155 -0.00012485]
[2248  516  469  550  603  667  693  729  794  831] [-0.01199183 -0.01080513 -0.00961844 -0.00843174 -0.00724504 -0.00605834
 -0.00487164 -0.00368495 -0.00249825 -0.00131155 -0.00012485]
-0.216932
0.260704
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.063760 minutes
Epoch 0
Fine tuning took 0.062883 minutes
Epoch 0
Fine tuning took 0.063723 minutes
Epoch 0
Fine tuning took 0.064829 minutes
Epoch 0
Fine tuning took 0.063870 minutes
Epoch 0
Fine tuning took 0.063532 minutes
Epoch 0
Fine tuning took 0.064536 minutes
Epoch 0
Fine tuning took 0.063111 minutes
Epoch 0
Fine tuning took 0.063923 minutes
Epoch 0
Fine tuning took 0.065157 minutes
{'zero': {0: [0.13300492610837439, 0.20566502463054187, 0.16995073891625614, 0.21551724137931033, 0.21921182266009853, 0.19950738916256158, 0.24507389162561577, 0.20566502463054187, 0.23645320197044334, 0.21921182266009853, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.57758620689655171, 0.54926108374384242, 0.54556650246305416, 0.52709359605911332, 0.53694581280788178, 0.50123152709359609, 0.57019704433497542, 0.48029556650246308, 0.45689655172413796, 0.49261083743842365], 5: [0.23891625615763548, 0.21674876847290642, 0.28078817733990147, 0.23891625615763548, 0.2536945812807882, 0.26354679802955666, 0.2536945812807882, 0.22413793103448276, 0.28325123152709358, 0.32389162561576357, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13300492610837439, 0.24261083743842365, 0.18103448275862069, 0.22660098522167488, 0.20812807881773399, 0.24384236453201971, 0.22660098522167488, 0.22044334975369459, 0.27709359605911332, 0.24014778325123154, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.53448275862068961, 0.54926108374384242, 0.55541871921182262, 0.52955665024630538, 0.53448275862068961, 0.50492610837438423, 0.5357142857142857, 0.46182266009852219, 0.43596059113300495, 0.49384236453201968], 5: [0.23891625615763548, 0.2229064039408867, 0.26970443349753692, 0.21798029556650247, 0.26231527093596058, 0.22167487684729065, 0.26847290640394089, 0.24384236453201971, 0.26108374384236455, 0.32389162561576357, 0.28078817733990147], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13300492610837439, 0.24507389162561577, 0.19211822660098521, 0.21182266009852216, 0.21674876847290642, 0.2536945812807882, 0.22167487684729065, 0.23275862068965517, 0.23522167487684728, 0.24876847290640394, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.52586206896551724, 0.54802955665024633, 0.53201970443349755, 0.49137931034482757, 0.5, 0.47167487684729065, 0.50615763546798032, 0.47413793103448276, 0.44458128078817732, 0.50123152709359609], 5: [0.23891625615763548, 0.22906403940886699, 0.25985221674876846, 0.25615763546798032, 0.29187192118226601, 0.24630541871921183, 0.30665024630541871, 0.26108374384236455, 0.29064039408866993, 0.30665024630541871, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13300492610837439, 0.24876847290640394, 0.19334975369458129, 0.25738916256157635, 0.21798029556650247, 0.23645320197044334, 0.27463054187192121, 0.19458128078817735, 0.26724137931034481, 0.2413793103448276, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.50615763546798032, 0.54802955665024633, 0.52955665024630538, 0.50862068965517238, 0.52709359605911332, 0.44581280788177341, 0.56527093596059108, 0.47290640394088668, 0.45566502463054187, 0.47290640394088668], 5: [0.23891625615763548, 0.24507389162561577, 0.25862068965517243, 0.21305418719211822, 0.27339901477832512, 0.23645320197044334, 0.27955665024630544, 0.24014778325123154, 0.25985221674876846, 0.30295566502463056, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151608 minutes
Weight histogram
[ 168  443  678  641 1495 4524 4715 9268 3913  480] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
[  134   154   231   324   476   854   999  1981  6517 14655] [-0.00013604  0.00017445  0.00048494  0.00079543  0.00110592  0.00141641
  0.0017269   0.00203739  0.00234788  0.00265837  0.00296886]
-1.31143
1.09692
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.45273
Epoch 1, cost is  2.40671
Epoch 2, cost is  2.37998
Epoch 3, cost is  2.35415
Epoch 4, cost is  2.337
Training took 0.115434 minutes
Weight histogram
[4563 5367 3591 2818 2539 1977 2173 1446 1533  318] [ -5.92379123e-02  -5.33110940e-02  -4.73842758e-02  -4.14574576e-02
  -3.55306393e-02  -2.96038211e-02  -2.36770028e-02  -1.77501846e-02
  -1.18233663e-02  -5.89654807e-03   3.02701774e-05]
[2687 1467 1444 1797 2179 2586 2969 3356 3631 4209] [ -5.92379123e-02  -5.33110940e-02  -4.73842758e-02  -4.14574576e-02
  -3.55306393e-02  -2.96038211e-02  -2.36770028e-02  -1.77501846e-02
  -1.18233663e-02  -5.89654807e-03   3.02701774e-05]
-1.20483
1.59556
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149275 minutes
Weight histogram
[ 223  554 1069 1208 2505 6480 7643 6481 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  273   310   461   634   970  1444   843  1996  4392 17027] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.117839 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 2368 3303  574] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.21651
Epoch 1, cost is  5.02701
Epoch 2, cost is  4.52435
Epoch 3, cost is  3.98095
Epoch 4, cost is  3.56912
Epoch 5, cost is  3.23987
Epoch 6, cost is  2.98492
Epoch 7, cost is  2.78306
Epoch 8, cost is  2.62051
Epoch 9, cost is  2.48454
Training took 0.685414 minutes
Weight histogram
[1616 1497 1205  966  865 1877   41   17    8    8] [-0.0065721  -0.00592732 -0.00528254 -0.00463776 -0.00399298 -0.00334821
 -0.00270343 -0.00205865 -0.00141387 -0.00076909 -0.00012431]
[1817  500  514  559  607  647  712  802  914 1028] [-0.0065721  -0.00592732 -0.00528254 -0.00463776 -0.00399298 -0.00334821
 -0.00270343 -0.00205865 -0.00141387 -0.00076909 -0.00012431]
-0.179131
0.189843
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076578 minutes
Epoch 0
Fine tuning took 0.077969 minutes
Epoch 0
Fine tuning took 0.078057 minutes
Epoch 0
Fine tuning took 0.076353 minutes
Epoch 0
Fine tuning took 0.077390 minutes
Epoch 0
Fine tuning took 0.077531 minutes
Epoch 0
Fine tuning took 0.077645 minutes
Epoch 0
Fine tuning took 0.076299 minutes
Epoch 0
Fine tuning took 0.078322 minutes
Epoch 0
Fine tuning took 0.078184 minutes
{'zero': {0: [0.15763546798029557, 0.2894088669950739, 0.24630541871921183, 0.26724137931034481, 0.22906403940886699, 0.27709359605911332, 0.25862068965517243, 0.26477832512315269, 0.23152709359605911, 0.23891625615763548, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51724137931034486, 0.40147783251231528, 0.45197044334975367, 0.44704433497536944, 0.46551724137931033, 0.44458128078817732, 0.44211822660098521, 0.44827586206896552, 0.41379310344827586, 0.43719211822660098, 0.41133004926108374], 5: [0.3251231527093596, 0.30911330049261082, 0.30172413793103448, 0.2857142857142857, 0.30541871921182268, 0.27832512315270935, 0.29926108374384236, 0.28694581280788178, 0.35467980295566504, 0.32389162561576357, 0.3251231527093596], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.31896551724137934, 0.2376847290640394, 0.24384236453201971, 0.23152709359605911, 0.23645320197044334, 0.28201970443349755, 0.28694581280788178, 0.26354679802955666, 0.2376847290640394, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51724137931034486, 0.4248768472906404, 0.46182266009852219, 0.45935960591133007, 0.43596059113300495, 0.45689655172413796, 0.41502463054187194, 0.44088669950738918, 0.41625615763546797, 0.42980295566502463, 0.3891625615763547], 5: [0.3251231527093596, 0.25615763546798032, 0.30049261083743845, 0.29679802955665024, 0.33251231527093594, 0.30665024630541871, 0.30295566502463056, 0.27216748768472904, 0.32019704433497537, 0.33251231527093594, 0.33990147783251229], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.30541871921182268, 0.2413793103448276, 0.24014778325123154, 0.25123152709359609, 0.25, 0.27463054187192121, 0.29926108374384236, 0.21674876847290642, 0.2413793103448276, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51724137931034486, 0.39778325123152708, 0.44827586206896552, 0.46674876847290642, 0.43965517241379309, 0.44334975369458129, 0.42241379310344829, 0.42980295566502463, 0.40147783251231528, 0.44088669950738918, 0.39408866995073893], 5: [0.3251231527093596, 0.29679802955665024, 0.31034482758620691, 0.29310344827586204, 0.30911330049261082, 0.30665024630541871, 0.30295566502463056, 0.27093596059113301, 0.3817733990147783, 0.31773399014778325, 0.33866995073891626], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.30788177339901479, 0.26231527093596058, 0.22413793103448276, 0.22413793103448276, 0.26847290640394089, 0.2857142857142857, 0.29064039408866993, 0.24630541871921183, 0.24014778325123154, 0.29556650246305421], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51724137931034486, 0.39901477832512317, 0.43842364532019706, 0.44458128078817732, 0.47783251231527096, 0.43719211822660098, 0.3891625615763547, 0.43472906403940886, 0.42241379310344829, 0.45689655172413796, 0.3854679802955665], 5: [0.3251231527093596, 0.29310344827586204, 0.29926108374384236, 0.33128078817733991, 0.29802955665024633, 0.29433497536945813, 0.3251231527093596, 0.27463054187192121, 0.33128078817733991, 0.30295566502463056, 0.31896551724137934], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235260 minutes
Weight histogram
[ 161  475  719  520  561 1635 2563 9230 9683  778] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
[  153   271   383   678   892  1153  2561  3562  5646 11026] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
-1.16234
1.29103
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.56639
Epoch 1, cost is  1.53489
Epoch 2, cost is  1.51262
Epoch 3, cost is  1.49399
Epoch 4, cost is  1.47947
Training took 0.222843 minutes
Weight histogram
[5766 4200 3949 2725 2308 1994 1566 1392 1420 1005] [ -5.90904839e-02  -5.31746720e-02  -4.72588601e-02  -4.13430482e-02
  -3.54272362e-02  -2.95114243e-02  -2.35956124e-02  -1.76798004e-02
  -1.17639885e-02  -5.84817659e-03   6.76353375e-05]
[2294 1347 1646 1895 2239 2463 3055 3157 3933 4296] [ -5.90904839e-02  -5.31746720e-02  -4.72588601e-02  -4.13430482e-02
  -3.54272362e-02  -2.95114243e-02  -2.35956124e-02  -1.76798004e-02
  -1.17639885e-02  -5.84817659e-03   6.76353375e-05]
-0.937644
1.69218
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148647 minutes
Weight histogram
[ 356 1117 1465  803 2030 6324 7596 6472 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  466   918  1151   306   449   802   843  1996  4392 17027] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.115482 minutes
Weight histogram
[4411 4567 3212 3042 2884 2094 1928 2056 3090 1066] [ -5.58358729e-02  -5.02455221e-02  -4.46551712e-02  -3.90648204e-02
  -3.34744696e-02  -2.78841188e-02  -2.22937680e-02  -1.67034171e-02
  -1.11130663e-02  -5.52271549e-03   6.76353375e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02455221e-02  -4.46551712e-02  -3.90648204e-02
  -3.34744696e-02  -2.78841188e-02  -2.22937680e-02  -1.67034171e-02
  -1.11130663e-02  -5.52271549e-03   6.76353375e-05]
-1.38457
1.55076
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.3186
Epoch 1, cost is  6.04562
Epoch 2, cost is  5.78776
Epoch 3, cost is  5.4307
Epoch 4, cost is  5.08037
Epoch 5, cost is  4.78988
Epoch 6, cost is  4.54632
Epoch 7, cost is  4.34328
Epoch 8, cost is  4.16634
Epoch 9, cost is  4.01389
Training took 0.247915 minutes
Weight histogram
[ 909  918  854  793  807  990 1477  954  363   35] [-0.02840349 -0.02558004 -0.0227566  -0.01993315 -0.01710971 -0.01428626
 -0.01146282 -0.00863937 -0.00581593 -0.00299249 -0.00016904]
[1601  890  572  579  600  664  723  784  832  855] [-0.02840349 -0.02558004 -0.0227566  -0.01993315 -0.01710971 -0.01428626
 -0.01146282 -0.00863937 -0.00581593 -0.00299249 -0.00016904]
-0.36644
0.599255
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076086 minutes
Epoch 0
Fine tuning took 0.075885 minutes
Epoch 0
Fine tuning took 0.075409 minutes
Epoch 0
Fine tuning took 0.076165 minutes
Epoch 0
Fine tuning took 0.075293 minutes
Epoch 0
Fine tuning took 0.076592 minutes
Epoch 0
Fine tuning took 0.076562 minutes
Epoch 0
Fine tuning took 0.076924 minutes
Epoch 0
Fine tuning took 0.077330 minutes
Epoch 0
Fine tuning took 0.076691 minutes
{'zero': {0: [0.12561576354679804, 0.082512315270935957, 0.31773399014778325, 0.31527093596059114, 0.23522167487684728, 0.22783251231527094, 0.25492610837438423, 0.21798029556650247, 0.15270935960591134, 0.17610837438423646, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.80049261083743839, 0.56773399014778325, 0.51724137931034486, 0.65394088669950734, 0.56650246305418717, 0.64532019704433496, 0.67241379310344829, 0.69458128078817738, 0.6428571428571429, 0.55049261083743839], 5: [0.14285714285714285, 0.11699507389162561, 0.1145320197044335, 0.16748768472906403, 0.11083743842364532, 0.20566502463054187, 0.099753694581280791, 0.10960591133004927, 0.15270935960591134, 0.18103448275862069, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12561576354679804, 0.1354679802955665, 0.38054187192118227, 0.41995073891625617, 0.31650246305418717, 0.24014778325123154, 0.33743842364532017, 0.23029556650246305, 0.22167487684729065, 0.23275862068965517, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.74753694581280783, 0.52093596059113301, 0.37807881773399016, 0.60467980295566504, 0.6280788177339901, 0.57758620689655171, 0.66256157635467983, 0.66502463054187189, 0.59482758620689657, 0.50246305418719217], 5: [0.14285714285714285, 0.11699507389162561, 0.098522167487684734, 0.2019704433497537, 0.078817733990147784, 0.13177339901477833, 0.084975369458128072, 0.10714285714285714, 0.11330049261083744, 0.17241379310344829, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12561576354679804, 0.12561576354679804, 0.39778325123152708, 0.36945812807881773, 0.26477832512315269, 0.2413793103448276, 0.32758620689655171, 0.25123152709359609, 0.22783251231527094, 0.21798029556650247, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.73399014778325122, 0.51231527093596063, 0.42733990147783252, 0.64162561576354682, 0.59852216748768472, 0.58251231527093594, 0.62068965517241381, 0.65270935960591137, 0.5923645320197044, 0.5], 5: [0.14285714285714285, 0.14039408866995073, 0.089901477832512317, 0.20320197044334976, 0.093596059113300489, 0.16009852216748768, 0.089901477832512317, 0.12807881773399016, 0.11945812807881774, 0.18965517241379309, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12561576354679804, 0.14532019704433496, 0.41625615763546797, 0.41256157635467983, 0.28325123152709358, 0.25, 0.32635467980295568, 0.24507389162561577, 0.22413793103448276, 0.2413793103448276, 0.33251231527093594], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.72290640394088668, 0.48768472906403942, 0.38423645320197042, 0.64039408866995073, 0.6071428571428571, 0.60591133004926112, 0.63916256157635465, 0.67980295566502458, 0.61945812807881773, 0.5], 5: [0.14285714285714285, 0.13177339901477833, 0.096059113300492605, 0.20320197044334976, 0.076354679802955669, 0.14285714285714285, 0.067733990147783252, 0.11576354679802955, 0.096059113300492605, 0.13916256157635468, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234705 minutes
Weight histogram
[ 161  475  719  520  561 1635 2563 9230 9683  778] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
[  153   271   383   678   892  1153  2561  3562  5646 11026] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
-1.16234
1.29103
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.56639
Epoch 1, cost is  1.53489
Epoch 2, cost is  1.51262
Epoch 3, cost is  1.49399
Epoch 4, cost is  1.47947
Training took 0.223926 minutes
Weight histogram
[5766 4200 3949 2725 2308 1994 1566 1392 1420 1005] [ -5.90904839e-02  -5.31746720e-02  -4.72588601e-02  -4.13430482e-02
  -3.54272362e-02  -2.95114243e-02  -2.35956124e-02  -1.76798004e-02
  -1.17639885e-02  -5.84817659e-03   6.76353375e-05]
[2294 1347 1646 1895 2239 2463 3055 3157 3933 4296] [ -5.90904839e-02  -5.31746720e-02  -4.72588601e-02  -4.13430482e-02
  -3.54272362e-02  -2.95114243e-02  -2.35956124e-02  -1.76798004e-02
  -1.17639885e-02  -5.84817659e-03   6.76353375e-05]
-0.937644
1.69218
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149185 minutes
Weight histogram
[ 356 1117 1465  803 2030 6324 7596 6472 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  466   918  1151   306   449   802   843  1996  4392 17027] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.116383 minutes
Weight histogram
[4411 4567 3212 3042 2884 2094 1928 2056 3090 1066] [ -5.58358729e-02  -5.02455221e-02  -4.46551712e-02  -3.90648204e-02
  -3.34744696e-02  -2.78841188e-02  -2.22937680e-02  -1.67034171e-02
  -1.11130663e-02  -5.52271549e-03   6.76353375e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02455221e-02  -4.46551712e-02  -3.90648204e-02
  -3.34744696e-02  -2.78841188e-02  -2.22937680e-02  -1.67034171e-02
  -1.11130663e-02  -5.52271549e-03   6.76353375e-05]
-1.38457
1.55076
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.71777
Epoch 1, cost is  5.44733
Epoch 2, cost is  5.13204
Epoch 3, cost is  4.6676
Epoch 4, cost is  4.27289
Epoch 5, cost is  3.96531
Epoch 6, cost is  3.72063
Epoch 7, cost is  3.51375
Epoch 8, cost is  3.34198
Epoch 9, cost is  3.19409
Training took 0.355140 minutes
Weight histogram
[1314 1187 1035  853  885 2032  599  136   39   20] [-0.01835062 -0.0165294  -0.01470819 -0.01288698 -0.01106576 -0.00924455
 -0.00742333 -0.00560212 -0.00378091 -0.00195969 -0.00013848]
[2000  549  516  562  623  697  735  776  827  815] [-0.01835062 -0.0165294  -0.01470819 -0.01288698 -0.01106576 -0.00924455
 -0.00742333 -0.00560212 -0.00378091 -0.00195969 -0.00013848]
-0.259161
0.341686
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082050 minutes
Epoch 0
Fine tuning took 0.081562 minutes
Epoch 0
Fine tuning took 0.081682 minutes
Epoch 0
Fine tuning took 0.081679 minutes
Epoch 0
Fine tuning took 0.081442 minutes
Epoch 0
Fine tuning took 0.081182 minutes
Epoch 0
Fine tuning took 0.080460 minutes
Epoch 0
Fine tuning took 0.081744 minutes
Epoch 0
Fine tuning took 0.080670 minutes
Epoch 0
Fine tuning took 0.082203 minutes
{'zero': {0: [0.16871921182266009, 0.14408866995073891, 0.10098522167487685, 0.14408866995073891, 0.14901477832512317, 0.14285714285714285, 0.1748768472906404, 0.13423645320197045, 0.1354679802955665, 0.17364532019704434, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.63300492610837433, 0.74753694581280783, 0.65886699507389157, 0.6785714285714286, 0.63916256157635465, 0.58251231527093594, 0.70443349753694584, 0.59852216748768472, 0.64408866995073888, 0.5714285714285714], 5: [0.21674876847290642, 0.2229064039408867, 0.15147783251231528, 0.19704433497536947, 0.17241379310344829, 0.21798029556650247, 0.24261083743842365, 0.16133004926108374, 0.26600985221674878, 0.18226600985221675, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16871921182266009, 0.27339901477832512, 0.20073891625615764, 0.29310344827586204, 0.30172413793103448, 0.25738916256157635, 0.21551724137931033, 0.17364532019704434, 0.23522167487684728, 0.18965517241379309, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.54064039408866993, 0.66748768472906406, 0.54926108374384242, 0.56280788177339902, 0.57758620689655171, 0.58620689655172409, 0.69334975369458129, 0.54679802955665024, 0.55172413793103448, 0.51724137931034486], 5: [0.21674876847290642, 0.18596059113300492, 0.13177339901477833, 0.15763546798029557, 0.1354679802955665, 0.16502463054187191, 0.19827586206896552, 0.13300492610837439, 0.21798029556650247, 0.25862068965517243, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16871921182266009, 0.25862068965517243, 0.19211822660098521, 0.27709359605911332, 0.25615763546798032, 0.23522167487684728, 0.20443349753694581, 0.15517241379310345, 0.19088669950738915, 0.19950738916256158, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.57389162561576357, 0.68349753694581283, 0.58251231527093594, 0.60837438423645318, 0.60098522167487689, 0.5788177339901478, 0.67733990147783252, 0.56773399014778325, 0.58866995073891626, 0.52093596059113301], 5: [0.21674876847290642, 0.16748768472906403, 0.12438423645320197, 0.14039408866995073, 0.1354679802955665, 0.16379310344827586, 0.21674876847290642, 0.16748768472906403, 0.2413793103448276, 0.21182266009852216, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16871921182266009, 0.29187192118226601, 0.21798029556650247, 0.31896551724137934, 0.28694581280788178, 0.24261083743842365, 0.19704433497536947, 0.16379310344827586, 0.23275862068965517, 0.17733990147783252, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.53694581280788178, 0.66256157635467983, 0.52586206896551724, 0.54926108374384242, 0.60221674876847286, 0.6280788177339901, 0.68103448275862066, 0.5357142857142857, 0.59359605911330049, 0.52093596059113301], 5: [0.21674876847290642, 0.17118226600985223, 0.11945812807881774, 0.15517241379310345, 0.16379310344827586, 0.15517241379310345, 0.1748768472906404, 0.15517241379310345, 0.23152709359605911, 0.22906403940886699, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234707 minutes
Weight histogram
[ 161  475  719  520  561 1635 2563 9230 9683  778] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
[  153   271   383   678   892  1153  2561  3562  5646 11026] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
-1.16234
1.29103
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.56639
Epoch 1, cost is  1.53489
Epoch 2, cost is  1.51262
Epoch 3, cost is  1.49399
Epoch 4, cost is  1.47947
Training took 0.222601 minutes
Weight histogram
[5766 4200 3949 2725 2308 1994 1566 1392 1420 1005] [ -5.90904839e-02  -5.31746720e-02  -4.72588601e-02  -4.13430482e-02
  -3.54272362e-02  -2.95114243e-02  -2.35956124e-02  -1.76798004e-02
  -1.17639885e-02  -5.84817659e-03   6.76353375e-05]
[2294 1347 1646 1895 2239 2463 3055 3157 3933 4296] [ -5.90904839e-02  -5.31746720e-02  -4.72588601e-02  -4.13430482e-02
  -3.54272362e-02  -2.95114243e-02  -2.35956124e-02  -1.76798004e-02
  -1.17639885e-02  -5.84817659e-03   6.76353375e-05]
-0.937644
1.69218
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149525 minutes
Weight histogram
[ 356 1117 1465  803 2030 6324 7596 6472 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  466   918  1151   306   449   802   843  1996  4392 17027] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.116419 minutes
Weight histogram
[4411 4567 3212 3042 2884 2094 1928 2056 3090 1066] [ -5.58358729e-02  -5.02455221e-02  -4.46551712e-02  -3.90648204e-02
  -3.34744696e-02  -2.78841188e-02  -2.22937680e-02  -1.67034171e-02
  -1.11130663e-02  -5.52271549e-03   6.76353375e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02455221e-02  -4.46551712e-02  -3.90648204e-02
  -3.34744696e-02  -2.78841188e-02  -2.22937680e-02  -1.67034171e-02
  -1.11130663e-02  -5.52271549e-03   6.76353375e-05]
-1.38457
1.55076
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.14291
Epoch 1, cost is  4.91472
Epoch 2, cost is  4.59712
Epoch 3, cost is  4.15941
Epoch 4, cost is  3.77345
Epoch 5, cost is  3.47266
Epoch 6, cost is  3.22814
Epoch 7, cost is  3.02506
Epoch 8, cost is  2.85763
Epoch 9, cost is  2.71644
Training took 0.544986 minutes
Weight histogram
[1686 1571 1293 2733  463  188   88   43   22   13] [-0.01290335 -0.01162615 -0.01034895 -0.00907175 -0.00779455 -0.00651734
 -0.00524014 -0.00396294 -0.00268574 -0.00140854 -0.00013134]
[1998  554  530  569  619  673  711  760  824  862] [-0.01290335 -0.01162615 -0.01034895 -0.00907175 -0.00779455 -0.00651734
 -0.00524014 -0.00396294 -0.00268574 -0.00140854 -0.00013134]
-0.22277
0.258137
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090038 minutes
Epoch 0
Fine tuning took 0.091258 minutes
Epoch 0
Fine tuning took 0.091780 minutes
Epoch 0
Fine tuning took 0.090246 minutes
Epoch 0
Fine tuning took 0.090290 minutes
Epoch 0
Fine tuning took 0.091122 minutes
Epoch 0
Fine tuning took 0.091425 minutes
Epoch 0
Fine tuning took 0.091361 minutes
Epoch 0
Fine tuning took 0.090286 minutes
Epoch 0
Fine tuning took 0.090178 minutes
{'zero': {0: [0.13916256157635468, 0.19581280788177341, 0.15763546798029557, 0.17118226600985223, 0.21551724137931033, 0.15886699507389163, 0.19827586206896552, 0.18472906403940886, 0.20443349753694581, 0.24014778325123154, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.59359605911330049, 0.63793103448275867, 0.59975369458128081, 0.55665024630541871, 0.63669950738916259, 0.52709359605911332, 0.5923645320197044, 0.53940886699507384, 0.5, 0.57266009852216748], 5: [0.21182266009852216, 0.2105911330049261, 0.20443349753694581, 0.22906403940886699, 0.22783251231527094, 0.20443349753694581, 0.27463054187192121, 0.2229064039408867, 0.25615763546798032, 0.25985221674876846, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13916256157635468, 0.20320197044334976, 0.19211822660098521, 0.20320197044334976, 0.23152709359605911, 0.16995073891625614, 0.21921182266009853, 0.20812807881773399, 0.19704433497536947, 0.2376847290640394, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.57266009852216748, 0.55418719211822665, 0.61699507389162567, 0.50615763546798032, 0.59359605911330049, 0.50985221674876846, 0.56773399014778325, 0.55911330049261088, 0.48522167487684731, 0.52093596059113301], 5: [0.21182266009852216, 0.22413793103448276, 0.2536945812807882, 0.17980295566502463, 0.26231527093596058, 0.23645320197044334, 0.27093596059113301, 0.22413793103448276, 0.24384236453201971, 0.27709359605911332, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13916256157635468, 0.22536945812807882, 0.19704433497536947, 0.21182266009852216, 0.21674876847290642, 0.17733990147783252, 0.18596059113300492, 0.19950738916256158, 0.21305418719211822, 0.25, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.55172413793103448, 0.59482758620689657, 0.59975369458128081, 0.53448275862068961, 0.57266009852216748, 0.54802955665024633, 0.57635467980295563, 0.50123152709359609, 0.48645320197044334, 0.55788177339901479], 5: [0.21182266009852216, 0.2229064039408867, 0.20812807881773399, 0.18842364532019704, 0.24876847290640394, 0.25, 0.26600985221674878, 0.22413793103448276, 0.2857142857142857, 0.26354679802955666, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13916256157635468, 0.22536945812807882, 0.17733990147783252, 0.20320197044334976, 0.23645320197044334, 0.19704433497536947, 0.23891625615763548, 0.21305418719211822, 0.21798029556650247, 0.2536945812807882, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.51724137931034486, 0.58004926108374388, 0.58990147783251234, 0.51231527093596063, 0.58251231527093594, 0.51354679802955661, 0.56157635467980294, 0.52709359605911332, 0.48891625615763545, 0.54187192118226601], 5: [0.21182266009852216, 0.25738916256157635, 0.24261083743842365, 0.20689655172413793, 0.25123152709359609, 0.22044334975369459, 0.24753694581280788, 0.22536945812807882, 0.25492610837438423, 0.25738916256157635, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235129 minutes
Weight histogram
[ 161  475  719  520  561 1635 2563 9230 9683  778] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
[  153   271   383   678   892  1153  2561  3562  5646 11026] [ -1.30836663e-04   4.84569886e-05   2.27750640e-04   4.07044291e-04
   5.86337943e-04   7.65631594e-04   9.44925245e-04   1.12421890e-03
   1.30351255e-03   1.48280620e-03   1.66209985e-03]
-1.16234
1.29103
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.56639
Epoch 1, cost is  1.53489
Epoch 2, cost is  1.51262
Epoch 3, cost is  1.49399
Epoch 4, cost is  1.47947
Training took 0.223986 minutes
Weight histogram
[5766 4200 3949 2725 2308 1994 1566 1392 1420 1005] [ -5.90904839e-02  -5.31746720e-02  -4.72588601e-02  -4.13430482e-02
  -3.54272362e-02  -2.95114243e-02  -2.35956124e-02  -1.76798004e-02
  -1.17639885e-02  -5.84817659e-03   6.76353375e-05]
[2294 1347 1646 1895 2239 2463 3055 3157 3933 4296] [ -5.90904839e-02  -5.31746720e-02  -4.72588601e-02  -4.13430482e-02
  -3.54272362e-02  -2.95114243e-02  -2.35956124e-02  -1.76798004e-02
  -1.17639885e-02  -5.84817659e-03   6.76353375e-05]
-0.937644
1.69218
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150460 minutes
Weight histogram
[ 356 1117 1465  803 2030 6324 7596 6472 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  466   918  1151   306   449   802   843  1996  4392 17027] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.115596 minutes
Weight histogram
[4411 4567 3212 3042 2884 2094 1928 2056 3090 1066] [ -5.58358729e-02  -5.02455221e-02  -4.46551712e-02  -3.90648204e-02
  -3.34744696e-02  -2.78841188e-02  -2.22937680e-02  -1.67034171e-02
  -1.11130663e-02  -5.52271549e-03   6.76353375e-05]
[4864 1388 1542 2004 2295 2572 2932 3258 3508 3987] [ -5.58358729e-02  -5.02455221e-02  -4.46551712e-02  -3.90648204e-02
  -3.34744696e-02  -2.78841188e-02  -2.22937680e-02  -1.67034171e-02
  -1.11130663e-02  -5.52271549e-03   6.76353375e-05]
-1.38457
1.55076
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.93415
Epoch 1, cost is  4.64303
Epoch 2, cost is  4.05803
Epoch 3, cost is  3.56293
Epoch 4, cost is  3.20008
Epoch 5, cost is  2.92384
Epoch 6, cost is  2.706
Epoch 7, cost is  2.53038
Epoch 8, cost is  2.39063
Epoch 9, cost is  2.27928
Training took 0.942923 minutes
Weight histogram
[1760 1557 1394 1014 1956  315   60   24   12    8] [-0.00793658 -0.00715525 -0.00637391 -0.00559258 -0.00481125 -0.00402992
 -0.00324858 -0.00246725 -0.00168592 -0.00090459 -0.00012325]
[1644  504  523  591  628  689  753  829  933 1006] [-0.00793658 -0.00715525 -0.00637391 -0.00559258 -0.00481125 -0.00402992
 -0.00324858 -0.00246725 -0.00168592 -0.00090459 -0.00012325]
-0.175686
0.188538
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.110036 minutes
Epoch 0
Fine tuning took 0.110659 minutes
Epoch 0
Fine tuning took 0.109553 minutes
Epoch 0
Fine tuning took 0.109728 minutes
Epoch 0
Fine tuning took 0.110338 minutes
Epoch 0
Fine tuning took 0.110012 minutes
Epoch 0
Fine tuning took 0.108541 minutes
Epoch 0
Fine tuning took 0.109743 minutes
Epoch 0
Fine tuning took 0.108772 minutes
Epoch 0
Fine tuning took 0.109666 minutes
{'zero': {0: [0.13054187192118227, 0.25862068965517243, 0.22044334975369459, 0.24876847290640394, 0.27093596059113301, 0.20812807881773399, 0.27339901477832512, 0.21674876847290642, 0.24261083743842365, 0.23645320197044334, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.48645320197044334, 0.49261083743842365, 0.48891625615763545, 0.46305418719211822, 0.5, 0.41748768472906406, 0.49507389162561577, 0.4642857142857143, 0.44211822660098521, 0.49261083743842365], 5: [0.26477832512315269, 0.25492610837438423, 0.28694581280788178, 0.26231527093596058, 0.26600985221674878, 0.29187192118226601, 0.30911330049261082, 0.28817733990147781, 0.29310344827586204, 0.32142857142857145, 0.2894088669950739], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13054187192118227, 0.25615763546798032, 0.23522167487684728, 0.24876847290640394, 0.2857142857142857, 0.23152709359605911, 0.25862068965517243, 0.25738916256157635, 0.28078817733990147, 0.24753694581280788, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.47536945812807879, 0.48152709359605911, 0.48029556650246308, 0.38300492610837439, 0.47536945812807879, 0.41256157635467983, 0.45443349753694579, 0.4642857142857143, 0.44088669950738918, 0.45935960591133007], 5: [0.26477832512315269, 0.26847290640394089, 0.28325123152709358, 0.27093596059113301, 0.33128078817733991, 0.29310344827586204, 0.3288177339901478, 0.28817733990147781, 0.25492610837438423, 0.31157635467980294, 0.28078817733990147], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13054187192118227, 0.27339901477832512, 0.25985221674876846, 0.25615763546798032, 0.27093596059113301, 0.22044334975369459, 0.26600985221674878, 0.24630541871921183, 0.28817733990147781, 0.23522167487684728, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.42857142857142855, 0.47536945812807879, 0.46182266009852219, 0.44581280788177341, 0.48522167487684731, 0.41379310344827586, 0.46182266009852219, 0.39532019704433496, 0.48029556650246308, 0.46305418719211822], 5: [0.26477832512315269, 0.29802955665024633, 0.26477832512315269, 0.28201970443349755, 0.28325123152709358, 0.29433497536945813, 0.32019704433497537, 0.29187192118226601, 0.31650246305418717, 0.28448275862068967, 0.32389162561576357], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13054187192118227, 0.29187192118226601, 0.25738916256157635, 0.23645320197044334, 0.26600985221674878, 0.27339901477832512, 0.24384236453201971, 0.23152709359605911, 0.26724137931034481, 0.22044334975369459, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.42980295566502463, 0.47290640394088668, 0.50369458128078815, 0.43842364532019706, 0.44334975369458129, 0.45073891625615764, 0.48029556650246308, 0.42733990147783252, 0.46305418719211822, 0.45566502463054187], 5: [0.26477832512315269, 0.27832512315270935, 0.26970443349753692, 0.25985221674876846, 0.29556650246305421, 0.28325123152709358, 0.30541871921182268, 0.28817733990147781, 0.30541871921182268, 0.31650246305418717, 0.32019704433497537], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421576 minutes
Weight histogram
[ 151  494  667  567 1599 8559 9696 3028 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1133  988  801 1112 1452 2691 2427 4062 3377 8282] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.15433
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.999849
Epoch 1, cost is  0.980127
Epoch 2, cost is  0.964346
Epoch 3, cost is  0.953911
Epoch 4, cost is  0.942164
Training took 0.644792 minutes
Weight histogram
[6488 5022 3798 2674 2141 1670 1362 1108  993 1069] [ -4.33397591e-02  -3.90114727e-02  -3.46831863e-02  -3.03548998e-02
  -2.60266134e-02  -2.16983270e-02  -1.73700405e-02  -1.30417541e-02
  -8.71346769e-03  -4.38518127e-03  -5.68948381e-05]
[1978 1343 1446 1774 2144 2671 3052 3269 3947 4701] [ -4.33397591e-02  -3.90114727e-02  -3.46831863e-02  -3.03548998e-02
  -2.60266134e-02  -2.16983270e-02  -1.73700405e-02  -1.30417541e-02
  -8.71346769e-03  -4.38518127e-03  -5.68948381e-05]
-0.86577
1.59978
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148252 minutes
Weight histogram
[ 538 1619  977  608 2029 6324 7596 6472 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2163   152   219   306   449   802   844  1995  4389 17031] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.115233 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 2086 2525 1634] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4863 1388 1541 2004 2295 2572 2932 3259 3508 3988] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.45399
Epoch 1, cost is  6.10547
Epoch 2, cost is  5.64293
Epoch 3, cost is  5.23526
Epoch 4, cost is  4.9266
Epoch 5, cost is  4.67158
Epoch 6, cost is  4.46138
Epoch 7, cost is  4.28033
Epoch 8, cost is  4.12811
Epoch 9, cost is  3.99409
Training took 0.321740 minutes
Weight histogram
[ 989  978  881  810  756  665  681  824 1351  165] [-0.03172233 -0.02856681 -0.02541129 -0.02225577 -0.01910025 -0.01594474
 -0.01278922 -0.0096337  -0.00647818 -0.00332266 -0.00016714]
[1367  594  581  624  684  739  801  857  926  927] [-0.03172233 -0.02856681 -0.02541129 -0.02225577 -0.01910025 -0.01594474
 -0.01278922 -0.0096337  -0.00647818 -0.00332266 -0.00016714]
-0.484167
0.639102
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.141971 minutes
Epoch 0
Fine tuning took 0.141968 minutes
Epoch 0
Fine tuning took 0.142767 minutes
Epoch 0
Fine tuning took 0.142344 minutes
Epoch 0
Fine tuning took 0.141842 minutes
Epoch 0
Fine tuning took 0.142551 minutes
Epoch 0
Fine tuning took 0.141471 minutes
Epoch 0
Fine tuning took 0.142646 minutes
Epoch 0
Fine tuning took 0.141798 minutes
Epoch 0
Fine tuning took 0.143095 minutes
{'zero': {0: [0.065270935960591137, 0.10344827586206896, 0.12807881773399016, 0.23029556650246305, 0.087438423645320201, 0.04064039408866995, 0.19704433497536947, 0.14285714285714285, 0.34113300492610837, 0.21798029556650247, 0.37438423645320196], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.83374384236453203, 0.69334975369458129, 0.82635467980295563, 0.66502463054187189, 0.70197044334975367, 0.81157635467980294, 0.74507389162561577, 0.75862068965517238, 0.53325123152709364, 0.66871921182266014, 0.56034482758620685], 5: [0.10098522167487685, 0.20320197044334976, 0.045566502463054187, 0.10467980295566502, 0.2105911330049261, 0.14778325123152711, 0.057881773399014777, 0.098522167487684734, 0.12561576354679804, 0.11330049261083744, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.065270935960591137, 0.094827586206896547, 0.14778325123152711, 0.20812807881773399, 0.093596059113300489, 0.066502463054187194, 0.21182266009852216, 0.13669950738916256, 0.33128078817733991, 0.27832512315270935, 0.44334975369458129], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.83374384236453203, 0.70812807881773399, 0.79679802955665024, 0.68226600985221675, 0.66133004926108374, 0.75615763546798032, 0.73768472906403937, 0.75985221674876846, 0.51600985221674878, 0.55788177339901479, 0.47290640394088668], 5: [0.10098522167487685, 0.19704433497536947, 0.055418719211822662, 0.10960591133004927, 0.24507389162561577, 0.17733990147783252, 0.050492610837438424, 0.10344827586206896, 0.15270935960591134, 0.16379310344827586, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.065270935960591137, 0.080049261083743842, 0.15517241379310345, 0.2229064039408867, 0.091133004926108374, 0.036945812807881777, 0.24876847290640394, 0.13300492610837439, 0.33866995073891626, 0.27093596059113301, 0.39778325123152708], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.83374384236453203, 0.71798029556650245, 0.79802955665024633, 0.64408866995073888, 0.66009852216748766, 0.80541871921182262, 0.68226600985221675, 0.78940886699507384, 0.52955665024630538, 0.57635467980295563, 0.51477832512315269], 5: [0.10098522167487685, 0.2019704433497537, 0.046798029556650245, 0.13300492610837439, 0.24876847290640394, 0.15763546798029557, 0.068965517241379309, 0.077586206896551727, 0.13177339901477833, 0.15270935960591134, 0.087438423645320201], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.065270935960591137, 0.10591133004926108, 0.17857142857142858, 0.22536945812807882, 0.084975369458128072, 0.050492610837438424, 0.25492610837438423, 0.14039408866995073, 0.35221674876847292, 0.28448275862068967, 0.40763546798029554], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.83374384236453203, 0.71059113300492616, 0.76847290640394084, 0.67733990147783252, 0.68226600985221675, 0.77216748768472909, 0.69211822660098521, 0.78325123152709364, 0.51231527093596063, 0.57512315270935965, 0.49137931034482757], 5: [0.10098522167487685, 0.18349753694581281, 0.05295566502463054, 0.097290640394088676, 0.23275862068965517, 0.17733990147783252, 0.05295566502463054, 0.076354679802955669, 0.1354679802955665, 0.14039408866995073, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420930 minutes
Weight histogram
[ 151  494  667  567 1599 8559 9696 3028 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1133  988  801 1112 1452 2691 2427 4062 3377 8282] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.15433
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.999849
Epoch 1, cost is  0.980127
Epoch 2, cost is  0.964346
Epoch 3, cost is  0.953911
Epoch 4, cost is  0.942164
Training took 0.645475 minutes
Weight histogram
[6488 5022 3798 2674 2141 1670 1362 1108  993 1069] [ -4.33397591e-02  -3.90114727e-02  -3.46831863e-02  -3.03548998e-02
  -2.60266134e-02  -2.16983270e-02  -1.73700405e-02  -1.30417541e-02
  -8.71346769e-03  -4.38518127e-03  -5.68948381e-05]
[1978 1343 1446 1774 2144 2671 3052 3269 3947 4701] [ -4.33397591e-02  -3.90114727e-02  -3.46831863e-02  -3.03548998e-02
  -2.60266134e-02  -2.16983270e-02  -1.73700405e-02  -1.30417541e-02
  -8.71346769e-03  -4.38518127e-03  -5.68948381e-05]
-0.86577
1.59978
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148710 minutes
Weight histogram
[ 538 1619  977  608 2029 6324 7596 6472 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2163   152   219   306   449   802   844  1995  4389 17031] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.116602 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 2086 2525 1634] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4863 1388 1541 2004 2295 2572 2932 3259 3508 3988] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.96255
Epoch 1, cost is  5.50886
Epoch 2, cost is  4.92554
Epoch 3, cost is  4.47762
Epoch 4, cost is  4.11574
Epoch 5, cost is  3.81922
Epoch 6, cost is  3.58139
Epoch 7, cost is  3.38792
Epoch 8, cost is  3.23086
Epoch 9, cost is  3.09646
Training took 0.499212 minutes
Weight histogram
[1130 1036  954  906  865  752  679 1338  412   28] [-0.02028304 -0.01826994 -0.01625684 -0.01424374 -0.01223064 -0.01021754
 -0.00820444 -0.00619134 -0.00417824 -0.00216514 -0.00015205]
[1427  578  617  654  671  691  756  824  900  982] [-0.02028304 -0.01826994 -0.01625684 -0.01424374 -0.01223064 -0.01021754
 -0.00820444 -0.00619134 -0.00417824 -0.00216514 -0.00015205]
-0.349079
0.391335
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.149715 minutes
Epoch 0
Fine tuning took 0.149021 minutes
Epoch 0
Fine tuning took 0.150133 minutes
Epoch 0
Fine tuning took 0.149973 minutes
Epoch 0
Fine tuning took 0.149909 minutes
Epoch 0
Fine tuning took 0.150822 minutes
Epoch 0
Fine tuning took 0.150112 minutes
Epoch 0
Fine tuning took 0.149491 minutes
Epoch 0
Fine tuning took 0.150322 minutes
Epoch 0
Fine tuning took 0.149966 minutes
{'zero': {0: [0.11699507389162561, 0.050492610837438424, 0.16133004926108374, 0.088669950738916259, 0.17610837438423646, 0.16009852216748768, 0.20812807881773399, 0.28201970443349755, 0.21305418719211822, 0.20073891625615764, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.7573891625615764, 0.7068965517241379, 0.73891625615763545, 0.63300492610837433, 0.65640394088669951, 0.62561576354679804, 0.58866995073891626, 0.61576354679802958, 0.59113300492610843, 0.72044334975369462], 5: [0.14162561576354679, 0.19211822660098521, 0.13177339901477833, 0.17241379310344829, 0.19088669950738915, 0.18349753694581281, 0.16625615763546797, 0.12931034482758622, 0.17118226600985223, 0.20812807881773399, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11699507389162561, 0.18226600985221675, 0.30049261083743845, 0.15640394088669951, 0.27709359605911332, 0.24507389162561577, 0.30049261083743845, 0.33620689655172414, 0.31527093596059114, 0.28078817733990147, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.63669950738916259, 0.58004926108374388, 0.67980295566502458, 0.55049261083743839, 0.57266009852216748, 0.53694581280788178, 0.50369458128078815, 0.53694581280788178, 0.50246305418719217, 0.60098522167487689], 5: [0.14162561576354679, 0.18103448275862069, 0.11945812807881774, 0.16379310344827586, 0.17241379310344829, 0.18226600985221675, 0.1625615763546798, 0.16009852216748768, 0.14778325123152711, 0.21674876847290642, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11699507389162561, 0.14655172413793102, 0.29926108374384236, 0.14408866995073891, 0.24630541871921183, 0.20935960591133004, 0.2857142857142857, 0.34729064039408869, 0.28325123152709358, 0.27216748768472904, 0.22044334975369459], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.62315270935960587, 0.58743842364532017, 0.66995073891625612, 0.55418719211822665, 0.6219211822660099, 0.55665024630541871, 0.50246305418719217, 0.55911330049261088, 0.51231527093596063, 0.60098522167487689], 5: [0.14162561576354679, 0.23029556650246305, 0.11330049261083744, 0.18596059113300492, 0.19950738916256158, 0.16871921182266009, 0.15763546798029557, 0.15024630541871922, 0.15763546798029557, 0.21551724137931033, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11699507389162561, 0.2019704433497537, 0.33004926108374383, 0.17733990147783252, 0.27216748768472904, 0.24876847290640394, 0.31034482758620691, 0.36330049261083741, 0.32758620689655171, 0.30541871921182268, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.60837438423645318, 0.56034482758620685, 0.66133004926108374, 0.57019704433497542, 0.60467980295566504, 0.56896551724137934, 0.5073891625615764, 0.53201970443349755, 0.4605911330049261, 0.65147783251231528], 5: [0.14162561576354679, 0.18965517241379309, 0.10960591133004927, 0.16133004926108374, 0.15763546798029557, 0.14655172413793102, 0.1206896551724138, 0.12931034482758622, 0.14039408866995073, 0.23399014778325122, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421519 minutes
Weight histogram
[ 151  494  667  567 1599 8559 9696 3028 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1133  988  801 1112 1452 2691 2427 4062 3377 8282] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.15433
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.999849
Epoch 1, cost is  0.980127
Epoch 2, cost is  0.964346
Epoch 3, cost is  0.953911
Epoch 4, cost is  0.942164
Training took 0.645977 minutes
Weight histogram
[6488 5022 3798 2674 2141 1670 1362 1108  993 1069] [ -4.33397591e-02  -3.90114727e-02  -3.46831863e-02  -3.03548998e-02
  -2.60266134e-02  -2.16983270e-02  -1.73700405e-02  -1.30417541e-02
  -8.71346769e-03  -4.38518127e-03  -5.68948381e-05]
[1978 1343 1446 1774 2144 2671 3052 3269 3947 4701] [ -4.33397591e-02  -3.90114727e-02  -3.46831863e-02  -3.03548998e-02
  -2.60266134e-02  -2.16983270e-02  -1.73700405e-02  -1.30417541e-02
  -8.71346769e-03  -4.38518127e-03  -5.68948381e-05]
-0.86577
1.59978
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149098 minutes
Weight histogram
[ 538 1619  977  608 2029 6324 7596 6472 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2163   152   219   306   449   802   844  1995  4389 17031] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.117696 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 2086 2525 1634] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4863 1388 1541 2004 2295 2572 2932 3259 3508 3988] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.34808
Epoch 1, cost is  4.85087
Epoch 2, cost is  4.25997
Epoch 3, cost is  3.81306
Epoch 4, cost is  3.46141
Epoch 5, cost is  3.19257
Epoch 6, cost is  2.98546
Epoch 7, cost is  2.81854
Epoch 8, cost is  2.68567
Epoch 9, cost is  2.57529
Training took 0.820487 minutes
Weight histogram
[1416 1301 1106  990  892  746 1396  201   37   15] [-0.0134927  -0.01215713 -0.01082157 -0.00948601 -0.00815044 -0.00681488
 -0.00547931 -0.00414375 -0.00280819 -0.00147262 -0.00013706]
[1438  580  604  623  644  697  767  834  912 1001] [-0.0134927  -0.01215713 -0.01082157 -0.00948601 -0.00815044 -0.00681488
 -0.00547931 -0.00414375 -0.00280819 -0.00147262 -0.00013706]
-0.205822
0.309724
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164675 minutes
Epoch 0
Fine tuning took 0.164069 minutes
Epoch 0
Fine tuning took 0.165017 minutes
Epoch 0
Fine tuning took 0.164426 minutes
Epoch 0
Fine tuning took 0.165188 minutes
Epoch 0
Fine tuning took 0.164271 minutes
Epoch 0
Fine tuning took 0.165397 minutes
Epoch 0
Fine tuning took 0.164776 minutes
Epoch 0
Fine tuning took 0.165444 minutes
Epoch 0
Fine tuning took 0.164089 minutes
{'zero': {0: [0.16009852216748768, 0.19950738916256158, 0.2019704433497537, 0.16748768472906403, 0.18965517241379309, 0.19088669950738915, 0.22167487684729065, 0.25862068965517243, 0.23399014778325122, 0.18596059113300492, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.5788177339901478, 0.58990147783251234, 0.6071428571428571, 0.59113300492610843, 0.56280788177339902, 0.52463054187192115, 0.54802955665024633, 0.5073891625615764, 0.54926108374384242, 0.53201970443349755], 5: [0.1354679802955665, 0.22167487684729065, 0.20812807881773399, 0.22536945812807882, 0.21921182266009853, 0.24630541871921183, 0.2536945812807882, 0.19334975369458129, 0.25862068965517243, 0.26477832512315269, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16009852216748768, 0.21305418719211822, 0.22044334975369459, 0.20812807881773399, 0.23522167487684728, 0.23399014778325122, 0.24753694581280788, 0.24384236453201971, 0.27463054187192121, 0.24876847290640394, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.57389162561576357, 0.55788177339901479, 0.59482758620689657, 0.52093596059113301, 0.5357142857142857, 0.54679802955665024, 0.55049261083743839, 0.47783251231527096, 0.47660098522167488, 0.5073891625615764], 5: [0.1354679802955665, 0.21305418719211822, 0.22167487684729065, 0.19704433497536947, 0.24384236453201971, 0.23029556650246305, 0.20566502463054187, 0.20566502463054187, 0.24753694581280788, 0.27463054187192121, 0.26231527093596058], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16009852216748768, 0.25246305418719212, 0.19950738916256158, 0.17118226600985223, 0.23645320197044334, 0.22660098522167488, 0.24753694581280788, 0.2229064039408867, 0.24753694581280788, 0.22783251231527094, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.54187192118226601, 0.56773399014778325, 0.63054187192118227, 0.52093596059113301, 0.56157635467980294, 0.5, 0.5714285714285714, 0.46674876847290642, 0.51231527093596063, 0.50492610837438423], 5: [0.1354679802955665, 0.20566502463054187, 0.23275862068965517, 0.19827586206896552, 0.24261083743842365, 0.21182266009852216, 0.25246305418719212, 0.20566502463054187, 0.2857142857142857, 0.25985221674876846, 0.27586206896551724], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16009852216748768, 0.22536945812807882, 0.20566502463054187, 0.14778325123152711, 0.23399014778325122, 0.22783251231527094, 0.24014778325123154, 0.22413793103448276, 0.23645320197044334, 0.21428571428571427, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.57758620689655171, 0.55788177339901479, 0.6576354679802956, 0.52586206896551724, 0.55172413793103448, 0.49261083743842365, 0.57266009852216748, 0.48399014778325122, 0.50615763546798032, 0.51477832512315269], 5: [0.1354679802955665, 0.19704433497536947, 0.23645320197044334, 0.19458128078817735, 0.24014778325123154, 0.22044334975369459, 0.26724137931034481, 0.20320197044334976, 0.27955665024630544, 0.27955665024630544, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422441 minutes
Weight histogram
[ 151  494  667  567 1599 8559 9696 3028 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1133  988  801 1112 1452 2691 2427 4062 3377 8282] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.15433
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.999849
Epoch 1, cost is  0.980127
Epoch 2, cost is  0.964346
Epoch 3, cost is  0.953911
Epoch 4, cost is  0.942164
Training took 0.645409 minutes
Weight histogram
[6488 5022 3798 2674 2141 1670 1362 1108  993 1069] [ -4.33397591e-02  -3.90114727e-02  -3.46831863e-02  -3.03548998e-02
  -2.60266134e-02  -2.16983270e-02  -1.73700405e-02  -1.30417541e-02
  -8.71346769e-03  -4.38518127e-03  -5.68948381e-05]
[1978 1343 1446 1774 2144 2671 3052 3269 3947 4701] [ -4.33397591e-02  -3.90114727e-02  -3.46831863e-02  -3.03548998e-02
  -2.60266134e-02  -2.16983270e-02  -1.73700405e-02  -1.30417541e-02
  -8.71346769e-03  -4.38518127e-03  -5.68948381e-05]
-0.86577
1.59978
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148531 minutes
Weight histogram
[ 538 1619  977  608 2029 6324 7596 6472 2051  136] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2163   152   219   306   449   802   844  1995  4389 17031] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.62297
Epoch 1, cost is  2.57064
Epoch 2, cost is  2.53856
Epoch 3, cost is  2.51085
Epoch 4, cost is  2.49037
Training took 0.114575 minutes
Weight histogram
[4410 4553 3218 3047 2886 2085 1906 2086 2525 1634] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
[4863 1388 1541 2004 2295 2572 2932 3259 3508 3988] [ -5.58358729e-02  -5.02490976e-02  -4.46623223e-02  -3.90755471e-02
  -3.34887718e-02  -2.79019965e-02  -2.23152212e-02  -1.67284460e-02
  -1.11416707e-02  -5.55489543e-03   3.18798448e-05]
-1.38457
1.55076
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.74047
Epoch 1, cost is  4.26515
Epoch 2, cost is  3.72497
Epoch 3, cost is  3.30618
Epoch 4, cost is  2.98618
Epoch 5, cost is  2.75214
Epoch 6, cost is  2.57891
Epoch 7, cost is  2.44482
Epoch 8, cost is  2.33526
Epoch 9, cost is  2.24386
Training took 1.470806 minutes
Weight histogram
[1847 1584 1313 1273 1669  247   97   42   18   10] [-0.00915086 -0.00825087 -0.00735089 -0.00645091 -0.00555093 -0.00465095
 -0.00375097 -0.00285098 -0.001951   -0.00105102 -0.00015104]
[1411  579  574  582  625  685  763  859  966 1056] [-0.00915086 -0.00825087 -0.00735089 -0.00645091 -0.00555093 -0.00465095
 -0.00375097 -0.00285098 -0.001951   -0.00105102 -0.00015104]
-0.187185
0.218415
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.194132 minutes
Epoch 0
Fine tuning took 0.193909 minutes
Epoch 0
Fine tuning took 0.194317 minutes
Epoch 0
Fine tuning took 0.193451 minutes
Epoch 0
Fine tuning took 0.193538 minutes
Epoch 0
Fine tuning took 0.193261 minutes
Epoch 0
Fine tuning took 0.193012 minutes
Epoch 0
Fine tuning took 0.193163 minutes
Epoch 0
Fine tuning took 0.193801 minutes
Epoch 0
Fine tuning took 0.193285 minutes
{'zero': {0: [0.13054187192118227, 0.23029556650246305, 0.21428571428571427, 0.22783251231527094, 0.26847290640394089, 0.24261083743842365, 0.25862068965517243, 0.19704433497536947, 0.2229064039408867, 0.19334975369458129, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66379310344827591, 0.48645320197044334, 0.45443349753694579, 0.52586206896551724, 0.45443349753694579, 0.48645320197044334, 0.47413793103448276, 0.53817733990147787, 0.42241379310344829, 0.45935960591133007, 0.45073891625615764], 5: [0.20566502463054187, 0.28325123152709358, 0.33128078817733991, 0.24630541871921183, 0.27709359605911332, 0.27093596059113301, 0.26724137931034481, 0.26477832512315269, 0.35467980295566504, 0.34729064039408869, 0.29802955665024633], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13054187192118227, 0.25, 0.22167487684729065, 0.24384236453201971, 0.24261083743842365, 0.23522167487684728, 0.26477832512315269, 0.21798029556650247, 0.26600985221674878, 0.21428571428571427, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66379310344827591, 0.49753694581280788, 0.47044334975369456, 0.50862068965517238, 0.45689655172413796, 0.48522167487684731, 0.45073891625615764, 0.52093596059113301, 0.42610837438423643, 0.44950738916256155, 0.43596059113300495], 5: [0.20566502463054187, 0.25246305418719212, 0.30788177339901479, 0.24753694581280788, 0.30049261083743845, 0.27955665024630544, 0.28448275862068967, 0.26108374384236455, 0.30788177339901479, 0.33620689655172414, 0.31280788177339902], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13054187192118227, 0.27093596059113301, 0.22536945812807882, 0.21921182266009853, 0.25985221674876846, 0.24014778325123154, 0.28325123152709358, 0.23522167487684728, 0.24876847290640394, 0.22783251231527094, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66379310344827591, 0.47783251231527096, 0.4605911330049261, 0.51354679802955661, 0.45689655172413796, 0.48891625615763545, 0.45073891625615764, 0.50615763546798032, 0.42241379310344829, 0.46305418719211822, 0.46182266009852219], 5: [0.20566502463054187, 0.25123152709359609, 0.31403940886699505, 0.26724137931034481, 0.28325123152709358, 0.27093596059113301, 0.26600985221674878, 0.25862068965517243, 0.3288177339901478, 0.30911330049261082, 0.31527093596059114], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13054187192118227, 0.24507389162561577, 0.2229064039408867, 0.21428571428571427, 0.23275862068965517, 0.23645320197044334, 0.29926108374384236, 0.23645320197044334, 0.28201970443349755, 0.20320197044334976, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66379310344827591, 0.48768472906403942, 0.48029556650246308, 0.4963054187192118, 0.49014778325123154, 0.49261083743842365, 0.43965517241379309, 0.51354679802955661, 0.40517241379310343, 0.48399014778325122, 0.45689655172413796], 5: [0.20566502463054187, 0.26724137931034481, 0.29679802955665024, 0.2894088669950739, 0.27709359605911332, 0.27093596059113301, 0.26108374384236455, 0.25, 0.31280788177339902, 0.31280788177339902, 0.31034482758620691], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105487 minutes
Weight histogram
[1649 3715 6982 7271 5251 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  135   146   226   304   461   716   742  1480  3845 22320] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-2.15646
2.22154
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.73598
Epoch 1, cost is  3.66755
Epoch 2, cost is  3.64978
Epoch 3, cost is  3.62291
Epoch 4, cost is  3.60259
Training took 0.073949 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 3455  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2797 2360 2176 2455 2234 2602 3350 3265 4531 4605] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.62395
2.50898
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150872 minutes
Weight histogram
[   48   106   545  1430  1710  8153 12689  7355   361     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 18720   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115410 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 3097 2920  651] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.02659
Epoch 1, cost is  5.71671
Epoch 2, cost is  5.61731
Epoch 3, cost is  5.46859
Epoch 4, cost is  5.23741
Epoch 5, cost is  5.03232
Epoch 6, cost is  4.83954
Epoch 7, cost is  4.66597
Epoch 8, cost is  4.50683
Epoch 9, cost is  4.36861
Training took 0.181461 minutes
Weight histogram
[2002 2437 3123 2078  956  650  490  259  103   52] [ -2.92806122e-02  -2.63615435e-02  -2.34424747e-02  -2.05234059e-02
  -1.76043371e-02  -1.46852683e-02  -1.17661995e-02  -8.84713074e-03
  -5.92806195e-03  -3.00899317e-03  -8.99243823e-05]
[ 928 1851 1906 1061 1025 1057 1052 1070 1104 1096] [ -2.92806122e-02  -2.63615435e-02  -2.34424747e-02  -2.05234059e-02
  -1.76043371e-02  -1.46852683e-02  -1.17661995e-02  -8.84713074e-03
  -5.92806195e-03  -3.00899317e-03  -8.99243823e-05]
-0.366673
0.362733
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044106 minutes
Epoch 0
Fine tuning took 0.042654 minutes
Epoch 0
Fine tuning took 0.042970 minutes
Epoch 0
Fine tuning took 0.044105 minutes
Epoch 0
Fine tuning took 0.042738 minutes
Epoch 0
Fine tuning took 0.041310 minutes
Epoch 0
Fine tuning took 0.043570 minutes
Epoch 0
Fine tuning took 0.040973 minutes
Epoch 0
Fine tuning took 0.040536 minutes
Epoch 0
Fine tuning took 0.043558 minutes
{'zero': {0: [0.24753694581280788, 0.14901477832512317, 0.21798029556650247, 0.096059113300492605, 0.20320197044334976, 0.1539408866995074, 0.18472906403940886, 0.16009852216748768, 0.16625615763546797, 0.17118226600985223, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.65270935960591137, 0.65517241379310343, 0.77463054187192115, 0.70812807881773399, 0.74630541871921185, 0.68596059113300489, 0.72413793103448276, 0.7068965517241379, 0.68719211822660098, 0.75], 5: [0.13916256157635468, 0.19827586206896552, 0.1268472906403941, 0.12931034482758622, 0.088669950738916259, 0.099753694581280791, 0.12931034482758622, 0.11576354679802955, 0.1268472906403941, 0.14162561576354679, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24753694581280788, 0.37931034482758619, 0.24630541871921183, 0.18472906403940886, 0.50492610837438423, 0.35467980295566504, 0.35098522167487683, 0.29802955665024633, 0.27216748768472904, 0.39039408866995073, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.44458128078817732, 0.49753694581280788, 0.70935960591133007, 0.41256157635467983, 0.53078817733990147, 0.43842364532019706, 0.65024630541871919, 0.56034482758620685, 0.40763546798029554, 0.66625615763546797], 5: [0.13916256157635468, 0.17610837438423646, 0.25615763546798032, 0.10591133004926108, 0.082512315270935957, 0.1145320197044335, 0.2105911330049261, 0.051724137931034482, 0.16748768472906403, 0.2019704433497537, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24753694581280788, 0.34482758620689657, 0.25985221674876846, 0.19458128078817735, 0.42733990147783252, 0.32142857142857145, 0.34113300492610837, 0.28078817733990147, 0.25985221674876846, 0.37807881773399016, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.44458128078817732, 0.51477832512315269, 0.7068965517241379, 0.46674876847290642, 0.56034482758620685, 0.44950738916256155, 0.65270935960591137, 0.57635467980295563, 0.42241379310344829, 0.6219211822660099], 5: [0.13916256157635468, 0.2105911330049261, 0.22536945812807882, 0.098522167487684734, 0.10591133004926108, 0.11822660098522167, 0.20935960591133004, 0.066502463054187194, 0.16379310344827586, 0.19950738916256158, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24753694581280788, 0.36945812807881773, 0.26354679802955666, 0.18349753694581281, 0.52832512315270941, 0.3854679802955665, 0.38054187192118227, 0.29064039408866993, 0.2857142857142857, 0.39655172413793105, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.46182266009852219, 0.46551724137931033, 0.73152709359605916, 0.39655172413793105, 0.51108374384236455, 0.35591133004926107, 0.67487684729064035, 0.56896551724137934, 0.3891625615763547, 0.73029556650246308], 5: [0.13916256157635468, 0.16871921182266009, 0.27093596059113301, 0.084975369458128072, 0.075123152709359611, 0.10344827586206896, 0.26354679802955666, 0.034482758620689655, 0.14532019704433496, 0.21428571428571427, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105193 minutes
Weight histogram
[1649 3715 6982 7271 5251 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  135   146   226   304   461   716   742  1480  3845 22320] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-2.15646
2.22154
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.73598
Epoch 1, cost is  3.66755
Epoch 2, cost is  3.64978
Epoch 3, cost is  3.62291
Epoch 4, cost is  3.60259
Training took 0.071916 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 3455  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2797 2360 2176 2455 2234 2602 3350 3265 4531 4605] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.62395
2.50898
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148246 minutes
Weight histogram
[   48   106   545  1430  1710  8153 12689  7355   361     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 18720   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115377 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 3097 2920  651] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.66725
Epoch 1, cost is  5.48101
Epoch 2, cost is  5.37557
Epoch 3, cost is  5.11679
Epoch 4, cost is  4.81919
Epoch 5, cost is  4.55181
Epoch 6, cost is  4.32758
Epoch 7, cost is  4.14166
Epoch 8, cost is  3.97296
Epoch 9, cost is  3.81911
Training took 0.247906 minutes
Weight histogram
[2099 4757 3386  816  470  272  168   96   52   34] [-0.0187807 -0.0169174 -0.0150541 -0.0131908 -0.0113275 -0.0094642
 -0.0076009 -0.0057376 -0.0038743 -0.002011  -0.0001477]
[2995 1261  798  860  879  928 1013 1115 1174 1127] [-0.0187807 -0.0169174 -0.0150541 -0.0131908 -0.0113275 -0.0094642
 -0.0076009 -0.0057376 -0.0038743 -0.002011  -0.0001477]
-0.330819
0.272233
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048135 minutes
Epoch 0
Fine tuning took 0.047331 minutes
Epoch 0
Fine tuning took 0.047829 minutes
Epoch 0
Fine tuning took 0.047709 minutes
Epoch 0
Fine tuning took 0.045278 minutes
Epoch 0
Fine tuning took 0.047003 minutes
Epoch 0
Fine tuning took 0.045573 minutes
Epoch 0
Fine tuning took 0.047497 minutes
Epoch 0
Fine tuning took 0.046219 minutes
Epoch 0
Fine tuning took 0.047698 minutes
{'zero': {0: [0.16625615763546797, 0.21798029556650247, 0.19581280788177341, 0.15147783251231528, 0.2019704433497537, 0.13669950738916256, 0.16871921182266009, 0.18349753694581281, 0.21551724137931033, 0.18226600985221675, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.5714285714285714, 0.62315270935960587, 0.66871921182266014, 0.62684729064039413, 0.62684729064039413, 0.61699507389162567, 0.58374384236453203, 0.58251231527093594, 0.55172413793103448, 0.63300492610837433], 5: [0.1539408866995074, 0.2105911330049261, 0.18103448275862069, 0.17980295566502463, 0.17118226600985223, 0.23645320197044334, 0.21428571428571427, 0.23275862068965517, 0.2019704433497537, 0.26600985221674878, 0.25], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16625615763546797, 0.20935960591133004, 0.19950738916256158, 0.18842364532019704, 0.1625615763546798, 0.15024630541871922, 0.17980295566502463, 0.21921182266009853, 0.23522167487684728, 0.19950738916256158, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.60098522167487689, 0.6071428571428571, 0.64655172413793105, 0.6428571428571429, 0.63177339901477836, 0.58620689655172409, 0.57512315270935965, 0.57266009852216748, 0.55172413793103448, 0.63423645320197042], 5: [0.1539408866995074, 0.18965517241379309, 0.19334975369458129, 0.16502463054187191, 0.19458128078817735, 0.21798029556650247, 0.23399014778325122, 0.20566502463054187, 0.19211822660098521, 0.24876847290640394, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16625615763546797, 0.20566502463054187, 0.19827586206896552, 0.17241379310344829, 0.19211822660098521, 0.15886699507389163, 0.20443349753694581, 0.23029556650246305, 0.22906403940886699, 0.22660098522167488, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.60098522167487689, 0.60467980295566504, 0.64532019704433496, 0.60591133004926112, 0.58497536945812811, 0.59729064039408863, 0.53448275862068961, 0.57389162561576357, 0.53078817733990147, 0.61699507389162567], 5: [0.1539408866995074, 0.19334975369458129, 0.19704433497536947, 0.18226600985221675, 0.2019704433497537, 0.25615763546798032, 0.19827586206896552, 0.23522167487684728, 0.19704433497536947, 0.24261083743842365, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16625615763546797, 0.19950738916256158, 0.17610837438423646, 0.14901477832512317, 0.20443349753694581, 0.14655172413793102, 0.18349753694581281, 0.18103448275862069, 0.20935960591133004, 0.16625615763546797, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.63916256157635465, 0.64408866995073888, 0.69211822660098521, 0.64039408866995073, 0.63916256157635465, 0.60960591133004927, 0.59729064039408863, 0.61699507389162567, 0.58743842364532017, 0.63054187192118227], 5: [0.1539408866995074, 0.16133004926108374, 0.17980295566502463, 0.15886699507389163, 0.15517241379310345, 0.21428571428571427, 0.20689655172413793, 0.22167487684729065, 0.17364532019704434, 0.24630541871921183, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107563 minutes
Weight histogram
[1649 3715 6982 7271 5251 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  135   146   226   304   461   716   742  1480  3845 22320] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-2.15646
2.22154
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.73598
Epoch 1, cost is  3.66755
Epoch 2, cost is  3.64978
Epoch 3, cost is  3.62291
Epoch 4, cost is  3.60259
Training took 0.075668 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 3455  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2797 2360 2176 2455 2234 2602 3350 3265 4531 4605] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.62395
2.50898
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148461 minutes
Weight histogram
[   48   106   545  1430  1710  8153 12689  7355   361     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 18720   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115358 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 3097 2920  651] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.5884
Epoch 1, cost is  5.46505
Epoch 2, cost is  5.17283
Epoch 3, cost is  4.72054
Epoch 4, cost is  4.35632
Epoch 5, cost is  4.09241
Epoch 6, cost is  3.85946
Epoch 7, cost is  3.64994
Epoch 8, cost is  3.45853
Epoch 9, cost is  3.29215
Training took 0.335602 minutes
Weight histogram
[1666 2670 2078 1696 2872  964  110   48   26   20] [-0.01086076 -0.0097855  -0.00871024 -0.00763497 -0.00655971 -0.00548445
 -0.00440919 -0.00333393 -0.00225867 -0.00118341 -0.00010814]
[3060  771  769  813  904 1052 1119 1184 1240 1238] [-0.01086076 -0.0097855  -0.00871024 -0.00763497 -0.00655971 -0.00548445
 -0.00440919 -0.00333393 -0.00225867 -0.00118341 -0.00010814]
-0.229213
0.227813
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050866 minutes
Epoch 0
Fine tuning took 0.051391 minutes
Epoch 0
Fine tuning took 0.050796 minutes
Epoch 0
Fine tuning took 0.050394 minutes
Epoch 0
Fine tuning took 0.049575 minutes
Epoch 0
Fine tuning took 0.050072 minutes
Epoch 0
Fine tuning took 0.050352 minutes
Epoch 0
Fine tuning took 0.049097 minutes
Epoch 0
Fine tuning took 0.051014 minutes
Epoch 0
Fine tuning took 0.049356 minutes
{'zero': {0: [0.15517241379310345, 0.19458128078817735, 0.19458128078817735, 0.19211822660098521, 0.18349753694581281, 0.18965517241379309, 0.19458128078817735, 0.22044334975369459, 0.25615763546798032, 0.19950738916256158, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.53817733990147787, 0.53201970443349755, 0.55541871921182262, 0.52586206896551724, 0.48522167487684731, 0.46551724137931033, 0.44458128078817732, 0.46182266009852219, 0.47290640394088668, 0.51724137931034486], 5: [0.19704433497536947, 0.26724137931034481, 0.27339901477832512, 0.25246305418719212, 0.29064039408866993, 0.3251231527093596, 0.33990147783251229, 0.33497536945812806, 0.28201970443349755, 0.32758620689655171, 0.30049261083743845], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15517241379310345, 0.16625615763546797, 0.18596059113300492, 0.21428571428571427, 0.20443349753694581, 0.2229064039408867, 0.23645320197044334, 0.24753694581280788, 0.28694581280788178, 0.22413793103448276, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.59729064039408863, 0.56527093596059108, 0.55172413793103448, 0.51108374384236455, 0.44950738916256155, 0.44827586206896552, 0.44211822660098521, 0.41379310344827586, 0.46674876847290642, 0.4605911330049261], 5: [0.19704433497536947, 0.23645320197044334, 0.24876847290640394, 0.23399014778325122, 0.28448275862068967, 0.32758620689655171, 0.31527093596059114, 0.31034482758620691, 0.29926108374384236, 0.30911330049261082, 0.32389162561576357], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15517241379310345, 0.17241379310344829, 0.21428571428571427, 0.20689655172413793, 0.20566502463054187, 0.20812807881773399, 0.21305418719211822, 0.23275862068965517, 0.27586206896551724, 0.21182266009852216, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.56773399014778325, 0.52093596059113301, 0.53940886699507384, 0.53078817733990147, 0.48645320197044334, 0.47660098522167488, 0.40147783251231528, 0.45812807881773399, 0.43349753694581283, 0.48891625615763545], 5: [0.19704433497536947, 0.25985221674876846, 0.26477832512315269, 0.2536945812807882, 0.26354679802955666, 0.30541871921182268, 0.31034482758620691, 0.36576354679802958, 0.26600985221674878, 0.35467980295566504, 0.32389162561576357], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15517241379310345, 0.2105911330049261, 0.19334975369458129, 0.2019704433497537, 0.21428571428571427, 0.21182266009852216, 0.23645320197044334, 0.22536945812807882, 0.26970443349753692, 0.20812807881773399, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.53694581280788178, 0.53817733990147787, 0.56034482758620685, 0.50123152709359609, 0.46305418719211822, 0.45935960591133007, 0.47290640394088668, 0.43103448275862066, 0.46551724137931033, 0.47783251231527096], 5: [0.19704433497536947, 0.25246305418719212, 0.26847290640394089, 0.2376847290640394, 0.28448275862068967, 0.3251231527093596, 0.30418719211822659, 0.30172413793103448, 0.29926108374384236, 0.32635467980295568, 0.3460591133004926], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.109124 minutes
Weight histogram
[1649 3715 6982 7271 5251 2171 1670 1208  420   38] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
[  135   146   226   304   461   716   742  1480  3845 22320] [-0.00235201 -0.00171017 -0.00106832 -0.00042647  0.00021538  0.00085722
  0.00149907  0.00214092  0.00278277  0.00342462  0.00406646]
-2.15646
2.22154
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.73598
Epoch 1, cost is  3.66755
Epoch 2, cost is  3.64978
Epoch 3, cost is  3.62291
Epoch 4, cost is  3.60259
Training took 0.074180 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 3455  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2797 2360 2176 2455 2234 2602 3350 3265 4531 4605] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.62395
2.50898
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147689 minutes
Weight histogram
[   48   106   545  1430  1710  8153 12689  7355   361     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 18720   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115745 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 3097 2920  651] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.5786
Epoch 1, cost is  5.38882
Epoch 2, cost is  4.8986
Epoch 3, cost is  4.3897
Epoch 4, cost is  4.01496
Epoch 5, cost is  3.71927
Epoch 6, cost is  3.45813
Epoch 7, cost is  3.23971
Epoch 8, cost is  3.05811
Epoch 9, cost is  2.90935
Training took 0.537477 minutes
Weight histogram
[1772 2369 2064 1474 1221 2795  405   25   13   12] [ -5.72379166e-03  -5.16113650e-03  -4.59848133e-03  -4.03582617e-03
  -3.47317100e-03  -2.91051584e-03  -2.34786067e-03  -1.78520550e-03
  -1.22255034e-03  -6.59895174e-04  -9.72400085e-05]
[2746  783  787  861  960 1040 1119 1212 1313 1329] [ -5.72379166e-03  -5.16113650e-03  -4.59848133e-03  -4.03582617e-03
  -3.47317100e-03  -2.91051584e-03  -2.34786067e-03  -1.78520550e-03
  -1.22255034e-03  -6.59895174e-04  -9.72400085e-05]
-0.181317
0.203843
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.060306 minutes
Epoch 0
Fine tuning took 0.061440 minutes
Epoch 0
Fine tuning took 0.060190 minutes
Epoch 0
Fine tuning took 0.061716 minutes
Epoch 0
Fine tuning took 0.061745 minutes
Epoch 0
Fine tuning took 0.060578 minutes
Epoch 0
Fine tuning took 0.061451 minutes
Epoch 0
Fine tuning took 0.061167 minutes
Epoch 0
Fine tuning took 0.062381 minutes
Epoch 0
Fine tuning took 0.061923 minutes
{'zero': {0: [0.16133004926108374, 0.18472906403940886, 0.2376847290640394, 0.22044334975369459, 0.21921182266009853, 0.20935960591133004, 0.22783251231527094, 0.22536945812807882, 0.27093596059113301, 0.23275862068965517, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.53201970443349755, 0.48399014778325122, 0.47413793103448276, 0.48275862068965519, 0.44334975369458129, 0.44458128078817732, 0.42733990147783252, 0.39532019704433496, 0.37192118226600984, 0.44088669950738918], 5: [0.22660098522167488, 0.28325123152709358, 0.27832512315270935, 0.30541871921182268, 0.29802955665024633, 0.34729064039408869, 0.32758620689655171, 0.34729064039408869, 0.33374384236453203, 0.39532019704433496, 0.35467980295566504], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.18965517241379309, 0.24014778325123154, 0.2413793103448276, 0.19458128078817735, 0.2229064039408867, 0.19704433497536947, 0.22536945812807882, 0.24014778325123154, 0.21551724137931033, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.52832512315270941, 0.48522167487684731, 0.49753694581280788, 0.50369458128078815, 0.41256157635467983, 0.44088669950738918, 0.46182266009852219, 0.43349753694581283, 0.39655172413793105, 0.44334975369458129], 5: [0.22660098522167488, 0.28201970443349755, 0.27463054187192121, 0.26108374384236455, 0.30172413793103448, 0.3645320197044335, 0.36206896551724138, 0.31280788177339902, 0.32635467980295568, 0.38793103448275862, 0.37315270935960593], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.17610837438423646, 0.24507389162561577, 0.22044334975369459, 0.19704433497536947, 0.20443349753694581, 0.21798029556650247, 0.23029556650246305, 0.24384236453201971, 0.19581280788177341, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.54433497536945807, 0.50123152709359609, 0.44581280788177341, 0.49384236453201968, 0.44704433497536944, 0.43103448275862066, 0.41502463054187194, 0.45566502463054187, 0.4211822660098522, 0.46182266009852219], 5: [0.22660098522167488, 0.27955665024630544, 0.2536945812807882, 0.33374384236453203, 0.30911330049261082, 0.34852216748768472, 0.35098522167487683, 0.35467980295566504, 0.30049261083743845, 0.38300492610837439, 0.3608374384236453], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.17610837438423646, 0.26108374384236455, 0.23399014778325122, 0.19211822660098521, 0.21921182266009853, 0.22167487684729065, 0.23399014778325122, 0.2536945812807882, 0.21551724137931033, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.54802955665024633, 0.44581280788177341, 0.43842364532019706, 0.49876847290640391, 0.46182266009852219, 0.43842364532019706, 0.41871921182266009, 0.44950738916256155, 0.41995073891625617, 0.45935960591133007], 5: [0.22660098522167488, 0.27586206896551724, 0.29310344827586204, 0.32758620689655171, 0.30911330049261082, 0.31896551724137934, 0.33990147783251229, 0.34729064039408869, 0.29679802955665024, 0.3645320197044335, 0.36945812807881773], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148502 minutes
Weight histogram
[  232   569   912  1127  5158  6891 11157  2308  1427   594] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
[  134   155   234   328   497   848  1016  2009  6975 18179] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
-1.31697
1.09692
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.34321
Epoch 1, cost is  2.30581
Epoch 2, cost is  2.27706
Epoch 3, cost is  2.26176
Epoch 4, cost is  2.23969
Training took 0.115387 minutes
Weight histogram
[5989 5786 4270 3268 2884 2217 2219 1766 1609  367] [ -6.30858988e-02  -5.67742819e-02  -5.04626650e-02  -4.41510481e-02
  -3.78394312e-02  -3.15278143e-02  -2.52161974e-02  -1.89045805e-02
  -1.25929636e-02  -6.28134672e-03   3.02701774e-05]
[2839 1561 1655 2101 2592 3051 3458 3983 4345 4790] [ -6.30858988e-02  -5.67742819e-02  -5.04626650e-02  -4.41510481e-02
  -3.78394312e-02  -3.15278143e-02  -2.52161974e-02  -1.89045805e-02
  -1.25929636e-02  -6.28134672e-03   3.02701774e-05]
-1.25836
1.69177
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149070 minutes
Weight histogram
[ 223  554 1069 1221 2639 6960 9079 7989 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  277   318   476   661   999  1404   929  2039  5908 19389] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.114238 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 2440 3563  665] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.25698
Epoch 1, cost is  6.026
Epoch 2, cost is  5.87439
Epoch 3, cost is  5.6428
Epoch 4, cost is  5.27739
Epoch 5, cost is  4.94728
Epoch 6, cost is  4.68183
Epoch 7, cost is  4.47203
Epoch 8, cost is  4.30442
Epoch 9, cost is  4.15609
Training took 0.207054 minutes
Weight histogram
[1315 1380 1311 1248 2381 2587 1317  472   96   43] [-0.02656188 -0.02392104 -0.0212802  -0.01863937 -0.01599853 -0.01335769
 -0.01071685 -0.00807602 -0.00543518 -0.00279434 -0.0001535 ]
[2714 1807  843  741  804  889  969 1058 1154 1171] [-0.02656188 -0.02392104 -0.0212802  -0.01863937 -0.01599853 -0.01335769
 -0.01071685 -0.00807602 -0.00543518 -0.00279434 -0.0001535 ]
-0.348383
0.440929
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.053739 minutes
Epoch 0
Fine tuning took 0.052216 minutes
Epoch 0
Fine tuning took 0.054724 minutes
Epoch 0
Fine tuning took 0.052961 minutes
Epoch 0
Fine tuning took 0.053519 minutes
Epoch 0
Fine tuning took 0.052608 minutes
Epoch 0
Fine tuning took 0.054484 minutes
Epoch 0
Fine tuning took 0.054409 minutes
Epoch 0
Fine tuning took 0.053603 minutes
Epoch 0
Fine tuning took 0.053599 minutes
{'zero': {0: [0.33990147783251229, 0.25246305418719212, 0.34359605911330049, 0.24630541871921183, 0.30049261083743845, 0.23645320197044334, 0.30911330049261082, 0.26847290640394089, 0.19458128078817735, 0.16379310344827586, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51108374384236455, 0.54433497536945807, 0.43596059113300495, 0.53694581280788178, 0.57019704433497542, 0.63177339901477836, 0.49261083743842365, 0.64162561576354682, 0.71059113300492616, 0.63793103448275867, 0.6428571428571429], 5: [0.14901477832512317, 0.20320197044334976, 0.22044334975369459, 0.21674876847290642, 0.12931034482758622, 0.13177339901477833, 0.19827586206896552, 0.089901477832512317, 0.094827586206896547, 0.19827586206896552, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33990147783251229, 0.36330049261083741, 0.34482758620689657, 0.37931034482758619, 0.41009852216748771, 0.41748768472906406, 0.44088669950738918, 0.43596059113300495, 0.27832512315270935, 0.2229064039408867, 0.35591133004926107], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51108374384236455, 0.38054187192118227, 0.37684729064039407, 0.43103448275862066, 0.47167487684729065, 0.43226600985221675, 0.33251231527093594, 0.45935960591133007, 0.59359605911330049, 0.53694581280788178, 0.46674876847290642], 5: [0.14901477832512317, 0.25615763546798032, 0.27832512315270935, 0.18965517241379309, 0.11822660098522167, 0.15024630541871922, 0.22660098522167488, 0.10467980295566502, 0.12807881773399016, 0.24014778325123154, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33990147783251229, 0.35467980295566504, 0.34113300492610837, 0.35344827586206895, 0.41009852216748771, 0.45566502463054187, 0.43719211822660098, 0.37438423645320196, 0.26970443349753692, 0.25615763546798032, 0.37315270935960593], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51108374384236455, 0.4211822660098522, 0.40886699507389163, 0.42980295566502463, 0.45073891625615764, 0.39778325123152708, 0.32142857142857145, 0.48768472906403942, 0.6219211822660099, 0.51108374384236455, 0.45935960591133007], 5: [0.14901477832512317, 0.22413793103448276, 0.25, 0.21674876847290642, 0.13916256157635468, 0.14655172413793102, 0.2413793103448276, 0.13793103448275862, 0.10837438423645321, 0.23275862068965517, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33990147783251229, 0.39039408866995073, 0.34359605911330049, 0.37192118226600984, 0.44581280788177341, 0.44458128078817732, 0.42857142857142855, 0.42610837438423643, 0.26600985221674878, 0.25862068965517243, 0.38054187192118227], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51108374384236455, 0.37931034482758619, 0.40147783251231528, 0.41256157635467983, 0.42241379310344829, 0.41379310344827586, 0.29679802955665024, 0.44950738916256155, 0.58620689655172409, 0.50615763546798032, 0.44581280788177341], 5: [0.14901477832512317, 0.23029556650246305, 0.25492610837438423, 0.21551724137931033, 0.13177339901477833, 0.14162561576354679, 0.27463054187192121, 0.12438423645320197, 0.14778325123152711, 0.23522167487684728, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147779 minutes
Weight histogram
[  232   569   912  1127  5158  6891 11157  2308  1427   594] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
[  134   155   234   328   497   848  1016  2009  6975 18179] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
-1.31697
1.09692
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.34321
Epoch 1, cost is  2.30581
Epoch 2, cost is  2.27706
Epoch 3, cost is  2.26176
Epoch 4, cost is  2.23969
Training took 0.114742 minutes
Weight histogram
[5989 5786 4270 3268 2884 2217 2219 1766 1609  367] [ -6.30858988e-02  -5.67742819e-02  -5.04626650e-02  -4.41510481e-02
  -3.78394312e-02  -3.15278143e-02  -2.52161974e-02  -1.89045805e-02
  -1.25929636e-02  -6.28134672e-03   3.02701774e-05]
[2839 1561 1655 2101 2592 3051 3458 3983 4345 4790] [ -6.30858988e-02  -5.67742819e-02  -5.04626650e-02  -4.41510481e-02
  -3.78394312e-02  -3.15278143e-02  -2.52161974e-02  -1.89045805e-02
  -1.25929636e-02  -6.28134672e-03   3.02701774e-05]
-1.25836
1.69177
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148266 minutes
Weight histogram
[ 223  554 1069 1221 2639 6960 9079 7989 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  277   318   476   661   999  1404   929  2039  5908 19389] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115665 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 2440 3563  665] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.67441
Epoch 1, cost is  5.46089
Epoch 2, cost is  5.31783
Epoch 3, cost is  5.00014
Epoch 4, cost is  4.59149
Epoch 5, cost is  4.2816
Epoch 6, cost is  4.03257
Epoch 7, cost is  3.816
Epoch 8, cost is  3.62537
Epoch 9, cost is  3.46455
Training took 0.298904 minutes
Weight histogram
[2198 1951 1663 4484 1132  385  179   85   45   28] [-0.01784189 -0.0160722  -0.01430251 -0.01253282 -0.01076313 -0.00899344
 -0.00722375 -0.00545406 -0.00368437 -0.00191468 -0.00014499]
[3589  918  684  770  861  935 1010 1085 1139 1159] [-0.01784189 -0.0160722  -0.01430251 -0.01253282 -0.01076313 -0.00899344
 -0.00722375 -0.00545406 -0.00368437 -0.00191468 -0.00014499]
-0.283091
0.336729
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.059598 minutes
Epoch 0
Fine tuning took 0.059146 minutes
Epoch 0
Fine tuning took 0.058713 minutes
Epoch 0
Fine tuning took 0.060012 minutes
Epoch 0
Fine tuning took 0.059479 minutes
Epoch 0
Fine tuning took 0.059088 minutes
Epoch 0
Fine tuning took 0.058381 minutes
Epoch 0
Fine tuning took 0.057828 minutes
Epoch 0
Fine tuning took 0.057843 minutes
Epoch 0
Fine tuning took 0.059561 minutes
{'zero': {0: [0.21428571428571427, 0.17241379310344829, 0.16133004926108374, 0.17118226600985223, 0.22660098522167488, 0.15024630541871922, 0.16379310344827586, 0.20320197044334976, 0.17118226600985223, 0.2413793103448276, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62684729064039413, 0.63300492610837433, 0.66625615763546797, 0.69704433497536944, 0.59729064039408863, 0.65147783251231528, 0.65886699507389157, 0.59975369458128081, 0.66256157635467983, 0.56034482758620685, 0.65886699507389157], 5: [0.15886699507389163, 0.19458128078817735, 0.17241379310344829, 0.13177339901477833, 0.17610837438423646, 0.19827586206896552, 0.17733990147783252, 0.19704433497536947, 0.16625615763546797, 0.19827586206896552, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21428571428571427, 0.24507389162561577, 0.21551724137931033, 0.21921182266009853, 0.29310344827586204, 0.20443349753694581, 0.19827586206896552, 0.21182266009852216, 0.18349753694581281, 0.25, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62684729064039413, 0.53201970443349755, 0.63300492610837433, 0.64655172413793105, 0.52339901477832518, 0.59605911330049266, 0.59605911330049266, 0.63423645320197042, 0.6576354679802956, 0.57019704433497542, 0.55295566502463056], 5: [0.15886699507389163, 0.2229064039408867, 0.15147783251231528, 0.13423645320197045, 0.18349753694581281, 0.19950738916256158, 0.20566502463054187, 0.1539408866995074, 0.15886699507389163, 0.17980295566502463, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21428571428571427, 0.2894088669950739, 0.22413793103448276, 0.24261083743842365, 0.2536945812807882, 0.22167487684729065, 0.18596059113300492, 0.21921182266009853, 0.17118226600985223, 0.23275862068965517, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62684729064039413, 0.5, 0.61083743842364535, 0.6145320197044335, 0.55665024630541871, 0.56773399014778325, 0.6145320197044335, 0.6145320197044335, 0.66871921182266014, 0.59852216748768472, 0.57758620689655171], 5: [0.15886699507389163, 0.2105911330049261, 0.16502463054187191, 0.14285714285714285, 0.18965517241379309, 0.2105911330049261, 0.19950738916256158, 0.16625615763546797, 0.16009852216748768, 0.16871921182266009, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21428571428571427, 0.28325123152709358, 0.21428571428571427, 0.22783251231527094, 0.27709359605911332, 0.19088669950738915, 0.2105911330049261, 0.22660098522167488, 0.15886699507389163, 0.23029556650246305, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62684729064039413, 0.49876847290640391, 0.6219211822660099, 0.61945812807881773, 0.52093596059113301, 0.6219211822660099, 0.58251231527093594, 0.60221674876847286, 0.67610837438423643, 0.57266009852216748, 0.6145320197044335], 5: [0.15886699507389163, 0.21798029556650247, 0.16379310344827586, 0.15270935960591134, 0.2019704433497537, 0.18719211822660098, 0.20689655172413793, 0.17118226600985223, 0.16502463054187191, 0.19704433497536947, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148436 minutes
Weight histogram
[  232   569   912  1127  5158  6891 11157  2308  1427   594] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
[  134   155   234   328   497   848  1016  2009  6975 18179] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
-1.31697
1.09692
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.34321
Epoch 1, cost is  2.30581
Epoch 2, cost is  2.27706
Epoch 3, cost is  2.26176
Epoch 4, cost is  2.23969
Training took 0.117232 minutes
Weight histogram
[5989 5786 4270 3268 2884 2217 2219 1766 1609  367] [ -6.30858988e-02  -5.67742819e-02  -5.04626650e-02  -4.41510481e-02
  -3.78394312e-02  -3.15278143e-02  -2.52161974e-02  -1.89045805e-02
  -1.25929636e-02  -6.28134672e-03   3.02701774e-05]
[2839 1561 1655 2101 2592 3051 3458 3983 4345 4790] [ -6.30858988e-02  -5.67742819e-02  -5.04626650e-02  -4.41510481e-02
  -3.78394312e-02  -3.15278143e-02  -2.52161974e-02  -1.89045805e-02
  -1.25929636e-02  -6.28134672e-03   3.02701774e-05]
-1.25836
1.69177
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149155 minutes
Weight histogram
[ 223  554 1069 1221 2639 6960 9079 7989 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  277   318   476   661   999  1404   929  2039  5908 19389] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.116711 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 2440 3563  665] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.33062
Epoch 1, cost is  5.18456
Epoch 2, cost is  4.97615
Epoch 3, cost is  4.53191
Epoch 4, cost is  4.15947
Epoch 5, cost is  3.87165
Epoch 6, cost is  3.62703
Epoch 7, cost is  3.39685
Epoch 8, cost is  3.20135
Epoch 9, cost is  3.0331
Training took 0.414582 minutes
Weight histogram
[2701 2821 5205  756  315  167   90   49   28   18] [-0.01199183 -0.01080513 -0.00961844 -0.00843174 -0.00724504 -0.00605834
 -0.00487164 -0.00368495 -0.00249825 -0.00131155 -0.00012485]
[3342  767  708  834  912 1009 1048 1099 1191 1240] [-0.01199183 -0.01080513 -0.00961844 -0.00843174 -0.00724504 -0.00605834
 -0.00487164 -0.00368495 -0.00249825 -0.00131155 -0.00012485]
-0.216932
0.260704
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.064993 minutes
Epoch 0
Fine tuning took 0.064514 minutes
Epoch 0
Fine tuning took 0.063657 minutes
Epoch 0
Fine tuning took 0.063404 minutes
Epoch 0
Fine tuning took 0.064679 minutes
Epoch 0
Fine tuning took 0.063468 minutes
Epoch 0
Fine tuning took 0.064674 minutes
Epoch 0
Fine tuning took 0.063185 minutes
Epoch 0
Fine tuning took 0.062800 minutes
Epoch 0
Fine tuning took 0.064781 minutes
{'zero': {0: [0.16625615763546797, 0.16009852216748768, 0.20566502463054187, 0.15270935960591134, 0.18226600985221675, 0.17241379310344829, 0.16748768472906403, 0.19088669950738915, 0.23029556650246305, 0.23522167487684728, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66995073891625612, 0.6280788177339901, 0.58251231527093594, 0.66625615763546797, 0.60960591133004927, 0.57266009852216748, 0.6145320197044335, 0.55911330049261088, 0.56650246305418717, 0.52586206896551724, 0.56527093596059108], 5: [0.16379310344827586, 0.21182266009852216, 0.21182266009852216, 0.18103448275862069, 0.20812807881773399, 0.25492610837438423, 0.21798029556650247, 0.25, 0.20320197044334976, 0.23891625615763548, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16625615763546797, 0.1539408866995074, 0.20812807881773399, 0.15517241379310345, 0.19704433497536947, 0.21551724137931033, 0.15270935960591134, 0.2105911330049261, 0.2413793103448276, 0.22044334975369459, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66995073891625612, 0.65640394088669951, 0.56527093596059108, 0.6785714285714286, 0.59113300492610843, 0.50492610837438423, 0.65886699507389157, 0.53940886699507384, 0.55418719211822665, 0.55049261083743839, 0.60591133004926112], 5: [0.16379310344827586, 0.18965517241379309, 0.22660098522167488, 0.16625615763546797, 0.21182266009852216, 0.27955665024630544, 0.18842364532019704, 0.25, 0.20443349753694581, 0.22906403940886699, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16625615763546797, 0.16502463054187191, 0.19950738916256158, 0.17118226600985223, 0.17364532019704434, 0.20689655172413793, 0.16748768472906403, 0.17980295566502463, 0.24014778325123154, 0.24630541871921183, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66995073891625612, 0.6280788177339901, 0.57758620689655171, 0.65886699507389157, 0.6280788177339901, 0.54187192118226601, 0.58374384236453203, 0.53694581280788178, 0.56280788177339902, 0.53201970443349755, 0.55418719211822665], 5: [0.16379310344827586, 0.20689655172413793, 0.2229064039408867, 0.16995073891625614, 0.19827586206896552, 0.25123152709359609, 0.24876847290640394, 0.28325123152709358, 0.19704433497536947, 0.22167487684729065, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16625615763546797, 0.18965517241379309, 0.2376847290640394, 0.15147783251231528, 0.15147783251231528, 0.19950738916256158, 0.15640394088669951, 0.17980295566502463, 0.22783251231527094, 0.21921182266009853, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.66995073891625612, 0.61206896551724133, 0.53940886699507384, 0.6428571428571429, 0.60591133004926112, 0.55911330049261088, 0.59359605911330049, 0.58004926108374388, 0.56403940886699511, 0.53325123152709364, 0.61945812807881773], 5: [0.16379310344827586, 0.19827586206896552, 0.2229064039408867, 0.20566502463054187, 0.24261083743842365, 0.2413793103448276, 0.25, 0.24014778325123154, 0.20812807881773399, 0.24753694581280788, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148334 minutes
Weight histogram
[  232   569   912  1127  5158  6891 11157  2308  1427   594] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
[  134   155   234   328   497   848  1016  2009  6975 18179] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
-1.31697
1.09692
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.34321
Epoch 1, cost is  2.30581
Epoch 2, cost is  2.27706
Epoch 3, cost is  2.26176
Epoch 4, cost is  2.23969
Training took 0.114897 minutes
Weight histogram
[5989 5786 4270 3268 2884 2217 2219 1766 1609  367] [ -6.30858988e-02  -5.67742819e-02  -5.04626650e-02  -4.41510481e-02
  -3.78394312e-02  -3.15278143e-02  -2.52161974e-02  -1.89045805e-02
  -1.25929636e-02  -6.28134672e-03   3.02701774e-05]
[2839 1561 1655 2101 2592 3051 3458 3983 4345 4790] [ -6.30858988e-02  -5.67742819e-02  -5.04626650e-02  -4.41510481e-02
  -3.78394312e-02  -3.15278143e-02  -2.52161974e-02  -1.89045805e-02
  -1.25929636e-02  -6.28134672e-03   3.02701774e-05]
-1.25836
1.69177
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147841 minutes
Weight histogram
[ 223  554 1069 1221 2639 6960 9079 7989 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  277   318   476   661   999  1404   929  2039  5908 19389] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.114796 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 2440 3563  665] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.29344
Epoch 1, cost is  5.0773
Epoch 2, cost is  4.53313
Epoch 3, cost is  4.01164
Epoch 4, cost is  3.61038
Epoch 5, cost is  3.28588
Epoch 6, cost is  3.02762
Epoch 7, cost is  2.82
Epoch 8, cost is  2.65124
Epoch 9, cost is  2.50845
Training took 0.682839 minutes
Weight histogram
[2234 2327 1867 1485 1298 2825   64   25   13   12] [-0.0065721  -0.00592648 -0.00528087 -0.00463525 -0.00398963 -0.00334401
 -0.0026984  -0.00205278 -0.00140716 -0.00076155 -0.00011593]
[2710  755  787  853  927  984 1081 1221 1372 1460] [-0.0065721  -0.00592648 -0.00528087 -0.00463525 -0.00398963 -0.00334401
 -0.0026984  -0.00205278 -0.00140716 -0.00076155 -0.00011593]
-0.180576
0.194157
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077197 minutes
Epoch 0
Fine tuning took 0.077239 minutes
Epoch 0
Fine tuning took 0.076268 minutes
Epoch 0
Fine tuning took 0.078206 minutes
Epoch 0
Fine tuning took 0.076686 minutes
Epoch 0
Fine tuning took 0.077952 minutes
Epoch 0
Fine tuning took 0.077551 minutes
Epoch 0
Fine tuning took 0.076726 minutes
Epoch 0
Fine tuning took 0.077102 minutes
Epoch 0
Fine tuning took 0.076645 minutes
{'zero': {0: [0.17610837438423646, 0.23152709359605911, 0.22660098522167488, 0.22660098522167488, 0.21428571428571427, 0.22536945812807882, 0.19827586206896552, 0.25985221674876846, 0.24876847290640394, 0.20443349753694581, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.51354679802955661, 0.52463054187192115, 0.52955665024630538, 0.55172413793103448, 0.45073891625615764, 0.49753694581280788, 0.43349753694581283, 0.46798029556650245, 0.43472906403940886, 0.50492610837438423], 5: [0.22536945812807882, 0.25492610837438423, 0.24876847290640394, 0.24384236453201971, 0.23399014778325122, 0.32389162561576357, 0.30418719211822659, 0.30665024630541871, 0.28325123152709358, 0.3608374384236453, 0.31280788177339902], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17610837438423646, 0.21798029556650247, 0.20320197044334976, 0.21305418719211822, 0.23275862068965517, 0.2105911330049261, 0.20566502463054187, 0.24876847290640394, 0.28325123152709358, 0.18596059113300492, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.53201970443349755, 0.55665024630541871, 0.52832512315270941, 0.50123152709359609, 0.45197044334975367, 0.48645320197044334, 0.45443349753694579, 0.44458128078817732, 0.46551724137931033, 0.49384236453201968], 5: [0.22536945812807882, 0.25, 0.24014778325123154, 0.25862068965517243, 0.26600985221674878, 0.33743842364532017, 0.30788177339901479, 0.29679802955665024, 0.27216748768472904, 0.34852216748768472, 0.31034482758620691], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17610837438423646, 0.22044334975369459, 0.2019704433497537, 0.24753694581280788, 0.22660098522167488, 0.22536945812807882, 0.20566502463054187, 0.24384236453201971, 0.27709359605911332, 0.20566502463054187, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.51354679802955661, 0.55418719211822665, 0.51724137931034486, 0.48522167487684731, 0.45320197044334976, 0.4605911330049261, 0.42241379310344829, 0.45197044334975367, 0.47906403940886699, 0.48645320197044334], 5: [0.22536945812807882, 0.26600985221674878, 0.24384236453201971, 0.23522167487684728, 0.28817733990147781, 0.32142857142857145, 0.33374384236453203, 0.33374384236453203, 0.27093596059113301, 0.31527093596059114, 0.32019704433497537], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17610837438423646, 0.24630541871921183, 0.22413793103448276, 0.21798029556650247, 0.18842364532019704, 0.21674876847290642, 0.23275862068965517, 0.25246305418719212, 0.26724137931034481, 0.21428571428571427, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.49753694581280788, 0.53201970443349755, 0.54433497536945807, 0.53817733990147787, 0.47536945812807879, 0.49507389162561577, 0.42857142857142855, 0.45320197044334976, 0.45812807881773399, 0.52832512315270941], 5: [0.22536945812807882, 0.25615763546798032, 0.24384236453201971, 0.2376847290640394, 0.27339901477832512, 0.30788177339901479, 0.27216748768472904, 0.31896551724137934, 0.27955665024630544, 0.32758620689655171, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235555 minutes
Weight histogram
[  184   588   776   472  1138  2279  6552 13877  4019   490] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
[ 163  284  426  686  963 1363 2990 4837 9099 9564] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
-1.28613
1.29103
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.4091
Epoch 1, cost is  1.38633
Epoch 2, cost is  1.36966
Epoch 3, cost is  1.35387
Epoch 4, cost is  1.34109
Training took 0.224096 minutes
Weight histogram
[7046 5051 4510 3138 2529 2243 1773 1510 1457 1118] [ -6.26062006e-02  -5.63388170e-02  -5.00714334e-02  -4.38040498e-02
  -3.75366662e-02  -3.12692826e-02  -2.50018990e-02  -1.87345154e-02
  -1.24671318e-02  -6.19974825e-03   6.76353375e-05]
[2408 1505 1854 2159 2584 3109 3242 4006 4649 4859] [ -6.26062006e-02  -5.63388170e-02  -5.00714334e-02  -4.38040498e-02
  -3.75366662e-02  -3.12692826e-02  -2.50018990e-02  -1.87345154e-02
  -1.24671318e-02  -6.19974825e-03   6.76353375e-05]
-1.00462
1.75301
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149050 minutes
Weight histogram
[ 356 1117 1465  816 2164 6804 9032 7980 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  474   970  1108   321   440   822   929  2039  5908 19389] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115844 minutes
Weight histogram
[5785 4636 4156 3521 3069 2578 2010 2276 2992 1377] [ -5.99095523e-02  -5.39118336e-02  -4.79141148e-02  -4.19163960e-02
  -3.59186773e-02  -2.99209585e-02  -2.39232397e-02  -1.79255210e-02
  -1.19278022e-02  -5.93008343e-03   6.76353375e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39118336e-02  -4.79141148e-02  -4.19163960e-02
  -3.59186773e-02  -2.99209585e-02  -2.39232397e-02  -1.79255210e-02
  -1.19278022e-02  -5.93008343e-03   6.76353375e-05]
-1.57414
1.75275
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.3247
Epoch 1, cost is  6.05062
Epoch 2, cost is  5.79433
Epoch 3, cost is  5.43227
Epoch 4, cost is  5.08258
Epoch 5, cost is  4.79235
Epoch 6, cost is  4.55292
Epoch 7, cost is  4.35427
Epoch 8, cost is  4.18058
Epoch 9, cost is  4.02694
Training took 0.249869 minutes
Weight histogram
[1295 1393 1291 1213 1226 1492 2205 1429  553   53] [-0.02840349 -0.02557967 -0.02275585 -0.01993204 -0.01710822 -0.0142844
 -0.01146058 -0.00863677 -0.00581295 -0.00298913 -0.00016531]
[2399 1334  869  878  911 1006 1094 1183 1261 1215] [-0.02840349 -0.02557967 -0.02275585 -0.01993204 -0.01710822 -0.0142844
 -0.01146058 -0.00863677 -0.00581295 -0.00298913 -0.00016531]
-0.367253
0.599255
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077192 minutes
Epoch 0
Fine tuning took 0.076979 minutes
Epoch 0
Fine tuning took 0.075852 minutes
Epoch 0
Fine tuning took 0.077108 minutes
Epoch 0
Fine tuning took 0.076646 minutes
Epoch 0
Fine tuning took 0.076726 minutes
Epoch 0
Fine tuning took 0.076935 minutes
Epoch 0
Fine tuning took 0.077106 minutes
Epoch 0
Fine tuning took 0.076239 minutes
Epoch 0
Fine tuning took 0.075544 minutes
{'zero': {0: [0.28817733990147781, 0.27339901477832512, 0.1748768472906404, 0.28201970443349755, 0.33251231527093594, 0.19704433497536947, 0.16995073891625614, 0.39039408866995073, 0.16502463054187191, 0.20935960591133004, 0.39778325123152708], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54926108374384242, 0.6071428571428571, 0.61083743842364535, 0.6219211822660099, 0.49753694581280788, 0.58866995073891626, 0.53078817733990147, 0.51600985221674878, 0.78078817733990147, 0.57635467980295563, 0.42980295566502463], 5: [0.1625615763546798, 0.11945812807881774, 0.21428571428571427, 0.096059113300492605, 0.16995073891625614, 0.21428571428571427, 0.29926108374384236, 0.093596059113300489, 0.054187192118226604, 0.21428571428571427, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28817733990147781, 0.3645320197044335, 0.17118226600985223, 0.32019704433497537, 0.44458128078817732, 0.27832512315270935, 0.20689655172413793, 0.54556650246305416, 0.21921182266009853, 0.2413793103448276, 0.43226600985221675], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54926108374384242, 0.50985221674876846, 0.57512315270935965, 0.58004926108374388, 0.3891625615763547, 0.48522167487684731, 0.45566502463054187, 0.34975369458128081, 0.7142857142857143, 0.45073891625615764, 0.39039408866995073], 5: [0.1625615763546798, 0.12561576354679804, 0.2536945812807882, 0.099753694581280791, 0.16625615763546797, 0.23645320197044334, 0.33743842364532017, 0.10467980295566502, 0.066502463054187194, 0.30788177339901479, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28817733990147781, 0.35098522167487683, 0.17980295566502463, 0.33866995073891626, 0.41133004926108374, 0.29433497536945813, 0.17980295566502463, 0.50123152709359609, 0.22660098522167488, 0.2376847290640394, 0.43965517241379309], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54926108374384242, 0.51847290640394084, 0.58620689655172409, 0.55541871921182262, 0.42241379310344829, 0.49014778325123154, 0.51724137931034486, 0.40763546798029554, 0.71798029556650245, 0.45812807881773399, 0.40886699507389163], 5: [0.1625615763546798, 0.13054187192118227, 0.23399014778325122, 0.10591133004926108, 0.16625615763546797, 0.21551724137931033, 0.30295566502463056, 0.091133004926108374, 0.055418719211822662, 0.30418719211822659, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28817733990147781, 0.37192118226600984, 0.17118226600985223, 0.33497536945812806, 0.43719211822660098, 0.28325123152709358, 0.18965517241379309, 0.51724137931034486, 0.20566502463054187, 0.28817733990147781, 0.47167487684729065], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.54926108374384242, 0.49876847290640391, 0.57635467980295563, 0.57635467980295563, 0.39408866995073893, 0.46674876847290642, 0.48152709359605911, 0.37315270935960593, 0.71798029556650245, 0.3817733990147783, 0.36206896551724138], 5: [0.1625615763546798, 0.12931034482758622, 0.25246305418719212, 0.088669950738916259, 0.16871921182266009, 0.25, 0.3288177339901478, 0.10960591133004927, 0.076354679802955669, 0.33004926108374383, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235384 minutes
Weight histogram
[  184   588   776   472  1138  2279  6552 13877  4019   490] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
[ 163  284  426  686  963 1363 2990 4837 9099 9564] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
-1.28613
1.29103
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.4091
Epoch 1, cost is  1.38633
Epoch 2, cost is  1.36966
Epoch 3, cost is  1.35387
Epoch 4, cost is  1.34109
Training took 0.223524 minutes
Weight histogram
[7046 5051 4510 3138 2529 2243 1773 1510 1457 1118] [ -6.26062006e-02  -5.63388170e-02  -5.00714334e-02  -4.38040498e-02
  -3.75366662e-02  -3.12692826e-02  -2.50018990e-02  -1.87345154e-02
  -1.24671318e-02  -6.19974825e-03   6.76353375e-05]
[2408 1505 1854 2159 2584 3109 3242 4006 4649 4859] [ -6.26062006e-02  -5.63388170e-02  -5.00714334e-02  -4.38040498e-02
  -3.75366662e-02  -3.12692826e-02  -2.50018990e-02  -1.87345154e-02
  -1.24671318e-02  -6.19974825e-03   6.76353375e-05]
-1.00462
1.75301
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147750 minutes
Weight histogram
[ 356 1117 1465  816 2164 6804 9032 7980 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  474   970  1108   321   440   822   929  2039  5908 19389] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.117096 minutes
Weight histogram
[5785 4636 4156 3521 3069 2578 2010 2276 2992 1377] [ -5.99095523e-02  -5.39118336e-02  -4.79141148e-02  -4.19163960e-02
  -3.59186773e-02  -2.99209585e-02  -2.39232397e-02  -1.79255210e-02
  -1.19278022e-02  -5.93008343e-03   6.76353375e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39118336e-02  -4.79141148e-02  -4.19163960e-02
  -3.59186773e-02  -2.99209585e-02  -2.39232397e-02  -1.79255210e-02
  -1.19278022e-02  -5.93008343e-03   6.76353375e-05]
-1.57414
1.75275
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.73855
Epoch 1, cost is  5.46963
Epoch 2, cost is  5.14742
Epoch 3, cost is  4.67697
Epoch 4, cost is  4.27993
Epoch 5, cost is  3.97985
Epoch 6, cost is  3.742
Epoch 7, cost is  3.54037
Epoch 8, cost is  3.37041
Epoch 9, cost is  3.22296
Training took 0.353526 minutes
Weight histogram
[1921 1774 1563 1289 1349 3043  912  210   59   30] [-0.01835062 -0.01652912 -0.01470763 -0.01288614 -0.01106465 -0.00924315
 -0.00742166 -0.00560017 -0.00377867 -0.00195718 -0.00013569]
[2989  828  775  840  933 1041 1107 1166 1248 1223] [-0.01835062 -0.01652912 -0.01470763 -0.01288614 -0.01106465 -0.00924315
 -0.00742166 -0.00560017 -0.00377867 -0.00195718 -0.00013569]
-0.26917
0.341686
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080197 minutes
Epoch 0
Fine tuning took 0.080460 minutes
Epoch 0
Fine tuning took 0.082132 minutes
Epoch 0
Fine tuning took 0.080824 minutes
Epoch 0
Fine tuning took 0.081169 minutes
Epoch 0
Fine tuning took 0.081010 minutes
Epoch 0
Fine tuning took 0.080057 minutes
Epoch 0
Fine tuning took 0.081361 minutes
Epoch 0
Fine tuning took 0.081263 minutes
Epoch 0
Fine tuning took 0.080686 minutes
{'zero': {0: [0.27216748768472904, 0.21428571428571427, 0.25985221674876846, 0.23275862068965517, 0.22167487684729065, 0.19088669950738915, 0.22536945812807882, 0.16871921182266009, 0.20566502463054187, 0.20320197044334976, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.62561576354679804, 0.60467980295566504, 0.6219211822660099, 0.69581280788177335, 0.66871921182266014, 0.66625615763546797, 0.73152709359605916, 0.67980295566502458, 0.63177339901477836, 0.74014778325123154], 5: [0.14162561576354679, 0.16009852216748768, 0.1354679802955665, 0.14532019704433496, 0.082512315270935957, 0.14039408866995073, 0.10837438423645321, 0.099753694581280791, 0.1145320197044335, 0.16502463054187191, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27216748768472904, 0.28201970443349755, 0.34359605911330049, 0.38669950738916259, 0.3288177339901478, 0.33004926108374383, 0.32266009852216748, 0.24261083743842365, 0.24753694581280788, 0.25985221674876846, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.52339901477832518, 0.48768472906403942, 0.48152709359605911, 0.55418719211822665, 0.53078817733990147, 0.51354679802955661, 0.66502463054187189, 0.61206896551724133, 0.52709359605911332, 0.60960591133004927], 5: [0.14162561576354679, 0.19458128078817735, 0.16871921182266009, 0.13177339901477833, 0.11699507389162561, 0.13916256157635468, 0.16379310344827586, 0.092364532019704432, 0.14039408866995073, 0.21305418719211822, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27216748768472904, 0.32019704433497537, 0.32635467980295568, 0.34975369458128081, 0.35714285714285715, 0.35098522167487683, 0.31280788177339902, 0.24876847290640394, 0.23399014778325122, 0.30295566502463056, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.49753694581280788, 0.49876847290640391, 0.52216748768472909, 0.54433497536945807, 0.50246305418719217, 0.54064039408866993, 0.6354679802955665, 0.64778325123152714, 0.48275862068965519, 0.58004926108374388], 5: [0.14162561576354679, 0.18226600985221675, 0.1748768472906404, 0.12807881773399016, 0.098522167487684734, 0.14655172413793102, 0.14655172413793102, 0.11576354679802955, 0.11822660098522167, 0.21428571428571427, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27216748768472904, 0.31773399014778325, 0.33004926108374383, 0.37807881773399016, 0.3645320197044335, 0.3817733990147783, 0.29064039408866993, 0.22536945812807882, 0.25246305418719212, 0.25492610837438423, 0.23645320197044334], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.49384236453201968, 0.48399014778325122, 0.47044334975369456, 0.51847290640394084, 0.49384236453201968, 0.54802955665024633, 0.6711822660098522, 0.62438423645320196, 0.52216748768472909, 0.61083743842364535], 5: [0.14162561576354679, 0.18842364532019704, 0.18596059113300492, 0.15147783251231528, 0.11699507389162561, 0.12438423645320197, 0.16133004926108374, 0.10344827586206896, 0.12315270935960591, 0.2229064039408867, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235481 minutes
Weight histogram
[  184   588   776   472  1138  2279  6552 13877  4019   490] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
[ 163  284  426  686  963 1363 2990 4837 9099 9564] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
-1.28613
1.29103
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.4091
Epoch 1, cost is  1.38633
Epoch 2, cost is  1.36966
Epoch 3, cost is  1.35387
Epoch 4, cost is  1.34109
Training took 0.224251 minutes
Weight histogram
[7046 5051 4510 3138 2529 2243 1773 1510 1457 1118] [ -6.26062006e-02  -5.63388170e-02  -5.00714334e-02  -4.38040498e-02
  -3.75366662e-02  -3.12692826e-02  -2.50018990e-02  -1.87345154e-02
  -1.24671318e-02  -6.19974825e-03   6.76353375e-05]
[2408 1505 1854 2159 2584 3109 3242 4006 4649 4859] [ -6.26062006e-02  -5.63388170e-02  -5.00714334e-02  -4.38040498e-02
  -3.75366662e-02  -3.12692826e-02  -2.50018990e-02  -1.87345154e-02
  -1.24671318e-02  -6.19974825e-03   6.76353375e-05]
-1.00462
1.75301
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148643 minutes
Weight histogram
[ 356 1117 1465  816 2164 6804 9032 7980 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  474   970  1108   321   440   822   929  2039  5908 19389] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.117058 minutes
Weight histogram
[5785 4636 4156 3521 3069 2578 2010 2276 2992 1377] [ -5.99095523e-02  -5.39118336e-02  -4.79141148e-02  -4.19163960e-02
  -3.59186773e-02  -2.99209585e-02  -2.39232397e-02  -1.79255210e-02
  -1.19278022e-02  -5.93008343e-03   6.76353375e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39118336e-02  -4.79141148e-02  -4.19163960e-02
  -3.59186773e-02  -2.99209585e-02  -2.39232397e-02  -1.79255210e-02
  -1.19278022e-02  -5.93008343e-03   6.76353375e-05]
-1.57414
1.75275
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.17783
Epoch 1, cost is  4.95037
Epoch 2, cost is  4.62225
Epoch 3, cost is  4.17175
Epoch 4, cost is  3.78294
Epoch 5, cost is  3.48772
Epoch 6, cost is  3.24917
Epoch 7, cost is  3.04731
Epoch 8, cost is  2.87877
Epoch 9, cost is  2.73268
Training took 0.544219 minutes
Weight histogram
[2452 2359 1952 4126  721  287  135   65   33   20] [-0.01290335 -0.01162595 -0.01034855 -0.00907115 -0.00779375 -0.00651635
 -0.00523895 -0.00396155 -0.00268415 -0.00140675 -0.00012934]
[2990  836  799  849  929 1012 1074 1151 1247 1263] [-0.01290335 -0.01162595 -0.01034855 -0.00907115 -0.00779375 -0.00651635
 -0.00523895 -0.00396155 -0.00268415 -0.00140675 -0.00012934]
-0.23005
0.266365
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090990 minutes
Epoch 0
Fine tuning took 0.090975 minutes
Epoch 0
Fine tuning took 0.090777 minutes
Epoch 0
Fine tuning took 0.091839 minutes
Epoch 0
Fine tuning took 0.089648 minutes
Epoch 0
Fine tuning took 0.091163 minutes
Epoch 0
Fine tuning took 0.091208 minutes
Epoch 0
Fine tuning took 0.090311 minutes
Epoch 0
Fine tuning took 0.090030 minutes
Epoch 0
Fine tuning took 0.089759 minutes
{'zero': {0: [0.18226600985221675, 0.18226600985221675, 0.17980295566502463, 0.11083743842364532, 0.17980295566502463, 0.18349753694581281, 0.16625615763546797, 0.23152709359605911, 0.19581280788177341, 0.22167487684729065, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.62438423645320196, 0.66133004926108374, 0.71921182266009853, 0.6576354679802956, 0.6219211822660099, 0.61945812807881773, 0.57266009852216748, 0.65024630541871919, 0.56773399014778325, 0.62315270935960587], 5: [0.14408866995073891, 0.19334975369458129, 0.15886699507389163, 0.16995073891625614, 0.1625615763546798, 0.19458128078817735, 0.21428571428571427, 0.19581280788177341, 0.1539408866995074, 0.2105911330049261, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18226600985221675, 0.22044334975369459, 0.19950738916256158, 0.14039408866995073, 0.16625615763546797, 0.17364532019704434, 0.19458128078817735, 0.19827586206896552, 0.18472906403940886, 0.19458128078817735, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.5788177339901478, 0.63793103448275867, 0.68596059113300489, 0.63423645320197042, 0.59975369458128081, 0.56773399014778325, 0.61330049261083741, 0.64901477832512311, 0.60837438423645318, 0.61945812807881773], 5: [0.14408866995073891, 0.20073891625615764, 0.1625615763546798, 0.17364532019704434, 0.19950738916256158, 0.22660098522167488, 0.2376847290640394, 0.18842364532019704, 0.16625615763546797, 0.19704433497536947, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18226600985221675, 0.2019704433497537, 0.17857142857142858, 0.16379310344827586, 0.21798029556650247, 0.18349753694581281, 0.22906403940886699, 0.25246305418719212, 0.17610837438423646, 0.19211822660098521, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.58374384236453203, 0.64039408866995073, 0.64655172413793105, 0.60591133004926112, 0.59605911330049266, 0.55665024630541871, 0.53694581280788178, 0.6576354679802956, 0.6219211822660099, 0.60344827586206895], 5: [0.14408866995073891, 0.21428571428571427, 0.18103448275862069, 0.18965517241379309, 0.17610837438423646, 0.22044334975369459, 0.21428571428571427, 0.2105911330049261, 0.16625615763546797, 0.18596059113300492, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18226600985221675, 0.23029556650246305, 0.18472906403940886, 0.13300492610837439, 0.18472906403940886, 0.17364532019704434, 0.20320197044334976, 0.21182266009852216, 0.16871921182266009, 0.19704433497536947, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.59852216748768472, 0.62068965517241381, 0.70812807881773399, 0.64408866995073888, 0.63300492610837433, 0.5923645320197044, 0.60591133004926112, 0.67364532019704437, 0.62561576354679804, 0.64532019704433496], 5: [0.14408866995073891, 0.17118226600985223, 0.19458128078817735, 0.15886699507389163, 0.17118226600985223, 0.19334975369458129, 0.20443349753694581, 0.18226600985221675, 0.15763546798029557, 0.17733990147783252, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234572 minutes
Weight histogram
[  184   588   776   472  1138  2279  6552 13877  4019   490] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
[ 163  284  426  686  963 1363 2990 4837 9099 9564] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
-1.28613
1.29103
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.4091
Epoch 1, cost is  1.38633
Epoch 2, cost is  1.36966
Epoch 3, cost is  1.35387
Epoch 4, cost is  1.34109
Training took 0.222747 minutes
Weight histogram
[7046 5051 4510 3138 2529 2243 1773 1510 1457 1118] [ -6.26062006e-02  -5.63388170e-02  -5.00714334e-02  -4.38040498e-02
  -3.75366662e-02  -3.12692826e-02  -2.50018990e-02  -1.87345154e-02
  -1.24671318e-02  -6.19974825e-03   6.76353375e-05]
[2408 1505 1854 2159 2584 3109 3242 4006 4649 4859] [ -6.26062006e-02  -5.63388170e-02  -5.00714334e-02  -4.38040498e-02
  -3.75366662e-02  -3.12692826e-02  -2.50018990e-02  -1.87345154e-02
  -1.24671318e-02  -6.19974825e-03   6.76353375e-05]
-1.00462
1.75301
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149044 minutes
Weight histogram
[ 356 1117 1465  816 2164 6804 9032 7980 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  474   970  1108   321   440   822   929  2039  5908 19389] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.118184 minutes
Weight histogram
[5785 4636 4156 3521 3069 2578 2010 2276 2992 1377] [ -5.99095523e-02  -5.39118336e-02  -4.79141148e-02  -4.19163960e-02
  -3.59186773e-02  -2.99209585e-02  -2.39232397e-02  -1.79255210e-02
  -1.19278022e-02  -5.93008343e-03   6.76353375e-05]
[5005 1529 1820 2324 2623 3089 3388 3838 4233 4551] [ -5.99095523e-02  -5.39118336e-02  -4.79141148e-02  -4.19163960e-02
  -3.59186773e-02  -2.99209585e-02  -2.39232397e-02  -1.79255210e-02
  -1.19278022e-02  -5.93008343e-03   6.76353375e-05]
-1.57414
1.75275
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.9735
Epoch 1, cost is  4.67053
Epoch 2, cost is  4.06699
Epoch 3, cost is  3.57344
Epoch 4, cost is  3.21728
Epoch 5, cost is  2.94462
Epoch 6, cost is  2.72181
Epoch 7, cost is  2.53991
Epoch 8, cost is  2.39277
Epoch 9, cost is  2.27402
Training took 0.943435 minutes
Weight histogram
[2540 2381 2091 1535 2679  766   91   36   19   12] [-0.00793658 -0.00715502 -0.00637345 -0.00559189 -0.00481033 -0.00402877
 -0.0032472  -0.00246564 -0.00168408 -0.00090251 -0.00012095]
[2461  764  790  892  954 1049 1144 1262 1414 1420] [-0.00793658 -0.00715502 -0.00637345 -0.00559189 -0.00481033 -0.00402877
 -0.0032472  -0.00246564 -0.00168408 -0.00090251 -0.00012095]
-0.177287
0.189962
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.109548 minutes
Epoch 0
Fine tuning took 0.109794 minutes
Epoch 0
Fine tuning took 0.109211 minutes
Epoch 0
Fine tuning took 0.109603 minutes
Epoch 0
Fine tuning took 0.109178 minutes
Epoch 0
Fine tuning took 0.109314 minutes
Epoch 0
Fine tuning took 0.109356 minutes
Epoch 0
Fine tuning took 0.109380 minutes
Epoch 0
Fine tuning took 0.110095 minutes
Epoch 0
Fine tuning took 0.109785 minutes
{'zero': {0: [0.17241379310344829, 0.20689655172413793, 0.18472906403940886, 0.21674876847290642, 0.20320197044334976, 0.18103448275862069, 0.17241379310344829, 0.20320197044334976, 0.24261083743842365, 0.2105911330049261, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.54187192118226601, 0.60098522167487689, 0.5788177339901478, 0.56157635467980294, 0.4963054187192118, 0.51477832512315269, 0.50862068965517238, 0.50123152709359609, 0.48029556650246308, 0.53940886699507384], 5: [0.20320197044334976, 0.25123152709359609, 0.21428571428571427, 0.20443349753694581, 0.23522167487684728, 0.32266009852216748, 0.31280788177339902, 0.28817733990147781, 0.25615763546798032, 0.30911330049261082, 0.28448275862068967], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17241379310344829, 0.2105911330049261, 0.20073891625615764, 0.20566502463054187, 0.19088669950738915, 0.22536945812807882, 0.21551724137931033, 0.22906403940886699, 0.26477832512315269, 0.24014778325123154, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.56773399014778325, 0.55295566502463056, 0.56773399014778325, 0.56650246305418717, 0.44704433497536944, 0.51600985221674878, 0.51231527093596063, 0.48768472906403942, 0.47906403940886699, 0.52463054187192115], 5: [0.20320197044334976, 0.22167487684729065, 0.24630541871921183, 0.22660098522167488, 0.24261083743842365, 0.32758620689655171, 0.26847290640394089, 0.25862068965517243, 0.24753694581280788, 0.28078817733990147, 0.28448275862068967], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17241379310344829, 0.20689655172413793, 0.23152709359605911, 0.22536945812807882, 0.2019704433497537, 0.17610837438423646, 0.18103448275862069, 0.22906403940886699, 0.26231527093596058, 0.23522167487684728, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.57019704433497542, 0.54802955665024633, 0.53817733990147787, 0.55049261083743839, 0.48645320197044334, 0.54802955665024633, 0.49753694581280788, 0.47044334975369456, 0.46798029556650245, 0.53940886699507384], 5: [0.20320197044334976, 0.2229064039408867, 0.22044334975369459, 0.23645320197044334, 0.24753694581280788, 0.33743842364532017, 0.27093596059113301, 0.27339901477832512, 0.26724137931034481, 0.29679802955665024, 0.27093596059113301], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17241379310344829, 0.22660098522167488, 0.24261083743842365, 0.21551724137931033, 0.21305418719211822, 0.16625615763546797, 0.19827586206896552, 0.21428571428571427, 0.23891625615763548, 0.22413793103448276, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.53694581280788178, 0.53940886699507384, 0.55911330049261088, 0.56157635467980294, 0.52216748768472909, 0.50985221674876846, 0.53078817733990147, 0.49014778325123154, 0.49507389162561577, 0.53201970443349755], 5: [0.20320197044334976, 0.23645320197044334, 0.21798029556650247, 0.22536945812807882, 0.22536945812807882, 0.31157635467980294, 0.29187192118226601, 0.25492610837438423, 0.27093596059113301, 0.28078817733990147, 0.29556650246305421], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.422246 minutes
Weight histogram
[ 151  494  667 1423 4334 9018 9696 3028 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1204 1058  929 1397 2570 1802 3962 3906 8434 5113] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.15433
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.896433
Epoch 1, cost is  0.879236
Epoch 2, cost is  0.866073
Epoch 3, cost is  0.856286
Epoch 4, cost is  0.84768
Training took 0.647134 minutes
Weight histogram
[7644 5786 4483 3296 2540 1755 1467 1266 1005 1133] [ -4.57002521e-02  -4.11359163e-02  -3.65715806e-02  -3.20072449e-02
  -2.74429092e-02  -2.28785734e-02  -1.83142377e-02  -1.37499020e-02
  -9.18556628e-03  -4.62123056e-03  -5.68948381e-05]
[2096 1438 1615 2052 2402 3204 3488 3771 4913 5396] [ -4.57002521e-02  -4.11359163e-02  -3.65715806e-02  -3.20072449e-02
  -2.74429092e-02  -2.28785734e-02  -1.83142377e-02  -1.37499020e-02
  -9.18556628e-03  -4.62123056e-03  -5.68948381e-05]
-0.921768
1.6542
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148855 minutes
Weight histogram
[ 538 1619  977  621 2163 6804 9032 7980 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2165   157   229   321   441   821   929  2036  5911 19390] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115426 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 2288 2607 1773] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5003 1529 1820 2324 2623 3088 3389 3840 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.4594
Epoch 1, cost is  6.10015
Epoch 2, cost is  5.62787
Epoch 3, cost is  5.22604
Epoch 4, cost is  4.92146
Epoch 5, cost is  4.67568
Epoch 6, cost is  4.46651
Epoch 7, cost is  4.28951
Epoch 8, cost is  4.13864
Epoch 9, cost is  4.00459
Training took 0.322081 minutes
Weight histogram
[1492 1484 1311 1218 1142  995 1010 1230 2005  263] [-0.03172233 -0.02856675 -0.02541117 -0.02225559 -0.01910001 -0.01594443
 -0.01278885 -0.00963327 -0.00647769 -0.00332211 -0.00016653]
[2033  886  870  936 1026 1116 1201 1286 1386 1410] [-0.03172233 -0.02856675 -0.02541117 -0.02225559 -0.01910001 -0.01594443
 -0.01278885 -0.00963327 -0.00647769 -0.00332211 -0.00016653]
-0.484167
0.639102
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.142864 minutes
Epoch 0
Fine tuning took 0.142420 minutes
Epoch 0
Fine tuning took 0.142415 minutes
Epoch 0
Fine tuning took 0.143248 minutes
Epoch 0
Fine tuning took 0.141962 minutes
Epoch 0
Fine tuning took 0.141456 minutes
Epoch 0
Fine tuning took 0.141829 minutes
Epoch 0
Fine tuning took 0.142104 minutes
Epoch 0
Fine tuning took 0.141895 minutes
Epoch 0
Fine tuning took 0.141862 minutes
{'zero': {0: [0.18472906403940886, 0.44581280788177341, 0.28078817733990147, 0.029556650246305417, 0.3682266009852217, 0.097290640394088676, 0.12931034482758622, 0.22044334975369459, 0.14901477832512317, 0.14901477832512317, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.50246305418719217, 0.67241379310344829, 0.88423645320197042, 0.29679802955665024, 0.75862068965517238, 0.63054187192118227, 0.73152709359605916, 0.75985221674876846, 0.73768472906403937, 0.65640394088669951], 5: [0.10098522167487685, 0.051724137931034482, 0.046798029556650245, 0.086206896551724144, 0.33497536945812806, 0.14408866995073891, 0.24014778325123154, 0.048029556650246302, 0.091133004926108374, 0.11330049261083744, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18472906403940886, 0.44458128078817732, 0.25862068965517243, 0.032019704433497539, 0.40024630541871919, 0.11206896551724138, 0.1354679802955665, 0.24261083743842365, 0.15147783251231528, 0.16133004926108374, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.50492610837438423, 0.69458128078817738, 0.8928571428571429, 0.26970443349753692, 0.73891625615763545, 0.63423645320197042, 0.71059113300492616, 0.73275862068965514, 0.68349753694581283, 0.57019704433497542], 5: [0.10098522167487685, 0.050492610837438424, 0.046798029556650245, 0.075123152709359611, 0.33004926108374383, 0.14901477832512317, 0.23029556650246305, 0.046798029556650245, 0.11576354679802955, 0.15517241379310345, 0.26231527093596058], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18472906403940886, 0.46798029556650245, 0.30049261083743845, 0.044334975369458129, 0.37192118226600984, 0.12438423645320197, 0.1206896551724138, 0.2376847290640394, 0.16995073891625614, 0.17610837438423646, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.48275862068965519, 0.64532019704433496, 0.8780788177339901, 0.28448275862068967, 0.72906403940886699, 0.62931034482758619, 0.70197044334975367, 0.71305418719211822, 0.66871921182266014, 0.56896551724137934], 5: [0.10098522167487685, 0.049261083743842367, 0.054187192118226604, 0.077586206896551727, 0.34359605911330049, 0.14655172413793102, 0.25, 0.060344827586206899, 0.11699507389162561, 0.15517241379310345, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18472906403940886, 0.46551724137931033, 0.26231527093596058, 0.033251231527093597, 0.39285714285714285, 0.10837438423645321, 0.1206896551724138, 0.25985221674876846, 0.14162561576354679, 0.19581280788177341, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.47290640394088668, 0.69458128078817738, 0.90147783251231528, 0.28325123152709358, 0.72044334975369462, 0.63177339901477836, 0.67980295566502458, 0.72167487684729059, 0.65147783251231528, 0.52463054187192115], 5: [0.10098522167487685, 0.061576354679802957, 0.043103448275862072, 0.065270935960591137, 0.32389162561576357, 0.17118226600985223, 0.24753694581280788, 0.060344827586206899, 0.13669950738916256, 0.15270935960591134, 0.26477832512315269], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.424075 minutes
Weight histogram
[ 151  494  667 1423 4334 9018 9696 3028 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1204 1058  929 1397 2570 1802 3962 3906 8434 5113] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.15433
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.896433
Epoch 1, cost is  0.879236
Epoch 2, cost is  0.866073
Epoch 3, cost is  0.856286
Epoch 4, cost is  0.84768
Training took 0.646072 minutes
Weight histogram
[7644 5786 4483 3296 2540 1755 1467 1266 1005 1133] [ -4.57002521e-02  -4.11359163e-02  -3.65715806e-02  -3.20072449e-02
  -2.74429092e-02  -2.28785734e-02  -1.83142377e-02  -1.37499020e-02
  -9.18556628e-03  -4.62123056e-03  -5.68948381e-05]
[2096 1438 1615 2052 2402 3204 3488 3771 4913 5396] [ -4.57002521e-02  -4.11359163e-02  -3.65715806e-02  -3.20072449e-02
  -2.74429092e-02  -2.28785734e-02  -1.83142377e-02  -1.37499020e-02
  -9.18556628e-03  -4.62123056e-03  -5.68948381e-05]
-0.921768
1.6542
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149279 minutes
Weight histogram
[ 538 1619  977  621 2163 6804 9032 7980 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2165   157   229   321   441   821   929  2036  5911 19390] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.116134 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 2288 2607 1773] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5003 1529 1820 2324 2623 3088 3389 3840 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.98022
Epoch 1, cost is  5.51722
Epoch 2, cost is  4.93241
Epoch 3, cost is  4.48999
Epoch 4, cost is  4.12606
Epoch 5, cost is  3.8306
Epoch 6, cost is  3.59944
Epoch 7, cost is  3.40896
Epoch 8, cost is  3.25444
Epoch 9, cost is  3.12176
Training took 0.499787 minutes
Weight histogram
[1661 1572 1443 1351 1315 1140 1007 1996  623   42] [-0.02028304 -0.01826947 -0.01625591 -0.01424235 -0.01222879 -0.01021523
 -0.00820166 -0.0061881  -0.00417454 -0.00216098 -0.00014742]
[2134  870  937  989 1011 1046 1144 1250 1370 1399] [-0.02028304 -0.01826947 -0.01625591 -0.01424235 -0.01222879 -0.01021523
 -0.00820166 -0.0061881  -0.00417454 -0.00216098 -0.00014742]
-0.349079
0.429552
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.148747 minutes
Epoch 0
Fine tuning took 0.150032 minutes
Epoch 0
Fine tuning took 0.150967 minutes
Epoch 0
Fine tuning took 0.150595 minutes
Epoch 0
Fine tuning took 0.150599 minutes
Epoch 0
Fine tuning took 0.149585 minutes
Epoch 0
Fine tuning took 0.149067 minutes
Epoch 0
Fine tuning took 0.150466 minutes
Epoch 0
Fine tuning took 0.150435 minutes
Epoch 0
Fine tuning took 0.149476 minutes
{'zero': {0: [0.24753694581280788, 0.14039408866995073, 0.20935960591133004, 0.22660098522167488, 0.23275862068965517, 0.20073891625615764, 0.22783251231527094, 0.24630541871921183, 0.16748768472906403, 0.23029556650246305, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.69334975369458129, 0.66871921182266014, 0.68103448275862066, 0.69950738916256161, 0.68719211822660098, 0.66009852216748766, 0.63916256157635465, 0.74630541871921185, 0.65640394088669951, 0.64901477832512311], 5: [0.1268472906403941, 0.16625615763546797, 0.12192118226600986, 0.092364532019704432, 0.067733990147783252, 0.11206896551724138, 0.11206896551724138, 0.1145320197044335, 0.086206896551724144, 0.11330049261083744, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24753694581280788, 0.24753694581280788, 0.30049261083743845, 0.32635467980295568, 0.37438423645320196, 0.32758620689655171, 0.28817733990147781, 0.2894088669950739, 0.19458128078817735, 0.23891625615763548, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.54556650246305416, 0.50492610837438423, 0.55788177339901479, 0.51477832512315269, 0.56280788177339902, 0.55788177339901479, 0.57019704433497542, 0.68596059113300489, 0.61206896551724133, 0.6071428571428571], 5: [0.1268472906403941, 0.20689655172413793, 0.19458128078817735, 0.11576354679802955, 0.11083743842364532, 0.10960591133004927, 0.1539408866995074, 0.14039408866995073, 0.11945812807881774, 0.14901477832512317, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24753694581280788, 0.25123152709359609, 0.28817733990147781, 0.31034482758620691, 0.35098522167487683, 0.35960591133004927, 0.27216748768472904, 0.26108374384236455, 0.18965517241379309, 0.24014778325123154, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.55665024630541871, 0.52586206896551724, 0.55418719211822665, 0.54556650246305416, 0.51600985221674878, 0.58990147783251234, 0.59113300492610843, 0.69088669950738912, 0.59729064039408863, 0.55911330049261088], 5: [0.1268472906403941, 0.19211822660098521, 0.18596059113300492, 0.1354679802955665, 0.10344827586206896, 0.12438423645320197, 0.13793103448275862, 0.14778325123152711, 0.11945812807881774, 0.1625615763546798, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24753694581280788, 0.27093596059113301, 0.30911330049261082, 0.32019704433497537, 0.38669950738916259, 0.34359605911330049, 0.25615763546798032, 0.27955665024630544, 0.20812807881773399, 0.2413793103448276, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.53325123152709364, 0.47044334975369456, 0.54433497536945807, 0.5073891625615764, 0.53325123152709364, 0.57635467980295563, 0.60591133004926112, 0.70197044334975367, 0.6145320197044335, 0.58128078817733986], 5: [0.1268472906403941, 0.19581280788177341, 0.22044334975369459, 0.1354679802955665, 0.10591133004926108, 0.12315270935960591, 0.16748768472906403, 0.1145320197044335, 0.089901477832512317, 0.14408866995073891, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421305 minutes
Weight histogram
[ 151  494  667 1423 4334 9018 9696 3028 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1204 1058  929 1397 2570 1802 3962 3906 8434 5113] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.15433
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.896433
Epoch 1, cost is  0.879236
Epoch 2, cost is  0.866073
Epoch 3, cost is  0.856286
Epoch 4, cost is  0.84768
Training took 0.645558 minutes
Weight histogram
[7644 5786 4483 3296 2540 1755 1467 1266 1005 1133] [ -4.57002521e-02  -4.11359163e-02  -3.65715806e-02  -3.20072449e-02
  -2.74429092e-02  -2.28785734e-02  -1.83142377e-02  -1.37499020e-02
  -9.18556628e-03  -4.62123056e-03  -5.68948381e-05]
[2096 1438 1615 2052 2402 3204 3488 3771 4913 5396] [ -4.57002521e-02  -4.11359163e-02  -3.65715806e-02  -3.20072449e-02
  -2.74429092e-02  -2.28785734e-02  -1.83142377e-02  -1.37499020e-02
  -9.18556628e-03  -4.62123056e-03  -5.68948381e-05]
-0.921768
1.6542
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150360 minutes
Weight histogram
[ 538 1619  977  621 2163 6804 9032 7980 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2165   157   229   321   441   821   929  2036  5911 19390] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115693 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 2288 2607 1773] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5003 1529 1820 2324 2623 3088 3389 3840 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.38391
Epoch 1, cost is  4.87576
Epoch 2, cost is  4.2806
Epoch 3, cost is  3.83086
Epoch 4, cost is  3.47911
Epoch 5, cost is  3.21572
Epoch 6, cost is  3.01227
Epoch 7, cost is  2.8481
Epoch 8, cost is  2.71424
Epoch 9, cost is  2.60415
Training took 0.820149 minutes
Weight histogram
[2089 1957 1663 1489 1357 1114 2096  306   57   22] [-0.0134927  -0.01215669 -0.01082068 -0.00948467 -0.00814866 -0.00681265
 -0.00547664 -0.00414063 -0.00280462 -0.00146861 -0.00013261]
[2151  878  914  937  974 1054 1165 1264 1387 1426] [-0.0134927  -0.01215669 -0.01082068 -0.00948467 -0.00814866 -0.00681265
 -0.00547664 -0.00414063 -0.00280462 -0.00146861 -0.00013261]
-0.205822
0.322582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.165037 minutes
Epoch 0
Fine tuning took 0.164933 minutes
Epoch 0
Fine tuning took 0.165051 minutes
Epoch 0
Fine tuning took 0.164535 minutes
Epoch 0
Fine tuning took 0.164820 minutes
Epoch 0
Fine tuning took 0.165263 minutes
Epoch 0
Fine tuning took 0.165191 minutes
Epoch 0
Fine tuning took 0.164827 minutes
Epoch 0
Fine tuning took 0.165472 minutes
Epoch 0
Fine tuning took 0.164437 minutes
{'zero': {0: [0.2229064039408867, 0.19088669950738915, 0.23029556650246305, 0.1539408866995074, 0.21428571428571427, 0.17364532019704434, 0.18226600985221675, 0.20812807881773399, 0.21428571428571427, 0.20689655172413793, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.58374384236453203, 0.57758620689655171, 0.67733990147783252, 0.58497536945812811, 0.60591133004926112, 0.58374384236453203, 0.5788177339901478, 0.58620689655172409, 0.58128078817733986, 0.60837438423645318], 5: [0.16748768472906403, 0.22536945812807882, 0.19211822660098521, 0.16871921182266009, 0.20073891625615764, 0.22044334975369459, 0.23399014778325122, 0.21305418719211822, 0.19950738916256158, 0.21182266009852216, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2229064039408867, 0.2105911330049261, 0.23152709359605911, 0.20812807881773399, 0.23645320197044334, 0.2413793103448276, 0.19334975369458129, 0.2105911330049261, 0.17857142857142858, 0.19827586206896552, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.60344827586206895, 0.56650246305418717, 0.6354679802955665, 0.53448275862068961, 0.55049261083743839, 0.55665024630541871, 0.58128078817733986, 0.61330049261083741, 0.55665024630541871, 0.5431034482758621], 5: [0.16748768472906403, 0.18596059113300492, 0.2019704433497537, 0.15640394088669951, 0.22906403940886699, 0.20812807881773399, 0.25, 0.20812807881773399, 0.20812807881773399, 0.24507389162561577, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2229064039408867, 0.20935960591133004, 0.21305418719211822, 0.18472906403940886, 0.24261083743842365, 0.19827586206896552, 0.19950738916256158, 0.21305418719211822, 0.21182266009852216, 0.22783251231527094, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.56280788177339902, 0.59359605911330049, 0.64778325123152714, 0.55049261083743839, 0.58004926108374388, 0.55295566502463056, 0.57266009852216748, 0.55295566502463056, 0.54556650246305416, 0.55418719211822665], 5: [0.16748768472906403, 0.22783251231527094, 0.19334975369458129, 0.16748768472906403, 0.20689655172413793, 0.22167487684729065, 0.24753694581280788, 0.21428571428571427, 0.23522167487684728, 0.22660098522167488, 0.25246305418719212], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2229064039408867, 0.21798029556650247, 0.22536945812807882, 0.19581280788177341, 0.2229064039408867, 0.23522167487684728, 0.19211822660098521, 0.21182266009852216, 0.20443349753694581, 0.24630541871921183, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.56527093596059108, 0.55665024630541871, 0.62315270935960587, 0.55541871921182262, 0.53325123152709364, 0.54802955665024633, 0.59729064039408863, 0.58004926108374388, 0.51231527093596063, 0.59482758620689657], 5: [0.16748768472906403, 0.21674876847290642, 0.21798029556650247, 0.18103448275862069, 0.22167487684729065, 0.23152709359605911, 0.25985221674876846, 0.19088669950738915, 0.21551724137931033, 0.2413793103448276, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421890 minutes
Weight histogram
[ 151  494  667 1423 4334 9018 9696 3028 1327  237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1204 1058  929 1397 2570 1802 3962 3906 8434 5113] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.2662
1.15433
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.896433
Epoch 1, cost is  0.879236
Epoch 2, cost is  0.866073
Epoch 3, cost is  0.856286
Epoch 4, cost is  0.84768
Training took 0.646096 minutes
Weight histogram
[7644 5786 4483 3296 2540 1755 1467 1266 1005 1133] [ -4.57002521e-02  -4.11359163e-02  -3.65715806e-02  -3.20072449e-02
  -2.74429092e-02  -2.28785734e-02  -1.83142377e-02  -1.37499020e-02
  -9.18556628e-03  -4.62123056e-03  -5.68948381e-05]
[2096 1438 1615 2052 2402 3204 3488 3771 4913 5396] [ -4.57002521e-02  -4.11359163e-02  -3.65715806e-02  -3.20072449e-02
  -2.74429092e-02  -2.28785734e-02  -1.83142377e-02  -1.37499020e-02
  -9.18556628e-03  -4.62123056e-03  -5.68948381e-05]
-0.921768
1.6542
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148621 minutes
Weight histogram
[ 538 1619  977  621 2163 6804 9032 7980 2516  150] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2165   157   229   321   441   821   929  2036  5911 19390] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.48892
Epoch 1, cost is  2.45036
Epoch 2, cost is  2.4228
Epoch 3, cost is  2.40206
Epoch 4, cost is  2.38225
Training took 0.115547 minutes
Weight histogram
[5785 4625 4165 3509 3076 2578 1994 2288 2607 1773] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
[5003 1529 1820 2324 2623 3088 3389 3840 4233 4551] [ -5.99095523e-02  -5.39154091e-02  -4.79212659e-02  -4.19271227e-02
  -3.59329795e-02  -2.99388362e-02  -2.39446930e-02  -1.79505498e-02
  -1.19564066e-02  -5.96226337e-03   3.18798448e-05]
-1.57414
1.75275
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.79205
Epoch 1, cost is  4.29825
Epoch 2, cost is  3.74835
Epoch 3, cost is  3.32072
Epoch 4, cost is  3.00586
Epoch 5, cost is  2.77673
Epoch 6, cost is  2.60614
Epoch 7, cost is  2.46955
Epoch 8, cost is  2.35829
Epoch 9, cost is  2.26234
Training took 1.470630 minutes
Weight histogram
[2701 2397 1971 1901 2543  381  151   62   28   15] [-0.00915086 -0.00825041 -0.00734996 -0.00644952 -0.00554907 -0.00464862
 -0.00374818 -0.00284773 -0.00194729 -0.00104684 -0.00014639]
[2112  874  867  878  944 1038 1167 1302 1457 1511] [-0.00915086 -0.00825041 -0.00734996 -0.00644952 -0.00554907 -0.00464862
 -0.00374818 -0.00284773 -0.00194729 -0.00104684 -0.00014639]
-0.187185
0.221182
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.194030 minutes
Epoch 0
Fine tuning took 0.193753 minutes
Epoch 0
Fine tuning took 0.194386 minutes
Epoch 0
Fine tuning took 0.193332 minutes
Epoch 0
Fine tuning took 0.193035 minutes
Epoch 0
Fine tuning took 0.193685 minutes
Epoch 0
Fine tuning took 0.195021 minutes
Epoch 0
Fine tuning took 0.193826 minutes
Epoch 0
Fine tuning took 0.193286 minutes
Epoch 0
Fine tuning took 0.193929 minutes
{'zero': {0: [0.14901477832512317, 0.18842364532019704, 0.23645320197044334, 0.1748768472906404, 0.16995073891625614, 0.17733990147783252, 0.20320197044334976, 0.20320197044334976, 0.20689655172413793, 0.17118226600985223, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.56280788177339902, 0.51354679802955661, 0.56527093596059108, 0.58251231527093594, 0.52955665024630538, 0.53078817733990147, 0.51724137931034486, 0.56650246305418717, 0.51231527093596063, 0.56157635467980294], 5: [0.19334975369458129, 0.24876847290640394, 0.25, 0.25985221674876846, 0.24753694581280788, 0.29310344827586204, 0.26600985221674878, 0.27955665024630544, 0.22660098522167488, 0.31650246305418717, 0.28078817733990147], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14901477832512317, 0.19211822660098521, 0.23399014778325122, 0.17980295566502463, 0.16995073891625614, 0.21182266009852216, 0.19334975369458129, 0.2105911330049261, 0.24261083743842365, 0.20073891625615764, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.5714285714285714, 0.53201970443349755, 0.58990147783251234, 0.58743842364532017, 0.48768472906403942, 0.52339901477832518, 0.51724137931034486, 0.50246305418719217, 0.54187192118226601, 0.53940886699507384], 5: [0.19334975369458129, 0.23645320197044334, 0.23399014778325122, 0.23029556650246305, 0.24261083743842365, 0.30049261083743845, 0.28325123152709358, 0.27216748768472904, 0.25492610837438423, 0.25738916256157635, 0.29064039408866993], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14901477832512317, 0.17857142857142858, 0.19950738916256158, 0.17857142857142858, 0.18596059113300492, 0.17980295566502463, 0.18719211822660098, 0.21551724137931033, 0.25492610837438423, 0.22783251231527094, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.58004926108374388, 0.56403940886699511, 0.58128078817733986, 0.56280788177339902, 0.51970443349753692, 0.53694581280788178, 0.51477832512315269, 0.49261083743842365, 0.52216748768472909, 0.55172413793103448], 5: [0.19334975369458129, 0.2413793103448276, 0.23645320197044334, 0.24014778325123154, 0.25123152709359609, 0.30049261083743845, 0.27586206896551724, 0.26970443349753692, 0.25246305418719212, 0.25, 0.28201970443349755], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14901477832512317, 0.20320197044334976, 0.21798029556650247, 0.16502463054187191, 0.17610837438423646, 0.18719211822660098, 0.22044334975369459, 0.18842364532019704, 0.22413793103448276, 0.22660098522167488, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.54433497536945807, 0.54926108374384242, 0.5788177339901478, 0.55665024630541871, 0.49876847290640391, 0.49137931034482757, 0.53694581280788178, 0.54433497536945807, 0.49261083743842365, 0.5431034482758621], 5: [0.19334975369458129, 0.25246305418719212, 0.23275862068965517, 0.25615763546798032, 0.26724137931034481, 0.31403940886699505, 0.28817733990147781, 0.27463054187192121, 0.23152709359605911, 0.28078817733990147, 0.27709359605911332], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106027 minutes
Weight histogram
[1274  953 4330 6735 9347 6569 2446 1797  913   61] [-0.00381266 -0.00302475 -0.00223684 -0.00144892 -0.00066101  0.0001269
  0.00091481  0.00170273  0.00249064  0.00327855  0.00406646]
[  135   148   228   309   464   747   752  1688  4036 25918] [-0.00381266 -0.00302475 -0.00223684 -0.00144892 -0.00066101  0.0001269
  0.00091481  0.00170273  0.00249064  0.00327855  0.00406646]
-2.16556
2.88085
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.72018
Epoch 1, cost is  3.67154
Epoch 2, cost is  3.63023
Epoch 3, cost is  3.61612
Epoch 4, cost is  3.5864
Training took 0.072402 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 7505  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2992 2550 2386 2667 2568 3027 3654 4758 4978 4845] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.8193
2.61544
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150781 minutes
Weight histogram
[   48   106   545  1430  1710  8375 15211  8612   410     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 22770   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.115790 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2404 3865  724] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.01487
Epoch 1, cost is  5.70753
Epoch 2, cost is  5.61496
Epoch 3, cost is  5.46483
Epoch 4, cost is  5.23572
Epoch 5, cost is  5.03125
Epoch 6, cost is  4.83157
Epoch 7, cost is  4.64966
Epoch 8, cost is  4.48867
Epoch 9, cost is  4.35456
Training took 0.178206 minutes
Weight histogram
[2480 3308 4071 3009 1276  842  628  370  144   72] [ -2.92806122e-02  -2.63606362e-02  -2.34406602e-02  -2.05206842e-02
  -1.76007082e-02  -1.46807322e-02  -1.17607561e-02  -8.84078013e-03
  -5.92080411e-03  -3.00082809e-03  -8.08520781e-05]
[1196 2429 2621 1434 1404 1446 1440 1458 1520 1252] [ -2.92806122e-02  -2.63606362e-02  -2.34406602e-02  -2.05206842e-02
  -1.76007082e-02  -1.46807322e-02  -1.17607561e-02  -8.84078013e-03
  -5.92080411e-03  -3.00082809e-03  -8.08520781e-05]
-0.373684
0.362733
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044235 minutes
Epoch 0
Fine tuning took 0.040521 minutes
Epoch 0
Fine tuning took 0.042259 minutes
Epoch 0
Fine tuning took 0.041249 minutes
Epoch 0
Fine tuning took 0.042761 minutes
Epoch 0
Fine tuning took 0.042932 minutes
Epoch 0
Fine tuning took 0.042054 minutes
Epoch 0
Fine tuning took 0.042390 minutes
Epoch 0
Fine tuning took 0.044144 minutes
Epoch 0
Fine tuning took 0.041872 minutes
{'zero': {0: [0.33990147783251229, 0.23522167487684728, 0.13177339901477833, 0.10467980295566502, 0.12192118226600986, 0.24261083743842365, 0.23152709359605911, 0.20443349753694581, 0.22783251231527094, 0.17364532019704434, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.53694581280788178, 0.69827586206896552, 0.72413793103448276, 0.76231527093596063, 0.60837438423645318, 0.61206896551724133, 0.58990147783251234, 0.54679802955665024, 0.62438423645320196, 0.6145320197044335], 5: [0.12315270935960591, 0.22783251231527094, 0.16995073891625614, 0.17118226600985223, 0.11576354679802955, 0.14901477832512317, 0.15640394088669951, 0.20566502463054187, 0.22536945812807882, 0.2019704433497537, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33990147783251229, 0.31527093596059114, 0.22660098522167488, 0.14408866995073891, 0.12561576354679804, 0.31650246305418717, 0.27586206896551724, 0.42241379310344829, 0.31280788177339902, 0.15270935960591134, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.40640394088669951, 0.59852216748768472, 0.6785714285714286, 0.70443349753694584, 0.45073891625615764, 0.46182266009852219, 0.4248768472906404, 0.43842364532019706, 0.57635467980295563, 0.60591133004926112], 5: [0.12315270935960591, 0.27832512315270935, 0.1748768472906404, 0.17733990147783252, 0.16995073891625614, 0.23275862068965517, 0.26231527093596058, 0.15270935960591134, 0.24876847290640394, 0.27093596059113301, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33990147783251229, 0.32266009852216748, 0.24630541871921183, 0.15147783251231528, 0.14039408866995073, 0.28694581280788178, 0.25985221674876846, 0.3854679802955665, 0.27709359605911332, 0.20073891625615764, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.43596059113300495, 0.58990147783251234, 0.67241379310344829, 0.70812807881773399, 0.51970443349753692, 0.48152709359605911, 0.42857142857142855, 0.49261083743842365, 0.57019704433497542, 0.56527093596059108], 5: [0.12315270935960591, 0.2413793103448276, 0.16379310344827586, 0.17610837438423646, 0.15147783251231528, 0.19334975369458129, 0.25862068965517243, 0.18596059113300492, 0.23029556650246305, 0.22906403940886699, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33990147783251229, 0.3608374384236453, 0.25492610837438423, 0.12315270935960591, 0.093596059113300489, 0.29187192118226601, 0.26724137931034481, 0.40517241379310343, 0.30911330049261082, 0.1539408866995074, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.37684729064039407, 0.56527093596059108, 0.68842364532019706, 0.75369458128078815, 0.47413793103448276, 0.42857142857142855, 0.4211822660098522, 0.40270935960591131, 0.55049261083743839, 0.58251231527093594], 5: [0.12315270935960591, 0.26231527093596058, 0.17980295566502463, 0.18842364532019704, 0.15270935960591134, 0.23399014778325122, 0.30418719211822659, 0.17364532019704434, 0.28817733990147781, 0.29556650246305421, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.107658 minutes
Weight histogram
[1274  953 4330 6735 9347 6569 2446 1797  913   61] [-0.00381266 -0.00302475 -0.00223684 -0.00144892 -0.00066101  0.0001269
  0.00091481  0.00170273  0.00249064  0.00327855  0.00406646]
[  135   148   228   309   464   747   752  1688  4036 25918] [-0.00381266 -0.00302475 -0.00223684 -0.00144892 -0.00066101  0.0001269
  0.00091481  0.00170273  0.00249064  0.00327855  0.00406646]
-2.16556
2.88085
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.72018
Epoch 1, cost is  3.67154
Epoch 2, cost is  3.63023
Epoch 3, cost is  3.61612
Epoch 4, cost is  3.5864
Training took 0.075364 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 7505  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2992 2550 2386 2667 2568 3027 3654 4758 4978 4845] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.8193
2.61544
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148127 minutes
Weight histogram
[   48   106   545  1430  1710  8375 15211  8612   410     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 22770   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.114744 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2404 3865  724] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.69088
Epoch 1, cost is  5.51214
Epoch 2, cost is  5.39597
Epoch 3, cost is  5.1157
Epoch 4, cost is  4.80998
Epoch 5, cost is  4.53584
Epoch 6, cost is  4.30901
Epoch 7, cost is  4.12381
Epoch 8, cost is  3.95462
Epoch 9, cost is  3.80332
Training took 0.245632 minutes
Weight histogram
[2184 5795 5400 1257  680  390  241  134   73   46] [-0.0187807  -0.01691636 -0.01505202 -0.01318768 -0.01132334 -0.009459
 -0.00759467 -0.00573033 -0.00386599 -0.00200165 -0.00013731]
[3967 1667 1081 1171 1189 1277 1380 1536 1614 1318] [-0.0187807  -0.01691636 -0.01505202 -0.01318768 -0.01132334 -0.009459
 -0.00759467 -0.00573033 -0.00386599 -0.00200165 -0.00013731]
-0.330819
0.274802
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048207 minutes
Epoch 0
Fine tuning took 0.045524 minutes
Epoch 0
Fine tuning took 0.047700 minutes
Epoch 0
Fine tuning took 0.046385 minutes
Epoch 0
Fine tuning took 0.046319 minutes
Epoch 0
Fine tuning took 0.045422 minutes
Epoch 0
Fine tuning took 0.046453 minutes
Epoch 0
Fine tuning took 0.045366 minutes
Epoch 0
Fine tuning took 0.046010 minutes
Epoch 0
Fine tuning took 0.047032 minutes
{'zero': {0: [0.22783251231527094, 0.14655172413793102, 0.25, 0.14901477832512317, 0.20320197044334976, 0.23522167487684728, 0.24014778325123154, 0.22167487684729065, 0.19950738916256158, 0.18719211822660098, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.63793103448275867, 0.61083743842364535, 0.63177339901477836, 0.62438423645320196, 0.51970443349753692, 0.55172413793103448, 0.49137931034482757, 0.55172413793103448, 0.56773399014778325, 0.52586206896551724], 5: [0.17364532019704434, 0.21551724137931033, 0.13916256157635468, 0.21921182266009853, 0.17241379310344829, 0.24507389162561577, 0.20812807881773399, 0.28694581280788178, 0.24876847290640394, 0.24507389162561577, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22783251231527094, 0.19827586206896552, 0.25738916256157635, 0.18965517241379309, 0.24507389162561577, 0.24876847290640394, 0.22783251231527094, 0.2413793103448276, 0.20812807881773399, 0.19211822660098521, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.65270935960591137, 0.60837438423645318, 0.62068965517241381, 0.5788177339901478, 0.56280788177339902, 0.55665024630541871, 0.54802955665024633, 0.52955665024630538, 0.55541871921182262, 0.5073891625615764], 5: [0.17364532019704434, 0.14901477832512317, 0.13423645320197045, 0.18965517241379309, 0.17610837438423646, 0.18842364532019704, 0.21551724137931033, 0.2105911330049261, 0.26231527093596058, 0.25246305418719212, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22783251231527094, 0.20073891625615764, 0.24753694581280788, 0.1625615763546798, 0.21305418719211822, 0.24261083743842365, 0.26600985221674878, 0.23891625615763548, 0.22167487684729065, 0.22536945812807882, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.62438423645320196, 0.61206896551724133, 0.61699507389162567, 0.59975369458128081, 0.54433497536945807, 0.52216748768472909, 0.52709359605911332, 0.55418719211822665, 0.52832512315270941, 0.50862068965517238], 5: [0.17364532019704434, 0.1748768472906404, 0.14039408866995073, 0.22044334975369459, 0.18719211822660098, 0.21305418719211822, 0.21182266009852216, 0.23399014778325122, 0.22413793103448276, 0.24630541871921183, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22783251231527094, 0.19704433497536947, 0.25615763546798032, 0.18596059113300492, 0.23645320197044334, 0.28448275862068967, 0.22906403940886699, 0.22413793103448276, 0.19704433497536947, 0.20073891625615764, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.62931034482758619, 0.61576354679802958, 0.58990147783251234, 0.58743842364532017, 0.51354679802955661, 0.56403940886699511, 0.53940886699507384, 0.58374384236453203, 0.53940886699507384, 0.51354679802955661], 5: [0.17364532019704434, 0.17364532019704434, 0.12807881773399016, 0.22413793103448276, 0.17610837438423646, 0.2019704433497537, 0.20689655172413793, 0.23645320197044334, 0.21921182266009853, 0.25985221674876846, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.105960 minutes
Weight histogram
[1274  953 4330 6735 9347 6569 2446 1797  913   61] [-0.00381266 -0.00302475 -0.00223684 -0.00144892 -0.00066101  0.0001269
  0.00091481  0.00170273  0.00249064  0.00327855  0.00406646]
[  135   148   228   309   464   747   752  1688  4036 25918] [-0.00381266 -0.00302475 -0.00223684 -0.00144892 -0.00066101  0.0001269
  0.00091481  0.00170273  0.00249064  0.00327855  0.00406646]
-2.16556
2.88085
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.72018
Epoch 1, cost is  3.67154
Epoch 2, cost is  3.63023
Epoch 3, cost is  3.61612
Epoch 4, cost is  3.5864
Training took 0.075005 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 7505  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2992 2550 2386 2667 2568 3027 3654 4758 4978 4845] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.8193
2.61544
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149934 minutes
Weight histogram
[   48   106   545  1430  1710  8375 15211  8612   410     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 22770   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.115260 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2404 3865  724] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.62612
Epoch 1, cost is  5.49885
Epoch 2, cost is  5.18466
Epoch 3, cost is  4.72417
Epoch 4, cost is  4.35732
Epoch 5, cost is  4.08288
Epoch 6, cost is  3.8432
Epoch 7, cost is  3.63257
Epoch 8, cost is  3.44802
Epoch 9, cost is  3.28784
Training took 0.337455 minutes
Weight histogram
[1667 3585 2990 2309 3444 1902  170   69   36   28] [-0.01086076 -0.00978537 -0.00870997 -0.00763458 -0.00655919 -0.0054838
 -0.0044084  -0.00333301 -0.00225762 -0.00118223 -0.00010683]
[4050 1036 1034 1100 1228 1425 1509 1597 1683 1538] [-0.01086076 -0.00978537 -0.00870997 -0.00763458 -0.00655919 -0.0054838
 -0.0044084  -0.00333301 -0.00225762 -0.00118223 -0.00010683]
-0.231564
0.227813
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049168 minutes
Epoch 0
Fine tuning took 0.049395 minutes
Epoch 0
Fine tuning took 0.050928 minutes
Epoch 0
Fine tuning took 0.051186 minutes
Epoch 0
Fine tuning took 0.049799 minutes
Epoch 0
Fine tuning took 0.051518 minutes
Epoch 0
Fine tuning took 0.049206 minutes
Epoch 0
Fine tuning took 0.051872 minutes
Epoch 0
Fine tuning took 0.052372 minutes
Epoch 0
Fine tuning took 0.050691 minutes
{'zero': {0: [0.2019704433497537, 0.22044334975369459, 0.20689655172413793, 0.20566502463054187, 0.19827586206896552, 0.25862068965517243, 0.21551724137931033, 0.2376847290640394, 0.23029556650246305, 0.25, 0.29433497536945813], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.52709359605911332, 0.56650246305418717, 0.52586206896551724, 0.53694581280788178, 0.45812807881773399, 0.47660098522167488, 0.48645320197044334, 0.47167487684729065, 0.4642857142857143, 0.44211822660098521], 5: [0.22413793103448276, 0.25246305418719212, 0.22660098522167488, 0.26847290640394089, 0.26477832512315269, 0.28325123152709358, 0.30788177339901479, 0.27586206896551724, 0.29802955665024633, 0.2857142857142857, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2019704433497537, 0.21798029556650247, 0.2376847290640394, 0.18472906403940886, 0.19581280788177341, 0.26600985221674878, 0.2376847290640394, 0.28817733990147781, 0.24261083743842365, 0.24753694581280788, 0.30541871921182268], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.55788177339901479, 0.52339901477832518, 0.52463054187192115, 0.5073891625615764, 0.42241379310344829, 0.47783251231527096, 0.43842364532019706, 0.44950738916256155, 0.43472906403940886, 0.41133004926108374], 5: [0.22413793103448276, 0.22413793103448276, 0.23891625615763548, 0.29064039408866993, 0.29679802955665024, 0.31157635467980294, 0.28448275862068967, 0.27339901477832512, 0.30788177339901479, 0.31773399014778325, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2019704433497537, 0.23029556650246305, 0.2376847290640394, 0.18842364532019704, 0.2019704433497537, 0.24507389162561577, 0.23522167487684728, 0.29187192118226601, 0.25, 0.24014778325123154, 0.27955665024630544], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.54679802955665024, 0.54679802955665024, 0.53201970443349755, 0.49753694581280788, 0.42980295566502463, 0.49137931034482757, 0.44827586206896552, 0.45443349753694579, 0.42610837438423643, 0.44088669950738918], 5: [0.22413793103448276, 0.2229064039408867, 0.21551724137931033, 0.27955665024630544, 0.30049261083743845, 0.3251231527093596, 0.27339901477832512, 0.25985221674876846, 0.29556650246305421, 0.33374384236453203, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2019704433497537, 0.21428571428571427, 0.24876847290640394, 0.19088669950738915, 0.20073891625615764, 0.25862068965517243, 0.23275862068965517, 0.25985221674876846, 0.21921182266009853, 0.24630541871921183, 0.30911330049261082], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.5357142857142857, 0.51970443349753692, 0.52709359605911332, 0.4963054187192118, 0.42241379310344829, 0.47290640394088668, 0.45566502463054187, 0.47290640394088668, 0.43719211822660098, 0.42980295566502463], 5: [0.22413793103448276, 0.25, 0.23152709359605911, 0.28201970443349755, 0.30295566502463056, 0.31896551724137934, 0.29433497536945813, 0.28448275862068967, 0.30788177339901479, 0.31650246305418717, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.108029 minutes
Weight histogram
[1274  953 4330 6735 9347 6569 2446 1797  913   61] [-0.00381266 -0.00302475 -0.00223684 -0.00144892 -0.00066101  0.0001269
  0.00091481  0.00170273  0.00249064  0.00327855  0.00406646]
[  135   148   228   309   464   747   752  1688  4036 25918] [-0.00381266 -0.00302475 -0.00223684 -0.00144892 -0.00066101  0.0001269
  0.00091481  0.00170273  0.00249064  0.00327855  0.00406646]
-2.16556
2.88085
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.72018
Epoch 1, cost is  3.67154
Epoch 2, cost is  3.63023
Epoch 3, cost is  3.61612
Epoch 4, cost is  3.5864
Training took 0.072851 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 7505  413  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[2992 2550 2386 2667 2568 3027 3654 4758 4978 4845] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.8193
2.61544
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148165 minutes
Weight histogram
[   48   106   545  1430  1710  8375 15211  8612   410     3] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
[  235   285   368   548   914  1228  2430  7189 22770   483] [ -7.42217875e-04  -4.05574136e-04  -6.89303968e-05   2.67713342e-04
   6.04357081e-04   9.41000821e-04   1.27764456e-03   1.61428830e-03
   1.95093204e-03   2.28757578e-03   2.62421952e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.114982 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2404 3865  724] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_350-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.61536
Epoch 1, cost is  5.41712
Epoch 2, cost is  4.90669
Epoch 3, cost is  4.39251
Epoch 4, cost is  4.00725
Epoch 5, cost is  3.70236
Epoch 6, cost is  3.44864
Epoch 7, cost is  3.2358
Epoch 8, cost is  3.05612
Epoch 9, cost is  2.90398
Training took 0.537925 minutes
Weight histogram
[2006 3244 2795 2066 1660 3303 1055   37   18   16] [ -5.72379166e-03  -5.16108933e-03  -4.59838700e-03  -4.03568466e-03
  -3.47298233e-03  -2.91028000e-03  -2.34757766e-03  -1.78487533e-03
  -1.22217300e-03  -6.59470663e-04  -9.67683300e-05]
[3643 1047 1060 1160 1289 1400 1506 1627 1768 1700] [ -5.72379166e-03  -5.16108933e-03  -4.59838700e-03  -4.03568466e-03
  -3.47298233e-03  -2.91028000e-03  -2.34757766e-03  -1.78487533e-03
  -1.22217300e-03  -6.59470663e-04  -9.67683300e-05]
-0.185404
0.210455
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.060923 minutes
Epoch 0
Fine tuning took 0.061621 minutes
Epoch 0
Fine tuning took 0.060743 minutes
Epoch 0
Fine tuning took 0.061319 minutes
Epoch 0
Fine tuning took 0.060702 minutes
Epoch 0
Fine tuning took 0.061891 minutes
Epoch 0
Fine tuning took 0.061957 minutes
Epoch 0
Fine tuning took 0.060683 minutes
Epoch 0
Fine tuning took 0.061958 minutes
Epoch 0
Fine tuning took 0.062503 minutes
{'zero': {0: [0.20935960591133004, 0.22660098522167488, 0.2105911330049261, 0.22660098522167488, 0.22536945812807882, 0.28325123152709358, 0.26108374384236455, 0.28817733990147781, 0.29064039408866993, 0.23029556650246305, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.5, 0.5073891625615764, 0.47413793103448276, 0.49137931034482757, 0.41133004926108374, 0.41256157635467983, 0.3817733990147783, 0.43719211822660098, 0.43226600985221675, 0.41625615763546797], 5: [0.25123152709359609, 0.27339901477832512, 0.28201970443349755, 0.29926108374384236, 0.28325123152709358, 0.30541871921182268, 0.32635467980295568, 0.33004926108374383, 0.27216748768472904, 0.33743842364532017, 0.3608374384236453], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20935960591133004, 0.21551724137931033, 0.25, 0.23029556650246305, 0.24507389162561577, 0.26600985221674878, 0.21674876847290642, 0.30295566502463056, 0.23029556650246305, 0.22536945812807882, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.45689655172413796, 0.45935960591133007, 0.48891625615763545, 0.46551724137931033, 0.39408866995073893, 0.45689655172413796, 0.4039408866995074, 0.44950738916256155, 0.42610837438423643, 0.40147783251231528], 5: [0.25123152709359609, 0.32758620689655171, 0.29064039408866993, 0.28078817733990147, 0.2894088669950739, 0.33990147783251229, 0.32635467980295568, 0.29310344827586204, 0.32019704433497537, 0.34852216748768472, 0.35837438423645318], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20935960591133004, 0.22783251231527094, 0.20812807881773399, 0.23275862068965517, 0.20935960591133004, 0.29187192118226601, 0.21428571428571427, 0.24753694581280788, 0.26970443349753692, 0.18472906403940886, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.49384236453201968, 0.54187192118226601, 0.44334975369458129, 0.48891625615763545, 0.41379310344827586, 0.47906403940886699, 0.41625615763546797, 0.41256157635467983, 0.46551724137931033, 0.40270935960591131], 5: [0.25123152709359609, 0.27832512315270935, 0.25, 0.32389162561576357, 0.30172413793103448, 0.29433497536945813, 0.30665024630541871, 0.33620689655172414, 0.31773399014778325, 0.34975369458128081, 0.35837438423645318], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20935960591133004, 0.22906403940886699, 0.22044334975369459, 0.20566502463054187, 0.18965517241379309, 0.27339901477832512, 0.2229064039408867, 0.25492610837438423, 0.24753694581280788, 0.22167487684729065, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.46798029556650245, 0.47783251231527096, 0.48275862068965519, 0.51847290640394084, 0.42980295566502463, 0.47783251231527096, 0.43965517241379309, 0.4605911330049261, 0.43472906403940886, 0.42364532019704432], 5: [0.25123152709359609, 0.30295566502463056, 0.30172413793103448, 0.31157635467980294, 0.29187192118226601, 0.29679802955665024, 0.29926108374384236, 0.30541871921182268, 0.29187192118226601, 0.34359605911330049, 0.34975369458128081], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149319 minutes
Weight histogram
[  232   569   912  1127  5158  6891 11709  4947  2234   646] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
[  138   164   248   352   577   885  1238  2884 10531 17408] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
-1.31697
1.12623
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.23423
Epoch 1, cost is  2.20493
Epoch 2, cost is  2.18392
Epoch 3, cost is  2.16928
Epoch 4, cost is  2.15183
Training took 0.115001 minutes
Weight histogram
[7874 4930 5697 3754 3327 2477 2139 2134 1675  418] [ -6.64254948e-02  -5.97799183e-02  -5.31343418e-02  -4.64887653e-02
  -3.98431888e-02  -3.31976123e-02  -2.65520358e-02  -1.99064593e-02
  -1.32608828e-02  -6.61530633e-03   3.02701774e-05]
[2991 1680 1910 2443 3011 3535 4250 4263 5393 4949] [ -6.64254948e-02  -5.97799183e-02  -5.31343418e-02  -4.64887653e-02
  -3.98431888e-02  -3.31976123e-02  -2.65520358e-02  -1.99064593e-02
  -1.32608828e-02  -6.61530633e-03   3.02701774e-05]
-1.33932
1.79814
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148231 minutes
Weight histogram
[  223   554  1069  1221  2639  7235 10833  9589  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  283   333   491   708  1062  1359  1094  2259  7348 21513] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.114298 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2444 3795  754] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.27109
Epoch 1, cost is  6.04408
Epoch 2, cost is  5.89758
Epoch 3, cost is  5.67381
Epoch 4, cost is  5.32054
Epoch 5, cost is  4.99113
Epoch 6, cost is  4.72504
Epoch 7, cost is  4.50978
Epoch 8, cost is  4.333
Epoch 9, cost is  4.17871
Training took 0.206737 minutes
Weight histogram
[1630 1861 1783 1668 2933 3710 1780  649  128   58] [-0.02656188 -0.02392086 -0.02127983 -0.01863881 -0.01599779 -0.01335676
 -0.01071574 -0.00807472 -0.00543369 -0.00279267 -0.00015165]
[3645 2390 1124  999 1087 1201 1306 1421 1555 1472] [-0.02656188 -0.02392086 -0.02127983 -0.01863881 -0.01599779 -0.01335676
 -0.01071574 -0.00807472 -0.00543369 -0.00279267 -0.00015165]
-0.348383
0.440929
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052112 minutes
Epoch 0
Fine tuning took 0.055196 minutes
Epoch 0
Fine tuning took 0.054995 minutes
Epoch 0
Fine tuning took 0.052457 minutes
Epoch 0
Fine tuning took 0.055419 minutes
Epoch 0
Fine tuning took 0.053794 minutes
Epoch 0
Fine tuning took 0.053837 minutes
Epoch 0
Fine tuning took 0.054541 minutes
Epoch 0
Fine tuning took 0.054668 minutes
Epoch 0
Fine tuning took 0.052436 minutes
{'zero': {0: [0.3682266009852217, 0.28078817733990147, 0.43349753694581283, 0.26970443349753692, 0.31527093596059114, 0.30788177339901479, 0.26600985221674878, 0.18349753694581281, 0.25, 0.21428571428571427, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.43103448275862066, 0.37931034482758619, 0.5788177339901478, 0.55049261083743839, 0.58743842364532017, 0.55788177339901479, 0.59482758620689657, 0.57019704433497542, 0.6576354679802956, 0.5714285714285714], 5: [0.11822660098522167, 0.28817733990147781, 0.18719211822660098, 0.15147783251231528, 0.13423645320197045, 0.10467980295566502, 0.17610837438423646, 0.22167487684729065, 0.17980295566502463, 0.12807881773399016, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3682266009852217, 0.30049261083743845, 0.52463054187192115, 0.29433497536945813, 0.35960591133004927, 0.52093596059113301, 0.42241379310344829, 0.25246305418719212, 0.28078817733990147, 0.33866995073891626, 0.28694581280788178], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.47044334975369456, 0.31896551724137934, 0.58866995073891626, 0.48399014778325122, 0.3891625615763547, 0.46921182266009853, 0.50369458128078815, 0.49507389162561577, 0.52955665024630538, 0.53817733990147787], 5: [0.11822660098522167, 0.22906403940886699, 0.15640394088669951, 0.11699507389162561, 0.15640394088669951, 0.089901477832512317, 0.10837438423645321, 0.24384236453201971, 0.22413793103448276, 0.13177339901477833, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3682266009852217, 0.29802955665024633, 0.52463054187192115, 0.29679802955665024, 0.38054187192118227, 0.50615763546798032, 0.36576354679802958, 0.20320197044334976, 0.25738916256157635, 0.32389162561576357, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.46305418719211822, 0.33866995073891626, 0.59359605911330049, 0.49137931034482757, 0.40270935960591131, 0.48152709359605911, 0.56527093596059108, 0.46305418719211822, 0.52955665024630538, 0.51970443349753692], 5: [0.11822660098522167, 0.23891625615763548, 0.13669950738916256, 0.10960591133004927, 0.12807881773399016, 0.091133004926108374, 0.15270935960591134, 0.23152709359605911, 0.27955665024630544, 0.14655172413793102, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3682266009852217, 0.29802955665024633, 0.55418719211822665, 0.29926108374384236, 0.41748768472906406, 0.53325123152709364, 0.37315270935960593, 0.21428571428571427, 0.29433497536945813, 0.35221674876847292, 0.30788177339901479], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.45935960591133007, 0.31403940886699505, 0.59605911330049266, 0.46182266009852219, 0.36576354679802958, 0.46921182266009853, 0.52955665024630538, 0.45935960591133007, 0.51354679802955661, 0.51231527093596063], 5: [0.11822660098522167, 0.24261083743842365, 0.13177339901477833, 0.10467980295566502, 0.1206896551724138, 0.10098522167487685, 0.15763546798029557, 0.25615763546798032, 0.24630541871921183, 0.13423645320197045, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148909 minutes
Weight histogram
[  232   569   912  1127  5158  6891 11709  4947  2234   646] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
[  138   164   248   352   577   885  1238  2884 10531 17408] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
-1.31697
1.12623
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.23423
Epoch 1, cost is  2.20493
Epoch 2, cost is  2.18392
Epoch 3, cost is  2.16928
Epoch 4, cost is  2.15183
Training took 0.116591 minutes
Weight histogram
[7874 4930 5697 3754 3327 2477 2139 2134 1675  418] [ -6.64254948e-02  -5.97799183e-02  -5.31343418e-02  -4.64887653e-02
  -3.98431888e-02  -3.31976123e-02  -2.65520358e-02  -1.99064593e-02
  -1.32608828e-02  -6.61530633e-03   3.02701774e-05]
[2991 1680 1910 2443 3011 3535 4250 4263 5393 4949] [ -6.64254948e-02  -5.97799183e-02  -5.31343418e-02  -4.64887653e-02
  -3.98431888e-02  -3.31976123e-02  -2.65520358e-02  -1.99064593e-02
  -1.32608828e-02  -6.61530633e-03   3.02701774e-05]
-1.33932
1.79814
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148693 minutes
Weight histogram
[  223   554  1069  1221  2639  7235 10833  9589  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  283   333   491   708  1062  1359  1094  2259  7348 21513] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.116527 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2444 3795  754] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.70528
Epoch 1, cost is  5.49726
Epoch 2, cost is  5.35905
Epoch 3, cost is  5.05078
Epoch 4, cost is  4.64138
Epoch 5, cost is  4.32289
Epoch 6, cost is  4.05818
Epoch 7, cost is  3.83111
Epoch 8, cost is  3.63365
Epoch 9, cost is  3.47116
Training took 0.298184 minutes
Weight histogram
[2701 2676 2256 5998 1580  532  243  116   60   38] [-0.01784189 -0.0160718  -0.0143017  -0.01253161 -0.01076152 -0.00899143
 -0.00722133 -0.00545124 -0.00368115 -0.00191106 -0.00014097]
[4795 1221  924 1044 1161 1258 1356 1453 1541 1447] [-0.01784189 -0.0160718  -0.0143017  -0.01253161 -0.01076152 -0.00899143
 -0.00722133 -0.00545124 -0.00368115 -0.00191106 -0.00014097]
-0.283091
0.336729
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.058328 minutes
Epoch 0
Fine tuning took 0.058997 minutes
Epoch 0
Fine tuning took 0.060132 minutes
Epoch 0
Fine tuning took 0.058269 minutes
Epoch 0
Fine tuning took 0.059197 minutes
Epoch 0
Fine tuning took 0.059172 minutes
Epoch 0
Fine tuning took 0.058818 minutes
Epoch 0
Fine tuning took 0.058553 minutes
Epoch 0
Fine tuning took 0.059922 minutes
Epoch 0
Fine tuning took 0.060168 minutes
{'zero': {0: [0.2894088669950739, 0.19211822660098521, 0.25985221674876846, 0.13300492610837439, 0.20566502463054187, 0.22660098522167488, 0.23891625615763548, 0.20566502463054187, 0.19334975369458129, 0.19088669950738915, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.59605911330049266, 0.59359605911330049, 0.68349753694581283, 0.61822660098522164, 0.60221674876847286, 0.57758620689655171, 0.63300492610837433, 0.59482758620689657, 0.60960591133004927, 0.59113300492610843], 5: [0.13669950738916256, 0.21182266009852216, 0.14655172413793102, 0.18349753694581281, 0.17610837438423646, 0.17118226600985223, 0.18349753694581281, 0.16133004926108374, 0.21182266009852216, 0.19950738916256158, 0.20320197044334976], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2894088669950739, 0.25, 0.30911330049261082, 0.21428571428571427, 0.30665024630541871, 0.26847290640394089, 0.28448275862068967, 0.23152709359605911, 0.22536945812807882, 0.18103448275862069, 0.22783251231527094], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.52339901477832518, 0.50862068965517238, 0.57389162561576357, 0.47783251231527096, 0.54679802955665024, 0.53078817733990147, 0.60344827586206895, 0.62438423645320196, 0.61576354679802958, 0.5357142857142857], 5: [0.13669950738916256, 0.22660098522167488, 0.18226600985221675, 0.21182266009852216, 0.21551724137931033, 0.18472906403940886, 0.18472906403940886, 0.16502463054187191, 0.15024630541871922, 0.20320197044334976, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2894088669950739, 0.25738916256157635, 0.28078817733990147, 0.23399014778325122, 0.27093596059113301, 0.26847290640394089, 0.28325123152709358, 0.24384236453201971, 0.23399014778325122, 0.17857142857142858, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.52216748768472909, 0.53694581280788178, 0.55665024630541871, 0.53817733990147787, 0.52955665024630538, 0.51600985221674878, 0.57758620689655171, 0.59852216748768472, 0.61330049261083741, 0.54679802955665024], 5: [0.13669950738916256, 0.22044334975369459, 0.18226600985221675, 0.20935960591133004, 0.19088669950738915, 0.2019704433497537, 0.20073891625615764, 0.17857142857142858, 0.16748768472906403, 0.20812807881773399, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2894088669950739, 0.26108374384236455, 0.28694581280788178, 0.20320197044334976, 0.30418719211822659, 0.29926108374384236, 0.27709359605911332, 0.23152709359605911, 0.2229064039408867, 0.20320197044334976, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.47783251231527096, 0.54433497536945807, 0.58251231527093594, 0.49507389162561577, 0.51847290640394084, 0.54064039408866993, 0.60344827586206895, 0.59852216748768472, 0.58620689655172409, 0.59605911330049266], 5: [0.13669950738916256, 0.26108374384236455, 0.16871921182266009, 0.21428571428571427, 0.20073891625615764, 0.18226600985221675, 0.18226600985221675, 0.16502463054187191, 0.17857142857142858, 0.2105911330049261, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150070 minutes
Weight histogram
[  232   569   912  1127  5158  6891 11709  4947  2234   646] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
[  138   164   248   352   577   885  1238  2884 10531 17408] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
-1.31697
1.12623
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.23423
Epoch 1, cost is  2.20493
Epoch 2, cost is  2.18392
Epoch 3, cost is  2.16928
Epoch 4, cost is  2.15183
Training took 0.118593 minutes
Weight histogram
[7874 4930 5697 3754 3327 2477 2139 2134 1675  418] [ -6.64254948e-02  -5.97799183e-02  -5.31343418e-02  -4.64887653e-02
  -3.98431888e-02  -3.31976123e-02  -2.65520358e-02  -1.99064593e-02
  -1.32608828e-02  -6.61530633e-03   3.02701774e-05]
[2991 1680 1910 2443 3011 3535 4250 4263 5393 4949] [ -6.64254948e-02  -5.97799183e-02  -5.31343418e-02  -4.64887653e-02
  -3.98431888e-02  -3.31976123e-02  -2.65520358e-02  -1.99064593e-02
  -1.32608828e-02  -6.61530633e-03   3.02701774e-05]
-1.33932
1.79814
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148869 minutes
Weight histogram
[  223   554  1069  1221  2639  7235 10833  9589  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  283   333   491   708  1062  1359  1094  2259  7348 21513] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.116048 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2444 3795  754] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.3731
Epoch 1, cost is  5.22959
Epoch 2, cost is  5.02184
Epoch 3, cost is  4.57787
Epoch 4, cost is  4.19921
Epoch 5, cost is  3.8918
Epoch 6, cost is  3.62658
Epoch 7, cost is  3.38943
Epoch 8, cost is  3.1919
Epoch 9, cost is  3.02266
Training took 0.413250 minutes
Weight histogram
[3309 3832 6830 1308  440  230  122   67   37   25] [-0.01199183 -0.01080513 -0.00961844 -0.00843174 -0.00724504 -0.00605834
 -0.00487164 -0.00368495 -0.00249825 -0.00131155 -0.00012485]
[4456 1030  968 1136 1240 1361 1414 1489 1629 1477] [-0.01199183 -0.01080513 -0.00961844 -0.00843174 -0.00724504 -0.00605834
 -0.00487164 -0.00368495 -0.00249825 -0.00131155 -0.00012485]
-0.216932
0.260704
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.064134 minutes
Epoch 0
Fine tuning took 0.063357 minutes
Epoch 0
Fine tuning took 0.062738 minutes
Epoch 0
Fine tuning took 0.064348 minutes
Epoch 0
Fine tuning took 0.064781 minutes
Epoch 0
Fine tuning took 0.064737 minutes
Epoch 0
Fine tuning took 0.063298 minutes
Epoch 0
Fine tuning took 0.063884 minutes
Epoch 0
Fine tuning took 0.064838 minutes
Epoch 0
Fine tuning took 0.063553 minutes
{'zero': {0: [0.2105911330049261, 0.16133004926108374, 0.22044334975369459, 0.20812807881773399, 0.16625615763546797, 0.20812807881773399, 0.24507389162561577, 0.22536945812807882, 0.19458128078817735, 0.16625615763546797, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60221674876847286, 0.6280788177339901, 0.59729064039408863, 0.55172413793103448, 0.63423645320197042, 0.55172413793103448, 0.50246305418719217, 0.52709359605911332, 0.55911330049261088, 0.56157635467980294, 0.51970443349753692], 5: [0.18719211822660098, 0.2105911330049261, 0.18226600985221675, 0.24014778325123154, 0.19950738916256158, 0.24014778325123154, 0.25246305418719212, 0.24753694581280788, 0.24630541871921183, 0.27216748768472904, 0.25], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2105911330049261, 0.16133004926108374, 0.25985221674876846, 0.22167487684729065, 0.19581280788177341, 0.27093596059113301, 0.23522167487684728, 0.22044334975369459, 0.2229064039408867, 0.18103448275862069, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60221674876847286, 0.65024630541871919, 0.56527093596059108, 0.55788177339901479, 0.58743842364532017, 0.51108374384236455, 0.50369458128078815, 0.52463054187192115, 0.53325123152709364, 0.56280788177339902, 0.46305418719211822], 5: [0.18719211822660098, 0.18842364532019704, 0.1748768472906404, 0.22044334975369459, 0.21674876847290642, 0.21798029556650247, 0.26108374384236455, 0.25492610837438423, 0.24384236453201971, 0.25615763546798032, 0.2857142857142857], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2105911330049261, 0.18472906403940886, 0.25, 0.19458128078817735, 0.20320197044334976, 0.24753694581280788, 0.24014778325123154, 0.24014778325123154, 0.21551724137931033, 0.17857142857142858, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60221674876847286, 0.60960591133004927, 0.57635467980295563, 0.57512315270935965, 0.57635467980295563, 0.5357142857142857, 0.49137931034482757, 0.49876847290640391, 0.55172413793103448, 0.56280788177339902, 0.47660098522167488], 5: [0.18719211822660098, 0.20566502463054187, 0.17364532019704434, 0.23029556650246305, 0.22044334975369459, 0.21674876847290642, 0.26847290640394089, 0.26108374384236455, 0.23275862068965517, 0.25862068965517243, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2105911330049261, 0.14901477832512317, 0.23152709359605911, 0.18226600985221675, 0.19581280788177341, 0.24630541871921183, 0.21921182266009853, 0.21305418719211822, 0.20443349753694581, 0.17857142857142858, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60221674876847286, 0.65517241379310343, 0.58128078817733986, 0.55541871921182262, 0.59113300492610843, 0.51354679802955661, 0.48891625615763545, 0.54679802955665024, 0.55049261083743839, 0.56527093596059108, 0.5073891625615764], 5: [0.18719211822660098, 0.19581280788177341, 0.18719211822660098, 0.26231527093596058, 0.21305418719211822, 0.24014778325123154, 0.29187192118226601, 0.24014778325123154, 0.24507389162561577, 0.25615763546798032, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149130 minutes
Weight histogram
[  232   569   912  1127  5158  6891 11709  4947  2234   646] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
[  138   164   248   352   577   885  1238  2884 10531 17408] [-0.00013604  0.00023728  0.00061059  0.0009839   0.00135721  0.00173053
  0.00210384  0.00247715  0.00285046  0.00322377  0.00359709]
-1.31697
1.12623
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.23423
Epoch 1, cost is  2.20493
Epoch 2, cost is  2.18392
Epoch 3, cost is  2.16928
Epoch 4, cost is  2.15183
Training took 0.116561 minutes
Weight histogram
[7874 4930 5697 3754 3327 2477 2139 2134 1675  418] [ -6.64254948e-02  -5.97799183e-02  -5.31343418e-02  -4.64887653e-02
  -3.98431888e-02  -3.31976123e-02  -2.65520358e-02  -1.99064593e-02
  -1.32608828e-02  -6.61530633e-03   3.02701774e-05]
[2991 1680 1910 2443 3011 3535 4250 4263 5393 4949] [ -6.64254948e-02  -5.97799183e-02  -5.31343418e-02  -4.64887653e-02
  -3.98431888e-02  -3.31976123e-02  -2.65520358e-02  -1.99064593e-02
  -1.32608828e-02  -6.61530633e-03   3.02701774e-05]
-1.33932
1.79814
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149090 minutes
Weight histogram
[  223   554  1069  1221  2639  7235 10833  9589  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  283   333   491   708  1062  1359  1094  2259  7348 21513] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.116248 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2444 3795  754] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_500-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.33782
Epoch 1, cost is  5.1221
Epoch 2, cost is  4.5739
Epoch 3, cost is  4.04157
Epoch 4, cost is  3.61666
Epoch 5, cost is  3.27811
Epoch 6, cost is  3.01591
Epoch 7, cost is  2.80368
Epoch 8, cost is  2.63226
Epoch 9, cost is  2.49027
Training took 0.685990 minutes
Weight histogram
[2713 3170 2571 2031 1774 3782   91   34   18   16] [-0.0065721  -0.00592648 -0.00528087 -0.00463525 -0.00398963 -0.00334401
 -0.0026984  -0.00205278 -0.00140716 -0.00076155 -0.00011593]
[3619 1027 1071 1158 1268 1339 1481 1667 1875 1695] [-0.0065721  -0.00592648 -0.00528087 -0.00463525 -0.00398963 -0.00334401
 -0.0026984  -0.00205278 -0.00140716 -0.00076155 -0.00011593]
-0.181892
0.194772
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.077772 minutes
Epoch 0
Fine tuning took 0.078268 minutes
Epoch 0
Fine tuning took 0.078219 minutes
Epoch 0
Fine tuning took 0.078328 minutes
Epoch 0
Fine tuning took 0.077469 minutes
Epoch 0
Fine tuning took 0.077482 minutes
Epoch 0
Fine tuning took 0.076926 minutes
Epoch 0
Fine tuning took 0.076656 minutes
Epoch 0
Fine tuning took 0.077621 minutes
Epoch 0
Fine tuning took 0.078313 minutes
{'zero': {0: [0.18103448275862069, 0.23522167487684728, 0.22536945812807882, 0.17857142857142858, 0.22536945812807882, 0.25985221674876846, 0.23029556650246305, 0.2857142857142857, 0.25492610837438423, 0.20935960591133004, 0.30049261083743845], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60098522167487689, 0.51354679802955661, 0.54187192118226601, 0.54064039408866993, 0.4963054187192118, 0.44704433497536944, 0.45689655172413796, 0.43719211822660098, 0.47536945812807879, 0.47167487684729065, 0.40024630541871919], 5: [0.21798029556650247, 0.25123152709359609, 0.23275862068965517, 0.28078817733990147, 0.27832512315270935, 0.29310344827586204, 0.31280788177339902, 0.27709359605911332, 0.26970443349753692, 0.31896551724137934, 0.29926108374384236], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18103448275862069, 0.23029556650246305, 0.22413793103448276, 0.18472906403940886, 0.24014778325123154, 0.27709359605911332, 0.22536945812807882, 0.30172413793103448, 0.27339901477832512, 0.20073891625615764, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60098522167487689, 0.54064039408866993, 0.53078817733990147, 0.52832512315270941, 0.47783251231527096, 0.41748768472906406, 0.48029556650246308, 0.40640394088669951, 0.44704433497536944, 0.4963054187192118, 0.41256157635467983], 5: [0.21798029556650247, 0.22906403940886699, 0.24507389162561577, 0.28694581280788178, 0.28201970443349755, 0.30541871921182268, 0.29433497536945813, 0.29187192118226601, 0.27955665024630544, 0.30295566502463056, 0.32266009852216748], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18103448275862069, 0.23152709359605911, 0.21551724137931033, 0.20073891625615764, 0.23645320197044334, 0.26724137931034481, 0.20443349753694581, 0.30172413793103448, 0.2376847290640394, 0.21798029556650247, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60098522167487689, 0.53201970443349755, 0.55295566502463056, 0.4963054187192118, 0.5, 0.43596059113300495, 0.49876847290640391, 0.41995073891625617, 0.44704433497536944, 0.47536945812807879, 0.3891625615763547], 5: [0.21798029556650247, 0.23645320197044334, 0.23152709359605911, 0.30295566502463056, 0.26354679802955666, 0.29679802955665024, 0.29679802955665024, 0.27832512315270935, 0.31527093596059114, 0.30665024630541871, 0.33620689655172414], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18103448275862069, 0.22660098522167488, 0.21798029556650247, 0.23029556650246305, 0.24507389162561577, 0.27955665024630544, 0.19950738916256158, 0.29187192118226601, 0.21921182266009853, 0.22044334975369459, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60098522167487689, 0.51354679802955661, 0.51600985221674878, 0.49384236453201968, 0.50369458128078815, 0.41256157635467983, 0.50492610837438423, 0.45689655172413796, 0.46921182266009853, 0.45443349753694579, 0.41871921182266009], 5: [0.21798029556650247, 0.25985221674876846, 0.26600985221674878, 0.27586206896551724, 0.25123152709359609, 0.30788177339901479, 0.29556650246305421, 0.25123152709359609, 0.31157635467980294, 0.3251231527093596, 0.31896551724137934], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236176 minutes
Weight histogram
[  184   588   776   472  1138  2279  6609 15877  5899   603] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
[  172   297   479   788  1003  2052  2813  6659 10922  9240] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
-1.3595
1.46953
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.32659
Epoch 1, cost is  1.30467
Epoch 2, cost is  1.28782
Epoch 3, cost is  1.2739
Epoch 4, cost is  1.26296
Training took 0.222743 minutes
Weight histogram
[7643 6276 4651 4041 3126 2367 2019 1580 1495 1227] [ -6.63788766e-02  -5.97342254e-02  -5.30895742e-02  -4.64449230e-02
  -3.98002718e-02  -3.31556206e-02  -2.65109694e-02  -1.98663182e-02
  -1.32216670e-02  -6.57701585e-03   6.76353375e-05]
[2520 1667 2055 2478 2853 3517 3866 4843 5142 5484] [ -6.63788766e-02  -5.97342254e-02  -5.30895742e-02  -4.64449230e-02
  -3.98002718e-02  -3.31556206e-02  -2.65109694e-02  -1.98663182e-02
  -1.32216670e-02  -6.57701585e-03   6.76353375e-05]
-1.05917
1.82574
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148301 minutes
Weight histogram
[  356  1117  1465   816  2164  7079 10786  9580  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  500   995  1075   333   499   834  1094  2259  7345 21516] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.113867 minutes
Weight histogram
[7046 5160 5173 3689 3241 2907 2248 2406 3065 1515] [ -6.33401051e-02  -5.69993311e-02  -5.06585570e-02  -4.43177830e-02
  -3.79770089e-02  -3.16362349e-02  -2.52954608e-02  -1.89546868e-02
  -1.26139128e-02  -6.27313871e-03   6.76353375e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.69993311e-02  -5.06585570e-02  -4.43177830e-02
  -3.79770089e-02  -3.16362349e-02  -2.52954608e-02  -1.89546868e-02
  -1.26139128e-02  -6.27313871e-03   6.76353375e-05]
-1.76727
1.9195
... retrieved True_rbm_750-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.33616
Epoch 1, cost is  6.06592
Epoch 2, cost is  5.80331
Epoch 3, cost is  5.43481
Epoch 4, cost is  5.08697
Epoch 5, cost is  4.80026
Epoch 6, cost is  4.56937
Epoch 7, cost is  4.37806
Epoch 8, cost is  4.20936
Epoch 9, cost is  4.06006
Training took 0.248351 minutes
Weight histogram
[1705 1868 1738 1613 1640 1984 2917 1913  751   71] [-0.02840349 -0.02557967 -0.02275585 -0.01993204 -0.01710822 -0.0142844
 -0.01146058 -0.00863677 -0.00581295 -0.00298913 -0.00016531]
[3180 1769 1161 1173 1215 1340 1465 1585 1686 1626] [-0.02840349 -0.02557967 -0.02275585 -0.01993204 -0.01710822 -0.0142844
 -0.01146058 -0.00863677 -0.00581295 -0.00298913 -0.00016531]
-0.367641
0.599255
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.076652 minutes
Epoch 0
Fine tuning took 0.075737 minutes
Epoch 0
Fine tuning took 0.075985 minutes
Epoch 0
Fine tuning took 0.077826 minutes
Epoch 0
Fine tuning took 0.075687 minutes
Epoch 0
Fine tuning took 0.075527 minutes
Epoch 0
Fine tuning took 0.077239 minutes
Epoch 0
Fine tuning took 0.076929 minutes
Epoch 0
Fine tuning took 0.076673 minutes
Epoch 0
Fine tuning took 0.076952 minutes
{'zero': {0: [0.43842364532019706, 0.31896551724137934, 0.36945812807881773, 0.30665024630541871, 0.34852216748768472, 0.33251231527093594, 0.38423645320197042, 0.26970443349753692, 0.14162561576354679, 0.21305418719211822, 0.36945812807881773], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46182266009852219, 0.39778325123152708, 0.39901477832512317, 0.56527093596059108, 0.43965517241379309, 0.53325123152709364, 0.39162561576354682, 0.39901477832512317, 0.69458128078817738, 0.53448275862068961, 0.43965517241379309], 5: [0.099753694581280791, 0.28325123152709358, 0.23152709359605911, 0.12807881773399016, 0.21182266009852216, 0.13423645320197045, 0.22413793103448276, 0.33128078817733991, 0.16379310344827586, 0.25246305418719212, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.43842364532019706, 0.30541871921182268, 0.29926108374384236, 0.34729064039408869, 0.38423645320197042, 0.40640394088669951, 0.38423645320197042, 0.29433497536945813, 0.15640394088669951, 0.25492610837438423, 0.42610837438423643], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46182266009852219, 0.44211822660098521, 0.44334975369458129, 0.53201970443349755, 0.41995073891625617, 0.46798029556650245, 0.39778325123152708, 0.42241379310344829, 0.68965517241379315, 0.51477832512315269, 0.4248768472906404], 5: [0.099753694581280791, 0.25246305418719212, 0.25738916256157635, 0.1206896551724138, 0.19581280788177341, 0.12561576354679804, 0.21798029556650247, 0.28325123152709358, 0.1539408866995074, 0.23029556650246305, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.43842364532019706, 0.3460591133004926, 0.30788177339901479, 0.36699507389162561, 0.3891625615763547, 0.39901477832512317, 0.39162561576354682, 0.31157635467980294, 0.16133004926108374, 0.24753694581280788, 0.40517241379310343], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46182266009852219, 0.40024630541871919, 0.42980295566502463, 0.50123152709359609, 0.44088669950738918, 0.47536945812807879, 0.3854679802955665, 0.37068965517241381, 0.67487684729064035, 0.54433497536945807, 0.44088669950738918], 5: [0.099753694581280791, 0.2536945812807882, 0.26231527093596058, 0.13177339901477833, 0.16995073891625614, 0.12561576354679804, 0.2229064039408867, 0.31773399014778325, 0.16379310344827586, 0.20812807881773399, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.43842364532019706, 0.30172413793103448, 0.29926108374384236, 0.3460591133004926, 0.40640394088669951, 0.43472906403940886, 0.32758620689655171, 0.27955665024630544, 0.1268472906403941, 0.27339901477832512, 0.4248768472906404], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46182266009852219, 0.43349753694581283, 0.44581280788177341, 0.52463054187192115, 0.43226600985221675, 0.46921182266009853, 0.44704433497536944, 0.39655172413793105, 0.72906403940886699, 0.51847290640394084, 0.44581280788177341], 5: [0.099753694581280791, 0.26477832512315269, 0.25492610837438423, 0.12931034482758622, 0.16133004926108374, 0.096059113300492605, 0.22536945812807882, 0.32389162561576357, 0.14408866995073891, 0.20812807881773399, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235753 minutes
Weight histogram
[  184   588   776   472  1138  2279  6609 15877  5899   603] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
[  172   297   479   788  1003  2052  2813  6659 10922  9240] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
-1.3595
1.46953
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.32659
Epoch 1, cost is  1.30467
Epoch 2, cost is  1.28782
Epoch 3, cost is  1.2739
Epoch 4, cost is  1.26296
Training took 0.224547 minutes
Weight histogram
[7643 6276 4651 4041 3126 2367 2019 1580 1495 1227] [ -6.63788766e-02  -5.97342254e-02  -5.30895742e-02  -4.64449230e-02
  -3.98002718e-02  -3.31556206e-02  -2.65109694e-02  -1.98663182e-02
  -1.32216670e-02  -6.57701585e-03   6.76353375e-05]
[2520 1667 2055 2478 2853 3517 3866 4843 5142 5484] [ -6.63788766e-02  -5.97342254e-02  -5.30895742e-02  -4.64449230e-02
  -3.98002718e-02  -3.31556206e-02  -2.65109694e-02  -1.98663182e-02
  -1.32216670e-02  -6.57701585e-03   6.76353375e-05]
-1.05917
1.82574
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149022 minutes
Weight histogram
[  356  1117  1465   816  2164  7079 10786  9580  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  500   995  1075   333   499   834  1094  2259  7345 21516] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.115926 minutes
Weight histogram
[7046 5160 5173 3689 3241 2907 2248 2406 3065 1515] [ -6.33401051e-02  -5.69993311e-02  -5.06585570e-02  -4.43177830e-02
  -3.79770089e-02  -3.16362349e-02  -2.52954608e-02  -1.89546868e-02
  -1.26139128e-02  -6.27313871e-03   6.76353375e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.69993311e-02  -5.06585570e-02  -4.43177830e-02
  -3.79770089e-02  -3.16362349e-02  -2.52954608e-02  -1.89546868e-02
  -1.26139128e-02  -6.27313871e-03   6.76353375e-05]
-1.76727
1.9195
... retrieved True_rbm_750-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.76444
Epoch 1, cost is  5.49799
Epoch 2, cost is  5.16582
Epoch 3, cost is  4.68735
Epoch 4, cost is  4.29782
Epoch 5, cost is  4.00786
Epoch 6, cost is  3.77115
Epoch 7, cost is  3.56658
Epoch 8, cost is  3.39396
Epoch 9, cost is  3.24368
Training took 0.354203 minutes
Weight histogram
[2508 2378 2102 1725 1799 4041 1240  287   80   40] [-0.01835062 -0.01652909 -0.01470757 -0.01288604 -0.01106452 -0.00924299
 -0.00742146 -0.00559994 -0.00377841 -0.00195689 -0.00013536]
[3971 1106 1038 1129 1253 1404 1486 1570 1676 1567] [-0.01835062 -0.01652909 -0.01470757 -0.01288604 -0.01106452 -0.00924299
 -0.00742146 -0.00559994 -0.00377841 -0.00195689 -0.00013536]
-0.26917
0.341686
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081671 minutes
Epoch 0
Fine tuning took 0.080232 minutes
Epoch 0
Fine tuning took 0.081943 minutes
Epoch 0
Fine tuning took 0.080132 minutes
Epoch 0
Fine tuning took 0.081621 minutes
Epoch 0
Fine tuning took 0.081384 minutes
Epoch 0
Fine tuning took 0.080143 minutes
Epoch 0
Fine tuning took 0.082198 minutes
Epoch 0
Fine tuning took 0.081298 minutes
Epoch 0
Fine tuning took 0.081533 minutes
{'zero': {0: [0.33620689655172414, 0.22783251231527094, 0.20073891625615764, 0.21305418719211822, 0.19334975369458129, 0.26231527093596058, 0.23645320197044334, 0.21305418719211822, 0.23275862068965517, 0.14655172413793102, 0.22044334975369459], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.59975369458128081, 0.63300492610837433, 0.59605911330049266, 0.69827586206896552, 0.59852216748768472, 0.61576354679802958, 0.57512315270935965, 0.56157635467980294, 0.68226600985221675, 0.58251231527093594], 5: [0.11083743842364532, 0.17241379310344829, 0.16625615763546797, 0.19088669950738915, 0.10837438423645321, 0.13916256157635468, 0.14778325123152711, 0.21182266009852216, 0.20566502463054187, 0.17118226600985223, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33620689655172414, 0.23645320197044334, 0.28325123152709358, 0.29433497536945813, 0.24876847290640394, 0.25985221674876846, 0.31157635467980294, 0.30418719211822659, 0.28078817733990147, 0.24876847290640394, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.52955665024630538, 0.55788177339901479, 0.53201970443349755, 0.59113300492610843, 0.53078817733990147, 0.48152709359605911, 0.48645320197044334, 0.48891625615763545, 0.56157635467980294, 0.54802955665024633], 5: [0.11083743842364532, 0.23399014778325122, 0.15886699507389163, 0.17364532019704434, 0.16009852216748768, 0.20935960591133004, 0.20689655172413793, 0.20935960591133004, 0.23029556650246305, 0.18965517241379309, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33620689655172414, 0.2413793103448276, 0.27832512315270935, 0.27832512315270935, 0.26477832512315269, 0.27586206896551724, 0.30911330049261082, 0.26231527093596058, 0.27463054187192121, 0.22536945812807882, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.5788177339901478, 0.57635467980295563, 0.54556650246305416, 0.60221674876847286, 0.53694581280788178, 0.48275862068965519, 0.51231527093596063, 0.50615763546798032, 0.59605911330049266, 0.51724137931034486], 5: [0.11083743842364532, 0.17980295566502463, 0.14532019704433496, 0.17610837438423646, 0.13300492610837439, 0.18719211822660098, 0.20812807881773399, 0.22536945812807882, 0.21921182266009853, 0.17857142857142858, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33620689655172414, 0.25615763546798032, 0.27339901477832512, 0.29556650246305421, 0.22906403940886699, 0.25, 0.2857142857142857, 0.25246305418719212, 0.26231527093596058, 0.22167487684729065, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.52955665024630538, 0.53078817733990147, 0.52093596059113301, 0.61822660098522164, 0.52463054187192115, 0.48029556650246308, 0.52216748768472909, 0.5357142857142857, 0.58251231527093594, 0.54187192118226601], 5: [0.11083743842364532, 0.21428571428571427, 0.19581280788177341, 0.18349753694581281, 0.15270935960591134, 0.22536945812807882, 0.23399014778325122, 0.22536945812807882, 0.2019704433497537, 0.19581280788177341, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237420 minutes
Weight histogram
[  184   588   776   472  1138  2279  6609 15877  5899   603] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
[  172   297   479   788  1003  2052  2813  6659 10922  9240] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
-1.3595
1.46953
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.32659
Epoch 1, cost is  1.30467
Epoch 2, cost is  1.28782
Epoch 3, cost is  1.2739
Epoch 4, cost is  1.26296
Training took 0.222527 minutes
Weight histogram
[7643 6276 4651 4041 3126 2367 2019 1580 1495 1227] [ -6.63788766e-02  -5.97342254e-02  -5.30895742e-02  -4.64449230e-02
  -3.98002718e-02  -3.31556206e-02  -2.65109694e-02  -1.98663182e-02
  -1.32216670e-02  -6.57701585e-03   6.76353375e-05]
[2520 1667 2055 2478 2853 3517 3866 4843 5142 5484] [ -6.63788766e-02  -5.97342254e-02  -5.30895742e-02  -4.64449230e-02
  -3.98002718e-02  -3.31556206e-02  -2.65109694e-02  -1.98663182e-02
  -1.32216670e-02  -6.57701585e-03   6.76353375e-05]
-1.05917
1.82574
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148215 minutes
Weight histogram
[  356  1117  1465   816  2164  7079 10786  9580  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  500   995  1075   333   499   834  1094  2259  7345 21516] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.116510 minutes
Weight histogram
[7046 5160 5173 3689 3241 2907 2248 2406 3065 1515] [ -6.33401051e-02  -5.69993311e-02  -5.06585570e-02  -4.43177830e-02
  -3.79770089e-02  -3.16362349e-02  -2.52954608e-02  -1.89546868e-02
  -1.26139128e-02  -6.27313871e-03   6.76353375e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.69993311e-02  -5.06585570e-02  -4.43177830e-02
  -3.79770089e-02  -3.16362349e-02  -2.52954608e-02  -1.89546868e-02
  -1.26139128e-02  -6.27313871e-03   6.76353375e-05]
-1.76727
1.9195
... retrieved True_rbm_750-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.21661
Epoch 1, cost is  4.98752
Epoch 2, cost is  4.64186
Epoch 3, cost is  4.18465
Epoch 4, cost is  3.80686
Epoch 5, cost is  3.51721
Epoch 6, cost is  3.27241
Epoch 7, cost is  3.06634
Epoch 8, cost is  2.89222
Epoch 9, cost is  2.7436
Training took 0.544151 minutes
Weight histogram
[3136 3187 2647 5467 1030  390  184   88   44   27] [-0.01290335 -0.01162593 -0.01034852 -0.00907111 -0.00779369 -0.00651628
 -0.00523887 -0.00396145 -0.00268404 -0.00140662 -0.00012921]
[3973 1124 1070 1143 1257 1364 1445 1554 1680 1590] [-0.01290335 -0.01162593 -0.01034852 -0.00907111 -0.00779369 -0.00651628
 -0.00523887 -0.00396145 -0.00268404 -0.00140662 -0.00012921]
-0.23005
0.267331
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090656 minutes
Epoch 0
Fine tuning took 0.090712 minutes
Epoch 0
Fine tuning took 0.090423 minutes
Epoch 0
Fine tuning took 0.090660 minutes
Epoch 0
Fine tuning took 0.091419 minutes
Epoch 0
Fine tuning took 0.091746 minutes
Epoch 0
Fine tuning took 0.090892 minutes
Epoch 0
Fine tuning took 0.090062 minutes
Epoch 0
Fine tuning took 0.090772 minutes
Epoch 0
Fine tuning took 0.091288 minutes
{'zero': {0: [0.25738916256157635, 0.18719211822660098, 0.20566502463054187, 0.15270935960591134, 0.16748768472906403, 0.21921182266009853, 0.22413793103448276, 0.25492610837438423, 0.16748768472906403, 0.17118226600985223, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.61822660098522164, 0.62684729064039413, 0.64778325123152714, 0.66995073891625612, 0.56157635467980294, 0.55665024630541871, 0.56773399014778325, 0.6071428571428571, 0.59975369458128081, 0.56034482758620685], 5: [0.16009852216748768, 0.19458128078817735, 0.16748768472906403, 0.19950738916256158, 0.1625615763546798, 0.21921182266009853, 0.21921182266009853, 0.17733990147783252, 0.22536945812807882, 0.22906403940886699, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25738916256157635, 0.19334975369458129, 0.23029556650246305, 0.17118226600985223, 0.20443349753694581, 0.26108374384236455, 0.21428571428571427, 0.27709359605911332, 0.24014778325123154, 0.17857142857142858, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.6280788177339901, 0.62315270935960587, 0.61206896551724133, 0.58743842364532017, 0.52463054187192115, 0.56034482758620685, 0.5357142857142857, 0.55418719211822665, 0.59605911330049266, 0.55418719211822665], 5: [0.16009852216748768, 0.17857142857142858, 0.14655172413793102, 0.21674876847290642, 0.20812807881773399, 0.21428571428571427, 0.22536945812807882, 0.18719211822660098, 0.20566502463054187, 0.22536945812807882, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25738916256157635, 0.19581280788177341, 0.20566502463054187, 0.18472906403940886, 0.2413793103448276, 0.22660098522167488, 0.22783251231527094, 0.26231527093596058, 0.23645320197044334, 0.20320197044334976, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.60837438423645318, 0.61945812807881773, 0.59482758620689657, 0.56403940886699511, 0.54926108374384242, 0.53940886699507384, 0.51847290640394084, 0.54679802955665024, 0.58620689655172409, 0.53817733990147787], 5: [0.16009852216748768, 0.19581280788177341, 0.1748768472906404, 0.22044334975369459, 0.19458128078817735, 0.22413793103448276, 0.23275862068965517, 0.21921182266009853, 0.21674876847290642, 0.2105911330049261, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25738916256157635, 0.22413793103448276, 0.22536945812807882, 0.2105911330049261, 0.20320197044334976, 0.25246305418719212, 0.21182266009852216, 0.25985221674876846, 0.20320197044334976, 0.18842364532019704, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.60221674876847286, 0.61576354679802958, 0.58004926108374388, 0.6071428571428571, 0.51724137931034486, 0.57389162561576357, 0.54187192118226601, 0.58743842364532017, 0.60098522167487689, 0.54187192118226601], 5: [0.16009852216748768, 0.17364532019704434, 0.15886699507389163, 0.20935960591133004, 0.18965517241379309, 0.23029556650246305, 0.21428571428571427, 0.19827586206896552, 0.20935960591133004, 0.2105911330049261, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237610 minutes
Weight histogram
[  184   588   776   472  1138  2279  6609 15877  5899   603] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
[  172   297   479   788  1003  2052  2813  6659 10922  9240] [ -1.30836663e-04   6.63052269e-05   2.63447117e-04   4.60589006e-04
   6.57730896e-04   8.54872786e-04   1.05201468e-03   1.24915656e-03
   1.44629845e-03   1.64344034e-03   1.84058223e-03]
-1.3595
1.46953
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.32659
Epoch 1, cost is  1.30467
Epoch 2, cost is  1.28782
Epoch 3, cost is  1.2739
Epoch 4, cost is  1.26296
Training took 0.223014 minutes
Weight histogram
[7643 6276 4651 4041 3126 2367 2019 1580 1495 1227] [ -6.63788766e-02  -5.97342254e-02  -5.30895742e-02  -4.64449230e-02
  -3.98002718e-02  -3.31556206e-02  -2.65109694e-02  -1.98663182e-02
  -1.32216670e-02  -6.57701585e-03   6.76353375e-05]
[2520 1667 2055 2478 2853 3517 3866 4843 5142 5484] [ -6.63788766e-02  -5.97342254e-02  -5.30895742e-02  -4.64449230e-02
  -3.98002718e-02  -3.31556206e-02  -2.65109694e-02  -1.98663182e-02
  -1.32216670e-02  -6.57701585e-03   6.76353375e-05]
-1.05917
1.82574
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148208 minutes
Weight histogram
[  356  1117  1465   816  2164  7079 10786  9580  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[  500   995  1075   333   499   834  1094  2259  7345 21516] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.117255 minutes
Weight histogram
[7046 5160 5173 3689 3241 2907 2248 2406 3065 1515] [ -6.33401051e-02  -5.69993311e-02  -5.06585570e-02  -4.43177830e-02
  -3.79770089e-02  -3.16362349e-02  -2.52954608e-02  -1.89546868e-02
  -1.26139128e-02  -6.27313871e-03   6.76353375e-05]
[5149 1681 2141 2630 3052 3516 4105 4207 5149 4820] [ -6.33401051e-02  -5.69993311e-02  -5.06585570e-02  -4.43177830e-02
  -3.79770089e-02  -3.16362349e-02  -2.52954608e-02  -1.89546868e-02
  -1.26139128e-02  -6.27313871e-03   6.76353375e-05]
-1.76727
1.9195
... retrieved True_rbm_750-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.01692
Epoch 1, cost is  4.6955
Epoch 2, cost is  4.07876
Epoch 3, cost is  3.59539
Epoch 4, cost is  3.23926
Epoch 5, cost is  2.95796
Epoch 6, cost is  2.72849
Epoch 7, cost is  2.54336
Epoch 8, cost is  2.39584
Epoch 9, cost is  2.27569
Training took 0.944221 minutes
Weight histogram
[3252 3223 2790 2104 3210 1405  126   48   26   16] [-0.00793658 -0.00715469 -0.00637279 -0.0055909  -0.00480901 -0.00402711
 -0.00324522 -0.00246333 -0.00168143 -0.00089954 -0.00011765]
[3273 1026 1063 1211 1287 1417 1540 1705 1897 1781] [-0.00793658 -0.00715469 -0.00637279 -0.0055909  -0.00480901 -0.00402711
 -0.00324522 -0.00246333 -0.00168143 -0.00089954 -0.00011765]
-0.184484
0.189962
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.109393 minutes
Epoch 0
Fine tuning took 0.109521 minutes
Epoch 0
Fine tuning took 0.110380 minutes
Epoch 0
Fine tuning took 0.109159 minutes
Epoch 0
Fine tuning took 0.110108 minutes
Epoch 0
Fine tuning took 0.110385 minutes
Epoch 0
Fine tuning took 0.109529 minutes
Epoch 0
Fine tuning took 0.109359 minutes
Epoch 0
Fine tuning took 0.109341 minutes
Epoch 0
Fine tuning took 0.109608 minutes
{'zero': {0: [0.17857142857142858, 0.19088669950738915, 0.2019704433497537, 0.15640394088669951, 0.19211822660098521, 0.25, 0.2229064039408867, 0.24384236453201971, 0.24753694581280788, 0.22906403940886699, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60837438423645318, 0.58866995073891626, 0.59359605911330049, 0.56773399014778325, 0.59359605911330049, 0.45197044334975367, 0.48768472906403942, 0.4605911330049261, 0.47783251231527096, 0.45443349753694579, 0.50492610837438423], 5: [0.21305418719211822, 0.22044334975369459, 0.20443349753694581, 0.27586206896551724, 0.21428571428571427, 0.29802955665024633, 0.2894088669950739, 0.29556650246305421, 0.27463054187192121, 0.31650246305418717, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17857142857142858, 0.18719211822660098, 0.20443349753694581, 0.16502463054187191, 0.17241379310344829, 0.28201970443349755, 0.22044334975369459, 0.25615763546798032, 0.25862068965517243, 0.25738916256157635, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60837438423645318, 0.56034482758620685, 0.56280788177339902, 0.53078817733990147, 0.56527093596059108, 0.43103448275862066, 0.51600985221674878, 0.48152709359605911, 0.46674876847290642, 0.45197044334975367, 0.47290640394088668], 5: [0.21305418719211822, 0.25246305418719212, 0.23275862068965517, 0.30418719211822659, 0.26231527093596058, 0.28694581280788178, 0.26354679802955666, 0.26231527093596058, 0.27463054187192121, 0.29064039408866993, 0.26231527093596058], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17857142857142858, 0.19211822660098521, 0.21674876847290642, 0.15147783251231528, 0.23522167487684728, 0.2413793103448276, 0.23275862068965517, 0.25, 0.27709359605911332, 0.25492610837438423, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60837438423645318, 0.58374384236453203, 0.56034482758620685, 0.55911330049261088, 0.53325123152709364, 0.45935960591133007, 0.50862068965517238, 0.45935960591133007, 0.43842364532019706, 0.47413793103448276, 0.49507389162561577], 5: [0.21305418719211822, 0.22413793103448276, 0.2229064039408867, 0.2894088669950739, 0.23152709359605911, 0.29926108374384236, 0.25862068965517243, 0.29064039408866993, 0.28448275862068967, 0.27093596059113301, 0.26724137931034481], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17857142857142858, 0.22783251231527094, 0.24753694581280788, 0.18226600985221675, 0.19827586206896552, 0.29556650246305421, 0.24630541871921183, 0.24014778325123154, 0.25738916256157635, 0.2413793103448276, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.60837438423645318, 0.55418719211822665, 0.53940886699507384, 0.54926108374384242, 0.56157635467980294, 0.4248768472906404, 0.48029556650246308, 0.48152709359605911, 0.45935960591133007, 0.50369458128078815, 0.46182266009852219], 5: [0.21305418719211822, 0.21798029556650247, 0.21305418719211822, 0.26847290640394089, 0.24014778325123154, 0.27955665024630544, 0.27339901477832512, 0.27832512315270935, 0.28325123152709358, 0.25492610837438423, 0.2857142857142857], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421091 minutes
Weight histogram
[  151   494   667  1438  5039 11759 10267  3046  1327   237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1271 1041 1044 1566 2836 2314 4720 4689 8382 6562] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.33765
1.18315
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.810203
Epoch 1, cost is  0.793761
Epoch 2, cost is  0.782863
Epoch 3, cost is  0.775065
Epoch 4, cost is  0.767167
Training took 0.645106 minutes
Weight histogram
[8474 6994 5040 3768 2972 1956 1580 1401 1053 1187] [ -4.79458831e-02  -4.31569843e-02  -3.83680855e-02  -3.35791866e-02
  -2.87902878e-02  -2.40013890e-02  -1.92124902e-02  -1.44235913e-02
  -9.63469250e-03  -4.84579367e-03  -5.68948381e-05]
[2199 1539 1782 2322 2840 3457 3805 4757 5509 6215] [ -4.79458831e-02  -4.31569843e-02  -3.83680855e-02  -3.35791866e-02
  -2.87902878e-02  -2.40013890e-02  -1.92124902e-02  -1.44235913e-02
  -9.63469250e-03  -4.84579367e-03  -5.68948381e-05]
-0.961498
1.69389
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148455 minutes
Weight histogram
[  538  1619   977   621  2163  7079 10786  9580  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2168   162   239   332   497   837  1094  2254  7327 21540] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.115981 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2404 2699 1890] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5147 1681 2141 2630 3052 3516 4106 4207 5150 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_1250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/12/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46001
Epoch 1, cost is  6.10205
Epoch 2, cost is  5.6383
Epoch 3, cost is  5.23709
Epoch 4, cost is  4.93272
Epoch 5, cost is  4.68845
Epoch 6, cost is  4.48394
Epoch 7, cost is  4.30868
Epoch 8, cost is  4.16023
Epoch 9, cost is  4.0245
Training took 0.324196 minutes
Weight histogram
[1968 1991 1755 1626 1524 1331 1344 1658 2629  374] [-0.03172233 -0.02856656 -0.02541079 -0.02225502 -0.01909926 -0.01594349
 -0.01278772 -0.00963195 -0.00647618 -0.00332041 -0.00016465]
[2694 1183 1156 1245 1368 1488 1604 1716 1850 1896] [-0.03172233 -0.02856656 -0.02541079 -0.02225502 -0.01909926 -0.01594349
 -0.01278772 -0.00963195 -0.00647618 -0.00332041 -0.00016465]
-0.484167
0.639102
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.141094 minutes
Epoch 0
Fine tuning took 0.142919 minutes
Epoch 0
Fine tuning took 0.142132 minutes
Epoch 0
Fine tuning took 0.142840 minutes
Epoch 0
Fine tuning took 0.141639 minutes
Epoch 0
Fine tuning took 0.142221 minutes
Epoch 0
Fine tuning took 0.142317 minutes
Epoch 0
Fine tuning took 0.142116 minutes
Epoch 0
Fine tuning took 0.142212 minutes
Epoch 0
Fine tuning took 0.142252 minutes
{'zero': {0: [0.38669950738916259, 0.37931034482758619, 0.7857142857142857, 0.28448275862068967, 0.51231527093596063, 0.29433497536945813, 0.61699507389162567, 0.27093596059113301, 0.14655172413793102, 0.2413793103448276, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46798029556650245, 0.31527093596059114, 0.10591133004926108, 0.5788177339901478, 0.38054187192118227, 0.5714285714285714, 0.15886699507389163, 0.39901477832512317, 0.65270935960591137, 0.61576354679802958, 0.48891625615763545], 5: [0.14532019704433496, 0.30541871921182268, 0.10837438423645321, 0.13669950738916256, 0.10714285714285714, 0.13423645320197045, 0.22413793103448276, 0.33004926108374383, 0.20073891625615764, 0.14285714285714285, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.38669950738916259, 0.40886699507389163, 0.75123152709359609, 0.27832512315270935, 0.54679802955665024, 0.3682266009852217, 0.64532019704433496, 0.27955665024630544, 0.16009852216748768, 0.29556650246305421, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46798029556650245, 0.31157635467980294, 0.15270935960591134, 0.57758620689655171, 0.34236453201970446, 0.50615763546798032, 0.18719211822660098, 0.42364532019704432, 0.66256157635467983, 0.55911330049261088, 0.52463054187192115], 5: [0.14532019704433496, 0.27955665024630544, 0.096059113300492605, 0.14408866995073891, 0.11083743842364532, 0.12561576354679804, 0.16748768472906403, 0.29679802955665024, 0.17733990147783252, 0.14532019704433496, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.38669950738916259, 0.36945812807881773, 0.7573891625615764, 0.28201970443349755, 0.53694581280788178, 0.32142857142857145, 0.6354679802955665, 0.29064039408866993, 0.17364532019704434, 0.26354679802955666, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46798029556650245, 0.33743842364532017, 0.15024630541871922, 0.56157635467980294, 0.34729064039408869, 0.54433497536945807, 0.16133004926108374, 0.4211822660098522, 0.64778325123152714, 0.56403940886699511, 0.52216748768472909], 5: [0.14532019704433496, 0.29310344827586204, 0.092364532019704432, 0.15640394088669951, 0.11576354679802955, 0.13423645320197045, 0.20320197044334976, 0.28817733990147781, 0.17857142857142858, 0.17241379310344829, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.38669950738916259, 0.42733990147783252, 0.71551724137931039, 0.28448275862068967, 0.54187192118226601, 0.29187192118226601, 0.67241379310344829, 0.3251231527093596, 0.16502463054187191, 0.35960591133004927, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.46798029556650245, 0.31403940886699505, 0.16625615763546797, 0.5923645320197044, 0.34852216748768472, 0.56896551724137934, 0.15886699507389163, 0.4039408866995074, 0.66379310344827591, 0.48768472906403942, 0.49261083743842365], 5: [0.14532019704433496, 0.25862068965517243, 0.11822660098522167, 0.12315270935960591, 0.10960591133004927, 0.13916256157635468, 0.16871921182266009, 0.27093596059113301, 0.17118226600985223, 0.15270935960591134, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.424242 minutes
Weight histogram
[  151   494   667  1438  5039 11759 10267  3046  1327   237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1271 1041 1044 1566 2836 2314 4720 4689 8382 6562] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.33765
1.18315
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.810203
Epoch 1, cost is  0.793761
Epoch 2, cost is  0.782863
Epoch 3, cost is  0.775065
Epoch 4, cost is  0.767167
Training took 0.647380 minutes
Weight histogram
[8474 6994 5040 3768 2972 1956 1580 1401 1053 1187] [ -4.79458831e-02  -4.31569843e-02  -3.83680855e-02  -3.35791866e-02
  -2.87902878e-02  -2.40013890e-02  -1.92124902e-02  -1.44235913e-02
  -9.63469250e-03  -4.84579367e-03  -5.68948381e-05]
[2199 1539 1782 2322 2840 3457 3805 4757 5509 6215] [ -4.79458831e-02  -4.31569843e-02  -3.83680855e-02  -3.35791866e-02
  -2.87902878e-02  -2.40013890e-02  -1.92124902e-02  -1.44235913e-02
  -9.63469250e-03  -4.84579367e-03  -5.68948381e-05]
-0.961498
1.69389
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148955 minutes
Weight histogram
[  538  1619   977   621  2163  7079 10786  9580  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2168   162   239   332   497   837  1094  2254  7327 21540] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.118150 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2404 2699 1890] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5147 1681 2141 2630 3052 3516 4106 4207 5150 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/13/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.98791
Epoch 1, cost is  5.53045
Epoch 2, cost is  4.94587
Epoch 3, cost is  4.50179
Epoch 4, cost is  4.13732
Epoch 5, cost is  3.84586
Epoch 6, cost is  3.6162
Epoch 7, cost is  3.42965
Epoch 8, cost is  3.27642
Epoch 9, cost is  3.14222
Training took 0.501047 minutes
Weight histogram
[2196 2103 1933 1788 1759 1518 1354 2659  834   56] [-0.02028304 -0.01826947 -0.01625591 -0.01424235 -0.01222879 -0.01021523
 -0.00820166 -0.0061881  -0.00417454 -0.00216098 -0.00014742]
[2842 1161 1250 1323 1347 1402 1532 1680 1837 1826] [-0.02028304 -0.01826947 -0.01625591 -0.01424235 -0.01222879 -0.01021523
 -0.00820166 -0.0061881  -0.00417454 -0.00216098 -0.00014742]
-0.349079
0.429688
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.150263 minutes
Epoch 0
Fine tuning took 0.149472 minutes
Epoch 0
Fine tuning took 0.149252 minutes
Epoch 0
Fine tuning took 0.150544 minutes
Epoch 0
Fine tuning took 0.149191 minutes
Epoch 0
Fine tuning took 0.150263 minutes
Epoch 0
Fine tuning took 0.149610 minutes
Epoch 0
Fine tuning took 0.149433 minutes
Epoch 0
Fine tuning took 0.150005 minutes
Epoch 0
Fine tuning took 0.149761 minutes
{'zero': {0: [0.38793103448275862, 0.23275862068965517, 0.2413793103448276, 0.16995073891625614, 0.2413793103448276, 0.24261083743842365, 0.18842364532019704, 0.18965517241379309, 0.18103448275862069, 0.1625615763546798, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.64532019704433496, 0.61822660098522164, 0.67241379310344829, 0.62931034482758619, 0.59852216748768472, 0.54433497536945807, 0.63916256157635465, 0.64655172413793105, 0.65640394088669951, 0.57635467980295563], 5: [0.11576354679802955, 0.12192118226600986, 0.14039408866995073, 0.15763546798029557, 0.12931034482758622, 0.15886699507389163, 0.26724137931034481, 0.17118226600985223, 0.17241379310344829, 0.18103448275862069, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.38793103448275862, 0.32142857142857145, 0.35467980295566504, 0.21798029556650247, 0.30665024630541871, 0.33128078817733991, 0.23645320197044334, 0.25615763546798032, 0.21182266009852216, 0.20935960591133004, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.56650246305418717, 0.49137931034482757, 0.6280788177339901, 0.51108374384236455, 0.50492610837438423, 0.50369458128078815, 0.51600985221674878, 0.57635467980295563, 0.60098522167487689, 0.53817733990147787], 5: [0.11576354679802955, 0.11206896551724138, 0.1539408866995074, 0.1539408866995074, 0.18226600985221675, 0.16379310344827586, 0.25985221674876846, 0.22783251231527094, 0.21182266009852216, 0.18965517241379309, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.38793103448275862, 0.29064039408866993, 0.35960591133004927, 0.20935960591133004, 0.2536945812807882, 0.32635467980295568, 0.22167487684729065, 0.24630541871921183, 0.23029556650246305, 0.21305418719211822, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.5923645320197044, 0.49261083743842365, 0.63423645320197042, 0.58497536945812811, 0.50123152709359609, 0.50615763546798032, 0.53448275862068961, 0.5788177339901478, 0.60591133004926112, 0.52463054187192115], 5: [0.11576354679802955, 0.11699507389162561, 0.14778325123152711, 0.15640394088669951, 0.16133004926108374, 0.17241379310344829, 0.27216748768472904, 0.21921182266009853, 0.19088669950738915, 0.18103448275862069, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.38793103448275862, 0.28817733990147781, 0.36206896551724138, 0.18719211822660098, 0.27339901477832512, 0.32389162561576357, 0.23522167487684728, 0.27339901477832512, 0.22906403940886699, 0.22906403940886699, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.58374384236453203, 0.47290640394088668, 0.66256157635467983, 0.53448275862068961, 0.51970443349753692, 0.46798029556650245, 0.51108374384236455, 0.54926108374384242, 0.5431034482758621, 0.56773399014778325], 5: [0.11576354679802955, 0.12807881773399016, 0.16502463054187191, 0.15024630541871922, 0.19211822660098521, 0.15640394088669951, 0.29679802955665024, 0.21551724137931033, 0.22167487684729065, 0.22783251231527094, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.421682 minutes
Weight histogram
[  151   494   667  1438  5039 11759 10267  3046  1327   237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1271 1041 1044 1566 2836 2314 4720 4689 8382 6562] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.33765
1.18315
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.810203
Epoch 1, cost is  0.793761
Epoch 2, cost is  0.782863
Epoch 3, cost is  0.775065
Epoch 4, cost is  0.767167
Training took 0.644950 minutes
Weight histogram
[8474 6994 5040 3768 2972 1956 1580 1401 1053 1187] [ -4.79458831e-02  -4.31569843e-02  -3.83680855e-02  -3.35791866e-02
  -2.87902878e-02  -2.40013890e-02  -1.92124902e-02  -1.44235913e-02
  -9.63469250e-03  -4.84579367e-03  -5.68948381e-05]
[2199 1539 1782 2322 2840 3457 3805 4757 5509 6215] [ -4.79458831e-02  -4.31569843e-02  -3.83680855e-02  -3.35791866e-02
  -2.87902878e-02  -2.40013890e-02  -1.92124902e-02  -1.44235913e-02
  -9.63469250e-03  -4.84579367e-03  -5.68948381e-05]
-0.961498
1.69389
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148495 minutes
Weight histogram
[  538  1619   977   621  2163  7079 10786  9580  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2168   162   239   332   497   837  1094  2254  7327 21540] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.115866 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2404 2699 1890] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5147 1681 2141 2630 3052 3516 4106 4207 5150 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/14/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.40425
Epoch 1, cost is  4.8996
Epoch 2, cost is  4.29723
Epoch 3, cost is  3.84596
Epoch 4, cost is  3.49794
Epoch 5, cost is  3.2359
Epoch 6, cost is  3.03228
Epoch 7, cost is  2.86557
Epoch 8, cost is  2.72835
Epoch 9, cost is  2.61512
Training took 0.818407 minutes
Weight histogram
[2719 2631 2223 2000 1819 1472 2811  419   76   30] [-0.0134927  -0.01215659 -0.01082049 -0.00948439 -0.00814829 -0.00681219
 -0.00547608 -0.00413998 -0.00280388 -0.00146778 -0.00013167]
[2868 1172 1225 1251 1308 1421 1569 1700 1870 1816] [-0.0134927  -0.01215659 -0.01082049 -0.00948439 -0.00814829 -0.00681219
 -0.00547608 -0.00413998 -0.00280388 -0.00146778 -0.00013167]
-0.205822
0.322582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.164554 minutes
Epoch 0
Fine tuning took 0.164435 minutes
Epoch 0
Fine tuning took 0.164568 minutes
Epoch 0
Fine tuning took 0.165131 minutes
Epoch 0
Fine tuning took 0.164943 minutes
Epoch 0
Fine tuning took 0.164610 minutes
Epoch 0
Fine tuning took 0.165275 minutes
Epoch 0
Fine tuning took 0.164441 minutes
Epoch 0
Fine tuning took 0.164771 minutes
Epoch 0
Fine tuning took 0.165194 minutes
{'zero': {0: [0.34975369458128081, 0.17980295566502463, 0.22783251231527094, 0.16502463054187191, 0.24753694581280788, 0.27093596059113301, 0.24261083743842365, 0.21798029556650247, 0.2413793103448276, 0.19827586206896552, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52955665024630538, 0.60960591133004927, 0.61083743842364535, 0.63793103448275867, 0.55911330049261088, 0.53817733990147787, 0.56280788177339902, 0.57389162561576357, 0.51847290640394084, 0.58128078817733986, 0.51847290640394084], 5: [0.1206896551724138, 0.2105911330049261, 0.16133004926108374, 0.19704433497536947, 0.19334975369458129, 0.19088669950738915, 0.19458128078817735, 0.20812807881773399, 0.24014778325123154, 0.22044334975369459, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.34975369458128081, 0.23152709359605911, 0.24876847290640394, 0.17610837438423646, 0.28448275862068967, 0.29556650246305421, 0.25492610837438423, 0.24876847290640394, 0.24876847290640394, 0.18103448275862069, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52955665024630538, 0.57019704433497542, 0.57512315270935965, 0.62931034482758619, 0.49384236453201968, 0.49876847290640391, 0.52832512315270941, 0.55418719211822665, 0.53817733990147787, 0.59975369458128081, 0.53078817733990147], 5: [0.1206896551724138, 0.19827586206896552, 0.17610837438423646, 0.19458128078817735, 0.22167487684729065, 0.20566502463054187, 0.21674876847290642, 0.19704433497536947, 0.21305418719211822, 0.21921182266009853, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.34975369458128081, 0.23152709359605911, 0.26477832512315269, 0.19950738916256158, 0.25492610837438423, 0.28694581280788178, 0.25246305418719212, 0.24876847290640394, 0.27955665024630544, 0.20566502463054187, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52955665024630538, 0.57266009852216748, 0.55911330049261088, 0.58990147783251234, 0.52955665024630538, 0.51108374384236455, 0.51847290640394084, 0.54679802955665024, 0.49137931034482757, 0.58990147783251234, 0.51970443349753692], 5: [0.1206896551724138, 0.19581280788177341, 0.17610837438423646, 0.2105911330049261, 0.21551724137931033, 0.2019704433497537, 0.22906403940886699, 0.20443349753694581, 0.22906403940886699, 0.20443349753694581, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.34975369458128081, 0.22783251231527094, 0.25246305418719212, 0.1748768472906404, 0.28201970443349755, 0.31773399014778325, 0.24876847290640394, 0.25246305418719212, 0.23399014778325122, 0.21921182266009853, 0.27955665024630544], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.52955665024630538, 0.55172413793103448, 0.58497536945812811, 0.63300492610837433, 0.51477832512315269, 0.48645320197044334, 0.53448275862068961, 0.55541871921182262, 0.53694581280788178, 0.54679802955665024, 0.52093596059113301], 5: [0.1206896551724138, 0.22044334975369459, 0.1625615763546798, 0.19211822660098521, 0.20320197044334976, 0.19581280788177341, 0.21674876847290642, 0.19211822660098521, 0.22906403940886699, 0.23399014778325122, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-1000_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.424490 minutes
Weight histogram
[  151   494   667  1438  5039 11759 10267  3046  1327   237] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
[1271 1041 1044 1566 2836 2314 4720 4689 8382 6562] [ -8.98989456e-05   2.19262722e-05   1.33751490e-04   2.45576708e-04
   3.57401925e-04   4.69227143e-04   5.81052361e-04   6.92877579e-04
   8.04702796e-04   9.16528014e-04   1.02835323e-03]
-1.33765
1.18315
training layer 1, rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_1000-1000_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  0.810203
Epoch 1, cost is  0.793761
Epoch 2, cost is  0.782863
Epoch 3, cost is  0.775065
Epoch 4, cost is  0.767167
Training took 0.644859 minutes
Weight histogram
[8474 6994 5040 3768 2972 1956 1580 1401 1053 1187] [ -4.79458831e-02  -4.31569843e-02  -3.83680855e-02  -3.35791866e-02
  -2.87902878e-02  -2.40013890e-02  -1.92124902e-02  -1.44235913e-02
  -9.63469250e-03  -4.84579367e-03  -5.68948381e-05]
[2199 1539 1782 2322 2840 3457 3805 4757 5509 6215] [ -4.79458831e-02  -4.31569843e-02  -3.83680855e-02  -3.35791866e-02
  -2.87902878e-02  -2.40013890e-02  -1.92124902e-02  -1.44235913e-02
  -9.63469250e-03  -4.84579367e-03  -5.68948381e-05]
-0.961498
1.69389
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148352 minutes
Weight histogram
[  538  1619   977   621  2163  7079 10786  9580  2930   157] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
[ 2168   162   239   332   497   837  1094  2254  7327 21540] [ -1.46825420e-04   9.28043068e-05   3.32434033e-04   5.72063759e-04
   8.11693486e-04   1.05132321e-03   1.29095294e-03   1.53058266e-03
   1.77021239e-03   2.00984212e-03   2.24947184e-03]
-1.58174
1.13329
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.43117
Epoch 1, cost is  2.39493
Epoch 2, cost is  2.37112
Epoch 3, cost is  2.35755
Epoch 4, cost is  2.33923
Training took 0.115908 minutes
Weight histogram
[7045 5160 5172 3689 3241 2904 2246 2404 2699 1890] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
[5147 1681 2141 2630 3052 3516 4106 4207 5150 4820] [ -6.33401051e-02  -5.70029066e-02  -5.06657081e-02  -4.43285096e-02
  -3.79913111e-02  -3.16541126e-02  -2.53169141e-02  -1.89797156e-02
  -1.26425171e-02  -6.30531865e-03   3.18798448e-05]
-1.76727
1.9195
... retrieved True_rbm_1250-1000_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/15/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.8229
Epoch 1, cost is  4.32819
Epoch 2, cost is  3.76735
Epoch 3, cost is  3.33692
Epoch 4, cost is  3.02411
Epoch 5, cost is  2.79268
Epoch 6, cost is  2.61555
Epoch 7, cost is  2.4761
Epoch 8, cost is  2.36135
Epoch 9, cost is  2.26335
Training took 1.466422 minutes
Weight histogram
[3483 3230 2638 2528 3453  521  205   84   38   20] [-0.00915086 -0.00825041 -0.00734996 -0.00644952 -0.00554907 -0.00464862
 -0.00374818 -0.00284773 -0.00194729 -0.00104684 -0.00014639]
[2822 1171 1163 1184 1277 1410 1581 1760 1971 1861] [-0.00915086 -0.00825041 -0.00734996 -0.00644952 -0.00554907 -0.00464862
 -0.00374818 -0.00284773 -0.00194729 -0.00104684 -0.00014639]
-0.187185
0.221182
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.193090 minutes
Epoch 0
Fine tuning took 0.193384 minutes
Epoch 0
Fine tuning took 0.193782 minutes
Epoch 0
Fine tuning took 0.193470 minutes
Epoch 0
Fine tuning took 0.193979 minutes
Epoch 0
Fine tuning took 0.193008 minutes
Epoch 0
Fine tuning took 0.193727 minutes
Epoch 0
Fine tuning took 0.192580 minutes
Epoch 0
Fine tuning took 0.192668 minutes
Epoch 0
Fine tuning took 0.194248 minutes
{'zero': {0: [0.27339901477832512, 0.18103448275862069, 0.21305418719211822, 0.16748768472906403, 0.22536945812807882, 0.20689655172413793, 0.21182266009852216, 0.23645320197044334, 0.22783251231527094, 0.18965517241379309, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.60837438423645318, 0.55172413793103448, 0.55049261083743839, 0.55049261083743839, 0.53078817733990147, 0.49507389162561577, 0.52093596059113301, 0.52709359605911332, 0.55665024630541871, 0.48522167487684731], 5: [0.13669950738916256, 0.2105911330049261, 0.23522167487684728, 0.28201970443349755, 0.22413793103448276, 0.26231527093596058, 0.29310344827586204, 0.24261083743842365, 0.24507389162561577, 0.2536945812807882, 0.26600985221674878], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27339901477832512, 0.17241379310344829, 0.23152709359605911, 0.18596059113300492, 0.25985221674876846, 0.24753694581280788, 0.22660098522167488, 0.24753694581280788, 0.19950738916256158, 0.21305418719211822, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.61699507389162567, 0.53325123152709364, 0.50862068965517238, 0.5357142857142857, 0.4963054187192118, 0.46921182266009853, 0.47167487684729065, 0.53201970443349755, 0.53078817733990147, 0.48275862068965519], 5: [0.13669950738916256, 0.2105911330049261, 0.23522167487684728, 0.30541871921182268, 0.20443349753694581, 0.25615763546798032, 0.30418719211822659, 0.28078817733990147, 0.26847290640394089, 0.25615763546798032, 0.29926108374384236], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27339901477832512, 0.16009852216748768, 0.21674876847290642, 0.17118226600985223, 0.23522167487684728, 0.22413793103448276, 0.22660098522167488, 0.23152709359605911, 0.23275862068965517, 0.19827586206896552, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.62931034482758619, 0.57266009852216748, 0.56280788177339902, 0.54926108374384242, 0.51724137931034486, 0.50369458128078815, 0.49261083743842365, 0.50985221674876846, 0.52339901477832518, 0.44950738916256155], 5: [0.13669950738916256, 0.2105911330049261, 0.2105911330049261, 0.26600985221674878, 0.21551724137931033, 0.25862068965517243, 0.26970443349753692, 0.27586206896551724, 0.25738916256157635, 0.27832512315270935, 0.27586206896551724], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27339901477832512, 0.18103448275862069, 0.21551724137931033, 0.15024630541871922, 0.22167487684729065, 0.2536945812807882, 0.23275862068965517, 0.23152709359605911, 0.22167487684729065, 0.21674876847290642, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.60837438423645318, 0.57635467980295563, 0.55788177339901479, 0.56773399014778325, 0.49876847290640391, 0.50985221674876846, 0.48645320197044334, 0.49014778325123154, 0.52339901477832518, 0.47290640394088668], 5: [0.13669950738916256, 0.2105911330049261, 0.20812807881773399, 0.29187192118226601, 0.2105911330049261, 0.24753694581280788, 0.25738916256157635, 0.28201970443349755, 0.28817733990147781, 0.25985221674876846, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
training layer 0, rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-100_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.106897 minutes
Weight histogram
[1213 2834 4236 7322 9584 7464 2829 1913 1012   68] [ -4.09451174e-03  -3.27841416e-03  -2.46231658e-03  -1.64621901e-03
  -8.30121431e-04  -1.40238553e-05   8.02073721e-04   1.61817130e-03
   2.43426887e-03   3.25036645e-03   4.06646403e-03]
[  135   148   228   309   464   747   752  1688  4036 29968] [ -4.09451174e-03  -3.27841416e-03  -2.46231658e-03  -1.64621901e-03
  -8.30121431e-04  -1.40238553e-05   8.02073721e-04   1.61817130e-03
   2.43426887e-03   3.25036645e-03   4.06646403e-03]
-2.16556
2.88085
training layer 1, rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.56331
Epoch 1, cost is  3.53119
Epoch 2, cost is  3.50487
Epoch 3, cost is  3.49407
Epoch 4, cost is  3.47664
Training took 0.073849 minutes
Weight histogram
[7044 5020 3596 3489 4195 2760 8167 3801  241  162] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
[3180 2741 2744 2708 2949 3908 4085 5338 5360 5462] [-0.03430194 -0.0308985  -0.02749507 -0.02409163 -0.02068819 -0.01728475
 -0.01388131 -0.01047787 -0.00707443 -0.00367099 -0.00026755]
-1.91591
2.80449
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150379 minutes
Weight histogram
[   49   125   647  1474  2297 11030 17344  5537  1460   537] [ -7.42217875e-04  -3.93281446e-04  -4.43450175e-05   3.04591411e-04
   6.53527840e-04   1.00246427e-03   1.35140070e-03   1.70033713e-03
   2.04927356e-03   2.39820998e-03   2.74714641e-03]
[  235   285   368   548   914  1228  2430  7189 26600   703] [ -7.42217875e-04  -3.93281446e-04  -4.43450175e-05   3.04591411e-04
   6.53527840e-04   1.00246427e-03   1.35140070e-03   1.70033713e-03
   2.04927356e-03   2.39820998e-03   2.74714641e-03]
-1.58174
1.19951
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.3351
Epoch 1, cost is  2.29538
Epoch 2, cost is  2.27248
Epoch 3, cost is  2.25004
Epoch 4, cost is  2.2385
Training took 0.116064 minutes
Weight histogram
[7705 6151 5267 4521 3612 3226 2579 2582 4034  823] [ -6.75083622e-02  -6.07543380e-02  -5.40003138e-02  -4.72462896e-02
  -4.04922654e-02  -3.37382412e-02  -2.69842170e-02  -2.02301928e-02
  -1.34761686e-02  -6.72214436e-03   3.18798448e-05]
[5282 1833 2416 2967 3484 3986 4545 5253 5219 5515] [ -6.75083622e-02  -6.07543380e-02  -5.40003138e-02  -4.72462896e-02
  -4.04922654e-02  -3.37382412e-02  -2.69842170e-02  -2.02301928e-02
  -1.34761686e-02  -6.72214436e-03   3.18798448e-05]
-1.8914
2.0567
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN_ambi/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.92741
Epoch 1, cost is  5.61821
Epoch 2, cost is  5.53106
Epoch 3, cost is  5.36616
Epoch 4, cost is  5.13022
Epoch 5, cost is  4.93399
Epoch 6, cost is  4.74641
Epoch 7, cost is  4.58227
Epoch 8, cost is  4.43066
Epoch 9, cost is  4.3014
Training took 0.178938 minutes
Weight histogram
[2947 4156 4989 4000 1612 1025  758  478  192   93] [ -2.92806122e-02  -2.63606362e-02  -2.34406602e-02  -2.05206842e-02
  -1.76007082e-02  -1.46807322e-02  -1.17607561e-02  -8.84078013e-03
  -5.92080411e-03  -3.00082809e-03  -8.08520781e-05]
[1437 3066 3250 1836 18