Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentADBN7
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.289584 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.28993
Epoch 1, cost is  2.3845
Epoch 2, cost is  2.37728
Epoch 3, cost is  2.46177
Epoch 4, cost is  2.59221
Training took 0.175179 minutes
Weight histogram
[454 430 362 259 236 100  74  47  29  34] [-0.15648775 -0.14116519 -0.12584263 -0.11052007 -0.09519751 -0.07987494
 -0.06455238 -0.04922982 -0.03390726 -0.0185847  -0.00326214]
[ 66  76 110 154 182 226 256 283 324 348] [-0.15648775 -0.14116519 -0.12584263 -0.11052007 -0.09519751 -0.07987494
 -0.06455238 -0.04922982 -0.03390726 -0.0185847  -0.00326214]
-4.18323
6.59346
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.291126 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.35078
Epoch 1, cost is  2.35639
Epoch 2, cost is  2.35758
Epoch 3, cost is  2.46405
Epoch 4, cost is  2.59405
Training took 0.177534 minutes
Weight histogram
[899 810 733 532 449 245 156  94  58  74] [-0.15868975 -0.14314699 -0.12760423 -0.11206147 -0.09651871 -0.08097595
 -0.06543319 -0.04989042 -0.03434766 -0.0188049  -0.00326214]
[138 162 231 319 381 460 545 594 675 545] [-0.15868975 -0.14314699 -0.12760423 -0.11206147 -0.09651871 -0.08097595
 -0.06543319 -0.04989042 -0.03434766 -0.0188049  -0.00326214]
-4.22795
6.59346
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  4.48944
Epoch 1, cost is  3.71143
Epoch 2, cost is  3.94003
Epoch 3, cost is  4.35517
Epoch 4, cost is  4.74435
Training took 0.105397 minutes
Weight histogram
[330 346 354 284 278 147 113  67  36  70] [-0.31483021 -0.28365065 -0.2524711  -0.22129154 -0.19011198 -0.15893242
 -0.12775286 -0.0965733  -0.06539374 -0.03421418 -0.00303462]
[102 100 133 177 211 241 247 258 267 289] [-0.31483021 -0.28365065 -0.2524711  -0.22129154 -0.19011198 -0.15893242
 -0.12775286 -0.0965733  -0.06539374 -0.03421418 -0.00303462]
-13.1132
11.9953
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083701 minutes
Epoch 0
Fine tuning took 0.081653 minutes
Epoch 0
Fine tuning took 0.082068 minutes
{'zero': {0: [0.19088669950738915, 0.19704433497536947, 0.16502463054187191, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.73152709359605916, 0.77093596059113301, 0.80295566502463056], 5: [0.12438423645320197, 0.071428571428571425, 0.064039408866995079, 0.076354679802955669], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19088669950738915, 0.12315270935960591, 0.15147783251231528, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.77586206896551724, 0.71182266009852213, 0.74630541871921185], 5: [0.12438423645320197, 0.10098522167487685, 0.13669950738916256, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19088669950738915, 0.18103448275862069, 0.14901477832512317, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.71305418719211822, 0.72167487684729059, 0.75], 5: [0.12438423645320197, 0.10591133004926108, 0.12931034482758622, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19088669950738915, 0.11945812807881774, 0.12561576354679804, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.81896551724137934, 0.75985221674876846, 0.79679802955665024], 5: [0.12438423645320197, 0.061576354679802957, 0.1145320197044335, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.326029 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.28993
Epoch 1, cost is  2.3845
Epoch 2, cost is  2.37728
Epoch 3, cost is  2.46177
Epoch 4, cost is  2.59221
Training took 0.194173 minutes
Weight histogram
[454 430 362 259 236 100  74  47  29  34] [-0.15648775 -0.14116519 -0.12584263 -0.11052007 -0.09519751 -0.07987494
 -0.06455238 -0.04922982 -0.03390726 -0.0185847  -0.00326214]
[ 66  76 110 154 182 226 256 283 324 348] [-0.15648775 -0.14116519 -0.12584263 -0.11052007 -0.09519751 -0.07987494
 -0.06455238 -0.04922982 -0.03390726 -0.0185847  -0.00326214]
-4.18323
6.59346
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.311397 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.35078
Epoch 1, cost is  2.35639
Epoch 2, cost is  2.35758
Epoch 3, cost is  2.46405
Epoch 4, cost is  2.59405
Training took 0.175130 minutes
Weight histogram
[899 810 733 532 449 245 156  94  58  74] [-0.15868975 -0.14314699 -0.12760423 -0.11206147 -0.09651871 -0.08097595
 -0.06543319 -0.04989042 -0.03434766 -0.0188049  -0.00326214]
[138 162 231 319 381 460 545 594 675 545] [-0.15868975 -0.14314699 -0.12760423 -0.11206147 -0.09651871 -0.08097595
 -0.06543319 -0.04989042 -0.03434766 -0.0188049  -0.00326214]
-4.22795
6.59346
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.08057
Epoch 1, cost is  3.08795
Epoch 2, cost is  3.19206
Epoch 3, cost is  3.42339
Epoch 4, cost is  3.72968
Training took 0.125558 minutes
Weight histogram
[340 305 296 293 244 199 149  82  41  76] [-0.27973735 -0.2520653  -0.22439324 -0.19672118 -0.16904912 -0.14137707
 -0.11370501 -0.08603295 -0.05836089 -0.03068884 -0.00301678]
[103 102 150 178 210 234 253 264 256 275] [-0.27973735 -0.2520653  -0.22439324 -0.19672118 -0.16904912 -0.14137707
 -0.11370501 -0.08603295 -0.05836089 -0.03068884 -0.00301678]
-8.4687
9.99211
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.086154 minutes
Epoch 0
Fine tuning took 0.083952 minutes
Epoch 0
Fine tuning took 0.096905 minutes
{'zero': {0: [0.17241379310344829, 0.32019704433497537, 0.20935960591133004, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.59852216748768472, 0.73275862068965514, 0.77093596059113301], 5: [0.089901477832512317, 0.081280788177339899, 0.057881773399014777, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17241379310344829, 0.14162561576354679, 0.11083743842364532, 0.10960591133004927], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.76108374384236455, 0.74014778325123154, 0.74014778325123154], 5: [0.089901477832512317, 0.097290640394088676, 0.14901477832512317, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17241379310344829, 0.12807881773399016, 0.091133004926108374, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.74014778325123154, 0.7573891625615764, 0.7068965517241379], 5: [0.089901477832512317, 0.13177339901477833, 0.15147783251231528, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17241379310344829, 0.14655172413793102, 0.092364532019704432, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.74507389162561577, 0.80788177339901479, 0.74876847290640391], 5: [0.089901477832512317, 0.10837438423645321, 0.099753694581280791, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.289299 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.28993
Epoch 1, cost is  2.3845
Epoch 2, cost is  2.37728
Epoch 3, cost is  2.46177
Epoch 4, cost is  2.59221
Training took 0.176535 minutes
Weight histogram
[454 430 362 259 236 100  74  47  29  34] [-0.15648775 -0.14116519 -0.12584263 -0.11052007 -0.09519751 -0.07987494
 -0.06455238 -0.04922982 -0.03390726 -0.0185847  -0.00326214]
[ 66  76 110 154 182 226 256 283 324 348] [-0.15648775 -0.14116519 -0.12584263 -0.11052007 -0.09519751 -0.07987494
 -0.06455238 -0.04922982 -0.03390726 -0.0185847  -0.00326214]
-4.18323
6.59346
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.289523 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.35078
Epoch 1, cost is  2.35639
Epoch 2, cost is  2.35758
Epoch 3, cost is  2.46405
Epoch 4, cost is  2.59405
Training took 0.175556 minutes
Weight histogram
[899 810 733 532 449 245 156  94  58  74] [-0.15868975 -0.14314699 -0.12760423 -0.11206147 -0.09651871 -0.08097595
 -0.06543319 -0.04989042 -0.03434766 -0.0188049  -0.00326214]
[138 162 231 319 381 460 545 594 675 545] [-0.15868975 -0.14314699 -0.12760423 -0.11206147 -0.09651871 -0.08097595
 -0.06543319 -0.04989042 -0.03434766 -0.0188049  -0.00326214]
-4.22795
6.59346
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.55725
Epoch 1, cost is  2.13239
Epoch 2, cost is  2.04915
Epoch 3, cost is  2.11804
Epoch 4, cost is  2.24933
Training took 0.181901 minutes
Weight histogram
[322 333 317 294 216 199 139  77  51  77] [-0.18797739 -0.16948205 -0.15098671 -0.13249138 -0.11399604 -0.0955007
 -0.07700536 -0.05851002 -0.04001469 -0.02151935 -0.00302401]
[109  96 142 178 207 233 247 258 276 279] [-0.18797739 -0.16948205 -0.15098671 -0.13249138 -0.11399604 -0.0955007
 -0.07700536 -0.05851002 -0.04001469 -0.02151935 -0.00302401]
-6.41529
7.24751
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089087 minutes
Epoch 0
Fine tuning took 0.089148 minutes
Epoch 0
Fine tuning took 0.089123 minutes
{'zero': {0: [0.11083743842364532, 0.076354679802955669, 0.066502463054187194, 0.078817733990147784], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79064039408866993, 0.84359605911330049, 0.83990147783251234, 0.82635467980295563], 5: [0.098522167487684734, 0.080049261083743842, 0.093596059113300489, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11083743842364532, 0.075123152709359611, 0.070197044334975367, 0.059113300492610835], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79064039408866993, 0.81157635467980294, 0.80788177339901479, 0.80172413793103448], 5: [0.098522167487684734, 0.11330049261083744, 0.12192118226600986, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11083743842364532, 0.093596059113300489, 0.10098522167487685, 0.083743842364532015], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79064039408866993, 0.78325123152709364, 0.76231527093596063, 0.7573891625615764], 5: [0.098522167487684734, 0.12315270935960591, 0.13669950738916256, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11083743842364532, 0.073891625615763554, 0.051724137931034482, 0.020935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79064039408866993, 0.80911330049261088, 0.84482758620689657, 0.86330049261083741], 5: [0.098522167487684734, 0.11699507389162561, 0.10344827586206896, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.288895 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.62808
Epoch 1, cost is  3.70954
Epoch 2, cost is  3.06304
Epoch 3, cost is  2.75082
Epoch 4, cost is  2.55548
Training took 0.177064 minutes
Weight histogram
[483 332 274 202 153 128 116 111 100 126] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
[226 112 122 142 168 198 219 245 284 309] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
-0.996808
1.70958
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.289539 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.77101
Epoch 1, cost is  3.8688
Epoch 2, cost is  3.16563
Epoch 3, cost is  2.8295
Epoch 4, cost is  2.62839
Training took 0.174137 minutes
Weight histogram
[883 678 545 397 312 272 233 224 220 286] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
[467 228 250 289 345 401 450 502 574 544] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
-0.996808
1.70958
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  6.58423
Epoch 1, cost is  5.5178
Epoch 2, cost is  4.69802
Epoch 3, cost is  4.30566
Epoch 4, cost is  4.07126
Training took 0.102435 minutes
Weight histogram
[271 254 209 172 170 143 108 114 146 438] [-0.10474743 -0.09430198 -0.08385652 -0.07341107 -0.06296562 -0.05252016
 -0.04207471 -0.03162926 -0.0211838  -0.01073835 -0.0002929 ]
[411 125 133 144 163 170 189 210 218 262] [-0.10474743 -0.09430198 -0.08385652 -0.07341107 -0.06296562 -0.05252016
 -0.04207471 -0.03162926 -0.0211838  -0.01073835 -0.0002929 ]
-1.53641
3.25821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082900 minutes
Epoch 0
Fine tuning took 0.082149 minutes
Epoch 0
Fine tuning took 0.080714 minutes
{'zero': {0: [0.30665024630541871, 0.17733990147783252, 0.2105911330049261, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.62068965517241381, 0.67980295566502458, 0.66256157635467983], 5: [0.17980295566502463, 0.2019704433497537, 0.10960591133004927, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30665024630541871, 0.24753694581280788, 0.28201970443349755, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.53940886699507384, 0.6219211822660099, 0.6576354679802956], 5: [0.17980295566502463, 0.21305418719211822, 0.096059113300492605, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30665024630541871, 0.21428571428571427, 0.23891625615763548, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.59605911330049266, 0.6711822660098522, 0.63793103448275867], 5: [0.17980295566502463, 0.18965517241379309, 0.089901477832512317, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30665024630541871, 0.25492610837438423, 0.26600985221674878, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.58004926108374388, 0.63054187192118227, 0.6280788177339901], 5: [0.17980295566502463, 0.16502463054187191, 0.10344827586206896, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.289326 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.62808
Epoch 1, cost is  3.70954
Epoch 2, cost is  3.06304
Epoch 3, cost is  2.75082
Epoch 4, cost is  2.55548
Training took 0.175346 minutes
Weight histogram
[483 332 274 202 153 128 116 111 100 126] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
[226 112 122 142 168 198 219 245 284 309] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
-0.996808
1.70958
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.288848 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.77101
Epoch 1, cost is  3.8688
Epoch 2, cost is  3.16563
Epoch 3, cost is  2.8295
Epoch 4, cost is  2.62839
Training took 0.174408 minutes
Weight histogram
[883 678 545 397 312 272 233 224 220 286] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
[467 228 250 289 345 401 450 502 574 544] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
-0.996808
1.70958
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.54391
Epoch 1, cost is  5.32655
Epoch 2, cost is  4.23458
Epoch 3, cost is  3.72774
Epoch 4, cost is  3.43902
Training took 0.123696 minutes
Weight histogram
[242 275 192 182 181 121 144 111 176 401] [-0.0834422  -0.07512899 -0.06681578 -0.05850257 -0.05018936 -0.04187615
 -0.03356294 -0.02524973 -0.01693652 -0.00862332 -0.00031011]
[415 132 138 133 156 170 187 209 219 266] [-0.0834422  -0.07512899 -0.06681578 -0.05850257 -0.05018936 -0.04187615
 -0.03356294 -0.02524973 -0.01693652 -0.00862332 -0.00031011]
-1.37774
2.50553
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084013 minutes
Epoch 0
Fine tuning took 0.082457 minutes
Epoch 0
Fine tuning took 0.084318 minutes
{'zero': {0: [0.15517241379310345, 0.16009852216748768, 0.1145320197044335, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.6711822660098522, 0.76600985221674878, 0.72783251231527091], 5: [0.12438423645320197, 0.16871921182266009, 0.11945812807881774, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15517241379310345, 0.15517241379310345, 0.15763546798029557, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.69950738916256161, 0.74753694581280783, 0.72290640394088668], 5: [0.12438423645320197, 0.14532019704433496, 0.094827586206896547, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15517241379310345, 0.15147783251231528, 0.14778325123152711, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.70935960591133007, 0.76108374384236455, 0.74507389162561577], 5: [0.12438423645320197, 0.13916256157635468, 0.091133004926108374, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15517241379310345, 0.19088669950738915, 0.13916256157635468, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.68103448275862066, 0.7573891625615764, 0.74876847290640391], 5: [0.12438423645320197, 0.12807881773399016, 0.10344827586206896, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.290405 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.62808
Epoch 1, cost is  3.70954
Epoch 2, cost is  3.06304
Epoch 3, cost is  2.75082
Epoch 4, cost is  2.55548
Training took 0.177344 minutes
Weight histogram
[483 332 274 202 153 128 116 111 100 126] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
[226 112 122 142 168 198 219 245 284 309] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
-0.996808
1.70958
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.288915 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.77101
Epoch 1, cost is  3.8688
Epoch 2, cost is  3.16563
Epoch 3, cost is  2.8295
Epoch 4, cost is  2.62839
Training took 0.174620 minutes
Weight histogram
[883 678 545 397 312 272 233 224 220 286] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
[467 228 250 289 345 401 450 502 574 544] [-0.05949546 -0.05356827 -0.04764108 -0.04171389 -0.0357867  -0.02985951
 -0.02393232 -0.01800513 -0.01207794 -0.00615075 -0.00022356]
-0.996808
1.70958
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.45091
Epoch 1, cost is  5.01798
Epoch 2, cost is  3.56304
Epoch 3, cost is  2.92488
Epoch 4, cost is  2.57312
Training took 0.182544 minutes
Weight histogram
[339 241 214 170 149 141 119 136 459  57] [-0.05459121 -0.04916239 -0.04373356 -0.03830473 -0.0328759  -0.02744708
 -0.02201825 -0.01658942 -0.0111606  -0.00573177 -0.00030294]
[427 134 127 128 140 160 183 215 243 268] [-0.05459121 -0.04916239 -0.04373356 -0.03830473 -0.0328759  -0.02744708
 -0.02201825 -0.01658942 -0.0111606  -0.00573177 -0.00030294]
-0.984766
1.92752
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089238 minutes
Epoch 0
Fine tuning took 0.088868 minutes
Epoch 0
Fine tuning took 0.088668 minutes
{'zero': {0: [0.1354679802955665, 0.17241379310344829, 0.11330049261083744, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.70935960591133007, 0.7426108374384236, 0.76600985221674878], 5: [0.14532019704433496, 0.11822660098522167, 0.14408866995073891, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1354679802955665, 0.14285714285714285, 0.11330049261083744, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.77832512315270941, 0.71798029556650245, 0.75123152709359609], 5: [0.14532019704433496, 0.078817733990147784, 0.16871921182266009, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1354679802955665, 0.16748768472906403, 0.10960591133004927, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.7142857142857143, 0.73399014778325122, 0.75985221674876846], 5: [0.14532019704433496, 0.11822660098522167, 0.15640394088669951, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1354679802955665, 0.14655172413793102, 0.12931034482758622, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.77339901477832518, 0.70197044334975367, 0.75985221674876846], 5: [0.14532019704433496, 0.080049261083743842, 0.16871921182266009, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.289022 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.79915
Epoch 1, cost is  6.64996
Epoch 2, cost is  6.40913
Epoch 3, cost is  6.06021
Epoch 4, cost is  5.70765
Training took 0.175684 minutes
Weight histogram
[183 203 149 168 252 870  95  52  32  21] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
[819 222 157 135 125 123 117 109 108 110] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
-0.261653
0.455274
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.289257 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.807
Epoch 1, cost is  6.69224
Epoch 2, cost is  6.53639
Epoch 3, cost is  6.2619
Epoch 4, cost is  5.91215
Training took 0.177098 minutes
Weight histogram
[ 183  203  271  404  846 1742  192  103   64   42] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
[1777  479  320  278  245  235  231  216  159  110] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
-0.261653
0.455274
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  6.85226
Epoch 1, cost is  6.73827
Epoch 2, cost is  6.63108
Epoch 3, cost is  6.51152
Epoch 4, cost is  6.35453
Training took 0.102293 minutes
Weight histogram
[ 92 751 957  73  47  33  26  18  15  13] [ -7.25918217e-03  -6.53667706e-03  -5.81417195e-03  -5.09166684e-03
  -4.36916173e-03  -3.64665662e-03  -2.92415151e-03  -2.20164640e-03
  -1.47914129e-03  -7.56636179e-04  -3.41310697e-05]
[705 285 207 165 145 118 107 102 100  91] [ -7.25918217e-03  -6.53667706e-03  -5.81417195e-03  -5.09166684e-03
  -4.36916173e-03  -3.64665662e-03  -2.92415151e-03  -2.20164640e-03
  -1.47914129e-03  -7.56636179e-04  -3.41310697e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082841 minutes
Epoch 0
Fine tuning took 0.082580 minutes
Epoch 0
Fine tuning took 0.083127 minutes
{'zero': {0: [0.17857142857142858, 0.27832512315270935, 0.18226600985221675, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.48768472906403942, 0.65517241379310343, 0.46305418719211822], 5: [0.23152709359605911, 0.23399014778325122, 0.1625615763546798, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17857142857142858, 0.30172413793103448, 0.2105911330049261, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.46674876847290642, 0.64408866995073888, 0.43842364532019706], 5: [0.23152709359605911, 0.23152709359605911, 0.14532019704433496, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17857142857142858, 0.29802955665024633, 0.22044334975369459, 0.29064039408866993], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.45443349753694579, 0.63177339901477836, 0.45073891625615764], 5: [0.23152709359605911, 0.24753694581280788, 0.14778325123152711, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17857142857142858, 0.29433497536945813, 0.22167487684729065, 0.2857142857142857], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.45443349753694579, 0.60837438423645318, 0.43103448275862066], 5: [0.23152709359605911, 0.25123152709359609, 0.16995073891625614, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.291511 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.79915
Epoch 1, cost is  6.64996
Epoch 2, cost is  6.40913
Epoch 3, cost is  6.06021
Epoch 4, cost is  5.70765
Training took 0.174342 minutes
Weight histogram
[183 203 149 168 252 870  95  52  32  21] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
[819 222 157 135 125 123 117 109 108 110] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
-0.261653
0.455274
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.288531 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.807
Epoch 1, cost is  6.69224
Epoch 2, cost is  6.53639
Epoch 3, cost is  6.2619
Epoch 4, cost is  5.91215
Training took 0.176902 minutes
Weight histogram
[ 183  203  271  404  846 1742  192  103   64   42] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
[1777  479  320  278  245  235  231  216  159  110] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
-0.261653
0.455274
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.82051
Epoch 1, cost is  6.68792
Epoch 2, cost is  6.56378
Epoch 3, cost is  6.42278
Epoch 4, cost is  6.24526
Training took 0.123263 minutes
Weight histogram
[  93  118  449 1114   95   57   37   27   19   16] [ -8.75947252e-03  -7.88926397e-03  -7.01905541e-03  -6.14884686e-03
  -5.27863831e-03  -4.40842975e-03  -3.53822120e-03  -2.66801264e-03
  -1.79780409e-03  -9.27595537e-04  -5.73869838e-05]
[640 271 206 166 144 130 123 113 118 114] [ -8.75947252e-03  -7.88926397e-03  -7.01905541e-03  -6.14884686e-03
  -5.27863831e-03  -4.40842975e-03  -3.53822120e-03  -2.66801264e-03
  -1.79780409e-03  -9.27595537e-04  -5.73869838e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082887 minutes
Epoch 0
Fine tuning took 0.083392 minutes
Epoch 0
Fine tuning took 0.083045 minutes
{'zero': {0: [0.17733990147783252, 0.29926108374384236, 0.24014778325123154, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.47290640394088668, 0.63054187192118227, 0.52832512315270941], 5: [0.26354679802955666, 0.22783251231527094, 0.12931034482758622, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17733990147783252, 0.32142857142857145, 0.2413793103448276, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.46798029556650245, 0.6354679802955665, 0.51847290640394084], 5: [0.26354679802955666, 0.2105911330049261, 0.12315270935960591, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17733990147783252, 0.30049261083743845, 0.20812807881773399, 0.30788177339901479], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.45935960591133007, 0.68596059113300489, 0.51108374384236455], 5: [0.26354679802955666, 0.24014778325123154, 0.10591133004926108, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17733990147783252, 0.30049261083743845, 0.23399014778325122, 0.31527093596059114], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.47044334975369456, 0.62561576354679804, 0.48029556650246308], 5: [0.26354679802955666, 0.22906403940886699, 0.14039408866995073, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.291009 minutes
Weight histogram
[ 47 137 221 327 356 356 250 209 103  19] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
[ 77  63 103 109 155 175 227 302 318 496] [ -1.55507252e-04  -6.42013343e-05   2.71045836e-05   1.18410501e-04
   2.09716419e-04   3.01022337e-04   3.92328255e-04   4.83634173e-04
   5.74940091e-04   6.66246009e-04   7.57551927e-04]
-0.586796
0.457131
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.79915
Epoch 1, cost is  6.64996
Epoch 2, cost is  6.40913
Epoch 3, cost is  6.06021
Epoch 4, cost is  5.70765
Training took 0.194534 minutes
Weight histogram
[183 203 149 168 252 870  95  52  32  21] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
[819 222 157 135 125 123 117 109 108 110] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
-0.261653
0.455274
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.317162 minutes
Weight histogram
[ 56 276 428 675 803 746 556 346 130  34] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
[161 155 216 250 298 415 540 638 910 467] [ -1.55507252e-04  -6.21455678e-05   3.12161166e-05   1.24577801e-04
   2.17939485e-04   3.11301170e-04   4.04662854e-04   4.98024539e-04
   5.91386223e-04   6.84747908e-04   7.78109592e-04]
-0.586796
0.525278
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.807
Epoch 1, cost is  6.69224
Epoch 2, cost is  6.53639
Epoch 3, cost is  6.2619
Epoch 4, cost is  5.91215
Training took 0.194116 minutes
Weight histogram
[ 183  203  271  404  846 1742  192  103   64   42] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
[1777  479  320  278  245  235  231  216  159  110] [ -1.06642339e-02  -9.58978038e-03  -8.51532682e-03  -7.44087327e-03
  -6.36641971e-03  -5.29196615e-03  -4.21751259e-03  -3.14305904e-03
  -2.06860548e-03  -9.94151920e-04   8.03016374e-05]
-0.261653
0.455274
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.72713
Epoch 1, cost is  6.54375
Epoch 2, cost is  6.38434
Epoch 3, cost is  6.21316
Epoch 4, cost is  6.02399
Training took 0.196314 minutes
Weight histogram
[162 200 253 980 213  87  53  35  24  18] [ -1.00328391e-02  -9.03408598e-03  -8.03533282e-03  -7.03657966e-03
  -6.03782650e-03  -5.03907334e-03  -4.04032018e-03  -3.04156702e-03
  -2.04281386e-03  -1.04406070e-03  -4.53075372e-05]
[541 243 191 168 150 142 142 142 149 157] [ -1.00328391e-02  -9.03408598e-03  -8.03533282e-03  -7.03657966e-03
  -6.03782650e-03  -5.03907334e-03  -4.04032018e-03  -3.04156702e-03
  -2.04281386e-03  -1.04406070e-03  -4.53075372e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089253 minutes
Epoch 0
Fine tuning took 0.090488 minutes
Epoch 0
Fine tuning took 0.093165 minutes
{'zero': {0: [0.16871921182266009, 0.33866995073891626, 0.22783251231527094, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.42857142857142855, 0.62561576354679804, 0.54802955665024633], 5: [0.24876847290640394, 0.23275862068965517, 0.14655172413793102, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16871921182266009, 0.35960591133004927, 0.21182266009852216, 0.29187192118226601], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.41502463054187194, 0.6428571428571429, 0.50862068965517238], 5: [0.24876847290640394, 0.22536945812807882, 0.14532019704433496, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16871921182266009, 0.34236453201970446, 0.18472906403940886, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.42364532019704432, 0.66995073891625612, 0.50862068965517238], 5: [0.24876847290640394, 0.23399014778325122, 0.14532019704433496, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16871921182266009, 0.33251231527093594, 0.20320197044334976, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.43596059113300495, 0.63300492610837433, 0.51108374384236455], 5: [0.24876847290640394, 0.23152709359605911, 0.16379310344827586, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.259523 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46422
Epoch 1, cost is  2.40335
Epoch 2, cost is  2.45424
Epoch 3, cost is  2.52255
Epoch 4, cost is  2.60609
Training took 0.159326 minutes
Weight histogram
[792 808 460 701 509 371 213 102  48  46] [-0.21838783 -0.19687526 -0.17536269 -0.15385012 -0.13233755 -0.11082498
 -0.08931242 -0.06779985 -0.04628728 -0.02477471 -0.00326214]
[101 154 244 328 401 472 575 586 589 600] [-0.21838783 -0.19687526 -0.17536269 -0.15385012 -0.13233755 -0.11082498
 -0.08931242 -0.06779985 -0.04628728 -0.02477471 -0.00326214]
-7.27096
11.005
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235008 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.47677
Epoch 1, cost is  2.41059
Epoch 2, cost is  2.45771
Epoch 3, cost is  2.52444
Epoch 4, cost is  2.60089
Training took 0.159994 minutes
Weight histogram
[ 771  789  461 1179 1120  783  519  246  112   95] [-0.22774385 -0.20529568 -0.18284751 -0.16039934 -0.13795117 -0.115503
 -0.09305482 -0.07060665 -0.04815848 -0.02571031 -0.00326214]
[207 319 491 646 820 970 795 609 605 613] [-0.22774385 -0.20529568 -0.18284751 -0.16039934 -0.13795117 -0.115503
 -0.09305482 -0.07060665 -0.04815848 -0.02571031 -0.00326214]
-8.08871
8.8581
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.5426
Epoch 1, cost is  3.77643
Epoch 2, cost is  4.03784
Epoch 3, cost is  4.42496
Epoch 4, cost is  4.83303
Training took 0.114529 minutes
Weight histogram
[627 707 622 626 599 285 229 138  80 137] [-0.31483021 -0.28363975 -0.25244929 -0.22125883 -0.19006837 -0.15887792
 -0.12768746 -0.096497   -0.06530654 -0.03411608 -0.00292562]
[204 202 265 356 427 482 493 512 544 565] [-0.31483021 -0.28363975 -0.25244929 -0.22125883 -0.19006837 -0.15887792
 -0.12768746 -0.096497   -0.06530654 -0.03411608 -0.00292562]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.085721 minutes
Epoch 0
Fine tuning took 0.084225 minutes
Epoch 0
Fine tuning took 0.083003 minutes
{'zero': {0: [0.20443349753694581, 0.16379310344827586, 0.094827586206896547, 0.082512315270935957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.73891625615763545, 0.78694581280788178, 0.82019704433497542], 5: [0.080049261083743842, 0.097290640394088676, 0.11822660098522167, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20443349753694581, 0.13916256157635468, 0.14532019704433496, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.76970443349753692, 0.77832512315270941, 0.70197044334975367], 5: [0.080049261083743842, 0.091133004926108374, 0.076354679802955669, 0.087438423645320201], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20443349753694581, 0.19827586206896552, 0.17857142857142858, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.70073891625615758, 0.74014778325123154, 0.63916256157635465], 5: [0.080049261083743842, 0.10098522167487685, 0.081280788177339899, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20443349753694581, 0.15763546798029557, 0.18472906403940886, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.7857142857142857, 0.76231527093596063, 0.73522167487684731], 5: [0.080049261083743842, 0.056650246305418719, 0.05295566502463054, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234979 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46422
Epoch 1, cost is  2.40335
Epoch 2, cost is  2.45424
Epoch 3, cost is  2.52255
Epoch 4, cost is  2.60609
Training took 0.158388 minutes
Weight histogram
[792 808 460 701 509 371 213 102  48  46] [-0.21838783 -0.19687526 -0.17536269 -0.15385012 -0.13233755 -0.11082498
 -0.08931242 -0.06779985 -0.04628728 -0.02477471 -0.00326214]
[101 154 244 328 401 472 575 586 589 600] [-0.21838783 -0.19687526 -0.17536269 -0.15385012 -0.13233755 -0.11082498
 -0.08931242 -0.06779985 -0.04628728 -0.02477471 -0.00326214]
-7.27096
11.005
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234167 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.47677
Epoch 1, cost is  2.41059
Epoch 2, cost is  2.45771
Epoch 3, cost is  2.52444
Epoch 4, cost is  2.60089
Training took 0.159815 minutes
Weight histogram
[ 771  789  461 1179 1120  783  519  246  112   95] [-0.22774385 -0.20529568 -0.18284751 -0.16039934 -0.13795117 -0.115503
 -0.09305482 -0.07060665 -0.04815848 -0.02571031 -0.00326214]
[207 319 491 646 820 970 795 609 605 613] [-0.22774385 -0.20529568 -0.18284751 -0.16039934 -0.13795117 -0.115503
 -0.09305482 -0.07060665 -0.04815848 -0.02571031 -0.00326214]
-8.08871
8.8581
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.14972
Epoch 1, cost is  3.17527
Epoch 2, cost is  3.29957
Epoch 3, cost is  3.56873
Epoch 4, cost is  3.90898
Training took 0.122680 minutes
Weight histogram
[599 634 590 595 506 414 310 168  82 152] [-0.27973735 -0.252052   -0.22436665 -0.1966813  -0.16899595 -0.1413106
 -0.11362525 -0.0859399  -0.05825455 -0.03056919 -0.00288384]
[206 210 302 359 421 472 505 527 511 537] [-0.27973735 -0.252052   -0.22436665 -0.1966813  -0.16899595 -0.1413106
 -0.11362525 -0.0859399  -0.05825455 -0.03056919 -0.00288384]
-8.4687
10.8052
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083114 minutes
Epoch 0
Fine tuning took 0.083182 minutes
Epoch 0
Fine tuning took 0.083113 minutes
{'zero': {0: [0.16995073891625614, 0.35714285714285715, 0.19827586206896552, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.58128078817733986, 0.71674876847290636, 0.63177339901477836], 5: [0.083743842364532015, 0.061576354679802957, 0.084975369458128072, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16995073891625614, 0.1268472906403941, 0.14778325123152711, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.77955665024630538, 0.72906403940886699, 0.69088669950738912], 5: [0.083743842364532015, 0.093596059113300489, 0.12315270935960591, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16995073891625614, 0.13916256157635468, 0.14778325123152711, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.70566502463054193, 0.72536945812807885, 0.64162561576354682], 5: [0.083743842364532015, 0.15517241379310345, 0.1268472906403941, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16995073891625614, 0.081280788177339899, 0.099753694581280791, 0.088669950738916259], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.83128078817733986, 0.80295566502463056, 0.77709359605911332], 5: [0.083743842364532015, 0.087438423645320201, 0.097290640394088676, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234923 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.46422
Epoch 1, cost is  2.40335
Epoch 2, cost is  2.45424
Epoch 3, cost is  2.52255
Epoch 4, cost is  2.60609
Training took 0.159051 minutes
Weight histogram
[792 808 460 701 509 371 213 102  48  46] [-0.21838783 -0.19687526 -0.17536269 -0.15385012 -0.13233755 -0.11082498
 -0.08931242 -0.06779985 -0.04628728 -0.02477471 -0.00326214]
[101 154 244 328 401 472 575 586 589 600] [-0.21838783 -0.19687526 -0.17536269 -0.15385012 -0.13233755 -0.11082498
 -0.08931242 -0.06779985 -0.04628728 -0.02477471 -0.00326214]
-7.27096
11.005
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233919 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.47677
Epoch 1, cost is  2.41059
Epoch 2, cost is  2.45771
Epoch 3, cost is  2.52444
Epoch 4, cost is  2.60089
Training took 0.159867 minutes
Weight histogram
[ 771  789  461 1179 1120  783  519  246  112   95] [-0.22774385 -0.20529568 -0.18284751 -0.16039934 -0.13795117 -0.115503
 -0.09305482 -0.07060665 -0.04815848 -0.02571031 -0.00326214]
[207 319 491 646 820 970 795 609 605 613] [-0.22774385 -0.20529568 -0.18284751 -0.16039934 -0.13795117 -0.115503
 -0.09305482 -0.07060665 -0.04815848 -0.02571031 -0.00326214]
-8.08871
8.8581
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.64041
Epoch 1, cost is  2.25616
Epoch 2, cost is  2.15658
Epoch 3, cost is  2.21441
Epoch 4, cost is  2.35011
Training took 0.179383 minutes
Weight histogram
[579 652 638 582 490 401 295 155 104 154] [-0.18797739 -0.16947049 -0.1509636  -0.1324567  -0.11394981 -0.09544291
 -0.07693601 -0.05842912 -0.03992222 -0.02141533 -0.00290843]
[219 200 286 363 423 466 495 516 553 529] [-0.18797739 -0.16947049 -0.1509636  -0.1324567  -0.11394981 -0.09544291
 -0.07693601 -0.05842912 -0.03992222 -0.02141533 -0.00290843]
-6.41529
7.35325
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090007 minutes
Epoch 0
Fine tuning took 0.090244 minutes
Epoch 0
Fine tuning took 0.089362 minutes
{'zero': {0: [0.16133004926108374, 0.11699507389162561, 0.10221674876847291, 0.087438423645320201], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.72290640394088668, 0.75985221674876846, 0.81527093596059108], 5: [0.10467980295566502, 0.16009852216748768, 0.13793103448275862, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.15024630541871922, 0.14901477832512317, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.71059113300492616, 0.65517241379310343, 0.64532019704433496], 5: [0.10467980295566502, 0.13916256157635468, 0.19581280788177341, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.16133004926108374, 0.1748768472906404, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.69581280788177335, 0.66995073891625612, 0.63300492610837433], 5: [0.10467980295566502, 0.14285714285714285, 0.15517241379310345, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.13669950738916256, 0.18226600985221675, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.77216748768472909, 0.72660098522167482, 0.68596059113300489], 5: [0.10467980295566502, 0.091133004926108374, 0.091133004926108374, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235058 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.38794
Epoch 1, cost is  2.25095
Epoch 2, cost is  2.16081
Epoch 3, cost is  2.09359
Epoch 4, cost is  2.04414
Training took 0.158979 minutes
Weight histogram
[1089  813  512  445  338  209  192  149  147  156] [-0.07861641 -0.07077713 -0.06293784 -0.05509855 -0.04725927 -0.03941998
 -0.0315807  -0.02374141 -0.01590213 -0.00806284 -0.00022356]
[284 180 228 288 342 420 511 551 600 646] [-0.07861641 -0.07077713 -0.06293784 -0.05509855 -0.04725927 -0.03941998
 -0.0315807  -0.02374141 -0.01590213 -0.00806284 -0.00022356]
-1.49784
2.40918
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234660 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42253
Epoch 1, cost is  2.27537
Epoch 2, cost is  2.18014
Epoch 3, cost is  2.11046
Epoch 4, cost is  2.05572
Training took 0.160725 minutes
Weight histogram
[1041  792  851  914  687  452  376  306  296  360] [-0.07905222 -0.07116935 -0.06328649 -0.05540362 -0.04752075 -0.03963789
 -0.03175502 -0.02387216 -0.01598929 -0.00810642 -0.00022356]
[587 375 473 600 724 865 695 536 588 632] [-0.07905222 -0.07116935 -0.06328649 -0.05540362 -0.04752075 -0.03963789
 -0.03175502 -0.02387216 -0.01598929 -0.00810642 -0.00022356]
-1.67825
2.03297
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.58931
Epoch 1, cost is  5.52814
Epoch 2, cost is  4.73844
Epoch 3, cost is  4.36811
Epoch 4, cost is  4.14717
Training took 0.106230 minutes
Weight histogram
[458 516 420 384 349 283 237 235 295 873] [-0.10474743 -0.09430067 -0.08385391 -0.07340715 -0.06296039 -0.05251362
 -0.04206686 -0.0316201  -0.02117334 -0.01072658 -0.00027982]
[799 259 263 286 326 346 381 420 447 523] [-0.10474743 -0.09430067 -0.08385391 -0.07340715 -0.06296039 -0.05251362
 -0.04206686 -0.0316201  -0.02117334 -0.01072658 -0.00027982]
-1.53641
3.25821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080569 minutes
Epoch 0
Fine tuning took 0.081155 minutes
Epoch 0
Fine tuning took 0.082172 minutes
{'zero': {0: [0.22413793103448276, 0.18103448275862069, 0.18842364532019704, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54187192118226601, 0.69581280788177335, 0.69088669950738912, 0.78817733990147787], 5: [0.23399014778325122, 0.12315270935960591, 0.1206896551724138, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22413793103448276, 0.25246305418719212, 0.21182266009852216, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54187192118226601, 0.62068965517241381, 0.66748768472906406, 0.8214285714285714], 5: [0.23399014778325122, 0.1268472906403941, 0.1206896551724138, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22413793103448276, 0.22906403940886699, 0.2019704433497537, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54187192118226601, 0.60344827586206895, 0.66625615763546797, 0.83620689655172409], 5: [0.23399014778325122, 0.16748768472906403, 0.13177339901477833, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22413793103448276, 0.25246305418719212, 0.25985221674876846, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54187192118226601, 0.58620689655172409, 0.62438423645320196, 0.81650246305418717], 5: [0.23399014778325122, 0.16133004926108374, 0.11576354679802955, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234512 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.38794
Epoch 1, cost is  2.25095
Epoch 2, cost is  2.16081
Epoch 3, cost is  2.09359
Epoch 4, cost is  2.04414
Training took 0.159326 minutes
Weight histogram
[1089  813  512  445  338  209  192  149  147  156] [-0.07861641 -0.07077713 -0.06293784 -0.05509855 -0.04725927 -0.03941998
 -0.0315807  -0.02374141 -0.01590213 -0.00806284 -0.00022356]
[284 180 228 288 342 420 511 551 600 646] [-0.07861641 -0.07077713 -0.06293784 -0.05509855 -0.04725927 -0.03941998
 -0.0315807  -0.02374141 -0.01590213 -0.00806284 -0.00022356]
-1.49784
2.40918
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234408 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42253
Epoch 1, cost is  2.27537
Epoch 2, cost is  2.18014
Epoch 3, cost is  2.11046
Epoch 4, cost is  2.05572
Training took 0.160872 minutes
Weight histogram
[1041  792  851  914  687  452  376  306  296  360] [-0.07905222 -0.07116935 -0.06328649 -0.05540362 -0.04752075 -0.03963789
 -0.03175502 -0.02387216 -0.01598929 -0.00810642 -0.00022356]
[587 375 473 600 724 865 695 536 588 632] [-0.07905222 -0.07116935 -0.06328649 -0.05540362 -0.04752075 -0.03963789
 -0.03175502 -0.02387216 -0.01598929 -0.00810642 -0.00022356]
-1.67825
2.03297
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55001
Epoch 1, cost is  5.32073
Epoch 2, cost is  4.26196
Epoch 3, cost is  3.79076
Epoch 4, cost is  3.50196
Training took 0.125583 minutes
Weight histogram
[382 546 442 389 363 258 267 241 366 796] [-0.0834422  -0.07512756 -0.06681292 -0.05849828 -0.05018364 -0.041869
 -0.03355437 -0.02523973 -0.01692509 -0.00861045 -0.00029581]
[814 271 272 266 314 347 377 425 450 514] [-0.0834422  -0.07512756 -0.06681292 -0.05849828 -0.05018364 -0.041869
 -0.03355437 -0.02523973 -0.01692509 -0.00861045 -0.00029581]
-1.37774
2.55887
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082893 minutes
Epoch 0
Fine tuning took 0.083514 minutes
Epoch 0
Fine tuning took 0.083017 minutes
{'zero': {0: [0.15763546798029557, 0.19827586206896552, 0.14285714285714285, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.64778325123152714, 0.71551724137931039, 0.77832512315270941], 5: [0.11206896551724138, 0.1539408866995074, 0.14162561576354679, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.19581280788177341, 0.20566502463054187, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.68349753694581283, 0.66133004926108374, 0.76600985221674878], 5: [0.11206896551724138, 0.1206896551724138, 0.13300492610837439, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.18349753694581281, 0.10344827586206896, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.71059113300492616, 0.76231527093596063, 0.79433497536945807], 5: [0.11206896551724138, 0.10591133004926108, 0.13423645320197045, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.20073891625615764, 0.15763546798029557, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.68965517241379315, 0.70443349753694584, 0.72783251231527091], 5: [0.11206896551724138, 0.10960591133004927, 0.13793103448275862, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234203 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.38794
Epoch 1, cost is  2.25095
Epoch 2, cost is  2.16081
Epoch 3, cost is  2.09359
Epoch 4, cost is  2.04414
Training took 0.159573 minutes
Weight histogram
[1089  813  512  445  338  209  192  149  147  156] [-0.07861641 -0.07077713 -0.06293784 -0.05509855 -0.04725927 -0.03941998
 -0.0315807  -0.02374141 -0.01590213 -0.00806284 -0.00022356]
[284 180 228 288 342 420 511 551 600 646] [-0.07861641 -0.07077713 -0.06293784 -0.05509855 -0.04725927 -0.03941998
 -0.0315807  -0.02374141 -0.01590213 -0.00806284 -0.00022356]
-1.49784
2.40918
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234739 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42253
Epoch 1, cost is  2.27537
Epoch 2, cost is  2.18014
Epoch 3, cost is  2.11046
Epoch 4, cost is  2.05572
Training took 0.160021 minutes
Weight histogram
[1041  792  851  914  687  452  376  306  296  360] [-0.07905222 -0.07116935 -0.06328649 -0.05540362 -0.04752075 -0.03963789
 -0.03175502 -0.02387216 -0.01598929 -0.00810642 -0.00022356]
[587 375 473 600 724 865 695 536 588 632] [-0.07905222 -0.07116935 -0.06328649 -0.05540362 -0.04752075 -0.03963789
 -0.03175502 -0.02387216 -0.01598929 -0.00810642 -0.00022356]
-1.67825
2.03297
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46048
Epoch 1, cost is  5.05032
Epoch 2, cost is  3.67881
Epoch 3, cost is  3.04973
Epoch 4, cost is  2.68176
Training took 0.179701 minutes
Weight histogram
[537 533 448 366 323 297 241 271 873 161] [-0.05459121 -0.04916124 -0.04373126 -0.03830129 -0.03287131 -0.02744134
 -0.02201136 -0.01658139 -0.01115141 -0.00572144 -0.00029147]
[842 269 255 263 288 324 371 427 486 525] [-0.05459121 -0.04916124 -0.04373126 -0.03830129 -0.03287131 -0.02744134
 -0.02201136 -0.01658139 -0.01115141 -0.00572144 -0.00029147]
-0.999991
1.92752
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088520 minutes
Epoch 0
Fine tuning took 0.088992 minutes
Epoch 0
Fine tuning took 0.089314 minutes
{'zero': {0: [0.14778325123152711, 0.13054187192118227, 0.14285714285714285, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75862068965517238, 0.76970443349753692, 0.73645320197044339, 0.75862068965517238], 5: [0.093596059113300489, 0.099753694581280791, 0.1206896551724138, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14778325123152711, 0.1625615763546798, 0.14162561576354679, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75862068965517238, 0.77832512315270941, 0.75492610837438423, 0.71551724137931039], 5: [0.093596059113300489, 0.059113300492610835, 0.10344827586206896, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14778325123152711, 0.14285714285714285, 0.14285714285714285, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75862068965517238, 0.77463054187192115, 0.75492610837438423, 0.72413793103448276], 5: [0.093596059113300489, 0.082512315270935957, 0.10221674876847291, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14778325123152711, 0.15147783251231528, 0.15147783251231528, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75862068965517238, 0.7857142857142857, 0.72536945812807885, 0.74384236453201968], 5: [0.093596059113300489, 0.062807881773399021, 0.12315270935960591, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.250182 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34107
Epoch 1, cost is  5.01136
Epoch 2, cost is  4.71897
Epoch 3, cost is  4.47777
Epoch 4, cost is  4.27554
Training took 0.180392 minutes
Weight histogram
[ 388  388  400  388  387  375  348 1137  181   58] [ -2.26953253e-02  -2.04177626e-02  -1.81401999e-02  -1.58626372e-02
  -1.35850745e-02  -1.13075118e-02  -9.02994914e-03  -6.75238645e-03
  -4.47482375e-03  -2.19726106e-03   8.03016374e-05]
[1199  384  335  319  300  289  290  297  312  325] [ -2.26953253e-02  -2.04177626e-02  -1.81401999e-02  -1.58626372e-02
  -1.35850745e-02  -1.13075118e-02  -9.02994914e-03  -6.75238645e-03
  -4.47482375e-03  -2.19726106e-03   8.03016374e-05]
-0.437315
0.802743
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.258781 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.54375
Epoch 1, cost is  5.21689
Epoch 2, cost is  4.93655
Epoch 3, cost is  4.69446
Epoch 4, cost is  4.48936
Training took 0.179160 minutes
Weight histogram
[ 352  343  360  354  525  644  741 2433  229   94] [ -1.96170993e-02  -1.76473592e-02  -1.56776191e-02  -1.37078790e-02
  -1.17381389e-02  -9.76839884e-03  -7.79865874e-03  -5.82891865e-03
  -3.85917855e-03  -1.88943846e-03   8.03016374e-05]
[2527  729  630  460  278  274  286  279  297  315] [ -1.96170993e-02  -1.76473592e-02  -1.56776191e-02  -1.37078790e-02
  -1.17381389e-02  -9.76839884e-03  -7.79865874e-03  -5.82891865e-03
  -3.85917855e-03  -1.88943846e-03   8.03016374e-05]
-0.401421
0.547953
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.87142
Epoch 1, cost is  6.78883
Epoch 2, cost is  6.71401
Epoch 3, cost is  6.64077
Epoch 4, cost is  6.56358
Training took 0.113781 minutes
Weight histogram
[  92 1768 1683  170  108   74   56   40   31   28] [ -7.25918217e-03  -6.53627398e-03  -5.81336580e-03  -5.09045762e-03
  -4.36754944e-03  -3.64464126e-03  -2.92173307e-03  -2.19882489e-03
  -1.47591671e-03  -7.53008526e-04  -3.01003438e-05]
[1871  740  524  252  145  118  107  102  100   91] [ -7.25918217e-03  -6.53627398e-03  -5.81336580e-03  -5.09045762e-03
  -4.36754944e-03  -3.64464126e-03  -2.92173307e-03  -2.19882489e-03
  -1.47591671e-03  -7.53008526e-04  -3.01003438e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081412 minutes
Epoch 0
Fine tuning took 0.083187 minutes
Epoch 0
Fine tuning took 0.084067 minutes
{'zero': {0: [0.097290640394088676, 0.23029556650246305, 0.15886699507389163, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.62931034482758619, 0.59113300492610843, 0.43596059113300495], 5: [0.3682266009852217, 0.14039408866995073, 0.25, 0.32635467980295568], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.097290640394088676, 0.21305418719211822, 0.13916256157635468, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.64901477832512311, 0.60837438423645318, 0.43226600985221675], 5: [0.3682266009852217, 0.13793103448275862, 0.25246305418719212, 0.37807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.097290640394088676, 0.19704433497536947, 0.16625615763546797, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.66995073891625612, 0.57266009852216748, 0.42610837438423643], 5: [0.3682266009852217, 0.13300492610837439, 0.26108374384236455, 0.36699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.097290640394088676, 0.22413793103448276, 0.15147783251231528, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.64408866995073888, 0.62068965517241381, 0.45443349753694579], 5: [0.3682266009852217, 0.13177339901477833, 0.22783251231527094, 0.34236453201970446], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236376 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34107
Epoch 1, cost is  5.01136
Epoch 2, cost is  4.71897
Epoch 3, cost is  4.47777
Epoch 4, cost is  4.27554
Training took 0.161124 minutes
Weight histogram
[ 388  388  400  388  387  375  348 1137  181   58] [ -2.26953253e-02  -2.04177626e-02  -1.81401999e-02  -1.58626372e-02
  -1.35850745e-02  -1.13075118e-02  -9.02994914e-03  -6.75238645e-03
  -4.47482375e-03  -2.19726106e-03   8.03016374e-05]
[1199  384  335  319  300  289  290  297  312  325] [ -2.26953253e-02  -2.04177626e-02  -1.81401999e-02  -1.58626372e-02
  -1.35850745e-02  -1.13075118e-02  -9.02994914e-03  -6.75238645e-03
  -4.47482375e-03  -2.19726106e-03   8.03016374e-05]
-0.437315
0.802743
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.251400 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.54375
Epoch 1, cost is  5.21689
Epoch 2, cost is  4.93655
Epoch 3, cost is  4.69446
Epoch 4, cost is  4.48936
Training took 0.177869 minutes
Weight histogram
[ 352  343  360  354  525  644  741 2433  229   94] [ -1.96170993e-02  -1.76473592e-02  -1.56776191e-02  -1.37078790e-02
  -1.17381389e-02  -9.76839884e-03  -7.79865874e-03  -5.82891865e-03
  -3.85917855e-03  -1.88943846e-03   8.03016374e-05]
[2527  729  630  460  278  274  286  279  297  315] [ -1.96170993e-02  -1.76473592e-02  -1.56776191e-02  -1.37078790e-02
  -1.17381389e-02  -9.76839884e-03  -7.79865874e-03  -5.82891865e-03
  -3.85917855e-03  -1.88943846e-03   8.03016374e-05]
-0.401421
0.547953
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.84917
Epoch 1, cost is  6.75689
Epoch 2, cost is  6.67752
Epoch 3, cost is  6.60003
Epoch 4, cost is  6.51606
Training took 0.134143 minutes
Weight histogram
[  93  119  449 2819  227  128   83   58   40   34] [ -8.75947252e-03  -7.88876204e-03  -7.01805155e-03  -6.14734107e-03
  -5.27663058e-03  -4.40592010e-03  -3.53520962e-03  -2.66449913e-03
  -1.79378865e-03  -9.23078166e-04  -5.23676827e-05]
[1714  711  520  363  144  130  123  113  118  114] [ -8.75947252e-03  -7.88876204e-03  -7.01805155e-03  -6.14734107e-03
  -5.27663058e-03  -4.40592010e-03  -3.53520962e-03  -2.66449913e-03
  -1.79378865e-03  -9.23078166e-04  -5.23676827e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087688 minutes
Epoch 0
Fine tuning took 0.082943 minutes
Epoch 0
Fine tuning took 0.083250 minutes
{'zero': {0: [0.10960591133004927, 0.23522167487684728, 0.10714285714285714, 0.29556650246305421], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.65640394088669951, 0.59482758620689657, 0.36945812807881773], 5: [0.34359605911330049, 0.10837438423645321, 0.29802955665024633, 0.33497536945812806], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10960591133004927, 0.24014778325123154, 0.098522167487684734, 0.28448275862068967], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.67364532019704437, 0.60344827586206895, 0.36576354679802958], 5: [0.34359605911330049, 0.086206896551724144, 0.29802955665024633, 0.34975369458128081], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10960591133004927, 0.25738916256157635, 0.099753694581280791, 0.28078817733990147], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.66256157635467983, 0.58004926108374388, 0.32758620689655171], 5: [0.34359605911330049, 0.080049261083743842, 0.32019704433497537, 0.39162561576354682], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10960591133004927, 0.24261083743842365, 0.097290640394088676, 0.31896551724137934], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.65886699507389157, 0.62684729064039413, 0.3608374384236453], 5: [0.34359605911330049, 0.098522167487684734, 0.27586206896551724, 0.32019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.254712 minutes
Weight histogram
[ 87 246 427 497 404 422 662 834 395  76] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 97 119 163 222 310 441 571 551 702 874] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-0.763917
0.658344
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34107
Epoch 1, cost is  5.01136
Epoch 2, cost is  4.71897
Epoch 3, cost is  4.47777
Epoch 4, cost is  4.27554
Training took 0.178151 minutes
Weight histogram
[ 388  388  400  388  387  375  348 1137  181   58] [ -2.26953253e-02  -2.04177626e-02  -1.81401999e-02  -1.58626372e-02
  -1.35850745e-02  -1.13075118e-02  -9.02994914e-03  -6.75238645e-03
  -4.47482375e-03  -2.19726106e-03   8.03016374e-05]
[1199  384  335  319  300  289  290  297  312  325] [ -2.26953253e-02  -2.04177626e-02  -1.81401999e-02  -1.58626372e-02
  -1.35850745e-02  -1.13075118e-02  -9.02994914e-03  -6.75238645e-03
  -4.47482375e-03  -2.19726106e-03   8.03016374e-05]
-0.437315
0.802743
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264873 minutes
Weight histogram
[101 381 654 911 916 786 902 890 487  47] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
[ 206  266  328  450  639  875 1099  548  724  940] [ -1.55507252e-04  -4.43150129e-05   6.68772263e-05   1.78069466e-04
   2.89261705e-04   4.00453944e-04   5.11646183e-04   6.22838423e-04
   7.34030662e-04   8.45222901e-04   9.56415141e-04]
-0.768339
0.702152
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.54375
Epoch 1, cost is  5.21689
Epoch 2, cost is  4.93655
Epoch 3, cost is  4.69446
Epoch 4, cost is  4.48936
Training took 0.178052 minutes
Weight histogram
[ 352  343  360  354  525  644  741 2433  229   94] [ -1.96170993e-02  -1.76473592e-02  -1.56776191e-02  -1.37078790e-02
  -1.17381389e-02  -9.76839884e-03  -7.79865874e-03  -5.82891865e-03
  -3.85917855e-03  -1.88943846e-03   8.03016374e-05]
[2527  729  630  460  278  274  286  279  297  315] [ -1.96170993e-02  -1.76473592e-02  -1.56776191e-02  -1.37078790e-02
  -1.17381389e-02  -9.76839884e-03  -7.79865874e-03  -5.82891865e-03
  -3.85917855e-03  -1.88943846e-03   8.03016374e-05]
-0.401421
0.547953
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78219
Epoch 1, cost is  6.66495
Epoch 2, cost is  6.57326
Epoch 3, cost is  6.48364
Epoch 4, cost is  6.38804
Training took 0.196555 minutes
Weight histogram
[ 162  200  253 2374  571  204  120   76   51   39] [ -1.00328391e-02  -9.03362027e-03  -8.03440140e-03  -7.03518252e-03
  -6.03596365e-03  -5.03674478e-03  -4.03752591e-03  -3.03830704e-03
  -2.03908816e-03  -1.03986929e-03  -4.06504187e-05]
[1454  651  509  424  280  142  142  142  149  157] [ -1.00328391e-02  -9.03362027e-03  -8.03440140e-03  -7.03518252e-03
  -6.03596365e-03  -5.03674478e-03  -4.03752591e-03  -3.03830704e-03
  -2.03908816e-03  -1.03986929e-03  -4.06504187e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087881 minutes
Epoch 0
Fine tuning took 0.088454 minutes
Epoch 0
Fine tuning took 0.088111 minutes
{'zero': {0: [0.11945812807881774, 0.18103448275862069, 0.20689655172413793, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53325123152709364, 0.65270935960591137, 0.62315270935960587, 0.49753694581280788], 5: [0.34729064039408869, 0.16625615763546797, 0.16995073891625614, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11945812807881774, 0.15517241379310345, 0.18596059113300492, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53325123152709364, 0.6785714285714286, 0.64039408866995073, 0.47783251231527096], 5: [0.34729064039408869, 0.16625615763546797, 0.17364532019704434, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11945812807881774, 0.18965517241379309, 0.19581280788177341, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53325123152709364, 0.64408866995073888, 0.58990147783251234, 0.45812807881773399], 5: [0.34729064039408869, 0.16625615763546797, 0.21428571428571427, 0.29310344827586204], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11945812807881774, 0.15517241379310345, 0.19704433497536947, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53325123152709364, 0.68965517241379315, 0.63423645320197042, 0.49753694581280788], 5: [0.34729064039408869, 0.15517241379310345, 0.16871921182266009, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234922 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.00598
Epoch 1, cost is  2.88247
Epoch 2, cost is  2.91557
Epoch 3, cost is  2.97871
Epoch 4, cost is  3.05718
Training took 0.158260 minutes
Weight histogram
[ 926  865  369 1123  809  933  563  316  109   62] [-0.30425924 -0.27415953 -0.24405982 -0.21396011 -0.1838604  -0.15376069
 -0.12366098 -0.09356127 -0.06346156 -0.03336185 -0.00326214]
[144 276 415 556 708 794 792 825 781 784] [-0.30425924 -0.27415953 -0.24405982 -0.21396011 -0.1838604  -0.15376069
 -0.12366098 -0.09356127 -0.06346156 -0.03336185 -0.00326214]
-10.3072
17.9116
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236158 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.06073
Epoch 1, cost is  2.91198
Epoch 2, cost is  2.92895
Epoch 3, cost is  2.98493
Epoch 4, cost is  3.05288
Training took 0.158619 minutes
Weight histogram
[ 981  875  527 1041  715 1728 1197  675  230  131] [-0.30905092 -0.27847204 -0.24789316 -0.21731428 -0.18673541 -0.15615653
 -0.12557765 -0.09499877 -0.0644199  -0.03384102 -0.00326214]
[ 298  544  825 1130 1229  838  808  837  800  791] [-0.30905092 -0.27847204 -0.24789316 -0.21731428 -0.18673541 -0.15615653
 -0.12557765 -0.09499877 -0.0644199  -0.03384102 -0.00326214]
-8.57089
11.365
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.56116
Epoch 1, cost is  3.82924
Epoch 2, cost is  4.05449
Epoch 3, cost is  4.50711
Epoch 4, cost is  4.93556
Training took 0.103758 minutes
Weight histogram
[ 847 1104  968  936  887  440  353  211  128  201] [-0.31483021 -0.28363837 -0.25244653 -0.22125469 -0.19006285 -0.15887101
 -0.12767917 -0.09648733 -0.06529549 -0.03410365 -0.00291181]
[306 302 397 535 642 732 739 780 822 820] [-0.31483021 -0.28363837 -0.25244653 -0.22125469 -0.19006285 -0.15887101
 -0.12767917 -0.09648733 -0.06529549 -0.03410365 -0.00291181]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083333 minutes
Epoch 0
Fine tuning took 0.081747 minutes
Epoch 0
Fine tuning took 0.082358 minutes
{'zero': {0: [0.21551724137931033, 0.15517241379310345, 0.21182266009852216, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.78201970443349755, 0.75369458128078815, 0.83004926108374388], 5: [0.16009852216748768, 0.062807881773399021, 0.034482758620689655, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21551724137931033, 0.2229064039408867, 0.24384236453201971, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.6785714285714286, 0.65517241379310343, 0.61945812807881773], 5: [0.16009852216748768, 0.098522167487684734, 0.10098522167487685, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21551724137931033, 0.23399014778325122, 0.24630541871921183, 0.3251231527093596], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.6280788177339901, 0.61206896551724133, 0.54926108374384242], 5: [0.16009852216748768, 0.13793103448275862, 0.14162561576354679, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21551724137931033, 0.2229064039408867, 0.25985221674876846, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.70197044334975367, 0.65024630541871919, 0.65886699507389157], 5: [0.16009852216748768, 0.075123152709359611, 0.089901477832512317, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235226 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.00598
Epoch 1, cost is  2.88247
Epoch 2, cost is  2.91557
Epoch 3, cost is  2.97871
Epoch 4, cost is  3.05718
Training took 0.160095 minutes
Weight histogram
[ 926  865  369 1123  809  933  563  316  109   62] [-0.30425924 -0.27415953 -0.24405982 -0.21396011 -0.1838604  -0.15376069
 -0.12366098 -0.09356127 -0.06346156 -0.03336185 -0.00326214]
[144 276 415 556 708 794 792 825 781 784] [-0.30425924 -0.27415953 -0.24405982 -0.21396011 -0.1838604  -0.15376069
 -0.12366098 -0.09356127 -0.06346156 -0.03336185 -0.00326214]
-10.3072
17.9116
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234703 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.06073
Epoch 1, cost is  2.91198
Epoch 2, cost is  2.92895
Epoch 3, cost is  2.98493
Epoch 4, cost is  3.05288
Training took 0.160999 minutes
Weight histogram
[ 981  875  527 1041  715 1728 1197  675  230  131] [-0.30905092 -0.27847204 -0.24789316 -0.21731428 -0.18673541 -0.15615653
 -0.12557765 -0.09499877 -0.0644199  -0.03384102 -0.00326214]
[ 298  544  825 1130 1229  838  808  837  800  791] [-0.30905092 -0.27847204 -0.24789316 -0.21731428 -0.18673541 -0.15615653
 -0.12557765 -0.09499877 -0.0644199  -0.03384102 -0.00326214]
-8.57089
11.365
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.16858
Epoch 1, cost is  3.22789
Epoch 2, cost is  3.37586
Epoch 3, cost is  3.64149
Epoch 4, cost is  4.02673
Training took 0.122728 minutes
Weight histogram
[786 954 930 891 781 653 473 255 124 228] [-0.27973735 -0.2520504  -0.22436344 -0.19667649 -0.16898953 -0.14130258
 -0.11361562 -0.08592866 -0.05824171 -0.03055475 -0.0028678 ]
[308 315 454 538 637 705 764 785 768 801] [-0.27973735 -0.2520504  -0.22436344 -0.19667649 -0.16898953 -0.14130258
 -0.11361562 -0.08592866 -0.05824171 -0.03055475 -0.0028678 ]
-8.4687
10.8052
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083339 minutes
Epoch 0
Fine tuning took 0.085476 minutes
Epoch 0
Fine tuning took 0.084077 minutes
{'zero': {0: [0.20320197044334976, 0.14162561576354679, 0.10714285714285714, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.79926108374384242, 0.83004926108374388, 0.82758620689655171], 5: [0.16871921182266009, 0.059113300492610835, 0.062807881773399021, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20320197044334976, 0.14655172413793102, 0.17118226600985223, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.75615763546798032, 0.72660098522167482, 0.70320197044334976], 5: [0.16871921182266009, 0.097290640394088676, 0.10221674876847291, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20320197044334976, 0.19088669950738915, 0.18226600985221675, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.68842364532019706, 0.68472906403940892, 0.66748768472906406], 5: [0.16871921182266009, 0.1206896551724138, 0.13300492610837439, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20320197044334976, 0.048029556650246302, 0.093596059113300489, 0.094827586206896547], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.84852216748768472, 0.81403940886699511, 0.83374384236453203], 5: [0.16871921182266009, 0.10344827586206896, 0.092364532019704432, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235137 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.00598
Epoch 1, cost is  2.88247
Epoch 2, cost is  2.91557
Epoch 3, cost is  2.97871
Epoch 4, cost is  3.05718
Training took 0.159390 minutes
Weight histogram
[ 926  865  369 1123  809  933  563  316  109   62] [-0.30425924 -0.27415953 -0.24405982 -0.21396011 -0.1838604  -0.15376069
 -0.12366098 -0.09356127 -0.06346156 -0.03336185 -0.00326214]
[144 276 415 556 708 794 792 825 781 784] [-0.30425924 -0.27415953 -0.24405982 -0.21396011 -0.1838604  -0.15376069
 -0.12366098 -0.09356127 -0.06346156 -0.03336185 -0.00326214]
-10.3072
17.9116
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234500 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.06073
Epoch 1, cost is  2.91198
Epoch 2, cost is  2.92895
Epoch 3, cost is  2.98493
Epoch 4, cost is  3.05288
Training took 0.162402 minutes
Weight histogram
[ 981  875  527 1041  715 1728 1197  675  230  131] [-0.30905092 -0.27847204 -0.24789316 -0.21731428 -0.18673541 -0.15615653
 -0.12557765 -0.09499877 -0.0644199  -0.03384102 -0.00326214]
[ 298  544  825 1130 1229  838  808  837  800  791] [-0.30905092 -0.27847204 -0.24789316 -0.21731428 -0.18673541 -0.15615653
 -0.12557765 -0.09499877 -0.0644199  -0.03384102 -0.00326214]
-8.57089
11.365
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.66928
Epoch 1, cost is  2.3129
Epoch 2, cost is  2.21795
Epoch 3, cost is  2.28019
Epoch 4, cost is  2.40771
Training took 0.184312 minutes
Weight histogram
[835 979 934 894 756 612 444 233 157 231] [-0.18797739 -0.16946947 -0.15096156 -0.13245365 -0.11394573 -0.09543782
 -0.0769299  -0.05842199 -0.03991407 -0.02140616 -0.00289825]
[331 302 436 553 634 701 745 773 828 772] [-0.18797739 -0.16946947 -0.15096156 -0.13245365 -0.11394573 -0.09543782
 -0.0769299  -0.05842199 -0.03991407 -0.02140616 -0.00289825]
-6.41529
7.90691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090623 minutes
Epoch 0
Fine tuning took 0.089427 minutes
Epoch 0
Fine tuning took 0.089018 minutes
{'zero': {0: [0.1539408866995074, 0.18472906403940886, 0.15024630541871922, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75123152709359609, 0.71921182266009853, 0.7931034482758621, 0.84113300492610843], 5: [0.094827586206896547, 0.096059113300492605, 0.056650246305418719, 0.02832512315270936], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1539408866995074, 0.16502463054187191, 0.18596059113300492, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75123152709359609, 0.72783251231527091, 0.72167487684729059, 0.71674876847290636], 5: [0.094827586206896547, 0.10714285714285714, 0.092364532019704432, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1539408866995074, 0.17857142857142858, 0.21674876847290642, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75123152709359609, 0.70566502463054193, 0.62438423645320196, 0.66625615763546797], 5: [0.094827586206896547, 0.11576354679802955, 0.15886699507389163, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1539408866995074, 0.1625615763546798, 0.10837438423645321, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75123152709359609, 0.75, 0.76724137931034486, 0.75369458128078815], 5: [0.094827586206896547, 0.087438423645320201, 0.12438423645320197, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235006 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.06428
Epoch 1, cost is  1.9765
Epoch 2, cost is  1.93416
Epoch 3, cost is  1.90453
Epoch 4, cost is  1.8835
Training took 0.159630 minutes
Weight histogram
[1449  739 1299  689  661  411  262  192  189  184] [-0.09678346 -0.08712747 -0.07747148 -0.06781549 -0.0581595  -0.04850351
 -0.03884752 -0.02919153 -0.01953554 -0.00987955 -0.00022356]
[331 254 345 442 559 696 755 857 893 943] [-0.09678346 -0.08712747 -0.07747148 -0.06781549 -0.0581595  -0.04850351
 -0.03884752 -0.02919153 -0.01953554 -0.00987955 -0.00022356]
-2.06019
3.34147
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235753 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.11906
Epoch 1, cost is  2.02152
Epoch 2, cost is  1.97165
Epoch 3, cost is  1.9406
Epoch 4, cost is  1.91447
Training took 0.159846 minutes
Weight histogram
[1421  776 1208  759 1347  849  526  421  368  425] [-0.09717651 -0.08748122 -0.07778592 -0.06809063 -0.05839533 -0.04870004
 -0.03900474 -0.02930944 -0.01961415 -0.00991885 -0.00022356]
[ 688  530  723  926 1142  687  744  844  885  931] [-0.09717651 -0.08748122 -0.07778592 -0.06809063 -0.05839533 -0.04870004
 -0.03900474 -0.02930944 -0.01961415 -0.00991885 -0.00022356]
-2.16201
2.72975
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.59519
Epoch 1, cost is  5.52408
Epoch 2, cost is  4.76573
Epoch 3, cost is  4.40972
Epoch 4, cost is  4.18955
Training took 0.106180 minutes
Weight histogram
[ 583  775  682  610  530  426  369  354  439 1307] [-0.10474743 -0.09429966 -0.08385189 -0.07340412 -0.06295635 -0.05250858
 -0.04206081 -0.03161304 -0.02116527 -0.0107175  -0.00026972]
[1183  387  389  430  488  522  577  637  673  789] [-0.10474743 -0.09429966 -0.08385189 -0.07340412 -0.06295635 -0.05250858
 -0.04206081 -0.03161304 -0.02116527 -0.0107175  -0.00026972]
-1.55839
3.25821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081651 minutes
Epoch 0
Fine tuning took 0.083033 minutes
Epoch 0
Fine tuning took 0.081822 minutes
{'zero': {0: [0.43226600985221675, 0.27709359605911332, 0.18472906403940886, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.28078817733990147, 0.44704433497536944, 0.61945812807881773, 0.61206896551724133], 5: [0.28694581280788178, 0.27586206896551724, 0.19581280788177341, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.43226600985221675, 0.30295566502463056, 0.17610837438423646, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.28078817733990147, 0.4039408866995074, 0.55049261083743839, 0.61822660098522164], 5: [0.28694581280788178, 0.29310344827586204, 0.27339901477832512, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.43226600985221675, 0.28325123152709358, 0.16625615763546797, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.28078817733990147, 0.44088669950738918, 0.58128078817733986, 0.60467980295566504], 5: [0.28694581280788178, 0.27586206896551724, 0.25246305418719212, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.43226600985221675, 0.28694581280788178, 0.16133004926108374, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.28078817733990147, 0.41748768472906406, 0.55418719211822665, 0.63054187192118227], 5: [0.28694581280788178, 0.29556650246305421, 0.28448275862068967, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234896 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.06428
Epoch 1, cost is  1.9765
Epoch 2, cost is  1.93416
Epoch 3, cost is  1.90453
Epoch 4, cost is  1.8835
Training took 0.159445 minutes
Weight histogram
[1449  739 1299  689  661  411  262  192  189  184] [-0.09678346 -0.08712747 -0.07747148 -0.06781549 -0.0581595  -0.04850351
 -0.03884752 -0.02919153 -0.01953554 -0.00987955 -0.00022356]
[331 254 345 442 559 696 755 857 893 943] [-0.09678346 -0.08712747 -0.07747148 -0.06781549 -0.0581595  -0.04850351
 -0.03884752 -0.02919153 -0.01953554 -0.00987955 -0.00022356]
-2.06019
3.34147
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234414 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.11906
Epoch 1, cost is  2.02152
Epoch 2, cost is  1.97165
Epoch 3, cost is  1.9406
Epoch 4, cost is  1.91447
Training took 0.161268 minutes
Weight histogram
[1421  776 1208  759 1347  849  526  421  368  425] [-0.09717651 -0.08748122 -0.07778592 -0.06809063 -0.05839533 -0.04870004
 -0.03900474 -0.02930944 -0.01961415 -0.00991885 -0.00022356]
[ 688  530  723  926 1142  687  744  844  885  931] [-0.09717651 -0.08748122 -0.07778592 -0.06809063 -0.05839533 -0.04870004
 -0.03900474 -0.02930944 -0.01961415 -0.00991885 -0.00022356]
-2.16201
2.72975
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55611
Epoch 1, cost is  5.31617
Epoch 2, cost is  4.2943
Epoch 3, cost is  3.8396
Epoch 4, cost is  3.57095
Training took 0.124473 minutes
Weight histogram
[ 516  821  694  607  542  387  396  378  544 1190] [-0.0834422  -0.07512688 -0.06681156 -0.05849624 -0.05018092 -0.0418656
 -0.03355028 -0.02523496 -0.01691964 -0.00860432 -0.00028899]
[1208  403  402  402  467  524  568  644  676  781] [-0.0834422  -0.07512688 -0.06681156 -0.05849624 -0.05018092 -0.0418656
 -0.03355028 -0.02523496 -0.01691964 -0.00860432 -0.00028899]
-1.37774
2.62931
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083335 minutes
Epoch 0
Fine tuning took 0.083361 minutes
Epoch 0
Fine tuning took 0.083070 minutes
{'zero': {0: [0.26354679802955666, 0.26108374384236455, 0.17364532019704434, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50492610837438423, 0.55541871921182262, 0.66871921182266014, 0.71674876847290636], 5: [0.23152709359605911, 0.18349753694581281, 0.15763546798029557, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26354679802955666, 0.14778325123152711, 0.14532019704433496, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50492610837438423, 0.57758620689655171, 0.56773399014778325, 0.68349753694581283], 5: [0.23152709359605911, 0.27463054187192121, 0.28694581280788178, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26354679802955666, 0.13177339901477833, 0.15763546798029557, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50492610837438423, 0.58251231527093594, 0.5714285714285714, 0.66995073891625612], 5: [0.23152709359605911, 0.2857142857142857, 0.27093596059113301, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26354679802955666, 0.1268472906403941, 0.13300492610837439, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50492610837438423, 0.58620689655172409, 0.59359605911330049, 0.66748768472906406], 5: [0.23152709359605911, 0.28694581280788178, 0.27339901477832512, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.251346 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.06428
Epoch 1, cost is  1.9765
Epoch 2, cost is  1.93416
Epoch 3, cost is  1.90453
Epoch 4, cost is  1.8835
Training took 0.178550 minutes
Weight histogram
[1449  739 1299  689  661  411  262  192  189  184] [-0.09678346 -0.08712747 -0.07747148 -0.06781549 -0.0581595  -0.04850351
 -0.03884752 -0.02919153 -0.01953554 -0.00987955 -0.00022356]
[331 254 345 442 559 696 755 857 893 943] [-0.09678346 -0.08712747 -0.07747148 -0.06781549 -0.0581595  -0.04850351
 -0.03884752 -0.02919153 -0.01953554 -0.00987955 -0.00022356]
-2.06019
3.34147
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.261391 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.11906
Epoch 1, cost is  2.02152
Epoch 2, cost is  1.97165
Epoch 3, cost is  1.9406
Epoch 4, cost is  1.91447
Training took 0.179979 minutes
Weight histogram
[1421  776 1208  759 1347  849  526  421  368  425] [-0.09717651 -0.08748122 -0.07778592 -0.06809063 -0.05839533 -0.04870004
 -0.03900474 -0.02930944 -0.01961415 -0.00991885 -0.00022356]
[ 688  530  723  926 1142  687  744  844  885  931] [-0.09717651 -0.08748122 -0.07778592 -0.06809063 -0.05839533 -0.04870004
 -0.03900474 -0.02930944 -0.01961415 -0.00991885 -0.00022356]
-2.16201
2.72975
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46857
Epoch 1, cost is  5.06803
Epoch 2, cost is  3.74095
Epoch 3, cost is  3.1159
Epoch 4, cost is  2.75543
Training took 0.203271 minutes
Weight histogram
[ 659  862  696  564  513  461  363  406 1219  332] [-0.05459121 -0.04916031 -0.04372942 -0.03829852 -0.03286762 -0.02743672
 -0.02200582 -0.01657492 -0.01114403 -0.00571313 -0.00028223]
[1252  403  384  402  438  489  559  639  731  778] [-0.05459121 -0.04916031 -0.04372942 -0.03829852 -0.03286762 -0.02743672
 -0.02200582 -0.01657492 -0.01114403 -0.00571313 -0.00028223]
-1.00853
1.9709
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087864 minutes
Epoch 0
Fine tuning took 0.092123 minutes
Epoch 0
Fine tuning took 0.087845 minutes
{'zero': {0: [0.20073891625615764, 0.15147783251231528, 0.15270935960591134, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68842364532019706, 0.7857142857142857, 0.77832512315270941, 0.84359605911330049], 5: [0.11083743842364532, 0.062807881773399021, 0.068965517241379309, 0.043103448275862072], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20073891625615764, 0.17980295566502463, 0.13793103448275862, 0.060344827586206899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68842364532019706, 0.74014778325123154, 0.81157635467980294, 0.90517241379310343], 5: [0.11083743842364532, 0.080049261083743842, 0.050492610837438424, 0.034482758620689655], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20073891625615764, 0.17118226600985223, 0.12931034482758622, 0.096059113300492605], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68842364532019706, 0.74384236453201968, 0.80665024630541871, 0.84605911330049266], 5: [0.11083743842364532, 0.084975369458128072, 0.064039408866995079, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20073891625615764, 0.17980295566502463, 0.14901477832512317, 0.065270935960591137], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68842364532019706, 0.74876847290640391, 0.81034482758620685, 0.90024630541871919], 5: [0.11083743842364532, 0.071428571428571425, 0.04064039408866995, 0.034482758620689655], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.255025 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.13917
Epoch 1, cost is  3.96771
Epoch 2, cost is  3.82788
Epoch 3, cost is  3.705
Epoch 4, cost is  3.59267
Training took 0.159242 minutes
Weight histogram
[ 721  610  559  536  581  591  534  571 1255  117] [ -3.39323729e-02  -3.05311055e-02  -2.71298380e-02  -2.37285706e-02
  -2.03273031e-02  -1.69260356e-02  -1.35247682e-02  -1.01235007e-02
  -6.72223328e-03  -3.32096582e-03   8.03016374e-05]
[1412  530  481  450  449  474  516  552  589  622] [ -3.39323729e-02  -3.05311055e-02  -2.71298380e-02  -2.37285706e-02
  -2.03273031e-02  -1.69260356e-02  -1.35247682e-02  -1.01235007e-02
  -6.72223328e-03  -3.32096582e-03   8.03016374e-05]
-0.564267
0.971113
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235738 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.38994
Epoch 1, cost is  4.20551
Epoch 2, cost is  4.05379
Epoch 3, cost is  3.91751
Epoch 4, cost is  3.79851
Training took 0.173780 minutes
Weight histogram
[ 636  571  536  566  518  559  848 1051 2630  185] [ -3.01458184e-02  -2.71232064e-02  -2.41005944e-02  -2.10779824e-02
  -1.80553704e-02  -1.50327584e-02  -1.20101464e-02  -8.98753436e-03
  -5.96492236e-03  -2.94231036e-03   8.03016374e-05]
[2985 1017  566  440  451  484  497  524  549  587] [ -3.01458184e-02  -2.71232064e-02  -2.41005944e-02  -2.10779824e-02
  -1.80553704e-02  -1.50327584e-02  -1.20101464e-02  -8.98753436e-03
  -5.96492236e-03  -2.94231036e-03   8.03016374e-05]
-0.496166
0.73336
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.87676
Epoch 1, cost is  6.80164
Epoch 2, cost is  6.73443
Epoch 3, cost is  6.66868
Epoch 4, cost is  6.60132
Training took 0.113256 minutes
Weight histogram
[  92 2901 2269  278  174  119   87   63   49   43] [ -7.25918217e-03  -6.53604660e-03  -5.81291103e-03  -5.08977546e-03
  -4.36663989e-03  -3.64350432e-03  -2.92036875e-03  -2.19723318e-03
  -1.47409761e-03  -7.50962043e-04  -2.78264743e-05]
[3287 1243  630  252  145  118  107  102  100   91] [ -7.25918217e-03  -6.53604660e-03  -5.81291103e-03  -5.08977546e-03
  -4.36663989e-03  -3.64350432e-03  -2.92036875e-03  -2.19723318e-03
  -1.47409761e-03  -7.50962043e-04  -2.78264743e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089293 minutes
Epoch 0
Fine tuning took 0.085070 minutes
Epoch 0
Fine tuning took 0.081401 minutes
{'zero': {0: [0.16502463054187191, 0.3288177339901478, 0.44211822660098521, 0.32019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4039408866995074, 0.52586206896551724, 0.3854679802955665, 0.42733990147783252], 5: [0.43103448275862066, 0.14532019704433496, 0.17241379310344829, 0.25246305418719212], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16502463054187191, 0.31527093596059114, 0.50369458128078815, 0.30049261083743845], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4039408866995074, 0.53448275862068961, 0.3460591133004926, 0.45443349753694579], 5: [0.43103448275862066, 0.15024630541871922, 0.15024630541871922, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16502463054187191, 0.32266009852216748, 0.47044334975369456, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4039408866995074, 0.50985221674876846, 0.36206896551724138, 0.40024630541871919], 5: [0.43103448275862066, 0.16748768472906403, 0.16748768472906403, 0.31157635467980294], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16502463054187191, 0.34113300492610837, 0.48891625615763545, 0.29556650246305421], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4039408866995074, 0.52216748768472909, 0.34975369458128081, 0.44827586206896552], 5: [0.43103448275862066, 0.13669950738916256, 0.16133004926108374, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234972 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.13917
Epoch 1, cost is  3.96771
Epoch 2, cost is  3.82788
Epoch 3, cost is  3.705
Epoch 4, cost is  3.59267
Training took 0.159285 minutes
Weight histogram
[ 721  610  559  536  581  591  534  571 1255  117] [ -3.39323729e-02  -3.05311055e-02  -2.71298380e-02  -2.37285706e-02
  -2.03273031e-02  -1.69260356e-02  -1.35247682e-02  -1.01235007e-02
  -6.72223328e-03  -3.32096582e-03   8.03016374e-05]
[1412  530  481  450  449  474  516  552  589  622] [ -3.39323729e-02  -3.05311055e-02  -2.71298380e-02  -2.37285706e-02
  -2.03273031e-02  -1.69260356e-02  -1.35247682e-02  -1.01235007e-02
  -6.72223328e-03  -3.32096582e-03   8.03016374e-05]
-0.564267
0.971113
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234611 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.38994
Epoch 1, cost is  4.20551
Epoch 2, cost is  4.05379
Epoch 3, cost is  3.91751
Epoch 4, cost is  3.79851
Training took 0.161995 minutes
Weight histogram
[ 636  571  536  566  518  559  848 1051 2630  185] [ -3.01458184e-02  -2.71232064e-02  -2.41005944e-02  -2.10779824e-02
  -1.80553704e-02  -1.50327584e-02  -1.20101464e-02  -8.98753436e-03
  -5.96492236e-03  -2.94231036e-03   8.03016374e-05]
[2985 1017  566  440  451  484  497  524  549  587] [ -3.01458184e-02  -2.71232064e-02  -2.41005944e-02  -2.10779824e-02
  -1.80553704e-02  -1.50327584e-02  -1.20101464e-02  -8.98753436e-03
  -5.96492236e-03  -2.94231036e-03   8.03016374e-05]
-0.496166
0.73336
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.85662
Epoch 1, cost is  6.77394
Epoch 2, cost is  6.70371
Epoch 3, cost is  6.63539
Epoch 4, cost is  6.56391
Training took 0.122238 minutes
Weight histogram
[  93  119  449 4495  375  206  132   91   63   52] [ -8.75947252e-03  -7.88862058e-03  -7.01776864e-03  -6.14691670e-03
  -5.27606477e-03  -4.40521283e-03  -3.53436089e-03  -2.66350895e-03
  -1.79265701e-03  -9.21805073e-04  -5.09531346e-05]
[3027 1199  744  363  144  130  123  113  118  114] [ -8.75947252e-03  -7.88862058e-03  -7.01776864e-03  -6.14691670e-03
  -5.27606477e-03  -4.40521283e-03  -3.53436089e-03  -2.66350895e-03
  -1.79265701e-03  -9.21805073e-04  -5.09531346e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083171 minutes
Epoch 0
Fine tuning took 0.083457 minutes
Epoch 0
Fine tuning took 0.083015 minutes
{'zero': {0: [0.18349753694581281, 0.33374384236453203, 0.30295566502463056, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.54802955665024633, 0.41133004926108374, 0.53078817733990147], 5: [0.45197044334975367, 0.11822660098522167, 0.2857142857142857, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18349753694581281, 0.3288177339901478, 0.32019704433497537, 0.3645320197044335], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.56527093596059108, 0.41625615763546797, 0.51231527093596063], 5: [0.45197044334975367, 0.10591133004926108, 0.26354679802955666, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18349753694581281, 0.31527093596059114, 0.28201970443349755, 0.36330049261083741], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.55172413793103448, 0.41995073891625617, 0.5073891625615764], 5: [0.45197044334975367, 0.13300492610837439, 0.29802955665024633, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18349753694581281, 0.29926108374384236, 0.31034482758620691, 0.3608374384236453], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3645320197044335, 0.57389162561576357, 0.40640394088669951, 0.50985221674876846], 5: [0.45197044334975367, 0.1268472906403941, 0.28325123152709358, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236151 minutes
Weight histogram
[  87  246  427  497  404  444 1057 1817  917  179] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
[ 104  153  200  250  406  578  630  726 1514 1514] [ -1.55507252e-04  -3.13544428e-05   9.27983667e-05   2.16951176e-04
   3.41103986e-04   4.65256795e-04   5.89409604e-04   7.13562414e-04
   8.37715223e-04   9.61868033e-04   1.08602084e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.13917
Epoch 1, cost is  3.96771
Epoch 2, cost is  3.82788
Epoch 3, cost is  3.705
Epoch 4, cost is  3.59267
Training took 0.161327 minutes
Weight histogram
[ 721  610  559  536  581  591  534  571 1255  117] [ -3.39323729e-02  -3.05311055e-02  -2.71298380e-02  -2.37285706e-02
  -2.03273031e-02  -1.69260356e-02  -1.35247682e-02  -1.01235007e-02
  -6.72223328e-03  -3.32096582e-03   8.03016374e-05]
[1412  530  481  450  449  474  516  552  589  622] [ -3.39323729e-02  -3.05311055e-02  -2.71298380e-02  -2.37285706e-02
  -2.03273031e-02  -1.69260356e-02  -1.35247682e-02  -1.01235007e-02
  -6.72223328e-03  -3.32096582e-03   8.03016374e-05]
-0.564267
0.971113
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236116 minutes
Weight histogram
[ 123  413  771  982  904  936 1547 1743  601   80] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
[ 223  304  390  561  839 1196  704  736 1522 1625] [ -1.55507252e-04  -3.69645088e-05   8.15782347e-05   2.00120978e-04
   3.18663722e-04   4.37206465e-04   5.55749208e-04   6.74291952e-04
   7.92834695e-04   9.11377439e-04   1.02992018e-03]
-1.08778
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.38994
Epoch 1, cost is  4.20551
Epoch 2, cost is  4.05379
Epoch 3, cost is  3.91751
Epoch 4, cost is  3.79851
Training took 0.161301 minutes
Weight histogram
[ 636  571  536  566  518  559  848 1051 2630  185] [ -3.01458184e-02  -2.71232064e-02  -2.41005944e-02  -2.10779824e-02
  -1.80553704e-02  -1.50327584e-02  -1.20101464e-02  -8.98753436e-03
  -5.96492236e-03  -2.94231036e-03   8.03016374e-05]
[2985 1017  566  440  451  484  497  524  549  587] [ -3.01458184e-02  -2.71232064e-02  -2.41005944e-02  -2.10779824e-02
  -1.80553704e-02  -1.50327584e-02  -1.20101464e-02  -8.98753436e-03
  -5.96492236e-03  -2.94231036e-03   8.03016374e-05]
-0.496166
0.73336
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79646
Epoch 1, cost is  6.69343
Epoch 2, cost is  6.61491
Epoch 3, cost is  6.53856
Epoch 4, cost is  6.45952
Training took 0.180241 minutes
Weight histogram
[ 162  200  254 3656 1018  335  190  120   80   60] [ -1.00328391e-02  -9.03352031e-03  -8.03420147e-03  -7.03488263e-03
  -6.03556380e-03  -5.03624496e-03  -4.03692612e-03  -3.03760729e-03
  -2.03828845e-03  -1.03896961e-03  -3.96507785e-05]
[2586 1127  857  493  280  142  142  142  149  157] [ -1.00328391e-02  -9.03352031e-03  -8.03420147e-03  -7.03488263e-03
  -6.03556380e-03  -5.03624496e-03  -4.03692612e-03  -3.03760729e-03
  -2.03828845e-03  -1.03896961e-03  -3.96507785e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088367 minutes
Epoch 0
Fine tuning took 0.088884 minutes
Epoch 0
Fine tuning took 0.088817 minutes
{'zero': {0: [0.15763546798029557, 0.36576354679802958, 0.39285714285714285, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3817733990147783, 0.49876847290640391, 0.47044334975369456, 0.63793103448275867], 5: [0.4605911330049261, 0.1354679802955665, 0.13669950738916256, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.33620689655172414, 0.45320197044334976, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3817733990147783, 0.5073891625615764, 0.43226600985221675, 0.62684729064039413], 5: [0.4605911330049261, 0.15640394088669951, 0.1145320197044335, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.37561576354679804, 0.4248768472906404, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3817733990147783, 0.49384236453201968, 0.47290640394088668, 0.62931034482758619], 5: [0.4605911330049261, 0.13054187192118227, 0.10221674876847291, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.35344827586206895, 0.43226600985221675, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3817733990147783, 0.46674876847290642, 0.45935960591133007, 0.65024630541871919], 5: [0.4605911330049261, 0.17980295566502463, 0.10837438423645321, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235145 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.37109
Epoch 1, cost is  3.1954
Epoch 2, cost is  3.19113
Epoch 3, cost is  3.22329
Epoch 4, cost is  3.28172
Training took 0.161922 minutes
Weight histogram
[1228  794 1126  878  982 1098 1066  647  196   85] [-0.37995917 -0.34228946 -0.30461976 -0.26695006 -0.22928036 -0.19161065
 -0.15394095 -0.11627125 -0.07860155 -0.04093184 -0.00326214]
[ 197  400  633  844  986  998  995  974 1051 1022] [-0.37995917 -0.34228946 -0.30461976 -0.26695006 -0.22928036 -0.19161065
 -0.15394095 -0.11627125 -0.07860155 -0.04093184 -0.00326214]
-12.5311
19.8031
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235100 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4999
Epoch 1, cost is  3.31368
Epoch 2, cost is  3.31467
Epoch 3, cost is  3.35823
Epoch 4, cost is  3.41654
Training took 0.160403 minutes
Weight histogram
[1163  830 1014 1007  982 1075 2027 1376  477  174] [-0.39638788 -0.3570753  -0.31776273 -0.27845015 -0.23913758 -0.19982501
 -0.16051243 -0.12119986 -0.08188729 -0.04257471 -0.00326214]
[ 404  818 1275 1535 1038 1041 1013 1002 1014  985] [-0.39638788 -0.3570753  -0.31776273 -0.27845015 -0.23913758 -0.19982501
 -0.16051243 -0.12119986 -0.08188729 -0.04257471 -0.00326214]
-11.1282
14.381
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.60032
Epoch 1, cost is  3.87902
Epoch 2, cost is  4.15976
Epoch 3, cost is  4.60215
Epoch 4, cost is  5.05729
Training took 0.103784 minutes
Weight histogram
[1054 1435 1277 1333 1213  592  474  286  170  266] [-0.31483021 -0.28363226 -0.25243431 -0.22123636 -0.1900384  -0.15884045
 -0.1276425  -0.09644455 -0.06524659 -0.03404864 -0.00285069]
[ 408  403  535  717  857  975  977 1032 1092 1104] [-0.31483021 -0.28363226 -0.25243431 -0.22123636 -0.1900384  -0.15884045
 -0.1276425  -0.09644455 -0.06524659 -0.03404864 -0.00285069]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082948 minutes
Epoch 0
Fine tuning took 0.081980 minutes
Epoch 0
Fine tuning took 0.082577 minutes
{'zero': {0: [0.33497536945812806, 0.12931034482758622, 0.14039408866995073, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.83374384236453203, 0.82019704433497542, 0.89532019704433496], 5: [0.12807881773399016, 0.036945812807881777, 0.039408866995073892, 0.027093596059113302], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33497536945812806, 0.17857142857142858, 0.14162561576354679, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.71305418719211822, 0.78078817733990147, 0.69088669950738912], 5: [0.12807881773399016, 0.10837438423645321, 0.077586206896551727, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33497536945812806, 0.17980295566502463, 0.14901477832512317, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.7068965517241379, 0.74507389162561577, 0.67980295566502458], 5: [0.12807881773399016, 0.11330049261083744, 0.10591133004926108, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33497536945812806, 0.17364532019704434, 0.14901477832512317, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.71798029556650245, 0.81034482758620685, 0.65517241379310343], 5: [0.12807881773399016, 0.10837438423645321, 0.04064039408866995, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235374 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.37109
Epoch 1, cost is  3.1954
Epoch 2, cost is  3.19113
Epoch 3, cost is  3.22329
Epoch 4, cost is  3.28172
Training took 0.158587 minutes
Weight histogram
[1228  794 1126  878  982 1098 1066  647  196   85] [-0.37995917 -0.34228946 -0.30461976 -0.26695006 -0.22928036 -0.19161065
 -0.15394095 -0.11627125 -0.07860155 -0.04093184 -0.00326214]
[ 197  400  633  844  986  998  995  974 1051 1022] [-0.37995917 -0.34228946 -0.30461976 -0.26695006 -0.22928036 -0.19161065
 -0.15394095 -0.11627125 -0.07860155 -0.04093184 -0.00326214]
-12.5311
19.8031
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234571 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4999
Epoch 1, cost is  3.31368
Epoch 2, cost is  3.31467
Epoch 3, cost is  3.35823
Epoch 4, cost is  3.41654
Training took 0.158699 minutes
Weight histogram
[1163  830 1014 1007  982 1075 2027 1376  477  174] [-0.39638788 -0.3570753  -0.31776273 -0.27845015 -0.23913758 -0.19982501
 -0.16051243 -0.12119986 -0.08188729 -0.04257471 -0.00326214]
[ 404  818 1275 1535 1038 1041 1013 1002 1014  985] [-0.39638788 -0.3570753  -0.31776273 -0.27845015 -0.23913758 -0.19982501
 -0.16051243 -0.12119986 -0.08188729 -0.04257471 -0.00326214]
-11.1282
14.381
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.2084
Epoch 1, cost is  3.27989
Epoch 2, cost is  3.43919
Epoch 3, cost is  3.72722
Epoch 4, cost is  4.10872
Training took 0.122269 minutes
Weight histogram
[ 906 1280 1272 1199 1090  904  638  343  165  303] [-0.27973735 -0.25204414 -0.22435094 -0.19665773 -0.16896452 -0.14127131
 -0.1135781  -0.08588489 -0.05819168 -0.03049847 -0.00280527]
[ 410  423  609  723  861  942 1027 1050 1032 1023] [-0.27973735 -0.25204414 -0.22435094 -0.19665773 -0.16896452 -0.14127131
 -0.1135781  -0.08588489 -0.05819168 -0.03049847 -0.00280527]
-9.2547
10.8052
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.085135 minutes
Epoch 0
Fine tuning took 0.084108 minutes
Epoch 0
Fine tuning took 0.083821 minutes
{'zero': {0: [0.21798029556650247, 0.23152709359605911, 0.26847290640394089, 0.10960591133004927], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.68472906403940892, 0.63054187192118227, 0.83620689655172409], 5: [0.14285714285714285, 0.083743842364532015, 0.10098522167487685, 0.054187192118226604], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21798029556650247, 0.13793103448275862, 0.12192118226600986, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.75985221674876846, 0.74876847290640391, 0.70812807881773399], 5: [0.14285714285714285, 0.10221674876847291, 0.12931034482758622, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21798029556650247, 0.17733990147783252, 0.14901477832512317, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.71798029556650245, 0.72906403940886699, 0.71182266009852213], 5: [0.14285714285714285, 0.10467980295566502, 0.12192118226600986, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21798029556650247, 0.11576354679802955, 0.088669950738916259, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.75615763546798032, 0.80418719211822665, 0.6785714285714286], 5: [0.14285714285714285, 0.12807881773399016, 0.10714285714285714, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234321 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.37109
Epoch 1, cost is  3.1954
Epoch 2, cost is  3.19113
Epoch 3, cost is  3.22329
Epoch 4, cost is  3.28172
Training took 0.159118 minutes
Weight histogram
[1228  794 1126  878  982 1098 1066  647  196   85] [-0.37995917 -0.34228946 -0.30461976 -0.26695006 -0.22928036 -0.19161065
 -0.15394095 -0.11627125 -0.07860155 -0.04093184 -0.00326214]
[ 197  400  633  844  986  998  995  974 1051 1022] [-0.37995917 -0.34228946 -0.30461976 -0.26695006 -0.22928036 -0.19161065
 -0.15394095 -0.11627125 -0.07860155 -0.04093184 -0.00326214]
-12.5311
19.8031
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234310 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4999
Epoch 1, cost is  3.31368
Epoch 2, cost is  3.31467
Epoch 3, cost is  3.35823
Epoch 4, cost is  3.41654
Training took 0.160130 minutes
Weight histogram
[1163  830 1014 1007  982 1075 2027 1376  477  174] [-0.39638788 -0.3570753  -0.31776273 -0.27845015 -0.23913758 -0.19982501
 -0.16051243 -0.12119986 -0.08188729 -0.04257471 -0.00326214]
[ 404  818 1275 1535 1038 1041 1013 1002 1014  985] [-0.39638788 -0.3570753  -0.31776273 -0.27845015 -0.23913758 -0.19982501
 -0.16051243 -0.12119986 -0.08188729 -0.04257471 -0.00326214]
-11.1282
14.381
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.69399
Epoch 1, cost is  2.35077
Epoch 2, cost is  2.25955
Epoch 3, cost is  2.31239
Epoch 4, cost is  2.49308
Training took 0.181268 minutes
Weight histogram
[1076 1310 1262 1174 1037  817  595  311  212  306] [-0.18797739 -0.16946226 -0.15094713 -0.13243199 -0.11391686 -0.09540173
 -0.0768866  -0.05837147 -0.03985634 -0.02134121 -0.00282607]
[ 440  404  584  737  845  933  996 1027 1099 1035] [-0.18797739 -0.16946226 -0.15094713 -0.13243199 -0.11391686 -0.09540173
 -0.0768866  -0.05837147 -0.03985634 -0.02134121 -0.00282607]
-6.41529
7.90691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090333 minutes
Epoch 0
Fine tuning took 0.090697 minutes
Epoch 0
Fine tuning took 0.089390 minutes
{'zero': {0: [0.1206896551724138, 0.099753694581280791, 0.064039408866995079, 0.066502463054187194], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73891625615763545, 0.81650246305418717, 0.86206896551724133, 0.89039408866995073], 5: [0.14039408866995073, 0.083743842364532015, 0.073891625615763554, 0.043103448275862072], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1206896551724138, 0.15024630541871922, 0.14408866995073891, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73891625615763545, 0.71674876847290636, 0.67487684729064035, 0.67733990147783252], 5: [0.14039408866995073, 0.13300492610837439, 0.18103448275862069, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1206896551724138, 0.16871921182266009, 0.16133004926108374, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73891625615763545, 0.68719211822660098, 0.69334975369458129, 0.69950738916256161], 5: [0.14039408866995073, 0.14408866995073891, 0.14532019704433496, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1206896551724138, 0.19334975369458129, 0.16009852216748768, 0.12807881773399016], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73891625615763545, 0.70566502463054193, 0.68965517241379315, 0.76970443349753692], 5: [0.14039408866995073, 0.10098522167487685, 0.15024630541871922, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235430 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.87303
Epoch 1, cost is  1.80228
Epoch 2, cost is  1.77538
Epoch 3, cost is  1.7549
Epoch 4, cost is  1.7411
Training took 0.159144 minutes
Weight histogram
[1858 1414  923 1454  769  621  389  259  193  220] [-0.11095173 -0.09987891 -0.08880609 -0.07773328 -0.06666046 -0.05558764
 -0.04451483 -0.03344201 -0.02236919 -0.01129637 -0.00022356]
[ 369  331  465  611  799  899 1031 1075 1225 1295] [-0.11095173 -0.09987891 -0.08880609 -0.07773328 -0.06666046 -0.05558764
 -0.04451483 -0.03344201 -0.02236919 -0.01129637 -0.00022356]
-2.58278
4.32786
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235114 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.94792
Epoch 1, cost is  1.87327
Epoch 2, cost is  1.84472
Epoch 3, cost is  1.82304
Epoch 4, cost is  1.80804
Training took 0.159670 minutes
Weight histogram
[1797 1202 1045 1385 1039 1370  838  535  418  496] [-0.11337756 -0.10206216 -0.09074676 -0.07943136 -0.06811596 -0.05680056
 -0.04548516 -0.03416976 -0.02285436 -0.01153896 -0.00022356]
[ 774  683  972 1273  939  882 1012 1068 1227 1295] [-0.11337756 -0.10206216 -0.09074676 -0.07943136 -0.06811596 -0.05680056
 -0.04548516 -0.03416976 -0.02285436 -0.01153896 -0.00022356]
-2.79564
3.3939
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.59403
Epoch 1, cost is  5.53473
Epoch 2, cost is  4.77627
Epoch 3, cost is  4.41622
Epoch 4, cost is  4.18619
Training took 0.104385 minutes
Weight histogram
[ 583 1085  948  881  671  612  507  474  603 1736] [-0.10474743 -0.09429944 -0.08385146 -0.07340348 -0.06295549 -0.05250751
 -0.04205952 -0.03161154 -0.02116355 -0.01071557 -0.00026758]
[1563  526  526  582  662  711  787  870  934  939] [-0.10474743 -0.09429944 -0.08385146 -0.07340348 -0.06295549 -0.05250751
 -0.04205952 -0.03161154 -0.02116355 -0.01071557 -0.00026758]
-1.55839
3.39939
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082566 minutes
Epoch 0
Fine tuning took 0.081891 minutes
Epoch 0
Fine tuning took 0.082289 minutes
{'zero': {0: [0.29926108374384236, 0.2857142857142857, 0.16009852216748768, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.25862068965517243, 0.41256157635467983, 0.59482758620689657, 0.61206896551724133], 5: [0.44211822660098521, 0.30172413793103448, 0.24507389162561577, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29926108374384236, 0.34975369458128081, 0.25738916256157635, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.25862068965517243, 0.27832512315270935, 0.42364532019704432, 0.4642857142857143], 5: [0.44211822660098521, 0.37192118226600984, 0.31896551724137934, 0.32266009852216748], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29926108374384236, 0.32142857142857145, 0.20320197044334976, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.25862068965517243, 0.27339901477832512, 0.46674876847290642, 0.4605911330049261], 5: [0.44211822660098521, 0.40517241379310343, 0.33004926108374383, 0.32019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29926108374384236, 0.35960591133004927, 0.21921182266009853, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.25862068965517243, 0.24014778325123154, 0.41379310344827586, 0.47783251231527096], 5: [0.44211822660098521, 0.40024630541871919, 0.36699507389162561, 0.33251231527093594], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.251598 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.87303
Epoch 1, cost is  1.80228
Epoch 2, cost is  1.77538
Epoch 3, cost is  1.7549
Epoch 4, cost is  1.7411
Training took 0.160880 minutes
Weight histogram
[1858 1414  923 1454  769  621  389  259  193  220] [-0.11095173 -0.09987891 -0.08880609 -0.07773328 -0.06666046 -0.05558764
 -0.04451483 -0.03344201 -0.02236919 -0.01129637 -0.00022356]
[ 369  331  465  611  799  899 1031 1075 1225 1295] [-0.11095173 -0.09987891 -0.08880609 -0.07773328 -0.06666046 -0.05558764
 -0.04451483 -0.03344201 -0.02236919 -0.01129637 -0.00022356]
-2.58278
4.32786
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237767 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.94792
Epoch 1, cost is  1.87327
Epoch 2, cost is  1.84472
Epoch 3, cost is  1.82304
Epoch 4, cost is  1.80804
Training took 0.183105 minutes
Weight histogram
[1797 1202 1045 1385 1039 1370  838  535  418  496] [-0.11337756 -0.10206216 -0.09074676 -0.07943136 -0.06811596 -0.05680056
 -0.04548516 -0.03416976 -0.02285436 -0.01153896 -0.00022356]
[ 774  683  972 1273  939  882 1012 1068 1227 1295] [-0.11337756 -0.10206216 -0.09074676 -0.07943136 -0.06811596 -0.05680056
 -0.04548516 -0.03416976 -0.02285436 -0.01153896 -0.00022356]
-2.79564
3.3939
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55576
Epoch 1, cost is  5.32408
Epoch 2, cost is  4.32052
Epoch 3, cost is  3.86926
Epoch 4, cost is  3.59874
Training took 0.134640 minutes
Weight histogram
[ 623 1092  957  828  711  542  522  512  736 1577] [-0.0834422  -0.07512601 -0.06680983 -0.05849365 -0.05017746 -0.04186128
 -0.0335451  -0.02522892 -0.01691273 -0.00859655 -0.00028037]
[1590  544  529  540  626  700  763  862  904 1042] [-0.0834422  -0.07512601 -0.06680983 -0.05849365 -0.05017746 -0.04186128
 -0.0335451  -0.02522892 -0.01691273 -0.00859655 -0.00028037]
-1.37774
2.70075
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.094581 minutes
Epoch 0
Fine tuning took 0.085904 minutes
Epoch 0
Fine tuning took 0.084508 minutes
{'zero': {0: [0.26108374384236455, 0.26724137931034481, 0.13669950738916256, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.55172413793103448, 0.69088669950738912, 0.65517241379310343], 5: [0.31157635467980294, 0.18103448275862069, 0.17241379310344829, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26108374384236455, 0.31280788177339902, 0.11822660098522167, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.49507389162561577, 0.5714285714285714, 0.61083743842364535], 5: [0.31157635467980294, 0.19211822660098521, 0.31034482758620691, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26108374384236455, 0.25615763546798032, 0.1145320197044335, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.51354679802955661, 0.59482758620689657, 0.6576354679802956], 5: [0.31157635467980294, 0.23029556650246305, 0.29064039408866993, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26108374384236455, 0.27586206896551724, 0.094827586206896547, 0.1145320197044335], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.50615763546798032, 0.58743842364532017, 0.65886699507389157], 5: [0.31157635467980294, 0.21798029556650247, 0.31773399014778325, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.259006 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.87303
Epoch 1, cost is  1.80228
Epoch 2, cost is  1.77538
Epoch 3, cost is  1.7549
Epoch 4, cost is  1.7411
Training took 0.176661 minutes
Weight histogram
[1858 1414  923 1454  769  621  389  259  193  220] [-0.11095173 -0.09987891 -0.08880609 -0.07773328 -0.06666046 -0.05558764
 -0.04451483 -0.03344201 -0.02236919 -0.01129637 -0.00022356]
[ 369  331  465  611  799  899 1031 1075 1225 1295] [-0.11095173 -0.09987891 -0.08880609 -0.07773328 -0.06666046 -0.05558764
 -0.04451483 -0.03344201 -0.02236919 -0.01129637 -0.00022356]
-2.58278
4.32786
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.255636 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.94792
Epoch 1, cost is  1.87327
Epoch 2, cost is  1.84472
Epoch 3, cost is  1.82304
Epoch 4, cost is  1.80804
Training took 0.175679 minutes
Weight histogram
[1797 1202 1045 1385 1039 1370  838  535  418  496] [-0.11337756 -0.10206216 -0.09074676 -0.07943136 -0.06811596 -0.05680056
 -0.04548516 -0.03416976 -0.02285436 -0.01153896 -0.00022356]
[ 774  683  972 1273  939  882 1012 1068 1227 1295] [-0.11337756 -0.10206216 -0.09074676 -0.07943136 -0.06811596 -0.05680056
 -0.04548516 -0.03416976 -0.02285436 -0.01153896 -0.00022356]
-2.79564
3.3939
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.47004
Epoch 1, cost is  5.0687
Epoch 2, cost is  3.76847
Epoch 3, cost is  3.15418
Epoch 4, cost is  2.7938
Training took 0.201723 minutes
Weight histogram
[ 773 1192  946  770  697  629  484  541 1526  542] [-0.05459121 -0.04915948 -0.04372774 -0.03829601 -0.03286427 -0.02743253
 -0.0220008  -0.01656906 -0.01113733 -0.00570559 -0.00027386]
[1653  540  515  543  588  656  750  853  974 1028] [-0.05459121 -0.04915948 -0.04372774 -0.03829601 -0.03286427 -0.02743253
 -0.0220008  -0.01656906 -0.01113733 -0.00570559 -0.00027386]
-1.11031
2.02951
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088890 minutes
Epoch 0
Fine tuning took 0.089483 minutes
Epoch 0
Fine tuning took 0.089319 minutes
{'zero': {0: [0.16748768472906403, 0.1748768472906404, 0.16995073891625614, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.62068965517241381, 0.63054187192118227, 0.67980295566502458], 5: [0.14778325123152711, 0.20443349753694581, 0.19950738916256158, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16748768472906403, 0.14655172413793102, 0.17118226600985223, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.65886699507389157, 0.66871921182266014, 0.69458128078817738], 5: [0.14778325123152711, 0.19458128078817735, 0.16009852216748768, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16748768472906403, 0.16133004926108374, 0.16748768472906403, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.66502463054187189, 0.69581280788177335, 0.69458128078817738], 5: [0.14778325123152711, 0.17364532019704434, 0.13669950738916256, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16748768472906403, 0.1354679802955665, 0.16748768472906403, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.66748768472906406, 0.66379310344827591, 0.72413793103448276], 5: [0.14778325123152711, 0.19704433497536947, 0.16871921182266009, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234513 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.45093
Epoch 1, cost is  3.34868
Epoch 2, cost is  3.26456
Epoch 3, cost is  3.18676
Epoch 4, cost is  3.11981
Training took 0.160342 minutes
Weight histogram
[1149  847  837  774  673  728  738  715 1439  200] [ -4.28546555e-02  -3.85611598e-02  -3.42676641e-02  -2.99741684e-02
  -2.56806727e-02  -2.13871769e-02  -1.70936812e-02  -1.28001855e-02
  -8.50668980e-03  -4.21319408e-03   8.03016374e-05]
[1574  647  582  579  624  687  748  824  888  947] [ -4.28546555e-02  -3.85611598e-02  -3.42676641e-02  -2.99741684e-02
  -2.56806727e-02  -2.13871769e-02  -1.70936812e-02  -1.28001855e-02
  -8.50668980e-03  -4.21319408e-03   8.03016374e-05]
-0.681835
1.19326
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233550 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.67198
Epoch 1, cost is  3.55224
Epoch 2, cost is  3.4574
Epoch 3, cost is  3.37671
Epoch 4, cost is  3.30397
Training took 0.159833 minutes
Weight histogram
[ 979  811  737  757  732  706  708 1171 3193  331] [ -3.97703387e-02  -3.57852747e-02  -3.18002106e-02  -2.78151466e-02
  -2.38300826e-02  -1.98450185e-02  -1.58599545e-02  -1.18748905e-02
  -7.88982643e-03  -3.90476239e-03   8.03016374e-05]
[3304 1082  573  591  632  666  712  796  857  912] [ -3.97703387e-02  -3.57852747e-02  -3.18002106e-02  -2.78151466e-02
  -2.38300826e-02  -1.98450185e-02  -1.58599545e-02  -1.18748905e-02
  -7.88982643e-03  -3.90476239e-03   8.03016374e-05]
-0.573101
0.899871
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.87839
Epoch 1, cost is  6.80594
Epoch 2, cost is  6.74115
Epoch 3, cost is  6.67815
Epoch 4, cost is  6.61505
Training took 0.102184 minutes
Weight histogram
[  92 4202 2677  392  242  164  119   87   66   59] [ -7.25918217e-03  -6.53598817e-03  -5.81279418e-03  -5.08960019e-03
  -4.36640619e-03  -3.64321220e-03  -2.92001820e-03  -2.19682421e-03
  -1.47363022e-03  -7.50436223e-04  -2.72422294e-05]
[4838 1717  630  252  145  118  107  102  100   91] [ -7.25918217e-03  -6.53598817e-03  -5.81279418e-03  -5.08960019e-03
  -4.36640619e-03  -3.64321220e-03  -2.92001820e-03  -2.19682421e-03
  -1.47363022e-03  -7.50436223e-04  -2.72422294e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.079926 minutes
Epoch 0
Fine tuning took 0.080584 minutes
Epoch 0
Fine tuning took 0.081298 minutes
{'zero': {0: [0.2376847290640394, 0.28201970443349755, 0.17241379310344829, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.55295566502463056, 0.70935960591133007, 0.76354679802955661], 5: [0.061576354679802957, 0.16502463054187191, 0.11822660098522167, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2376847290640394, 0.27586206896551724, 0.14532019704433496, 0.034482758620689655], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.50492610837438423, 0.74014778325123154, 0.76847290640394084], 5: [0.061576354679802957, 0.21921182266009853, 0.1145320197044335, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2376847290640394, 0.26354679802955666, 0.16379310344827586, 0.041871921182266007], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.54679802955665024, 0.69950738916256161, 0.74630541871921185], 5: [0.061576354679802957, 0.18965517241379309, 0.13669950738916256, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2376847290640394, 0.29064039408866993, 0.16995073891625614, 0.035714285714285712], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.53694581280788178, 0.69211822660098521, 0.76108374384236455], 5: [0.061576354679802957, 0.17241379310344829, 0.13793103448275862, 0.20320197044334976], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235765 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.45093
Epoch 1, cost is  3.34868
Epoch 2, cost is  3.26456
Epoch 3, cost is  3.18676
Epoch 4, cost is  3.11981
Training took 0.160752 minutes
Weight histogram
[1149  847  837  774  673  728  738  715 1439  200] [ -4.28546555e-02  -3.85611598e-02  -3.42676641e-02  -2.99741684e-02
  -2.56806727e-02  -2.13871769e-02  -1.70936812e-02  -1.28001855e-02
  -8.50668980e-03  -4.21319408e-03   8.03016374e-05]
[1574  647  582  579  624  687  748  824  888  947] [ -4.28546555e-02  -3.85611598e-02  -3.42676641e-02  -2.99741684e-02
  -2.56806727e-02  -2.13871769e-02  -1.70936812e-02  -1.28001855e-02
  -8.50668980e-03  -4.21319408e-03   8.03016374e-05]
-0.681835
1.19326
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235596 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.67198
Epoch 1, cost is  3.55224
Epoch 2, cost is  3.4574
Epoch 3, cost is  3.37671
Epoch 4, cost is  3.30397
Training took 0.160436 minutes
Weight histogram
[ 979  811  737  757  732  706  708 1171 3193  331] [ -3.97703387e-02  -3.57852747e-02  -3.18002106e-02  -2.78151466e-02
  -2.38300826e-02  -1.98450185e-02  -1.58599545e-02  -1.18748905e-02
  -7.88982643e-03  -3.90476239e-03   8.03016374e-05]
[3304 1082  573  591  632  666  712  796  857  912] [ -3.97703387e-02  -3.57852747e-02  -3.18002106e-02  -2.78151466e-02
  -2.38300826e-02  -1.98450185e-02  -1.58599545e-02  -1.18748905e-02
  -7.88982643e-03  -3.90476239e-03   8.03016374e-05]
-0.573101
0.899871
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.85886
Epoch 1, cost is  6.77944
Epoch 2, cost is  6.71235
Epoch 3, cost is  6.64776
Epoch 4, cost is  6.58178
Training took 0.122196 minutes
Weight histogram
[  93  119  449 6160  530  285  182  125   86   71] [ -8.75947252e-03  -7.88856793e-03  -7.01766335e-03  -6.14675876e-03
  -5.27585417e-03  -4.40494959e-03  -3.53404500e-03  -2.66314041e-03
  -1.79223583e-03  -9.21331240e-04  -5.04266536e-05]
[4469 1739  787  363  144  130  123  113  118  114] [ -8.75947252e-03  -7.88856793e-03  -7.01766335e-03  -6.14675876e-03
  -5.27585417e-03  -4.40494959e-03  -3.53404500e-03  -2.66314041e-03
  -1.79223583e-03  -9.21331240e-04  -5.04266536e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084203 minutes
Epoch 0
Fine tuning took 0.083645 minutes
Epoch 0
Fine tuning took 0.084333 minutes
{'zero': {0: [0.20320197044334976, 0.24384236453201971, 0.24630541871921183, 0.009852216748768473], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73891625615763545, 0.69211822660098521, 0.66009852216748766, 0.89532019704433496], 5: [0.057881773399014777, 0.064039408866995079, 0.093596059113300489, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20320197044334976, 0.26847290640394089, 0.24507389162561577, 0.009852216748768473], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73891625615763545, 0.68719211822660098, 0.63916256157635465, 0.88054187192118227], 5: [0.057881773399014777, 0.044334975369458129, 0.11576354679802955, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20320197044334976, 0.22783251231527094, 0.24261083743842365, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73891625615763545, 0.70073891625615758, 0.66502463054187189, 0.90640394088669951], 5: [0.057881773399014777, 0.071428571428571425, 0.092364532019704432, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20320197044334976, 0.2413793103448276, 0.20566502463054187, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73891625615763545, 0.70320197044334976, 0.72167487684729059, 0.90517241379310343], 5: [0.057881773399014777, 0.055418719211822662, 0.072660098522167482, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234923 minutes
Weight histogram
[ 139  425  624  532  767 2064 1439 1181  861   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 112  177  224  340  509  714  651  997 2018 2358] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.798044
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.45093
Epoch 1, cost is  3.34868
Epoch 2, cost is  3.26456
Epoch 3, cost is  3.18676
Epoch 4, cost is  3.11981
Training took 0.159858 minutes
Weight histogram
[1149  847  837  774  673  728  738  715 1439  200] [ -4.28546555e-02  -3.85611598e-02  -3.42676641e-02  -2.99741684e-02
  -2.56806727e-02  -2.13871769e-02  -1.70936812e-02  -1.28001855e-02
  -8.50668980e-03  -4.21319408e-03   8.03016374e-05]
[1574  647  582  579  624  687  748  824  888  947] [ -4.28546555e-02  -3.85611598e-02  -3.42676641e-02  -2.99741684e-02
  -2.56806727e-02  -2.13871769e-02  -1.70936812e-02  -1.28001855e-02
  -8.50668980e-03  -4.21319408e-03   8.03016374e-05]
-0.681835
1.19326
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234809 minutes
Weight histogram
[ 157  538 1005 1141  974 1538 2306 1762  634   70] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 241  333  443  643 1011 1213  684  958 2125 2474] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.67198
Epoch 1, cost is  3.55224
Epoch 2, cost is  3.4574
Epoch 3, cost is  3.37671
Epoch 4, cost is  3.30397
Training took 0.159521 minutes
Weight histogram
[ 979  811  737  757  732  706  708 1171 3193  331] [ -3.97703387e-02  -3.57852747e-02  -3.18002106e-02  -2.78151466e-02
  -2.38300826e-02  -1.98450185e-02  -1.58599545e-02  -1.18748905e-02
  -7.88982643e-03  -3.90476239e-03   8.03016374e-05]
[3304 1082  573  591  632  666  712  796  857  912] [ -3.97703387e-02  -3.57852747e-02  -3.18002106e-02  -2.78151466e-02
  -2.38300826e-02  -1.98450185e-02  -1.58599545e-02  -1.18748905e-02
  -7.88982643e-03  -3.90476239e-03   8.03016374e-05]
-0.573101
0.899871
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80092
Epoch 1, cost is  6.70279
Epoch 2, cost is  6.62843
Epoch 3, cost is  6.55835
Epoch 4, cost is  6.48785
Training took 0.179460 minutes
Weight histogram
[ 162  200  254 4905 1491  470  262  165  109   82] [ -1.00328391e-02  -9.03341289e-03  -8.03398663e-03  -7.03456038e-03
  -6.03513412e-03  -5.03570787e-03  -4.03628161e-03  -3.03685536e-03
  -2.03742910e-03  -1.03800285e-03  -3.85765925e-05]
[3832 1660 1103  493  280  142  142  142  149  157] [ -1.00328391e-02  -9.03341289e-03  -8.03398663e-03  -7.03456038e-03
  -6.03513412e-03  -5.03570787e-03  -4.03628161e-03  -3.03685536e-03
  -2.03742910e-03  -1.03800285e-03  -3.85765925e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088921 minutes
Epoch 0
Fine tuning took 0.087480 minutes
Epoch 0
Fine tuning took 0.087985 minutes
{'zero': {0: [0.17733990147783252, 0.23645320197044334, 0.20073891625615764, 0.045566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.62438423645320196, 0.74384236453201968, 0.48152709359605911], 5: [0.057881773399014777, 0.13916256157635468, 0.055418719211822662, 0.47290640394088668], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17733990147783252, 0.22044334975369459, 0.21305418719211822, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.60960591133004927, 0.73399014778325122, 0.45935960591133007], 5: [0.057881773399014777, 0.16995073891625614, 0.05295566502463054, 0.50123152709359609], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17733990147783252, 0.26847290640394089, 0.25246305418719212, 0.056650246305418719], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.6071428571428571, 0.7068965517241379, 0.44088669950738918], 5: [0.057881773399014777, 0.12438423645320197, 0.04064039408866995, 0.50246305418719217], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17733990147783252, 0.24630541871921183, 0.24753694581280788, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.6219211822660099, 0.69950738916256161, 0.48152709359605911], 5: [0.057881773399014777, 0.13177339901477833, 0.05295566502463054, 0.47906403940886699], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234809 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81393
Epoch 1, cost is  3.56809
Epoch 2, cost is  3.52501
Epoch 3, cost is  3.53263
Epoch 4, cost is  3.57177
Training took 0.159026 minutes
Weight histogram
[1525  923 1429 1024 1147 1239 1343 1003  384  108] [-0.45754325 -0.41211514 -0.36668703 -0.32125892 -0.27583081 -0.2304027
 -0.18497459 -0.13954648 -0.09411836 -0.04869025 -0.00326214]
[ 248  551  850 1133 1163 1179 1180 1218 1307 1296] [-0.45754325 -0.41211514 -0.36668703 -0.32125892 -0.27583081 -0.2304027
 -0.18497459 -0.13954648 -0.09411836 -0.04869025 -0.00326214]
-16.5938
19.8031
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233998 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.04003
Epoch 1, cost is  3.77958
Epoch 2, cost is  3.75663
Epoch 3, cost is  3.77919
Epoch 4, cost is  3.82483
Training took 0.159032 minutes
Weight histogram
[1487 1091 1280  944 1241 1261 1610 2134  874  228] [-0.47296545 -0.42599512 -0.37902479 -0.33205446 -0.28508413 -0.2381138
 -0.19114346 -0.14417313 -0.0972028  -0.05023247 -0.00326214]
[ 522 1132 1769 1412 1211 1222 1206 1189 1243 1244] [-0.47296545 -0.42599512 -0.37902479 -0.33205446 -0.28508413 -0.2381138
 -0.19114346 -0.14417313 -0.0972028  -0.05023247 -0.00326214]
-14.6912
20.6518
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.61129
Epoch 1, cost is  3.9087
Epoch 2, cost is  4.17498
Epoch 3, cost is  4.63632
Epoch 4, cost is  5.0673
Training took 0.106707 minutes
Weight histogram
[1300 1711 1627 1718 1489  770  605  370  204  331] [-0.31483021 -0.28363119 -0.25243216 -0.22123313 -0.1900341  -0.15883507
 -0.12763605 -0.09643702 -0.06523799 -0.03403896 -0.00283994]
[ 509  506  673  901 1075 1226 1229 1297 1379 1330] [-0.31483021 -0.28363119 -0.25243216 -0.22123313 -0.1900341  -0.15883507
 -0.12763605 -0.09643702 -0.06523799 -0.03403896 -0.00283994]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080946 minutes
Epoch 0
Fine tuning took 0.081096 minutes
Epoch 0
Fine tuning took 0.081128 minutes
{'zero': {0: [0.33866995073891626, 0.088669950738916259, 0.083743842364532015, 0.066502463054187194], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50862068965517238, 0.8854679802955665, 0.88054187192118227, 0.90394088669950734], 5: [0.15270935960591134, 0.025862068965517241, 0.035714285714285712, 0.029556650246305417], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33866995073891626, 0.23645320197044334, 0.17857142857142858, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50862068965517238, 0.5923645320197044, 0.69334975369458129, 0.64778325123152714], 5: [0.15270935960591134, 0.17118226600985223, 0.12807881773399016, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33866995073891626, 0.23152709359605911, 0.21305418719211822, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50862068965517238, 0.63793103448275867, 0.64901477832512311, 0.57758620689655171], 5: [0.15270935960591134, 0.13054187192118227, 0.13793103448275862, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33866995073891626, 0.25, 0.18965517241379309, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50862068965517238, 0.58990147783251234, 0.69827586206896552, 0.66502463054187189], 5: [0.15270935960591134, 0.16009852216748768, 0.11206896551724138, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235166 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81393
Epoch 1, cost is  3.56809
Epoch 2, cost is  3.52501
Epoch 3, cost is  3.53263
Epoch 4, cost is  3.57177
Training took 0.159460 minutes
Weight histogram
[1525  923 1429 1024 1147 1239 1343 1003  384  108] [-0.45754325 -0.41211514 -0.36668703 -0.32125892 -0.27583081 -0.2304027
 -0.18497459 -0.13954648 -0.09411836 -0.04869025 -0.00326214]
[ 248  551  850 1133 1163 1179 1180 1218 1307 1296] [-0.45754325 -0.41211514 -0.36668703 -0.32125892 -0.27583081 -0.2304027
 -0.18497459 -0.13954648 -0.09411836 -0.04869025 -0.00326214]
-16.5938
19.8031
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235193 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.04003
Epoch 1, cost is  3.77958
Epoch 2, cost is  3.75663
Epoch 3, cost is  3.77919
Epoch 4, cost is  3.82483
Training took 0.161646 minutes
Weight histogram
[1487 1091 1280  944 1241 1261 1610 2134  874  228] [-0.47296545 -0.42599512 -0.37902479 -0.33205446 -0.28508413 -0.2381138
 -0.19114346 -0.14417313 -0.0972028  -0.05023247 -0.00326214]
[ 522 1132 1769 1412 1211 1222 1206 1189 1243 1244] [-0.47296545 -0.42599512 -0.37902479 -0.33205446 -0.28508413 -0.2381138
 -0.19114346 -0.14417313 -0.0972028  -0.05023247 -0.00326214]
-14.6912
20.6518
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.20586
Epoch 1, cost is  3.27852
Epoch 2, cost is  3.44623
Epoch 3, cost is  3.73476
Epoch 4, cost is  4.12551
Training took 0.122389 minutes
Weight histogram
[1018 1618 1619 1506 1389 1149  801  441  209  375] [-0.27973735 -0.25203982 -0.22434228 -0.19664475 -0.16894721 -0.14124967
 -0.11355214 -0.0858546  -0.05815707 -0.03045953 -0.002762  ]
[ 511  529  762  905 1078 1177 1284 1310 1291 1278] [-0.27973735 -0.25203982 -0.22434228 -0.19664475 -0.16894721 -0.14124967
 -0.11355214 -0.0858546  -0.05815707 -0.03045953 -0.002762  ]
-9.2547
11.2219
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082570 minutes
Epoch 0
Fine tuning took 0.083733 minutes
Epoch 0
Fine tuning took 0.083200 minutes
{'zero': {0: [0.20566502463054187, 0.23152709359605911, 0.20566502463054187, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.70566502463054193, 0.75862068965517238, 0.72783251231527091], 5: [0.075123152709359611, 0.062807881773399021, 0.035714285714285712, 0.030788177339901478], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20566502463054187, 0.25738916256157635, 0.19827586206896552, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.6280788177339901, 0.62438423645320196, 0.60960591133004927], 5: [0.075123152709359611, 0.1145320197044335, 0.17733990147783252, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20566502463054187, 0.26724137931034481, 0.25862068965517243, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.61576354679802958, 0.57389162561576357, 0.59852216748768472], 5: [0.075123152709359611, 0.11699507389162561, 0.16748768472906403, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20566502463054187, 0.23891625615763548, 0.19950738916256158, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.57635467980295563, 0.61330049261083741, 0.54064039408866993], 5: [0.075123152709359611, 0.18472906403940886, 0.18719211822660098, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234715 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81393
Epoch 1, cost is  3.56809
Epoch 2, cost is  3.52501
Epoch 3, cost is  3.53263
Epoch 4, cost is  3.57177
Training took 0.161951 minutes
Weight histogram
[1525  923 1429 1024 1147 1239 1343 1003  384  108] [-0.45754325 -0.41211514 -0.36668703 -0.32125892 -0.27583081 -0.2304027
 -0.18497459 -0.13954648 -0.09411836 -0.04869025 -0.00326214]
[ 248  551  850 1133 1163 1179 1180 1218 1307 1296] [-0.45754325 -0.41211514 -0.36668703 -0.32125892 -0.27583081 -0.2304027
 -0.18497459 -0.13954648 -0.09411836 -0.04869025 -0.00326214]
-16.5938
19.8031
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236133 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.04003
Epoch 1, cost is  3.77958
Epoch 2, cost is  3.75663
Epoch 3, cost is  3.77919
Epoch 4, cost is  3.82483
Training took 0.159174 minutes
Weight histogram
[1487 1091 1280  944 1241 1261 1610 2134  874  228] [-0.47296545 -0.42599512 -0.37902479 -0.33205446 -0.28508413 -0.2381138
 -0.19114346 -0.14417313 -0.0972028  -0.05023247 -0.00326214]
[ 522 1132 1769 1412 1211 1222 1206 1189 1243 1244] [-0.47296545 -0.42599512 -0.37902479 -0.33205446 -0.28508413 -0.2381138
 -0.19114346 -0.14417313 -0.0972028  -0.05023247 -0.00326214]
-14.6912
20.6518
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.69532
Epoch 1, cost is  2.3504
Epoch 2, cost is  2.26002
Epoch 3, cost is  2.3361
Epoch 4, cost is  2.50319
Training took 0.180844 minutes
Weight histogram
[1316 1628 1572 1465 1318 1036  752  392  267  379] [-0.18797739 -0.16946226 -0.15094713 -0.13243199 -0.11391686 -0.09540173
 -0.0768866  -0.05837147 -0.03985634 -0.02134121 -0.00282607]
[ 551  512  746  937 1058 1176 1259 1286 1386 1214] [-0.18797739 -0.16946226 -0.15094713 -0.13243199 -0.11391686 -0.09540173
 -0.0768866  -0.05837147 -0.03985634 -0.02134121 -0.00282607]
-6.41529
7.90691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090465 minutes
Epoch 0
Fine tuning took 0.093619 minutes
Epoch 0
Fine tuning took 0.090362 minutes
{'zero': {0: [0.077586206896551727, 0.10837438423645321, 0.068965517241379309, 0.088669950738916259], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83004926108374388, 0.84359605911330049, 0.86945812807881773, 0.86945812807881773], 5: [0.092364532019704432, 0.048029556650246302, 0.061576354679802957, 0.041871921182266007], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.077586206896551727, 0.21798029556650247, 0.18842364532019704, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83004926108374388, 0.66379310344827591, 0.66502463054187189, 0.60221674876847286], 5: [0.092364532019704432, 0.11822660098522167, 0.14655172413793102, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.077586206896551727, 0.24384236453201971, 0.19704433497536947, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83004926108374388, 0.66256157635467983, 0.67364532019704437, 0.59975369458128081], 5: [0.092364532019704432, 0.093596059113300489, 0.12931034482758622, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.077586206896551727, 0.17118226600985223, 0.17980295566502463, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83004926108374388, 0.75246305418719217, 0.72413793103448276, 0.66995073891625612], 5: [0.092364532019704432, 0.076354679802955669, 0.096059113300492605, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.253892 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.76717
Epoch 1, cost is  1.69966
Epoch 2, cost is  1.67167
Epoch 3, cost is  1.65528
Epoch 4, cost is  1.64357
Training took 0.160938 minutes
Weight histogram
[2017 1943 1593 1178 1310  771  534  306  236  237] [-0.12362612 -0.11128586 -0.09894561 -0.08660535 -0.0742651  -0.06192484
 -0.04958458 -0.03724433 -0.02490407 -0.01256381 -0.00022356]
[ 410  401  573  799  973 1125 1213 1377 1542 1712] [-0.12362612 -0.11128586 -0.09894561 -0.08660535 -0.0742651  -0.06192484
 -0.04958458 -0.03724433 -0.02490407 -0.01256381 -0.00022356]
-2.88957
4.59218
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235967 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84323
Epoch 1, cost is  1.77241
Epoch 2, cost is  1.74308
Epoch 3, cost is  1.7262
Epoch 4, cost is  1.71508
Training took 0.160635 minutes
Weight histogram
[2016 1863 1485 1141 1403 1430 1128  667  482  535] [-0.12635487 -0.11374174 -0.10112861 -0.08851548 -0.07590235 -0.06328922
 -0.05067608 -0.03806295 -0.02544982 -0.01283669 -0.00022356]
[ 852  836 1205 1365  956 1113 1195 1381 1551 1696] [-0.12635487 -0.11374174 -0.10112861 -0.08851548 -0.07590235 -0.06328922
 -0.05067608 -0.03806295 -0.02544982 -0.01283669 -0.00022356]
-3.26361
4.33205
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.5947
Epoch 1, cost is  5.53368
Epoch 2, cost is  4.78722
Epoch 3, cost is  4.43895
Epoch 4, cost is  4.20501
Training took 0.110910 minutes
Weight histogram
[ 583 1392 1215 1160  808  799  650  601  752 2165] [-0.10474743 -0.09429904 -0.08385065 -0.07340226 -0.06295387 -0.05250548
 -0.0420571  -0.03160871 -0.02116032 -0.01071193 -0.00026354]
[1934  663  654  725  826  890  987 1089 1165 1192] [-0.10474743 -0.09429904 -0.08385065 -0.07340226 -0.06295387 -0.05250548
 -0.0420571  -0.03160871 -0.02116032 -0.01071193 -0.00026354]
-1.55839
3.39939
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.085989 minutes
Epoch 0
Fine tuning took 0.083152 minutes
Epoch 0
Fine tuning took 0.085371 minutes
{'zero': {0: [0.27339901477832512, 0.19704433497536947, 0.11083743842364532, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35344827586206895, 0.58251231527093594, 0.70197044334975367, 0.65640394088669951], 5: [0.37315270935960593, 0.22044334975369459, 0.18719211822660098, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27339901477832512, 0.20443349753694581, 0.10591133004926108, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35344827586206895, 0.56527093596059108, 0.65517241379310343, 0.66995073891625612], 5: [0.37315270935960593, 0.23029556650246305, 0.23891625615763548, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27339901477832512, 0.2019704433497537, 0.099753694581280791, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35344827586206895, 0.55049261083743839, 0.64655172413793105, 0.69950738916256161], 5: [0.37315270935960593, 0.24753694581280788, 0.2536945812807882, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27339901477832512, 0.20935960591133004, 0.098522167487684734, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35344827586206895, 0.55665024630541871, 0.64532019704433496, 0.68596059113300489], 5: [0.37315270935960593, 0.23399014778325122, 0.25615763546798032, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236178 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.76717
Epoch 1, cost is  1.69966
Epoch 2, cost is  1.67167
Epoch 3, cost is  1.65528
Epoch 4, cost is  1.64357
Training took 0.159469 minutes
Weight histogram
[2017 1943 1593 1178 1310  771  534  306  236  237] [-0.12362612 -0.11128586 -0.09894561 -0.08660535 -0.0742651  -0.06192484
 -0.04958458 -0.03724433 -0.02490407 -0.01256381 -0.00022356]
[ 410  401  573  799  973 1125 1213 1377 1542 1712] [-0.12362612 -0.11128586 -0.09894561 -0.08660535 -0.0742651  -0.06192484
 -0.04958458 -0.03724433 -0.02490407 -0.01256381 -0.00022356]
-2.88957
4.59218
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235426 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84323
Epoch 1, cost is  1.77241
Epoch 2, cost is  1.74308
Epoch 3, cost is  1.7262
Epoch 4, cost is  1.71508
Training took 0.160798 minutes
Weight histogram
[2016 1863 1485 1141 1403 1430 1128  667  482  535] [-0.12635487 -0.11374174 -0.10112861 -0.08851548 -0.07590235 -0.06328922
 -0.05067608 -0.03806295 -0.02544982 -0.01283669 -0.00022356]
[ 852  836 1205 1365  956 1113 1195 1381 1551 1696] [-0.12635487 -0.11374174 -0.10112861 -0.08851548 -0.07590235 -0.06328922
 -0.05067608 -0.03806295 -0.02544982 -0.01283669 -0.00022356]
-3.26361
4.33205
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55588
Epoch 1, cost is  5.32991
Epoch 2, cost is  4.33358
Epoch 3, cost is  3.88821
Epoch 4, cost is  3.62423
Training took 0.124547 minutes
Weight histogram
[ 708 1383 1215 1054  885  694  654  649  925 1958] [-0.0834422  -0.07512574 -0.06680928 -0.05849282 -0.05017636 -0.0418599
 -0.03354345 -0.02522699 -0.01691053 -0.00859407 -0.00027761]
[1971  684  657  677  783  878  956 1081 1132 1306] [-0.0834422  -0.07512574 -0.06680928 -0.05849282 -0.05017636 -0.0418599
 -0.03354345 -0.02522699 -0.01691053 -0.00859407 -0.00027761]
-1.38577
2.70075
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084705 minutes
Epoch 0
Fine tuning took 0.084234 minutes
Epoch 0
Fine tuning took 0.083511 minutes
{'zero': {0: [0.3288177339901478, 0.31650246305418717, 0.23029556650246305, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44334975369458129, 0.4963054187192118, 0.63300492610837433, 0.65147783251231528], 5: [0.22783251231527094, 0.18719211822660098, 0.13669950738916256, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3288177339901478, 0.20935960591133004, 0.1206896551724138, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44334975369458129, 0.49384236453201968, 0.67487684729064035, 0.64039408866995073], 5: [0.22783251231527094, 0.29679802955665024, 0.20443349753694581, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3288177339901478, 0.2105911330049261, 0.1354679802955665, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44334975369458129, 0.52463054187192115, 0.66502463054187189, 0.59729064039408863], 5: [0.22783251231527094, 0.26477832512315269, 0.19950738916256158, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3288177339901478, 0.17610837438423646, 0.11576354679802955, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44334975369458129, 0.46798029556650245, 0.69950738916256161, 0.57266009852216748], 5: [0.22783251231527094, 0.35591133004926107, 0.18472906403940886, 0.28201970443349755], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236240 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.76717
Epoch 1, cost is  1.69966
Epoch 2, cost is  1.67167
Epoch 3, cost is  1.65528
Epoch 4, cost is  1.64357
Training took 0.160411 minutes
Weight histogram
[2017 1943 1593 1178 1310  771  534  306  236  237] [-0.12362612 -0.11128586 -0.09894561 -0.08660535 -0.0742651  -0.06192484
 -0.04958458 -0.03724433 -0.02490407 -0.01256381 -0.00022356]
[ 410  401  573  799  973 1125 1213 1377 1542 1712] [-0.12362612 -0.11128586 -0.09894561 -0.08660535 -0.0742651  -0.06192484
 -0.04958458 -0.03724433 -0.02490407 -0.01256381 -0.00022356]
-2.88957
4.59218
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236484 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84323
Epoch 1, cost is  1.77241
Epoch 2, cost is  1.74308
Epoch 3, cost is  1.7262
Epoch 4, cost is  1.71508
Training took 0.159601 minutes
Weight histogram
[2016 1863 1485 1141 1403 1430 1128  667  482  535] [-0.12635487 -0.11374174 -0.10112861 -0.08851548 -0.07590235 -0.06328922
 -0.05067608 -0.03806295 -0.02544982 -0.01283669 -0.00022356]
[ 852  836 1205 1365  956 1113 1195 1381 1551 1696] [-0.12635487 -0.11374174 -0.10112861 -0.08851548 -0.07590235 -0.06328922
 -0.05067608 -0.03806295 -0.02544982 -0.01283669 -0.00022356]
-3.26361
4.33205
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.47066
Epoch 1, cost is  5.07897
Epoch 2, cost is  3.78951
Epoch 3, cost is  3.17747
Epoch 4, cost is  2.81628
Training took 0.181122 minutes
Weight histogram
[ 823 1584 1196  973  885  798  606  682 1816  762] [-0.05459121 -0.04915912 -0.04372704 -0.03829495 -0.03286286 -0.02743077
 -0.02199868 -0.01656659 -0.0111345  -0.00570242 -0.00027033]
[2052  678  646  685  738  824  941 1069 1218 1274] [-0.05459121 -0.04915912 -0.04372704 -0.03829495 -0.03286286 -0.02743077
 -0.02199868 -0.01656659 -0.0111345  -0.00570242 -0.00027033]
-1.11031
2.02951
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089049 minutes
Epoch 0
Fine tuning took 0.087883 minutes
Epoch 0
Fine tuning took 0.090046 minutes
{'zero': {0: [0.22906403940886699, 0.20320197044334976, 0.21674876847290642, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.61206896551724133, 0.66379310344827591, 0.67241379310344829], 5: [0.16748768472906403, 0.18472906403940886, 0.11945812807881774, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22906403940886699, 0.16133004926108374, 0.16502463054187191, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.67610837438423643, 0.72044334975369462, 0.71551724137931039], 5: [0.16748768472906403, 0.1625615763546798, 0.1145320197044335, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22906403940886699, 0.19088669950738915, 0.19211822660098521, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.66133004926108374, 0.68226600985221675, 0.69704433497536944], 5: [0.16748768472906403, 0.14778325123152711, 0.12561576354679804, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22906403940886699, 0.16871921182266009, 0.18842364532019704, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.6576354679802956, 0.68103448275862066, 0.72660098522167482], 5: [0.16748768472906403, 0.17364532019704434, 0.13054187192118227, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234735 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.02304
Epoch 1, cost is  2.95115
Epoch 2, cost is  2.89279
Epoch 3, cost is  2.83899
Epoch 4, cost is  2.79334
Training took 0.162218 minutes
Weight histogram
[1588 1173 1121  957  867  824  850  827 1572  346] [ -5.02910987e-02  -4.52539586e-02  -4.02168186e-02  -3.51796786e-02
  -3.01425385e-02  -2.51053985e-02  -2.00682585e-02  -1.50311185e-02
  -9.99397842e-03  -4.95683839e-03   8.03016374e-05]
[1700  738  677  714  805  874  998 1070 1239 1310] [ -5.02910987e-02  -4.52539586e-02  -4.02168186e-02  -3.51796786e-02
  -3.01425385e-02  -2.51053985e-02  -2.00682585e-02  -1.50311185e-02
  -9.99397842e-03  -4.95683839e-03   8.03016374e-05]
-0.764954
1.31614
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233960 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.20879
Epoch 1, cost is  3.13312
Epoch 2, cost is  3.07009
Epoch 3, cost is  3.01321
Epoch 4, cost is  2.95942
Training took 0.161474 minutes
Weight histogram
[1432 1046 1062  856  924  866  854 1069 3487  554] [ -4.75914180e-02  -4.28242461e-02  -3.80570741e-02  -3.32899021e-02
  -2.85227302e-02  -2.37555582e-02  -1.89883862e-02  -1.42212143e-02
  -9.45404230e-03  -4.68687033e-03   8.03016374e-05]
[3555 1045  684  740  777  849  962 1043 1208 1287] [ -4.75914180e-02  -4.28242461e-02  -3.80570741e-02  -3.32899021e-02
  -2.85227302e-02  -2.37555582e-02  -1.89883862e-02  -1.42212143e-02
  -9.45404230e-03  -4.68687033e-03   8.03016374e-05]
-0.663906
1.07037
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.87866
Epoch 1, cost is  6.80649
Epoch 2, cost is  6.74189
Epoch 3, cost is  6.67912
Epoch 4, cost is  6.61668
Training took 0.102527 minutes
Weight histogram
[  92 5406 3180  507  310  210  151  112   82   75] [ -7.25918217e-03  -6.53596509e-03  -5.81274802e-03  -5.08953094e-03
  -4.36631387e-03  -3.64309679e-03  -2.91987972e-03  -2.19666264e-03
  -1.47344556e-03  -7.50228489e-04  -2.70114142e-05]
[6397 2183  630  252  145  118  107  102  100   91] [ -7.25918217e-03  -6.53596509e-03  -5.81274802e-03  -5.08953094e-03
  -4.36631387e-03  -3.64309679e-03  -2.91987972e-03  -2.19666264e-03
  -1.47344556e-03  -7.50228489e-04  -2.70114142e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081519 minutes
Epoch 0
Fine tuning took 0.081918 minutes
Epoch 0
Fine tuning took 0.080422 minutes
{'zero': {0: [0.44334975369458129, 0.23029556650246305, 0.49014778325123154, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.53201970443349755, 0.36576354679802958, 0.46674876847290642], 5: [0.029556650246305417, 0.2376847290640394, 0.14408866995073891, 0.41502463054187194], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.44334975369458129, 0.25738916256157635, 0.54556650246305416, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.4963054187192118, 0.33004926108374383, 0.44211822660098521], 5: [0.029556650246305417, 0.24630541871921183, 0.12438423645320197, 0.45073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.44334975369458129, 0.23275862068965517, 0.51477832512315269, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.50985221674876846, 0.35098522167487683, 0.48399014778325122], 5: [0.029556650246305417, 0.25738916256157635, 0.13423645320197045, 0.39408866995073893], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.44334975369458129, 0.22783251231527094, 0.52463054187192115, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.51354679802955661, 0.36576354679802958, 0.44704433497536944], 5: [0.029556650246305417, 0.25862068965517243, 0.10960591133004927, 0.44950738916256155], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235044 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.02304
Epoch 1, cost is  2.95115
Epoch 2, cost is  2.89279
Epoch 3, cost is  2.83899
Epoch 4, cost is  2.79334
Training took 0.161376 minutes
Weight histogram
[1588 1173 1121  957  867  824  850  827 1572  346] [ -5.02910987e-02  -4.52539586e-02  -4.02168186e-02  -3.51796786e-02
  -3.01425385e-02  -2.51053985e-02  -2.00682585e-02  -1.50311185e-02
  -9.99397842e-03  -4.95683839e-03   8.03016374e-05]
[1700  738  677  714  805  874  998 1070 1239 1310] [ -5.02910987e-02  -4.52539586e-02  -4.02168186e-02  -3.51796786e-02
  -3.01425385e-02  -2.51053985e-02  -2.00682585e-02  -1.50311185e-02
  -9.99397842e-03  -4.95683839e-03   8.03016374e-05]
-0.764954
1.31614
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234221 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.20879
Epoch 1, cost is  3.13312
Epoch 2, cost is  3.07009
Epoch 3, cost is  3.01321
Epoch 4, cost is  2.95942
Training took 0.159472 minutes
Weight histogram
[1432 1046 1062  856  924  866  854 1069 3487  554] [ -4.75914180e-02  -4.28242461e-02  -3.80570741e-02  -3.32899021e-02
  -2.85227302e-02  -2.37555582e-02  -1.89883862e-02  -1.42212143e-02
  -9.45404230e-03  -4.68687033e-03   8.03016374e-05]
[3555 1045  684  740  777  849  962 1043 1208 1287] [ -4.75914180e-02  -4.28242461e-02  -3.80570741e-02  -3.32899021e-02
  -2.85227302e-02  -2.37555582e-02  -1.89883862e-02  -1.42212143e-02
  -9.45404230e-03  -4.68687033e-03   8.03016374e-05]
-0.663906
1.07037
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8591
Epoch 1, cost is  6.77971
Epoch 2, cost is  6.71329
Epoch 3, cost is  6.64929
Epoch 4, cost is  6.58395
Training took 0.124544 minutes
Weight histogram
[  93  119  449 7822  686  366  232  159  109   90] [ -8.75947252e-03  -7.88853231e-03  -7.01759209e-03  -6.14665188e-03
  -5.27571167e-03  -4.40477146e-03  -3.53383125e-03  -2.66289103e-03
  -1.79195082e-03  -9.21010609e-04  -5.00703973e-05]
[5910 2296  814  363  144  130  123  113  118  114] [ -8.75947252e-03  -7.88853231e-03  -7.01759209e-03  -6.14665188e-03
  -5.27571167e-03  -4.40477146e-03  -3.53383125e-03  -2.66289103e-03
  -1.79195082e-03  -9.21010609e-04  -5.00703973e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083847 minutes
Epoch 0
Fine tuning took 0.084154 minutes
Epoch 0
Fine tuning took 0.083466 minutes
{'zero': {0: [0.4211822660098522, 0.27216748768472904, 0.34113300492610837, 0.020935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.52093596059113301, 0.50862068965517238, 0.79064039408866993], 5: [0.025862068965517241, 0.20689655172413793, 0.15024630541871922, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.4211822660098522, 0.25862068965517243, 0.36699507389162561, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.55295566502463056, 0.45197044334975367, 0.8214285714285714], 5: [0.025862068965517241, 0.18842364532019704, 0.18103448275862069, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.4211822660098522, 0.24261083743842365, 0.33374384236453203, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.55295566502463056, 0.48152709359605911, 0.80295566502463056], 5: [0.025862068965517241, 0.20443349753694581, 0.18472906403940886, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.4211822660098522, 0.24507389162561577, 0.35098522167487683, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.54064039408866993, 0.47167487684729065, 0.79802955665024633], 5: [0.025862068965517241, 0.21428571428571427, 0.17733990147783252, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234659 minutes
Weight histogram
[ 139  425  624  532  773 2232 2378 2003  951   68] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 121  189  252  391  601  767  868 1847 2368 2721] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.865655
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.02304
Epoch 1, cost is  2.95115
Epoch 2, cost is  2.89279
Epoch 3, cost is  2.83899
Epoch 4, cost is  2.79334
Training took 0.160535 minutes
Weight histogram
[1588 1173 1121  957  867  824  850  827 1572  346] [ -5.02910987e-02  -4.52539586e-02  -4.02168186e-02  -3.51796786e-02
  -3.01425385e-02  -2.51053985e-02  -2.00682585e-02  -1.50311185e-02
  -9.99397842e-03  -4.95683839e-03   8.03016374e-05]
[1700  738  677  714  805  874  998 1070 1239 1310] [ -5.02910987e-02  -4.52539586e-02  -4.02168186e-02  -3.51796786e-02
  -3.01425385e-02  -2.51053985e-02  -2.00682585e-02  -1.50311185e-02
  -9.99397842e-03  -4.95683839e-03   8.03016374e-05]
-0.764954
1.31614
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234527 minutes
Weight histogram
[ 157  538 1005 1141  974 1574 2668 2885 1106  102] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
[ 263  352  504  769 1207 1057  799 1883 2387 2929] [ -1.55507252e-04  -2.05939185e-05   1.14319415e-04   2.49232749e-04
   3.84146083e-04   5.19059417e-04   6.53972750e-04   7.88886084e-04
   9.23799418e-04   1.05871275e-03   1.19362609e-03]
-1.24142
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.20879
Epoch 1, cost is  3.13312
Epoch 2, cost is  3.07009
Epoch 3, cost is  3.01321
Epoch 4, cost is  2.95942
Training took 0.160195 minutes
Weight histogram
[1432 1046 1062  856  924  866  854 1069 3487  554] [ -4.75914180e-02  -4.28242461e-02  -3.80570741e-02  -3.32899021e-02
  -2.85227302e-02  -2.37555582e-02  -1.89883862e-02  -1.42212143e-02
  -9.45404230e-03  -4.68687033e-03   8.03016374e-05]
[3555 1045  684  740  777  849  962 1043 1208 1287] [ -4.75914180e-02  -4.28242461e-02  -3.80570741e-02  -3.32899021e-02
  -2.85227302e-02  -2.37555582e-02  -1.89883862e-02  -1.42212143e-02
  -9.45404230e-03  -4.68687033e-03   8.03016374e-05]
-0.663906
1.07037
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8012
Epoch 1, cost is  6.70437
Epoch 2, cost is  6.63032
Epoch 3, cost is  6.56066
Epoch 4, cost is  6.49129
Training took 0.180817 minutes
Weight histogram
[ 162  200  255 6101 2013  608  334  210  139  103] [ -1.00328391e-02  -9.03339527e-03  -8.03395140e-03  -7.03450753e-03
  -6.03506366e-03  -5.03561979e-03  -4.03617592e-03  -3.03673205e-03
  -2.03728818e-03  -1.03784431e-03  -3.84004379e-05]
[5079 2202 1339  493  280  142  142  142  149  157] [ -1.00328391e-02  -9.03339527e-03  -8.03395140e-03  -7.03450753e-03
  -6.03506366e-03  -5.03561979e-03  -4.03617592e-03  -3.03673205e-03
  -2.03728818e-03  -1.03784431e-03  -3.84004379e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087938 minutes
Epoch 0
Fine tuning took 0.088552 minutes
Epoch 0
Fine tuning took 0.087649 minutes
{'zero': {0: [0.44581280788177341, 0.37807881773399016, 0.19088669950738915, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52463054187192115, 0.33004926108374383, 0.61206896551724133, 0.65886699507389157], 5: [0.029556650246305417, 0.29187192118226601, 0.19704433497536947, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.44581280788177341, 0.3891625615763547, 0.18349753694581281, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52463054187192115, 0.34729064039408869, 0.63177339901477836, 0.71182266009852213], 5: [0.029556650246305417, 0.26354679802955666, 0.18472906403940886, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.44581280788177341, 0.37931034482758619, 0.17857142857142858, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52463054187192115, 0.33743842364532017, 0.64162561576354682, 0.68103448275862066], 5: [0.029556650246305417, 0.28325123152709358, 0.17980295566502463, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.44581280788177341, 0.39285714285714285, 0.17980295566502463, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52463054187192115, 0.34852216748768472, 0.62684729064039413, 0.68103448275862066], 5: [0.029556650246305417, 0.25862068965517243, 0.19334975369458129, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234469 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.37639
Epoch 1, cost is  4.10643
Epoch 2, cost is  4.06274
Epoch 3, cost is  4.06371
Epoch 4, cost is  4.09893
Training took 0.160780 minutes
Weight histogram
[1697 1078 1272 1750 1246 1055 1967 1305  644  136] [-0.54230571 -0.48840135 -0.43449699 -0.38059264 -0.32668828 -0.27278392
 -0.21887957 -0.16497521 -0.11107085 -0.0571665  -0.00326214]
[ 317  728 1145 1344 1383 1360 1416 1536 1504 1417] [-0.54230571 -0.48840135 -0.43449699 -0.38059264 -0.32668828 -0.27278392
 -0.21887957 -0.16497521 -0.11107085 -0.0571665  -0.00326214]
-18.999
23.4654
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235135 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.4098
Epoch 1, cost is  4.10726
Epoch 2, cost is  4.0405
Epoch 3, cost is  4.03574
Epoch 4, cost is  4.05769
Training took 0.161758 minutes
Weight histogram
[1623 1039 1386 1630 1156 1266 1925 2383 1456  311] [-0.56565553 -0.50941619 -0.45317685 -0.39693751 -0.34069817 -0.28445884
 -0.2282195  -0.17198016 -0.11574082 -0.05950148 -0.00326214]
[ 658 1496 2044 1398 1425 1390 1378 1451 1471 1464] [-0.56565553 -0.50941619 -0.45317685 -0.39693751 -0.34069817 -0.28445884
 -0.2282195  -0.17198016 -0.11574082 -0.05950148 -0.00326214]
-16.7704
24.6905
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.61205
Epoch 1, cost is  3.92904
Epoch 2, cost is  4.16644
Epoch 3, cost is  4.57826
Epoch 4, cost is  5.02011
Training took 0.106464 minutes
Weight histogram
[1450 2071 1975 2103 1816  938  719  444  240  394] [-0.31483021 -0.28363012 -0.25243003 -0.22122994 -0.19002985 -0.15882976
 -0.12762967 -0.09642958 -0.06522949 -0.0340294  -0.00282931]
[ 609  609  810 1087 1300 1479 1487 1561 1664 1544] [-0.31483021 -0.28363012 -0.25243003 -0.22122994 -0.19002985 -0.15882976
 -0.12762967 -0.09642958 -0.06522949 -0.0340294  -0.00282931]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080412 minutes
Epoch 0
Fine tuning took 0.081689 minutes
Epoch 0
Fine tuning took 0.080956 minutes
{'zero': {0: [0.30172413793103448, 0.13423645320197045, 0.1539408866995074, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.79802955665024633, 0.64162561576354682, 0.66995073891625612], 5: [0.093596059113300489, 0.067733990147783252, 0.20443349753694581, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30172413793103448, 0.20443349753694581, 0.1539408866995074, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.71551724137931039, 0.73522167487684731, 0.68226600985221675], 5: [0.093596059113300489, 0.080049261083743842, 0.11083743842364532, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30172413793103448, 0.20320197044334976, 0.14901477832512317, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.70443349753694584, 0.73152709359605916, 0.71798029556650245], 5: [0.093596059113300489, 0.092364532019704432, 0.11945812807881774, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30172413793103448, 0.16133004926108374, 0.12807881773399016, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.77339901477832518, 0.76354679802955661, 0.71059113300492616], 5: [0.093596059113300489, 0.065270935960591137, 0.10837438423645321, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235264 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.37639
Epoch 1, cost is  4.10643
Epoch 2, cost is  4.06274
Epoch 3, cost is  4.06371
Epoch 4, cost is  4.09893
Training took 0.161468 minutes
Weight histogram
[1697 1078 1272 1750 1246 1055 1967 1305  644  136] [-0.54230571 -0.48840135 -0.43449699 -0.38059264 -0.32668828 -0.27278392
 -0.21887957 -0.16497521 -0.11107085 -0.0571665  -0.00326214]
[ 317  728 1145 1344 1383 1360 1416 1536 1504 1417] [-0.54230571 -0.48840135 -0.43449699 -0.38059264 -0.32668828 -0.27278392
 -0.21887957 -0.16497521 -0.11107085 -0.0571665  -0.00326214]
-18.999
23.4654
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235414 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.4098
Epoch 1, cost is  4.10726
Epoch 2, cost is  4.0405
Epoch 3, cost is  4.03574
Epoch 4, cost is  4.05769
Training took 0.161220 minutes
Weight histogram
[1623 1039 1386 1630 1156 1266 1925 2383 1456  311] [-0.56565553 -0.50941619 -0.45317685 -0.39693751 -0.34069817 -0.28445884
 -0.2282195  -0.17198016 -0.11574082 -0.05950148 -0.00326214]
[ 658 1496 2044 1398 1425 1390 1378 1451 1471 1464] [-0.56565553 -0.50941619 -0.45317685 -0.39693751 -0.34069817 -0.28445884
 -0.2282195  -0.17198016 -0.11574082 -0.05950148 -0.00326214]
-16.7704
24.6905
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.21513
Epoch 1, cost is  3.29947
Epoch 2, cost is  3.43161
Epoch 3, cost is  3.71954
Epoch 4, cost is  4.08845
Training took 0.122714 minutes
Weight histogram
[1140 1948 1938 1848 1676 1386  982  532  252  448] [-0.27973735 -0.2520392  -0.22434105 -0.1966429  -0.16894476 -0.14124661
 -0.11354846 -0.08585031 -0.05815216 -0.03045401 -0.00275586]
[ 612  636  916 1089 1294 1414 1540 1570 1544 1535] [-0.27973735 -0.2520392  -0.22434105 -0.1966429  -0.16894476 -0.14124661
 -0.11354846 -0.08585031 -0.05815216 -0.03045401 -0.00275586]
-9.2547
11.2219
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083310 minutes
Epoch 0
Fine tuning took 0.084452 minutes
Epoch 0
Fine tuning took 0.084162 minutes
{'zero': {0: [0.33990147783251229, 0.22167487684729065, 0.19950738916256158, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.62068965517241381, 0.64778325123152714, 0.66133004926108374], 5: [0.16133004926108374, 0.15763546798029557, 0.15270935960591134, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33990147783251229, 0.16748768472906403, 0.16871921182266009, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.68842364532019706, 0.66995073891625612, 0.69827586206896552], 5: [0.16133004926108374, 0.14408866995073891, 0.16133004926108374, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33990147783251229, 0.19088669950738915, 0.15270935960591134, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.66748768472906406, 0.7068965517241379, 0.68842364532019706], 5: [0.16133004926108374, 0.14162561576354679, 0.14039408866995073, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33990147783251229, 0.12561576354679804, 0.1206896551724138, 0.10960591133004927], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49876847290640391, 0.74630541871921185, 0.74014778325123154, 0.73522167487684731], 5: [0.16133004926108374, 0.12807881773399016, 0.13916256157635468, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234492 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.37639
Epoch 1, cost is  4.10643
Epoch 2, cost is  4.06274
Epoch 3, cost is  4.06371
Epoch 4, cost is  4.09893
Training took 0.160415 minutes
Weight histogram
[1697 1078 1272 1750 1246 1055 1967 1305  644  136] [-0.54230571 -0.48840135 -0.43449699 -0.38059264 -0.32668828 -0.27278392
 -0.21887957 -0.16497521 -0.11107085 -0.0571665  -0.00326214]
[ 317  728 1145 1344 1383 1360 1416 1536 1504 1417] [-0.54230571 -0.48840135 -0.43449699 -0.38059264 -0.32668828 -0.27278392
 -0.21887957 -0.16497521 -0.11107085 -0.0571665  -0.00326214]
-18.999
23.4654
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233821 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.4098
Epoch 1, cost is  4.10726
Epoch 2, cost is  4.0405
Epoch 3, cost is  4.03574
Epoch 4, cost is  4.05769
Training took 0.161894 minutes
Weight histogram
[1623 1039 1386 1630 1156 1266 1925 2383 1456  311] [-0.56565553 -0.50941619 -0.45317685 -0.39693751 -0.34069817 -0.28445884
 -0.2282195  -0.17198016 -0.11574082 -0.05950148 -0.00326214]
[ 658 1496 2044 1398 1425 1390 1378 1451 1471 1464] [-0.56565553 -0.50941619 -0.45317685 -0.39693751 -0.34069817 -0.28445884
 -0.2282195  -0.17198016 -0.11574082 -0.05950148 -0.00326214]
-16.7704
24.6905
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.70512
Epoch 1, cost is  2.37773
Epoch 2, cost is  2.28581
Epoch 3, cost is  2.3725
Epoch 4, cost is  2.52918
Training took 0.179718 minutes
Weight histogram
[1544 1949 1874 1768 1606 1252  908  476  322  451] [-0.18797739 -0.16945653 -0.15093567 -0.13241481 -0.11389395 -0.09537308
 -0.07685222 -0.05833136 -0.0398105  -0.02128964 -0.00276878]
[ 661  622  902 1132 1278 1413 1511 1548 1659 1424] [-0.18797739 -0.16945653 -0.15093567 -0.13241481 -0.11389395 -0.09537308
 -0.07685222 -0.05833136 -0.0398105  -0.02128964 -0.00276878]
-6.41529
7.90691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088786 minutes
Epoch 0
Fine tuning took 0.089671 minutes
Epoch 0
Fine tuning took 0.089048 minutes
{'zero': {0: [0.15270935960591134, 0.046798029556650245, 0.02832512315270936, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7857142857142857, 0.88669950738916259, 0.88177339901477836, 0.91009852216748766], 5: [0.061576354679802957, 0.066502463054187194, 0.089901477832512317, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15270935960591134, 0.1748768472906404, 0.14162561576354679, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7857142857142857, 0.72167487684729059, 0.70935960591133007, 0.68472906403940892], 5: [0.061576354679802957, 0.10344827586206896, 0.14901477832512317, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15270935960591134, 0.16133004926108374, 0.10960591133004927, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7857142857142857, 0.73399014778325122, 0.77586206896551724, 0.71921182266009853], 5: [0.061576354679802957, 0.10467980295566502, 0.1145320197044335, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15270935960591134, 0.18103448275862069, 0.12315270935960591, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7857142857142857, 0.69704433497536944, 0.73152709359605916, 0.70935960591133007], 5: [0.061576354679802957, 0.12192118226600986, 0.14532019704433496, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235417 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.77536
Epoch 1, cost is  1.70524
Epoch 2, cost is  1.67993
Epoch 3, cost is  1.66274
Epoch 4, cost is  1.65487
Training took 0.159359 minutes
Weight histogram
[2005 2115 1947 1866 1484 1057  741  404  262  269] [-0.13840507 -0.12458692 -0.11076877 -0.09695062 -0.08313246 -0.06931431
 -0.05549616 -0.04167801 -0.02785986 -0.01404171 -0.00022356]
[ 446  473  692  974 1145 1329 1463 1638 1909 2081] [-0.13840507 -0.12458692 -0.11076877 -0.09695062 -0.08313246 -0.06931431
 -0.05549616 -0.04167801 -0.02785986 -0.01404171 -0.00022356]
-3.49956
5.20579
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235480 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83523
Epoch 1, cost is  1.76109
Epoch 2, cost is  1.73279
Epoch 3, cost is  1.71583
Epoch 4, cost is  1.70715
Training took 0.158382 minutes
Weight histogram
[2247 2150 1665 1859 1343 1412 1541  827  541  590] [-0.13923551 -0.12533432 -0.11143312 -0.09753193 -0.08363073 -0.06972953
 -0.05582834 -0.04192714 -0.02802595 -0.01412475 -0.00022356]
[ 929  999 1468 1276 1135 1312 1479 1668 1903 2006] [-0.13923551 -0.12533432 -0.11143312 -0.09753193 -0.08363073 -0.06972953
 -0.05582834 -0.04192714 -0.02802595 -0.01412475 -0.00022356]
-3.64521
4.81954
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.59242
Epoch 1, cost is  5.53407
Epoch 2, cost is  4.80451
Epoch 3, cost is  4.46379
Epoch 4, cost is  4.23642
Training took 0.104757 minutes
Weight histogram
[ 583 1699 1486 1432  958  984  790  733  893 2592] [-0.10474743 -0.09429896 -0.08385049 -0.07340202 -0.06295356 -0.05250509
 -0.04205662 -0.03160815 -0.02115968 -0.01071122 -0.00026275]
[2297  801  783  870  992 1072 1188 1310 1400 1437] [-0.10474743 -0.09429896 -0.08385049 -0.07340202 -0.06295356 -0.05250509
 -0.04205662 -0.03160815 -0.02115968 -0.01071122 -0.00026275]
-1.55839
3.39939
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083197 minutes
Epoch 0
Fine tuning took 0.082974 minutes
Epoch 0
Fine tuning took 0.081781 minutes
{'zero': {0: [0.34113300492610837, 0.15640394088669951, 0.13669950738916256, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45689655172413796, 0.62684729064039413, 0.64778325123152714, 0.74014778325123154], 5: [0.2019704433497537, 0.21674876847290642, 0.21551724137931033, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.34113300492610837, 0.15147783251231528, 0.12561576354679804, 0.087438423645320201], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45689655172413796, 0.52093596059113301, 0.58004926108374388, 0.70197044334975367], 5: [0.2019704433497537, 0.32758620689655171, 0.29433497536945813, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.34113300492610837, 0.13916256157635468, 0.13300492610837439, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45689655172413796, 0.55788177339901479, 0.58497536945812811, 0.70935960591133007], 5: [0.2019704433497537, 0.30295566502463056, 0.28201970443349755, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.34113300492610837, 0.16625615763546797, 0.13300492610837439, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45689655172413796, 0.50369458128078815, 0.58497536945812811, 0.69088669950738912], 5: [0.2019704433497537, 0.33004926108374383, 0.28201970443349755, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234431 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.77536
Epoch 1, cost is  1.70524
Epoch 2, cost is  1.67993
Epoch 3, cost is  1.66274
Epoch 4, cost is  1.65487
Training took 0.159576 minutes
Weight histogram
[2005 2115 1947 1866 1484 1057  741  404  262  269] [-0.13840507 -0.12458692 -0.11076877 -0.09695062 -0.08313246 -0.06931431
 -0.05549616 -0.04167801 -0.02785986 -0.01404171 -0.00022356]
[ 446  473  692  974 1145 1329 1463 1638 1909 2081] [-0.13840507 -0.12458692 -0.11076877 -0.09695062 -0.08313246 -0.06931431
 -0.05549616 -0.04167801 -0.02785986 -0.01404171 -0.00022356]
-3.49956
5.20579
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235177 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83523
Epoch 1, cost is  1.76109
Epoch 2, cost is  1.73279
Epoch 3, cost is  1.71583
Epoch 4, cost is  1.70715
Training took 0.161499 minutes
Weight histogram
[2247 2150 1665 1859 1343 1412 1541  827  541  590] [-0.13923551 -0.12533432 -0.11143312 -0.09753193 -0.08363073 -0.06972953
 -0.05582834 -0.04192714 -0.02802595 -0.01412475 -0.00022356]
[ 929  999 1468 1276 1135 1312 1479 1668 1903 2006] [-0.13923551 -0.12533432 -0.11143312 -0.09753193 -0.08363073 -0.06972953
 -0.05582834 -0.04192714 -0.02802595 -0.01412475 -0.00022356]
-3.64521
4.81954
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55461
Epoch 1, cost is  5.32158
Epoch 2, cost is  4.34065
Epoch 3, cost is  3.9046
Epoch 4, cost is  3.64583
Training took 0.122989 minutes
Weight histogram
[ 769 1689 1483 1278 1061  849  791  789 1107 2334] [-0.0834422  -0.07512574 -0.06680928 -0.05849282 -0.05017636 -0.0418599
 -0.03354345 -0.02522699 -0.01691053 -0.00859407 -0.00027761]
[2345  824  788  813  939 1058 1147 1302 1361 1573] [-0.0834422  -0.07512574 -0.06680928 -0.05849282 -0.05017636 -0.0418599
 -0.03354345 -0.02522699 -0.01691053 -0.00859407 -0.00027761]
-1.44792
2.70075
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084308 minutes
Epoch 0
Fine tuning took 0.084389 minutes
Epoch 0
Fine tuning took 0.083422 minutes
{'zero': {0: [0.29926108374384236, 0.26108374384236455, 0.24876847290640394, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5, 0.56650246305418717, 0.58004926108374388, 0.61945812807881773], 5: [0.20073891625615764, 0.17241379310344829, 0.17118226600985223, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29926108374384236, 0.1206896551724138, 0.14285714285714285, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5, 0.71305418719211822, 0.63669950738916259, 0.69211822660098521], 5: [0.20073891625615764, 0.16625615763546797, 0.22044334975369459, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29926108374384236, 0.14408866995073891, 0.15270935960591134, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5, 0.65024630541871919, 0.59729064039408863, 0.66256157635467983], 5: [0.20073891625615764, 0.20566502463054187, 0.25, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29926108374384236, 0.10714285714285714, 0.12561576354679804, 0.071428571428571425], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5, 0.72290640394088668, 0.64532019704433496, 0.73645320197044339], 5: [0.20073891625615764, 0.16995073891625614, 0.22906403940886699, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235495 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.77536
Epoch 1, cost is  1.70524
Epoch 2, cost is  1.67993
Epoch 3, cost is  1.66274
Epoch 4, cost is  1.65487
Training took 0.161136 minutes
Weight histogram
[2005 2115 1947 1866 1484 1057  741  404  262  269] [-0.13840507 -0.12458692 -0.11076877 -0.09695062 -0.08313246 -0.06931431
 -0.05549616 -0.04167801 -0.02785986 -0.01404171 -0.00022356]
[ 446  473  692  974 1145 1329 1463 1638 1909 2081] [-0.13840507 -0.12458692 -0.11076877 -0.09695062 -0.08313246 -0.06931431
 -0.05549616 -0.04167801 -0.02785986 -0.01404171 -0.00022356]
-3.49956
5.20579
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234763 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83523
Epoch 1, cost is  1.76109
Epoch 2, cost is  1.73279
Epoch 3, cost is  1.71583
Epoch 4, cost is  1.70715
Training took 0.161732 minutes
Weight histogram
[2247 2150 1665 1859 1343 1412 1541  827  541  590] [-0.13923551 -0.12533432 -0.11143312 -0.09753193 -0.08363073 -0.06972953
 -0.05582834 -0.04192714 -0.02802595 -0.01412475 -0.00022356]
[ 929  999 1468 1276 1135 1312 1479 1668 1903 2006] [-0.13923551 -0.12533432 -0.11143312 -0.09753193 -0.08363073 -0.06972953
 -0.05582834 -0.04192714 -0.02802595 -0.01412475 -0.00022356]
-3.64521
4.81954
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46991
Epoch 1, cost is  5.07619
Epoch 2, cost is  3.80138
Epoch 3, cost is  3.19745
Epoch 4, cost is  2.84254
Training took 0.180706 minutes
Weight histogram
[ 872 1976 1434 1190 1072  967  732  825 2097  985] [-0.05459121 -0.04915912 -0.04372704 -0.03829495 -0.03286286 -0.02743077
 -0.02199868 -0.01656659 -0.0111345  -0.00570242 -0.00027033]
[2446  817  777  828  887  994 1133 1283 1463 1522] [-0.05459121 -0.04915912 -0.04372704 -0.03829495 -0.03286286 -0.02743077
 -0.02199868 -0.01656659 -0.0111345  -0.00570242 -0.00027033]
-1.11031
2.02951
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090121 minutes
Epoch 0
Fine tuning took 0.088717 minutes
Epoch 0
Fine tuning took 0.088080 minutes
{'zero': {0: [0.13423645320197045, 0.2229064039408867, 0.12315270935960591, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.66502463054187189, 0.76231527093596063, 0.74630541871921185], 5: [0.15640394088669951, 0.11206896551724138, 0.1145320197044335, 0.076354679802955669], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13423645320197045, 0.16502463054187191, 0.11083743842364532, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.77955665024630538, 0.7857142857142857, 0.82389162561576357], 5: [0.15640394088669951, 0.055418719211822662, 0.10344827586206896, 0.044334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13423645320197045, 0.18226600985221675, 0.10960591133004927, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.73152709359605916, 0.77586206896551724, 0.76724137931034486], 5: [0.15640394088669951, 0.086206896551724144, 0.1145320197044335, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13423645320197045, 0.16871921182266009, 0.088669950738916259, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.76724137931034486, 0.81403940886699511, 0.83990147783251234], 5: [0.15640394088669951, 0.064039408866995079, 0.097290640394088676, 0.038177339901477834], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235422 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.7769
Epoch 1, cost is  2.7201
Epoch 2, cost is  2.67339
Epoch 3, cost is  2.62925
Epoch 4, cost is  2.59036
Training took 0.159115 minutes
Weight histogram
[1809 1760 1317 1181 1131  904  972  979  927 1170] [ -5.68057373e-02  -5.11171334e-02  -4.54285295e-02  -3.97399256e-02
  -3.40513217e-02  -2.83627178e-02  -2.26741139e-02  -1.69855100e-02
  -1.12969062e-02  -5.60830226e-03   8.03016374e-05]
[1806  822  776  863  965 1098 1216 1420 1520 1664] [ -5.68057373e-02  -5.11171334e-02  -4.54285295e-02  -3.97399256e-02
  -3.40513217e-02  -2.83627178e-02  -2.26741139e-02  -1.69855100e-02
  -1.12969062e-02  -5.60830226e-03   8.03016374e-05]
-0.788649
1.43016
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233961 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.92745
Epoch 1, cost is  2.87068
Epoch 2, cost is  2.82408
Epoch 3, cost is  2.7784
Epoch 4, cost is  2.74152
Training took 0.160085 minutes
Weight histogram
[1751 1602 1198 1186 1009  983  978  982 2182 2304] [ -5.40938526e-02  -4.86764372e-02  -4.32590218e-02  -3.78416064e-02
  -3.24241909e-02  -2.70067755e-02  -2.15893601e-02  -1.61719446e-02
  -1.07545292e-02  -5.33711379e-03   8.03016374e-05]
[3775 1026  797  868  945 1075 1189 1400 1498 1602] [ -5.40938526e-02  -4.86764372e-02  -4.32590218e-02  -3.78416064e-02
  -3.24241909e-02  -2.70067755e-02  -2.15893601e-02  -1.61719446e-02
  -1.07545292e-02  -5.33711379e-03   8.03016374e-05]
-0.749458
1.20625
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8799
Epoch 1, cost is  6.80962
Epoch 2, cost is  6.74669
Epoch 3, cost is  6.68542
Epoch 4, cost is  6.62455
Training took 0.102513 minutes
Weight histogram
[  92 6299 3984  628  380  256  184  136  100   91] [ -7.25918217e-03  -6.53591616e-03  -5.81265016e-03  -5.08938416e-03
  -4.36611816e-03  -3.64285216e-03  -2.91958616e-03  -2.19632015e-03
  -1.47305415e-03  -7.49788151e-04  -2.65221497e-05]
[7952 2653  630  252  145  118  107  102  100   91] [ -7.25918217e-03  -6.53591616e-03  -5.81265016e-03  -5.08938416e-03
  -4.36611816e-03  -3.64285216e-03  -2.91958616e-03  -2.19632015e-03
  -1.47305415e-03  -7.49788151e-04  -2.65221497e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081147 minutes
Epoch 0
Fine tuning took 0.082133 minutes
Epoch 0
Fine tuning took 0.081010 minutes
{'zero': {0: [0.47044334975369456, 0.43965517241379309, 0.23029556650246305, 0.38300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32019704433497537, 0.44581280788177341, 0.49753694581280788, 0.51970443349753692], 5: [0.20935960591133004, 0.1145320197044335, 0.27216748768472904, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.47044334975369456, 0.41625615763546797, 0.23029556650246305, 0.34113300492610837], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32019704433497537, 0.46674876847290642, 0.51847290640394084, 0.56280788177339902], 5: [0.20935960591133004, 0.11699507389162561, 0.25123152709359609, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.47044334975369456, 0.43596059113300495, 0.23891625615763548, 0.3608374384236453], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32019704433497537, 0.43103448275862066, 0.48029556650246308, 0.55541871921182262], 5: [0.20935960591133004, 0.13300492610837439, 0.28078817733990147, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.47044334975369456, 0.45689655172413796, 0.24014778325123154, 0.40270935960591131], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32019704433497537, 0.41995073891625617, 0.52709359605911332, 0.48645320197044334], 5: [0.20935960591133004, 0.12315270935960591, 0.23275862068965517, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235285 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.7769
Epoch 1, cost is  2.7201
Epoch 2, cost is  2.67339
Epoch 3, cost is  2.62925
Epoch 4, cost is  2.59036
Training took 0.159038 minutes
Weight histogram
[1809 1760 1317 1181 1131  904  972  979  927 1170] [ -5.68057373e-02  -5.11171334e-02  -4.54285295e-02  -3.97399256e-02
  -3.40513217e-02  -2.83627178e-02  -2.26741139e-02  -1.69855100e-02
  -1.12969062e-02  -5.60830226e-03   8.03016374e-05]
[1806  822  776  863  965 1098 1216 1420 1520 1664] [ -5.68057373e-02  -5.11171334e-02  -4.54285295e-02  -3.97399256e-02
  -3.40513217e-02  -2.83627178e-02  -2.26741139e-02  -1.69855100e-02
  -1.12969062e-02  -5.60830226e-03   8.03016374e-05]
-0.788649
1.43016
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235668 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.92745
Epoch 1, cost is  2.87068
Epoch 2, cost is  2.82408
Epoch 3, cost is  2.7784
Epoch 4, cost is  2.74152
Training took 0.159624 minutes
Weight histogram
[1751 1602 1198 1186 1009  983  978  982 2182 2304] [ -5.40938526e-02  -4.86764372e-02  -4.32590218e-02  -3.78416064e-02
  -3.24241909e-02  -2.70067755e-02  -2.15893601e-02  -1.61719446e-02
  -1.07545292e-02  -5.33711379e-03   8.03016374e-05]
[3775 1026  797  868  945 1075 1189 1400 1498 1602] [ -5.40938526e-02  -4.86764372e-02  -4.32590218e-02  -3.78416064e-02
  -3.24241909e-02  -2.70067755e-02  -2.15893601e-02  -1.61719446e-02
  -1.07545292e-02  -5.33711379e-03   8.03016374e-05]
-0.749458
1.20625
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.86103
Epoch 1, cost is  6.78365
Epoch 2, cost is  6.71872
Epoch 3, cost is  6.65625
Epoch 4, cost is  6.59269
Training took 0.125594 minutes
Weight histogram
[  93  119  449 9473  849  449  283  193  133  109] [ -8.75947252e-03  -7.88845607e-03  -7.01743963e-03  -6.14642318e-03
  -5.27540674e-03  -4.40439029e-03  -3.53337385e-03  -2.66235740e-03
  -1.79134096e-03  -9.20324511e-04  -4.93080661e-05]
[7346 2860  839  363  144  130  123  113  118  114] [ -8.75947252e-03  -7.88845607e-03  -7.01743963e-03  -6.14642318e-03
  -5.27540674e-03  -4.40439029e-03  -3.53337385e-03  -2.66235740e-03
  -1.79134096e-03  -9.20324511e-04  -4.93080661e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081703 minutes
Epoch 0
Fine tuning took 0.084166 minutes
Epoch 0
Fine tuning took 0.084078 minutes
{'zero': {0: [0.46674876847290642, 0.21182266009852216, 0.22660098522167488, 0.37561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.33990147783251229, 0.35467980295566504, 0.44458128078817732, 0.51600985221674878], 5: [0.19334975369458129, 0.43349753694581283, 0.3288177339901478, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.46674876847290642, 0.20689655172413793, 0.22413793103448276, 0.38054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.33990147783251229, 0.3645320197044335, 0.47660098522167488, 0.51477832512315269], 5: [0.19334975369458129, 0.42857142857142855, 0.29926108374384236, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.46674876847290642, 0.22536945812807882, 0.2019704433497537, 0.40270935960591131], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.33990147783251229, 0.37315270935960593, 0.45566502463054187, 0.49384236453201968], 5: [0.19334975369458129, 0.40147783251231528, 0.34236453201970446, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.46674876847290642, 0.2105911330049261, 0.2229064039408867, 0.4211822660098522], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.33990147783251229, 0.37438423645320196, 0.46305418719211822, 0.47290640394088668], 5: [0.19334975369458129, 0.41502463054187194, 0.31403940886699505, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234344 minutes
Weight histogram
[ 139  425  624  532  773 2232 2482 2828 1947  168] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
[ 130  191  289  444  671  773 1035 2116 3154 3347] [ -1.55507252e-04   5.12526458e-06   1.65757781e-04   3.26390298e-04
   4.87022815e-04   6.47655332e-04   8.08287848e-04   9.68920365e-04
   1.12955288e-03   1.29018540e-03   1.45081792e-03]
-1.07829
0.900681
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.7769
Epoch 1, cost is  2.7201
Epoch 2, cost is  2.67339
Epoch 3, cost is  2.62925
Epoch 4, cost is  2.59036
Training took 0.160857 minutes
Weight histogram
[1809 1760 1317 1181 1131  904  972  979  927 1170] [ -5.68057373e-02  -5.11171334e-02  -4.54285295e-02  -3.97399256e-02
  -3.40513217e-02  -2.83627178e-02  -2.26741139e-02  -1.69855100e-02
  -1.12969062e-02  -5.60830226e-03   8.03016374e-05]
[1806  822  776  863  965 1098 1216 1420 1520 1664] [ -5.68057373e-02  -5.11171334e-02  -4.54285295e-02  -3.97399256e-02
  -3.40513217e-02  -2.83627178e-02  -2.26741139e-02  -1.69855100e-02
  -1.12969062e-02  -5.60830226e-03   8.03016374e-05]
-0.788649
1.43016
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235145 minutes
Weight histogram
[ 180  612 1121 1127 1092 2030 3210 2814 1638  351] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 2965] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.92745
Epoch 1, cost is  2.87068
Epoch 2, cost is  2.82408
Epoch 3, cost is  2.7784
Epoch 4, cost is  2.74152
Training took 0.162407 minutes
Weight histogram
[1751 1602 1198 1186 1009  983  978  982 2182 2304] [ -5.40938526e-02  -4.86764372e-02  -4.32590218e-02  -3.78416064e-02
  -3.24241909e-02  -2.70067755e-02  -2.15893601e-02  -1.61719446e-02
  -1.07545292e-02  -5.33711379e-03   8.03016374e-05]
[3775 1026  797  868  945 1075 1189 1400 1498 1602] [ -5.40938526e-02  -4.86764372e-02  -4.32590218e-02  -3.78416064e-02
  -3.24241909e-02  -2.70067755e-02  -2.15893601e-02  -1.61719446e-02
  -1.07545292e-02  -5.33711379e-03   8.03016374e-05]
-0.749458
1.20625
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80439
Epoch 1, cost is  6.70953
Epoch 2, cost is  6.63766
Epoch 3, cost is  6.56992
Epoch 4, cost is  6.50192
Training took 0.180299 minutes
Weight histogram
[ 162  200  256 7176 2648  751  408  255  169  125] [ -1.00328391e-02  -9.03331198e-03  -8.03378482e-03  -7.03425766e-03
  -6.03473049e-03  -5.03520333e-03  -4.03567617e-03  -3.03614901e-03
  -2.03662185e-03  -1.03709468e-03  -3.75675227e-05]
[6317 2750 1578  493  280  142  142  142  149  157] [ -1.00328391e-02  -9.03331198e-03  -8.03378482e-03  -7.03425766e-03
  -6.03473049e-03  -5.03520333e-03  -4.03567617e-03  -3.03614901e-03
  -2.03662185e-03  -1.03709468e-03  -3.75675227e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088947 minutes
Epoch 0
Fine tuning took 0.088245 minutes
Epoch 0
Fine tuning took 0.087340 minutes
{'zero': {0: [0.44950738916256155, 0.15147783251231528, 0.19088669950738915, 0.45197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35221674876847292, 0.77709359605911332, 0.64162561576354682, 0.3891625615763547], 5: [0.19827586206896552, 0.071428571428571425, 0.16748768472906403, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.44950738916256155, 0.1268472906403941, 0.18842364532019704, 0.47783251231527096], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35221674876847292, 0.79556650246305416, 0.63423645320197042, 0.40270935960591131], 5: [0.19827586206896552, 0.077586206896551727, 0.17733990147783252, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.44950738916256155, 0.14285714285714285, 0.21674876847290642, 0.45812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35221674876847292, 0.77586206896551724, 0.62315270935960587, 0.37931034482758619], 5: [0.19827586206896552, 0.081280788177339899, 0.16009852216748768, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.44950738916256155, 0.12315270935960591, 0.21674876847290642, 0.45320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35221674876847292, 0.77216748768472909, 0.62561576354679804, 0.41748768472906406], 5: [0.19827586206896552, 0.10467980295566502, 0.15763546798029557, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235854 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.03078
Epoch 1, cost is  4.66903
Epoch 2, cost is  4.59178
Epoch 3, cost is  4.57462
Epoch 4, cost is  4.599
Training took 0.160971 minutes
Weight histogram
[1841 1456 1341 1469 1950 1603 1455 1870 1000  190] [-0.62679327 -0.56444015 -0.50208704 -0.43973393 -0.37738082 -0.3150277
 -0.25267459 -0.19032148 -0.12796837 -0.06561525 -0.00326214]
[ 397  921 1435 1566 1528 1619 1727 1708 1634 1640] [-0.62679327 -0.56444015 -0.50208704 -0.43973393 -0.37738082 -0.3150277
 -0.25267459 -0.19032148 -0.12796837 -0.06561525 -0.00326214]
-23.9167
28.7097
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233981 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.07511
Epoch 1, cost is  4.6882
Epoch 2, cost is  4.58794
Epoch 3, cost is  4.55855
Epoch 4, cost is  4.57159
Training took 0.159772 minutes
Weight histogram
[1900 1501 1223 1618 1757 1554 1599 2473 2163  412] [-0.64954734 -0.58491882 -0.5202903  -0.45566178 -0.39103326 -0.32640474
 -0.26177622 -0.1971477  -0.13251918 -0.06789066 -0.00326214]
[ 811 1873 2071 1614 1553 1583 1625 1661 1695 1714] [-0.64954734 -0.58491882 -0.5202903  -0.45566178 -0.39103326 -0.32640474
 -0.26177622 -0.1971477  -0.13251918 -0.06789066 -0.00326214]
-20.0319
25.5675
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.6371
Epoch 1, cost is  3.933
Epoch 2, cost is  4.20129
Epoch 3, cost is  4.67537
Epoch 4, cost is  5.12856
Training took 0.102219 minutes
Weight histogram
[1535 2466 2350 2502 2129 1104  828  526  283  452] [-0.31483021 -0.28362628 -0.25242236 -0.22121843 -0.1900145  -0.15881057
 -0.12760664 -0.09640271 -0.06519878 -0.03399485 -0.00279092]
[ 710  714  948 1271 1517 1731 1736 1831 1952 1765] [-0.31483021 -0.28362628 -0.25242236 -0.22121843 -0.1900145  -0.15881057
 -0.12760664 -0.09640271 -0.06519878 -0.03399485 -0.00279092]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080216 minutes
Epoch 0
Fine tuning took 0.081841 minutes
Epoch 0
Fine tuning took 0.082046 minutes
{'zero': {0: [0.24384236453201971, 0.31773399014778325, 0.23275862068965517, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.55665024630541871, 0.68226600985221675, 0.76354679802955661], 5: [0.094827586206896547, 0.12561576354679804, 0.084975369458128072, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24384236453201971, 0.2536945812807882, 0.23029556650246305, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.63916256157635465, 0.63916256157635465, 0.68719211822660098], 5: [0.094827586206896547, 0.10714285714285714, 0.13054187192118227, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24384236453201971, 0.26600985221674878, 0.26477832512315269, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.6145320197044335, 0.59482758620689657, 0.60591133004926112], 5: [0.094827586206896547, 0.11945812807881774, 0.14039408866995073, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24384236453201971, 0.27463054187192121, 0.22413793103448276, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.60221674876847286, 0.66871921182266014, 0.71798029556650245], 5: [0.094827586206896547, 0.12315270935960591, 0.10714285714285714, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234863 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.03078
Epoch 1, cost is  4.66903
Epoch 2, cost is  4.59178
Epoch 3, cost is  4.57462
Epoch 4, cost is  4.599
Training took 0.162040 minutes
Weight histogram
[1841 1456 1341 1469 1950 1603 1455 1870 1000  190] [-0.62679327 -0.56444015 -0.50208704 -0.43973393 -0.37738082 -0.3150277
 -0.25267459 -0.19032148 -0.12796837 -0.06561525 -0.00326214]
[ 397  921 1435 1566 1528 1619 1727 1708 1634 1640] [-0.62679327 -0.56444015 -0.50208704 -0.43973393 -0.37738082 -0.3150277
 -0.25267459 -0.19032148 -0.12796837 -0.06561525 -0.00326214]
-23.9167
28.7097
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234683 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.07511
Epoch 1, cost is  4.6882
Epoch 2, cost is  4.58794
Epoch 3, cost is  4.55855
Epoch 4, cost is  4.57159
Training took 0.161373 minutes
Weight histogram
[1900 1501 1223 1618 1757 1554 1599 2473 2163  412] [-0.64954734 -0.58491882 -0.5202903  -0.45566178 -0.39103326 -0.32640474
 -0.26177622 -0.1971477  -0.13251918 -0.06789066 -0.00326214]
[ 811 1873 2071 1614 1553 1583 1625 1661 1695 1714] [-0.64954734 -0.58491882 -0.5202903  -0.45566178 -0.39103326 -0.32640474
 -0.26177622 -0.1971477  -0.13251918 -0.06789066 -0.00326214]
-20.0319
25.5675
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.23326
Epoch 1, cost is  3.30543
Epoch 2, cost is  3.45327
Epoch 3, cost is  3.73198
Epoch 4, cost is  4.14792
Training took 0.122059 minutes
Weight histogram
[1229 2306 2215 2213 1969 1657 1146  625  295  520] [-0.27973735 -0.25203754 -0.22433772 -0.19663791 -0.1689381  -0.14123828
 -0.11353847 -0.08583865 -0.05813884 -0.03043902 -0.00273921]
[ 713  743 1071 1272 1513 1649 1797 1828 1796 1793] [-0.27973735 -0.25203754 -0.22433772 -0.19663791 -0.1689381  -0.14123828
 -0.11353847 -0.08583865 -0.05813884 -0.03043902 -0.00273921]
-9.2547
11.2219
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082324 minutes
Epoch 0
Fine tuning took 0.082647 minutes
Epoch 0
Fine tuning took 0.083163 minutes
{'zero': {0: [0.27216748768472904, 0.48768472906403942, 0.42241379310344829, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.41379310344827586, 0.49014778325123154, 0.73891625615763545], 5: [0.11699507389162561, 0.098522167487684734, 0.087438423645320201, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27216748768472904, 0.21428571428571427, 0.18103448275862069, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.67610837438423643, 0.69581280788177335, 0.69458128078817738], 5: [0.11699507389162561, 0.10960591133004927, 0.12315270935960591, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27216748768472904, 0.24507389162561577, 0.20812807881773399, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.6428571428571429, 0.6711822660098522, 0.65517241379310343], 5: [0.11699507389162561, 0.11206896551724138, 0.1206896551724138, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27216748768472904, 0.20320197044334976, 0.22044334975369459, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.7068965517241379, 0.71059113300492616, 0.68349753694581283], 5: [0.11699507389162561, 0.089901477832512317, 0.068965517241379309, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234626 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.03078
Epoch 1, cost is  4.66903
Epoch 2, cost is  4.59178
Epoch 3, cost is  4.57462
Epoch 4, cost is  4.599
Training took 0.162212 minutes
Weight histogram
[1841 1456 1341 1469 1950 1603 1455 1870 1000  190] [-0.62679327 -0.56444015 -0.50208704 -0.43973393 -0.37738082 -0.3150277
 -0.25267459 -0.19032148 -0.12796837 -0.06561525 -0.00326214]
[ 397  921 1435 1566 1528 1619 1727 1708 1634 1640] [-0.62679327 -0.56444015 -0.50208704 -0.43973393 -0.37738082 -0.3150277
 -0.25267459 -0.19032148 -0.12796837 -0.06561525 -0.00326214]
-23.9167
28.7097
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234623 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.07511
Epoch 1, cost is  4.6882
Epoch 2, cost is  4.58794
Epoch 3, cost is  4.55855
Epoch 4, cost is  4.57159
Training took 0.161700 minutes
Weight histogram
[1900 1501 1223 1618 1757 1554 1599 2473 2163  412] [-0.64954734 -0.58491882 -0.5202903  -0.45566178 -0.39103326 -0.32640474
 -0.26177622 -0.1971477  -0.13251918 -0.06789066 -0.00326214]
[ 811 1873 2071 1614 1553 1583 1625 1661 1695 1714] [-0.64954734 -0.58491882 -0.5202903  -0.45566178 -0.39103326 -0.32640474
 -0.26177622 -0.1971477  -0.13251918 -0.06789066 -0.00326214]
-20.0319
25.5675
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.71658
Epoch 1, cost is  2.38913
Epoch 2, cost is  2.29088
Epoch 3, cost is  2.37114
Epoch 4, cost is  2.52692
Training took 0.179000 minutes
Weight histogram
[1774 2299 2158 2060 1884 1477 1066  556  378  523] [-0.18797739 -0.16945611 -0.15093482 -0.13241354 -0.11389226 -0.09537098
 -0.07684969 -0.05832841 -0.03980713 -0.02128585 -0.00276456]
[ 773  727 1061 1323 1501 1651 1756 1806 1934 1643] [-0.18797739 -0.16945611 -0.15093482 -0.13241354 -0.11389226 -0.09537098
 -0.07684969 -0.05832841 -0.03980713 -0.02128585 -0.00276456]
-6.41529
7.90691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089586 minutes
Epoch 0
Fine tuning took 0.089803 minutes
Epoch 0
Fine tuning took 0.088545 minutes
{'zero': {0: [0.14408866995073891, 0.070197044334975367, 0.077586206896551727, 0.084975369458128072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74753694581280783, 0.90024630541871919, 0.8645320197044335, 0.85344827586206895], 5: [0.10837438423645321, 0.029556650246305417, 0.057881773399014777, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14408866995073891, 0.1625615763546798, 0.17118226600985223, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74753694581280783, 0.75985221674876846, 0.73891625615763545, 0.75492610837438423], 5: [0.10837438423645321, 0.077586206896551727, 0.089901477832512317, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14408866995073891, 0.15270935960591134, 0.19950738916256158, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74753694581280783, 0.75862068965517238, 0.69088669950738912, 0.72660098522167482], 5: [0.10837438423645321, 0.088669950738916259, 0.10960591133004927, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14408866995073891, 0.10837438423645321, 0.17857142857142858, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74753694581280783, 0.85098522167487689, 0.74384236453201968, 0.84605911330049266], 5: [0.10837438423645321, 0.04064039408866995, 0.077586206896551727, 0.051724137931034482], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236806 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84438
Epoch 1, cost is  1.76255
Epoch 2, cost is  1.73662
Epoch 3, cost is  1.72067
Epoch 4, cost is  1.71175
Training took 0.159524 minutes
Weight histogram
[2793 2047 2365 1963 1479 1494  962  483  297  292] [-0.14982635 -0.13486607 -0.11990579 -0.10494551 -0.08998523 -0.07502495
 -0.06006467 -0.04510439 -0.03014412 -0.01518384 -0.00022356]
[ 483  552  835 1151 1363 1497 1749 2041 2260 2244] [-0.14982635 -0.13486607 -0.11990579 -0.10494551 -0.08998523 -0.07502495
 -0.06006467 -0.04510439 -0.03014412 -0.01518384 -0.00022356]
-4.3173
6.17562
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235476 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.85141
Epoch 1, cost is  1.76439
Epoch 2, cost is  1.73201
Epoch 3, cost is  1.71352
Epoch 4, cost is  1.7025
Training took 0.158560 minutes
Weight histogram
[2371 2520 2262 1715 1606 1597 1793 1046  649  641] [-0.15295617 -0.13768291 -0.12240965 -0.10713639 -0.09186313 -0.07658987
 -0.0613166  -0.04604334 -0.03077008 -0.01549682 -0.00022356]
[1002 1158 1710 1189 1338 1463 1751 2014 2160 2415] [-0.15295617 -0.13768291 -0.12240965 -0.10713639 -0.09186313 -0.07658987
 -0.0613166  -0.04604334 -0.03077008 -0.01549682 -0.00022356]
-4.06936
5.40729
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.5926
Epoch 1, cost is  5.52796
Epoch 2, cost is  4.80505
Epoch 3, cost is  4.47169
Epoch 4, cost is  4.25178
Training took 0.102350 minutes
Weight histogram
[ 583 2019 1777 1673 1132 1151  923  864 1035 3018] [-0.10474743 -0.09429832 -0.08384921 -0.0734001  -0.06295099 -0.05250188
 -0.04205277 -0.03160367 -0.02115456 -0.01070545 -0.00025634]
[2658  939  907 1016 1159 1257 1392 1533 1643 1671] [-0.10474743 -0.09429832 -0.08384921 -0.0734001  -0.06295099 -0.05250188
 -0.04205277 -0.03160367 -0.02115456 -0.01070545 -0.00025634]
-1.55839
3.39939
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081070 minutes
Epoch 0
Fine tuning took 0.083182 minutes
Epoch 0
Fine tuning took 0.083003 minutes
{'zero': {0: [0.56403940886699511, 0.28078817733990147, 0.37561576354679804, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.2019704433497537, 0.45073891625615764, 0.44827586206896552, 0.64778325123152714], 5: [0.23399014778325122, 0.26847290640394089, 0.17610837438423646, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.56403940886699511, 0.29802955665024633, 0.37438423645320196, 0.29064039408866993], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.2019704433497537, 0.39901477832512317, 0.41995073891625617, 0.57266009852216748], 5: [0.23399014778325122, 0.30295566502463056, 0.20566502463054187, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.56403940886699511, 0.29556650246305421, 0.39655172413793105, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.2019704433497537, 0.40147783251231528, 0.42241379310344829, 0.58990147783251234], 5: [0.23399014778325122, 0.30295566502463056, 0.18103448275862069, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.56403940886699511, 0.33004926108374383, 0.39039408866995073, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.2019704433497537, 0.35714285714285715, 0.40886699507389163, 0.5714285714285714], 5: [0.23399014778325122, 0.31280788177339902, 0.20073891625615764, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236083 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84438
Epoch 1, cost is  1.76255
Epoch 2, cost is  1.73662
Epoch 3, cost is  1.72067
Epoch 4, cost is  1.71175
Training took 0.159802 minutes
Weight histogram
[2793 2047 2365 1963 1479 1494  962  483  297  292] [-0.14982635 -0.13486607 -0.11990579 -0.10494551 -0.08998523 -0.07502495
 -0.06006467 -0.04510439 -0.03014412 -0.01518384 -0.00022356]
[ 483  552  835 1151 1363 1497 1749 2041 2260 2244] [-0.14982635 -0.13486607 -0.11990579 -0.10494551 -0.08998523 -0.07502495
 -0.06006467 -0.04510439 -0.03014412 -0.01518384 -0.00022356]
-4.3173
6.17562
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235044 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.85141
Epoch 1, cost is  1.76439
Epoch 2, cost is  1.73201
Epoch 3, cost is  1.71352
Epoch 4, cost is  1.7025
Training took 0.159185 minutes
Weight histogram
[2371 2520 2262 1715 1606 1597 1793 1046  649  641] [-0.15295617 -0.13768291 -0.12240965 -0.10713639 -0.09186313 -0.07658987
 -0.0613166  -0.04604334 -0.03077008 -0.01549682 -0.00022356]
[1002 1158 1710 1189 1338 1463 1751 2014 2160 2415] [-0.15295617 -0.13768291 -0.12240965 -0.10713639 -0.09186313 -0.07658987
 -0.0613166  -0.04604334 -0.03077008 -0.01549682 -0.00022356]
-4.06936
5.40729
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55441
Epoch 1, cost is  5.31987
Epoch 2, cost is  4.35464
Epoch 3, cost is  3.91677
Epoch 4, cost is  3.65847
Training took 0.124380 minutes
Weight histogram
[ 828 1975 1754 1521 1243  999  936  920 1295 2704] [-0.0834422 -0.0751255 -0.0668088 -0.0584921 -0.0501754 -0.0418587
 -0.033542  -0.0252253 -0.0169086 -0.0085919 -0.0002752]
[2717  963  919  951 1098 1238 1344 1521 1593 1831] [-0.0834422 -0.0751255 -0.0668088 -0.0584921 -0.0501754 -0.0418587
 -0.033542  -0.0252253 -0.0169086 -0.0085919 -0.0002752]
-1.44792
2.70075
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084916 minutes
Epoch 0
Fine tuning took 0.084460 minutes
Epoch 0
Fine tuning took 0.083744 minutes
{'zero': {0: [0.37931034482758619, 0.37561576354679804, 0.35098522167487683, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.45073891625615764, 0.47660098522167488, 0.5788177339901478], 5: [0.17241379310344829, 0.17364532019704434, 0.17241379310344829, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37931034482758619, 0.19458128078817735, 0.21182266009852216, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.56896551724137934, 0.51970443349753692, 0.70566502463054193], 5: [0.17241379310344829, 0.23645320197044334, 0.26847290640394089, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37931034482758619, 0.20935960591133004, 0.23152709359605911, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.58128078817733986, 0.55788177339901479, 0.63669950738916259], 5: [0.17241379310344829, 0.20935960591133004, 0.2105911330049261, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37931034482758619, 0.17857142857142858, 0.19211822660098521, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.56034482758620685, 0.58620689655172409, 0.71059113300492616], 5: [0.17241379310344829, 0.26108374384236455, 0.22167487684729065, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235289 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84438
Epoch 1, cost is  1.76255
Epoch 2, cost is  1.73662
Epoch 3, cost is  1.72067
Epoch 4, cost is  1.71175
Training took 0.161069 minutes
Weight histogram
[2793 2047 2365 1963 1479 1494  962  483  297  292] [-0.14982635 -0.13486607 -0.11990579 -0.10494551 -0.08998523 -0.07502495
 -0.06006467 -0.04510439 -0.03014412 -0.01518384 -0.00022356]
[ 483  552  835 1151 1363 1497 1749 2041 2260 2244] [-0.14982635 -0.13486607 -0.11990579 -0.10494551 -0.08998523 -0.07502495
 -0.06006467 -0.04510439 -0.03014412 -0.01518384 -0.00022356]
-4.3173
6.17562
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234624 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.85141
Epoch 1, cost is  1.76439
Epoch 2, cost is  1.73201
Epoch 3, cost is  1.71352
Epoch 4, cost is  1.7025
Training took 0.158460 minutes
Weight histogram
[2371 2520 2262 1715 1606 1597 1793 1046  649  641] [-0.15295617 -0.13768291 -0.12240965 -0.10713639 -0.09186313 -0.07658987
 -0.0613166  -0.04604334 -0.03077008 -0.01549682 -0.00022356]
[1002 1158 1710 1189 1338 1463 1751 2014 2160 2415] [-0.15295617 -0.13768291 -0.12240965 -0.10713639 -0.09186313 -0.07658987
 -0.0613166  -0.04604334 -0.03077008 -0.01549682 -0.00022356]
-4.06936
5.40729
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46962
Epoch 1, cost is  5.07226
Epoch 2, cost is  3.81568
Epoch 3, cost is  3.21777
Epoch 4, cost is  2.86192
Training took 0.183538 minutes
Weight histogram
[ 923 2366 1682 1394 1262 1138  857  967 2379 1207] [-0.05459121 -0.04915904 -0.04372687 -0.03829471 -0.03286254 -0.02743037
 -0.0219982  -0.01656603 -0.01113386 -0.00570169 -0.00026952]
[2838  957  907  972 1038 1166 1327 1502 1709 1759] [-0.05459121 -0.04915904 -0.04372687 -0.03829471 -0.03286254 -0.02743037
 -0.0219982  -0.01656603 -0.01113386 -0.00570169 -0.00026952]
-1.11031
2.02951
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088980 minutes
Epoch 0
Fine tuning took 0.090086 minutes
Epoch 0
Fine tuning took 0.088302 minutes
{'zero': {0: [0.29310344827586204, 0.16871921182266009, 0.21798029556650247, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.66256157635467983, 0.65517241379310343, 0.72536945812807885], 5: [0.15270935960591134, 0.16871921182266009, 0.1268472906403941, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29310344827586204, 0.11206896551724138, 0.23152709359605911, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.76477832512315269, 0.67610837438423643, 0.76354679802955661], 5: [0.15270935960591134, 0.12315270935960591, 0.092364532019704432, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29310344827586204, 0.14408866995073891, 0.17980295566502463, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.70443349753694584, 0.72167487684729059, 0.71921182266009853], 5: [0.15270935960591134, 0.15147783251231528, 0.098522167487684734, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29310344827586204, 0.13177339901477833, 0.21798029556650247, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.76108374384236455, 0.69334975369458129, 0.74753694581280783], 5: [0.15270935960591134, 0.10714285714285714, 0.088669950738916259, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235684 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.60743
Epoch 1, cost is  2.54826
Epoch 2, cost is  2.51045
Epoch 3, cost is  2.47918
Epoch 4, cost is  2.44612
Training took 0.158914 minutes
Weight histogram
[2330 1901 1754 1526 1181 1107 1014 1070 1023 1269] [ -6.22487292e-02  -5.60158261e-02  -4.97829230e-02  -4.35500199e-02
  -3.73171168e-02  -3.10842138e-02  -2.48513107e-02  -1.86184076e-02
  -1.23855045e-02  -6.15260144e-03   8.03016374e-05]
[1902  899  883 1002 1137 1317 1509 1674 1857 1995] [ -6.22487292e-02  -5.60158261e-02  -4.97829230e-02  -4.35500199e-02
  -3.73171168e-02  -3.10842138e-02  -2.48513107e-02  -1.86184076e-02
  -1.23855045e-02  -6.15260144e-03   8.03016374e-05]
-0.876824
1.43391
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233993 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.72579
Epoch 1, cost is  2.66928
Epoch 2, cost is  2.63015
Epoch 3, cost is  2.59145
Epoch 4, cost is  2.55997
Training took 0.159744 minutes
Weight histogram
[1990 1891 1746 1347 1132 1204 1101 1080 1900 2809] [ -6.02925904e-02  -5.42553012e-02  -4.82180120e-02  -4.21807228e-02
  -3.61434336e-02  -3.01061444e-02  -2.40688552e-02  -1.80315660e-02
  -1.19942768e-02  -5.95698757e-03   8.03016374e-05]
[3960 1011  906  997 1102 1290 1483 1651 1772 2028] [ -6.02925904e-02  -5.42553012e-02  -4.82180120e-02  -4.21807228e-02
  -3.61434336e-02  -3.01061444e-02  -2.40688552e-02  -1.80315660e-02
  -1.19942768e-02  -5.95698757e-03   8.03016374e-05]
-0.824163
1.23818
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.87989
Epoch 1, cost is  6.80936
Epoch 2, cost is  6.74636
Epoch 3, cost is  6.68483
Epoch 4, cost is  6.62343
Training took 0.102146 minutes
Weight histogram
[  92 7128 4850  750  451  302  217  160  118  107] [ -7.25918217e-03  -6.53586109e-03  -5.81254002e-03  -5.08921894e-03
  -4.36589787e-03  -3.64257679e-03  -2.91925572e-03  -2.19593464e-03
  -1.47261357e-03  -7.49292496e-04  -2.59714216e-05]
[9471 3159  630  252  145  118  107  102  100   91] [ -7.25918217e-03  -6.53586109e-03  -5.81254002e-03  -5.08921894e-03
  -4.36589787e-03  -3.64257679e-03  -2.91925572e-03  -2.19593464e-03
  -1.47261357e-03  -7.49292496e-04  -2.59714216e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.079933 minutes
Epoch 0
Fine tuning took 0.081426 minutes
Epoch 0
Fine tuning took 0.081261 minutes
{'zero': {0: [0.37315270935960593, 0.093596059113300489, 0.034482758620689655, 0.050492610837438424], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.19334975369458129, 0.69950738916256161, 0.6785714285714286, 0.72290640394088668], 5: [0.43349753694581283, 0.20689655172413793, 0.28694581280788178, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37315270935960593, 0.084975369458128072, 0.030788177339901478, 0.055418719211822662], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.19334975369458129, 0.68226600985221675, 0.67487684729064035, 0.75985221674876846], 5: [0.43349753694581283, 0.23275862068965517, 0.29433497536945813, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37315270935960593, 0.070197044334975367, 0.022167487684729065, 0.059113300492610835], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.19334975369458129, 0.6785714285714286, 0.66871921182266014, 0.74137931034482762], 5: [0.43349753694581283, 0.25123152709359609, 0.30911330049261082, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37315270935960593, 0.071428571428571425, 0.023399014778325122, 0.060344827586206899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.19334975369458129, 0.69950738916256161, 0.70197044334975367, 0.70197044334975367], 5: [0.43349753694581283, 0.22906403940886699, 0.27463054187192121, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236318 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.60743
Epoch 1, cost is  2.54826
Epoch 2, cost is  2.51045
Epoch 3, cost is  2.47918
Epoch 4, cost is  2.44612
Training took 0.160834 minutes
Weight histogram
[2330 1901 1754 1526 1181 1107 1014 1070 1023 1269] [ -6.22487292e-02  -5.60158261e-02  -4.97829230e-02  -4.35500199e-02
  -3.73171168e-02  -3.10842138e-02  -2.48513107e-02  -1.86184076e-02
  -1.23855045e-02  -6.15260144e-03   8.03016374e-05]
[1902  899  883 1002 1137 1317 1509 1674 1857 1995] [ -6.22487292e-02  -5.60158261e-02  -4.97829230e-02  -4.35500199e-02
  -3.73171168e-02  -3.10842138e-02  -2.48513107e-02  -1.86184076e-02
  -1.23855045e-02  -6.15260144e-03   8.03016374e-05]
-0.876824
1.43391
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234832 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.72579
Epoch 1, cost is  2.66928
Epoch 2, cost is  2.63015
Epoch 3, cost is  2.59145
Epoch 4, cost is  2.55997
Training took 0.160306 minutes
Weight histogram
[1990 1891 1746 1347 1132 1204 1101 1080 1900 2809] [ -6.02925904e-02  -5.42553012e-02  -4.82180120e-02  -4.21807228e-02
  -3.61434336e-02  -3.01061444e-02  -2.40688552e-02  -1.80315660e-02
  -1.19942768e-02  -5.95698757e-03   8.03016374e-05]
[3960 1011  906  997 1102 1290 1483 1651 1772 2028] [ -6.02925904e-02  -5.42553012e-02  -4.82180120e-02  -4.21807228e-02
  -3.61434336e-02  -3.01061444e-02  -2.40688552e-02  -1.80315660e-02
  -1.19942768e-02  -5.95698757e-03   8.03016374e-05]
-0.824163
1.23818
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8608
Epoch 1, cost is  6.78337
Epoch 2, cost is  6.71826
Epoch 3, cost is  6.65536
Epoch 4, cost is  6.59096
Training took 0.122209 minutes
Weight histogram
[   93   119   450 11121  1013   532   334   228   157   128] [ -8.75947252e-03  -7.88840984e-03  -7.01734717e-03  -6.14628449e-03
  -5.27522182e-03  -4.40415914e-03  -3.53309647e-03  -2.66203379e-03
  -1.79097112e-03  -9.19908442e-04  -4.88457663e-05]
[8750 3409  911  363  144  130  123  113  118  114] [ -8.75947252e-03  -7.88840984e-03  -7.01734717e-03  -6.14628449e-03
  -5.27522182e-03  -4.40415914e-03  -3.53309647e-03  -2.66203379e-03
  -1.79097112e-03  -9.19908442e-04  -4.88457663e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083842 minutes
Epoch 0
Fine tuning took 0.083034 minutes
Epoch 0
Fine tuning took 0.084153 minutes
{'zero': {0: [0.36699507389162561, 0.034482758620689655, 0.094827586206896547, 0.055418719211822662], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20566502463054187, 0.80665024630541871, 0.67610837438423643, 0.77955665024630538], 5: [0.42733990147783252, 0.15886699507389163, 0.22906403940886699, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.36699507389162561, 0.056650246305418719, 0.094827586206896547, 0.034482758620689655], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20566502463054187, 0.79187192118226601, 0.65147783251231528, 0.82019704433497542], 5: [0.42733990147783252, 0.15147783251231528, 0.2536945812807882, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.36699507389162561, 0.049261083743842367, 0.087438423645320201, 0.05295566502463054], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20566502463054187, 0.78201970443349755, 0.66133004926108374, 0.77463054187192115], 5: [0.42733990147783252, 0.16871921182266009, 0.25123152709359609, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.36699507389162561, 0.02832512315270936, 0.099753694581280791, 0.05295566502463054], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20566502463054187, 0.81650246305418717, 0.66009852216748766, 0.76724137931034486], 5: [0.42733990147783252, 0.15517241379310345, 0.24014778325123154, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235467 minutes
Weight histogram
[ 145  463  640  527  915 2542 2543 3552 2570  278] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 130  191  289  444  671  773 1035 2116 3154 5372] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.60743
Epoch 1, cost is  2.54826
Epoch 2, cost is  2.51045
Epoch 3, cost is  2.47918
Epoch 4, cost is  2.44612
Training took 0.161759 minutes
Weight histogram
[2330 1901 1754 1526 1181 1107 1014 1070 1023 1269] [ -6.22487292e-02  -5.60158261e-02  -4.97829230e-02  -4.35500199e-02
  -3.73171168e-02  -3.10842138e-02  -2.48513107e-02  -1.86184076e-02
  -1.23855045e-02  -6.15260144e-03   8.03016374e-05]
[1902  899  883 1002 1137 1317 1509 1674 1857 1995] [ -6.22487292e-02  -5.60158261e-02  -4.97829230e-02  -4.35500199e-02
  -3.73171168e-02  -3.10842138e-02  -2.48513107e-02  -1.86184076e-02
  -1.23855045e-02  -6.15260144e-03   8.03016374e-05]
-0.876824
1.43391
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234624 minutes
Weight histogram
[ 180  612 1121 1127 1097 2150 3916 3867 1777  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 285  386  578  955 1390  815 1022 2349 3430 4990] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.35812
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.72579
Epoch 1, cost is  2.66928
Epoch 2, cost is  2.63015
Epoch 3, cost is  2.59145
Epoch 4, cost is  2.55997
Training took 0.161144 minutes
Weight histogram
[1990 1891 1746 1347 1132 1204 1101 1080 1900 2809] [ -6.02925904e-02  -5.42553012e-02  -4.82180120e-02  -4.21807228e-02
  -3.61434336e-02  -3.01061444e-02  -2.40688552e-02  -1.80315660e-02
  -1.19942768e-02  -5.95698757e-03   8.03016374e-05]
[3960 1011  906  997 1102 1290 1483 1651 1772 2028] [ -6.02925904e-02  -5.42553012e-02  -4.82180120e-02  -4.21807228e-02
  -3.61434336e-02  -3.01061444e-02  -2.40688552e-02  -1.80315660e-02
  -1.19942768e-02  -5.95698757e-03   8.03016374e-05]
-0.824163
1.23818
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80432
Epoch 1, cost is  6.70914
Epoch 2, cost is  6.63667
Epoch 3, cost is  6.56799
Epoch 4, cost is  6.49893
Training took 0.179909 minutes
Weight histogram
[ 162  200  258 8225 3307  895  481  301  199  147] [ -1.00328391e-02  -9.03324239e-03  -8.03364564e-03  -7.03404889e-03
  -6.03445214e-03  -5.03485539e-03  -4.03525864e-03  -3.03566189e-03
  -2.03606514e-03  -1.03646839e-03  -3.68716392e-05]
[7528 3288 1854  493  280  142  142  142  149  157] [ -1.00328391e-02  -9.03324239e-03  -8.03364564e-03  -7.03404889e-03
  -6.03445214e-03  -5.03485539e-03  -4.03525864e-03  -3.03566189e-03
  -2.03606514e-03  -1.03646839e-03  -3.68716392e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088514 minutes
Epoch 0
Fine tuning took 0.088452 minutes
Epoch 0
Fine tuning took 0.088637 minutes
{'zero': {0: [0.38423645320197042, 0.15147783251231528, 0.083743842364532015, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20443349753694581, 0.69581280788177335, 0.47906403940886699, 0.52709359605911332], 5: [0.41133004926108374, 0.15270935960591134, 0.43719211822660098, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.38423645320197042, 0.16502463054187191, 0.11945812807881774, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20443349753694581, 0.70443349753694584, 0.43472906403940886, 0.50492610837438423], 5: [0.41133004926108374, 0.13054187192118227, 0.44581280788177341, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.38423645320197042, 0.16379310344827586, 0.11330049261083744, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20443349753694581, 0.68472906403940892, 0.46551724137931033, 0.51724137931034486], 5: [0.41133004926108374, 0.15147783251231528, 0.4211822660098522, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.38423645320197042, 0.18842364532019704, 0.10591133004926108, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20443349753694581, 0.68965517241379315, 0.44458128078817732, 0.49137931034482757], 5: [0.41133004926108374, 0.12192118226600986, 0.44950738916256155, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239976 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.29789
Epoch 1, cost is  4.91377
Epoch 2, cost is  4.82174
Epoch 3, cost is  4.79731
Epoch 4, cost is  4.80638
Training took 0.162779 minutes
Weight histogram
[1940 1823 1650 1695 1754 1834 1556 2308 1402  238] [-0.71033323 -0.63962612 -0.56891901 -0.4982119  -0.42750479 -0.35679768
 -0.28609058 -0.21538347 -0.14467636 -0.07396925 -0.00326214]
[ 473 1133 1683 1743 1760 1874 1930 1825 1853 1926] [-0.71033323 -0.63962612 -0.56891901 -0.4982119  -0.42750479 -0.35679768
 -0.28609058 -0.21538347 -0.14467636 -0.07396925 -0.00326214]
-25.8388
35.1438
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234524 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34196
Epoch 1, cost is  4.93722
Epoch 2, cost is  4.83817
Epoch 3, cost is  4.81227
Epoch 4, cost is  4.82095
Training took 0.159358 minutes
Weight histogram
[1869 1792 1696 1476 1904 1707 1774 2324 3121  562] [-0.74497366 -0.67080251 -0.59663136 -0.5224602  -0.44828905 -0.3741179
 -0.29994675 -0.2257756  -0.15160444 -0.07743329 -0.00326214]
[ 979 2325 2029 1799 1766 1793 1857 1901 1931 1845] [-0.74497366 -0.67080251 -0.59663136 -0.5224602  -0.44828905 -0.3741179
 -0.29994675 -0.2257756  -0.15160444 -0.07743329 -0.00326214]
-21.4493
26.1079
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.63631
Epoch 1, cost is  3.95477
Epoch 2, cost is  4.26845
Epoch 3, cost is  4.72805
Epoch 4, cost is  5.17189
Training took 0.104077 minutes
Weight histogram
[1776 2750 2682 2892 2429 1279  954  604  323  511] [-0.31483021 -0.28362204 -0.25241386 -0.22120569 -0.18999751 -0.15878934
 -0.12758117 -0.09637299 -0.06516482 -0.03395664 -0.00274847]
[ 810  818 1088 1455 1736 1978 1984 2097 2230 2004] [-0.31483021 -0.28362204 -0.25241386 -0.22120569 -0.18999751 -0.15878934
 -0.12758117 -0.09637299 -0.06516482 -0.03395664 -0.00274847]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.079839 minutes
Epoch 0
Fine tuning took 0.080104 minutes
Epoch 0
Fine tuning took 0.081346 minutes
{'zero': {0: [0.27216748768472904, 0.16871921182266009, 0.056650246305418719, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.74876847290640391, 0.78817733990147787, 0.63300492610837433], 5: [0.084975369458128072, 0.082512315270935957, 0.15517241379310345, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27216748768472904, 0.22044334975369459, 0.28201970443349755, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.68596059113300489, 0.61330049261083741, 0.60467980295566504], 5: [0.084975369458128072, 0.093596059113300489, 0.10467980295566502, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27216748768472904, 0.21674876847290642, 0.26847290640394089, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.67733990147783252, 0.59113300492610843, 0.59729064039408863], 5: [0.084975369458128072, 0.10591133004926108, 0.14039408866995073, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27216748768472904, 0.1539408866995074, 0.18965517241379309, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.73645320197044339, 0.68719211822660098, 0.70566502463054193], 5: [0.084975369458128072, 0.10960591133004927, 0.12315270935960591, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235744 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.29789
Epoch 1, cost is  4.91377
Epoch 2, cost is  4.82174
Epoch 3, cost is  4.79731
Epoch 4, cost is  4.80638
Training took 0.160253 minutes
Weight histogram
[1940 1823 1650 1695 1754 1834 1556 2308 1402  238] [-0.71033323 -0.63962612 -0.56891901 -0.4982119  -0.42750479 -0.35679768
 -0.28609058 -0.21538347 -0.14467636 -0.07396925 -0.00326214]
[ 473 1133 1683 1743 1760 1874 1930 1825 1853 1926] [-0.71033323 -0.63962612 -0.56891901 -0.4982119  -0.42750479 -0.35679768
 -0.28609058 -0.21538347 -0.14467636 -0.07396925 -0.00326214]
-25.8388
35.1438
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235161 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34196
Epoch 1, cost is  4.93722
Epoch 2, cost is  4.83817
Epoch 3, cost is  4.81227
Epoch 4, cost is  4.82095
Training took 0.162092 minutes
Weight histogram
[1869 1792 1696 1476 1904 1707 1774 2324 3121  562] [-0.74497366 -0.67080251 -0.59663136 -0.5224602  -0.44828905 -0.3741179
 -0.29994675 -0.2257756  -0.15160444 -0.07743329 -0.00326214]
[ 979 2325 2029 1799 1766 1793 1857 1901 1931 1845] [-0.74497366 -0.67080251 -0.59663136 -0.5224602  -0.44828905 -0.3741179
 -0.29994675 -0.2257756  -0.15160444 -0.07743329 -0.00326214]
-21.4493
26.1079
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.23488
Epoch 1, cost is  3.3535
Epoch 2, cost is  3.49342
Epoch 3, cost is  3.76065
Epoch 4, cost is  4.12384
Training took 0.123040 minutes
Weight histogram
[1347 2641 2536 2540 2270 1912 1311  713  338  592] [-0.27973735 -0.25203443 -0.22433151 -0.19662859 -0.16892567 -0.14122275
 -0.11351983 -0.08581691 -0.05811399 -0.03041107 -0.00270815]
[ 813  851 1224 1456 1734 1884 2058 2091 2052 2037] [-0.27973735 -0.25203443 -0.22433151 -0.19662859 -0.16892567 -0.14122275
 -0.11351983 -0.08581691 -0.05811399 -0.03041107 -0.00270815]
-9.51929
11.2219
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083153 minutes
Epoch 0
Fine tuning took 0.081946 minutes
Epoch 0
Fine tuning took 0.083624 minutes
{'zero': {0: [0.23275862068965517, 0.20812807881773399, 0.16133004926108374, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66625615763546797, 0.76600985221674878, 0.81896551724137934, 0.82389162561576357], 5: [0.10098522167487685, 0.025862068965517241, 0.019704433497536946, 0.018472906403940888], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23275862068965517, 0.16379310344827586, 0.21182266009852216, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66625615763546797, 0.7573891625615764, 0.70073891625615758, 0.71059113300492616], 5: [0.10098522167487685, 0.078817733990147784, 0.087438423645320201, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23275862068965517, 0.19827586206896552, 0.21428571428571427, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66625615763546797, 0.71798029556650245, 0.68472906403940892, 0.71305418719211822], 5: [0.10098522167487685, 0.083743842364532015, 0.10098522167487685, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23275862068965517, 0.14778325123152711, 0.23399014778325122, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66625615763546797, 0.79556650246305416, 0.70320197044334976, 0.68349753694581283], 5: [0.10098522167487685, 0.056650246305418719, 0.062807881773399021, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235987 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.29789
Epoch 1, cost is  4.91377
Epoch 2, cost is  4.82174
Epoch 3, cost is  4.79731
Epoch 4, cost is  4.80638
Training took 0.161988 minutes
Weight histogram
[1940 1823 1650 1695 1754 1834 1556 2308 1402  238] [-0.71033323 -0.63962612 -0.56891901 -0.4982119  -0.42750479 -0.35679768
 -0.28609058 -0.21538347 -0.14467636 -0.07396925 -0.00326214]
[ 473 1133 1683 1743 1760 1874 1930 1825 1853 1926] [-0.71033323 -0.63962612 -0.56891901 -0.4982119  -0.42750479 -0.35679768
 -0.28609058 -0.21538347 -0.14467636 -0.07396925 -0.00326214]
-25.8388
35.1438
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236165 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34196
Epoch 1, cost is  4.93722
Epoch 2, cost is  4.83817
Epoch 3, cost is  4.81227
Epoch 4, cost is  4.82095
Training took 0.161110 minutes
Weight histogram
[1869 1792 1696 1476 1904 1707 1774 2324 3121  562] [-0.74497366 -0.67080251 -0.59663136 -0.5224602  -0.44828905 -0.3741179
 -0.29994675 -0.2257756  -0.15160444 -0.07743329 -0.00326214]
[ 979 2325 2029 1799 1766 1793 1857 1901 1931 1845] [-0.74497366 -0.67080251 -0.59663136 -0.5224602  -0.44828905 -0.3741179
 -0.29994675 -0.2257756  -0.15160444 -0.07743329 -0.00326214]
-21.4493
26.1079
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.72585
Epoch 1, cost is  2.40768
Epoch 2, cost is  2.31367
Epoch 3, cost is  2.38824
Epoch 4, cost is  2.54676
Training took 0.179272 minutes
Weight histogram
[2008 2628 2462 2347 2184 1682 1225  635  437  592] [-0.18797739 -0.16944944 -0.15092148 -0.13239353 -0.11386558 -0.09533763
 -0.07680968 -0.05828172 -0.03975377 -0.02122582 -0.00269787]
[ 882  832 1212 1517 1713 1886 2004 2058 2207 1889] [-0.18797739 -0.16944944 -0.15092148 -0.13239353 -0.11386558 -0.09533763
 -0.07680968 -0.05828172 -0.03975377 -0.02122582 -0.00269787]
-6.41529
7.90691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087451 minutes
Epoch 0
Fine tuning took 0.088977 minutes
Epoch 0
Fine tuning took 0.087544 minutes
{'zero': {0: [0.18349753694581281, 0.12438423645320197, 0.009852216748768473, 0.013546798029556651], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.86699507389162567, 0.98275862068965514, 0.98152709359605916], 5: [0.080049261083743842, 0.0086206896551724137, 0.0073891625615763543, 0.0049261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18349753694581281, 0.18472906403940886, 0.087438423645320201, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.73768472906403937, 0.84729064039408863, 0.80172413793103448], 5: [0.080049261083743842, 0.077586206896551727, 0.065270935960591137, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18349753694581281, 0.16871921182266009, 0.10221674876847291, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.77955665024630538, 0.82266009852216748, 0.80911330049261088], 5: [0.080049261083743842, 0.051724137931034482, 0.075123152709359611, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18349753694581281, 0.16995073891625614, 0.070197044334975367, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.74753694581280783, 0.8780788177339901, 0.83251231527093594], 5: [0.080049261083743842, 0.082512315270935957, 0.051724137931034482, 0.059113300492610835], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236572 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.80887
Epoch 1, cost is  1.72791
Epoch 2, cost is  1.69519
Epoch 3, cost is  1.68121
Epoch 4, cost is  1.67219
Training took 0.159088 minutes
Weight histogram
[2607 2992 2305 2176 1979 1793 1060  623  349  316] [-0.16350926 -0.14718069 -0.13085212 -0.11452355 -0.09819498 -0.08186641
 -0.06553784 -0.04920927 -0.0328807  -0.01655213 -0.00022356]
[ 518  641  985 1309 1552 1757 2064 2387 2434 2553] [-0.16350926 -0.14718069 -0.13085212 -0.11452355 -0.09819498 -0.08186641
 -0.06553784 -0.04920927 -0.0328807  -0.01655213 -0.00022356]
-4.71067
6.67126
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235976 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83941
Epoch 1, cost is  1.7548
Epoch 2, cost is  1.72702
Epoch 3, cost is  1.70905
Epoch 4, cost is  1.69966
Training took 0.160289 minutes
Weight histogram
[2926 2429 2594 2094 1950 1690 1813 1304  741  684] [-0.16591623 -0.14934697 -0.1327777  -0.11620843 -0.09963916 -0.0830699
 -0.06650063 -0.04993136 -0.03336209 -0.01679282 -0.00022356]
[1083 1326 1806 1273 1527 1753 2056 2294 2558 2549] [-0.16591623 -0.14934697 -0.1327777  -0.11620843 -0.09963916 -0.0830699
 -0.06650063 -0.04993136 -0.03336209 -0.01679282 -0.00022356]
-4.49111
5.73821
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.59235
Epoch 1, cost is  5.52205
Epoch 2, cost is  4.8078
Epoch 3, cost is  4.48039
Epoch 4, cost is  4.2675
Training took 0.103895 minutes
Weight histogram
[ 603 2345 2048 1911 1329 1294 1055  993 1181 3441] [-0.10474743 -0.09429832 -0.08384921 -0.0734001  -0.06295099 -0.05250188
 -0.04205277 -0.03160367 -0.02115456 -0.01070545 -0.00025634]
[3015 1076 1031 1163 1325 1437 1597 1759 1881 1916] [-0.10474743 -0.09429832 -0.08384921 -0.0734001  -0.06295099 -0.05250188
 -0.04205277 -0.03160367 -0.02115456 -0.01070545 -0.00025634]
-1.55839
3.39939
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081104 minutes
Epoch 0
Fine tuning took 0.081302 minutes
Epoch 0
Fine tuning took 0.082409 minutes
{'zero': {0: [0.68596059113300489, 0.3682266009852217, 0.20812807881773399, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20320197044334976, 0.35221674876847292, 0.55295566502463056, 0.51847290640394084], 5: [0.11083743842364532, 0.27955665024630544, 0.23891625615763548, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.68596059113300489, 0.40270935960591131, 0.18842364532019704, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20320197044334976, 0.31157635467980294, 0.61699507389162567, 0.49384236453201968], 5: [0.11083743842364532, 0.2857142857142857, 0.19458128078817735, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.68596059113300489, 0.43472906403940886, 0.22536945812807882, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20320197044334976, 0.31157635467980294, 0.56527093596059108, 0.49137931034482757], 5: [0.11083743842364532, 0.2536945812807882, 0.20935960591133004, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.68596059113300489, 0.3682266009852217, 0.18103448275862069, 0.23645320197044334], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.20320197044334976, 0.30665024630541871, 0.58990147783251234, 0.50985221674876846], 5: [0.11083743842364532, 0.3251231527093596, 0.22906403940886699, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236092 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.80887
Epoch 1, cost is  1.72791
Epoch 2, cost is  1.69519
Epoch 3, cost is  1.68121
Epoch 4, cost is  1.67219
Training took 0.158964 minutes
Weight histogram
[2607 2992 2305 2176 1979 1793 1060  623  349  316] [-0.16350926 -0.14718069 -0.13085212 -0.11452355 -0.09819498 -0.08186641
 -0.06553784 -0.04920927 -0.0328807  -0.01655213 -0.00022356]
[ 518  641  985 1309 1552 1757 2064 2387 2434 2553] [-0.16350926 -0.14718069 -0.13085212 -0.11452355 -0.09819498 -0.08186641
 -0.06553784 -0.04920927 -0.0328807  -0.01655213 -0.00022356]
-4.71067
6.67126
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235612 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83941
Epoch 1, cost is  1.7548
Epoch 2, cost is  1.72702
Epoch 3, cost is  1.70905
Epoch 4, cost is  1.69966
Training took 0.158670 minutes
Weight histogram
[2926 2429 2594 2094 1950 1690 1813 1304  741  684] [-0.16591623 -0.14934697 -0.1327777  -0.11620843 -0.09963916 -0.0830699
 -0.06650063 -0.04993136 -0.03336209 -0.01679282 -0.00022356]
[1083 1326 1806 1273 1527 1753 2056 2294 2558 2549] [-0.16591623 -0.14934697 -0.1327777  -0.11620843 -0.09963916 -0.0830699
 -0.06650063 -0.04993136 -0.03336209 -0.01679282 -0.00022356]
-4.49111
5.73821
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55365
Epoch 1, cost is  5.30994
Epoch 2, cost is  4.35279
Epoch 3, cost is  3.92631
Epoch 4, cost is  3.67076
Training took 0.123416 minutes
Weight histogram
[ 879 2284 2016 1758 1430 1144 1087 1046 1484 3072] [-0.0834422  -0.07512538 -0.06680857 -0.05849175 -0.05017494 -0.04185813
 -0.03354131 -0.0252245  -0.01690768 -0.00859087 -0.00027406]
[3085 1102 1047 1092 1254 1420 1537 1745 1823 2095] [-0.0834422  -0.07512538 -0.06680857 -0.05849175 -0.05017494 -0.04185813
 -0.03354131 -0.0252245  -0.01690768 -0.00859087 -0.00027406]
-1.44792
2.70075
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084147 minutes
Epoch 0
Fine tuning took 0.084312 minutes
Epoch 0
Fine tuning took 0.084400 minutes
{'zero': {0: [0.45073891625615764, 0.23152709359605911, 0.2894088669950739, 0.29556650246305421], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3460591133004926, 0.50123152709359609, 0.55049261083743839, 0.52216748768472909], 5: [0.20320197044334976, 0.26724137931034481, 0.16009852216748768, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.45073891625615764, 0.14532019704433496, 0.1625615763546798, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3460591133004926, 0.57758620689655171, 0.58128078817733986, 0.60467980295566504], 5: [0.20320197044334976, 0.27709359605911332, 0.25615763546798032, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.45073891625615764, 0.20073891625615764, 0.19581280788177341, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3460591133004926, 0.55911330049261088, 0.56773399014778325, 0.58004926108374388], 5: [0.20320197044334976, 0.24014778325123154, 0.23645320197044334, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.45073891625615764, 0.14039408866995073, 0.14655172413793102, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3460591133004926, 0.58128078817733986, 0.61083743842364535, 0.63300492610837433], 5: [0.20320197044334976, 0.27832512315270935, 0.24261083743842365, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236370 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.80887
Epoch 1, cost is  1.72791
Epoch 2, cost is  1.69519
Epoch 3, cost is  1.68121
Epoch 4, cost is  1.67219
Training took 0.160033 minutes
Weight histogram
[2607 2992 2305 2176 1979 1793 1060  623  349  316] [-0.16350926 -0.14718069 -0.13085212 -0.11452355 -0.09819498 -0.08186641
 -0.06553784 -0.04920927 -0.0328807  -0.01655213 -0.00022356]
[ 518  641  985 1309 1552 1757 2064 2387 2434 2553] [-0.16350926 -0.14718069 -0.13085212 -0.11452355 -0.09819498 -0.08186641
 -0.06553784 -0.04920927 -0.0328807  -0.01655213 -0.00022356]
-4.71067
6.67126
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235877 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83941
Epoch 1, cost is  1.7548
Epoch 2, cost is  1.72702
Epoch 3, cost is  1.70905
Epoch 4, cost is  1.69966
Training took 0.160671 minutes
Weight histogram
[2926 2429 2594 2094 1950 1690 1813 1304  741  684] [-0.16591623 -0.14934697 -0.1327777  -0.11620843 -0.09963916 -0.0830699
 -0.06650063 -0.04993136 -0.03336209 -0.01679282 -0.00022356]
[1083 1326 1806 1273 1527 1753 2056 2294 2558 2549] [-0.16591623 -0.14934697 -0.1327777  -0.11620843 -0.09963916 -0.0830699
 -0.06650063 -0.04993136 -0.03336209 -0.01679282 -0.00022356]
-4.49111
5.73821
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46938
Epoch 1, cost is  5.06501
Epoch 2, cost is  3.82215
Epoch 3, cost is  3.23134
Epoch 4, cost is  2.88104
Training took 0.180236 minutes
Weight histogram
[ 958 2767 1918 1617 1455 1305  985 1105 2663 1427] [-0.05459121 -0.04915888 -0.04372654 -0.0382942  -0.03286186 -0.02742953
 -0.02199719 -0.01656485 -0.01113252 -0.00570018 -0.00026784]
[3226 1095 1039 1118 1189 1338 1522 1722 1954 1997] [-0.05459121 -0.04915888 -0.04372654 -0.0382942  -0.03286186 -0.02742953
 -0.02199719 -0.01656485 -0.01113252 -0.00570018 -0.00026784]
-1.11031
2.02951
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089070 minutes
Epoch 0
Fine tuning took 0.090071 minutes
Epoch 0
Fine tuning took 0.089798 minutes
{'zero': {0: [0.35344827586206895, 0.2857142857142857, 0.26477832512315269, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37438423645320196, 0.48645320197044334, 0.60467980295566504, 0.5788177339901478], 5: [0.27216748768472904, 0.22783251231527094, 0.13054187192118227, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.35344827586206895, 0.30172413793103448, 0.26477832512315269, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37438423645320196, 0.50862068965517238, 0.63916256157635465, 0.61699507389162567], 5: [0.27216748768472904, 0.18965517241379309, 0.096059113300492605, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.35344827586206895, 0.28078817733990147, 0.27093596059113301, 0.23645320197044334], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37438423645320196, 0.52463054187192115, 0.61822660098522164, 0.62068965517241381], 5: [0.27216748768472904, 0.19458128078817735, 0.11083743842364532, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.35344827586206895, 0.29433497536945813, 0.26970443349753692, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37438423645320196, 0.51724137931034486, 0.61330049261083741, 0.60467980295566504], 5: [0.27216748768472904, 0.18842364532019704, 0.11699507389162561, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235597 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42766
Epoch 1, cost is  2.38587
Epoch 2, cost is  2.3515
Epoch 3, cost is  2.32336
Epoch 4, cost is  2.30116
Training took 0.162235 minutes
Weight histogram
[2835 2299 2077 1620 1374 1300 1099 1155 1085 1356] [ -6.71114773e-02  -6.03922994e-02  -5.36731215e-02  -4.69539436e-02
  -4.02347657e-02  -3.35155878e-02  -2.67964099e-02  -2.00772320e-02
  -1.33580541e-02  -6.63887625e-03   8.03016374e-05]
[1992  970  987 1148 1333 1504 1799 1972 2166 2329] [ -6.71114773e-02  -6.03922994e-02  -5.36731215e-02  -4.69539436e-02
  -4.02347657e-02  -3.35155878e-02  -2.67964099e-02  -2.00772320e-02
  -1.33580541e-02  -6.63887625e-03   8.03016374e-05]
-0.932204
1.53651
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234873 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56717
Epoch 1, cost is  2.52008
Epoch 2, cost is  2.48623
Epoch 3, cost is  2.45591
Epoch 4, cost is  2.42439
Training took 0.161466 minutes
Weight histogram
[2753 2041 1996 1530 1452 1215 1195 1143 1889 3011] [ -6.50108978e-02  -5.85017779e-02  -5.19926579e-02  -4.54835380e-02
  -3.89744180e-02  -3.24652981e-02  -2.59561781e-02  -1.94470582e-02
  -1.29379383e-02  -6.42881831e-03   8.03016374e-05]
[4143 1003 1019 1127 1307 1478 1790 1914 2178 2266] [ -6.50108978e-02  -5.85017779e-02  -5.19926579e-02  -4.54835380e-02
  -3.89744180e-02  -3.24652981e-02  -2.59561781e-02  -1.94470582e-02
  -1.29379383e-02  -6.42881831e-03   8.03016374e-05]
-0.895509
1.35618
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.88083
Epoch 1, cost is  6.81161
Epoch 2, cost is  6.74967
Epoch 3, cost is  6.68921
Epoch 4, cost is  6.62896
Training took 0.103228 minutes
Weight histogram
[  92 7621 6043  878  524  348  250  185  136  123] [ -7.25918217e-03  -6.53581815e-03  -5.81245413e-03  -5.08909011e-03
  -4.36572609e-03  -3.64236208e-03  -2.91899806e-03  -2.19563404e-03
  -1.47227002e-03  -7.48906005e-04  -2.55419873e-05]
[10968  3687   630   252   145   118   107   102   100    91] [ -7.25918217e-03  -6.53581815e-03  -5.81245413e-03  -5.08909011e-03
  -4.36572609e-03  -3.64236208e-03  -2.91899806e-03  -2.19563404e-03
  -1.47227002e-03  -7.48906005e-04  -2.55419873e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080981 minutes
Epoch 0
Fine tuning took 0.080619 minutes
Epoch 0
Fine tuning took 0.081079 minutes
{'zero': {0: [0.28201970443349755, 0.2019704433497537, 0.41133004926108374, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.34975369458128081, 0.63054187192118227, 0.43226600985221675, 0.74507389162561577], 5: [0.3682266009852217, 0.16748768472906403, 0.15640394088669951, 0.062807881773399021], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28201970443349755, 0.19458128078817735, 0.4211822660098522, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.34975369458128081, 0.66009852216748766, 0.39408866995073893, 0.73645320197044339], 5: [0.3682266009852217, 0.14532019704433496, 0.18472906403940886, 0.087438423645320201], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28201970443349755, 0.20566502463054187, 0.4211822660098522, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.34975369458128081, 0.64408866995073888, 0.41133004926108374, 0.73522167487684731], 5: [0.3682266009852217, 0.15024630541871922, 0.16748768472906403, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28201970443349755, 0.20689655172413793, 0.41871921182266009, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.34975369458128081, 0.6576354679802956, 0.44581280788177341, 0.76970443349753692], 5: [0.3682266009852217, 0.1354679802955665, 0.1354679802955665, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236355 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42766
Epoch 1, cost is  2.38587
Epoch 2, cost is  2.3515
Epoch 3, cost is  2.32336
Epoch 4, cost is  2.30116
Training took 0.160674 minutes
Weight histogram
[2835 2299 2077 1620 1374 1300 1099 1155 1085 1356] [ -6.71114773e-02  -6.03922994e-02  -5.36731215e-02  -4.69539436e-02
  -4.02347657e-02  -3.35155878e-02  -2.67964099e-02  -2.00772320e-02
  -1.33580541e-02  -6.63887625e-03   8.03016374e-05]
[1992  970  987 1148 1333 1504 1799 1972 2166 2329] [ -6.71114773e-02  -6.03922994e-02  -5.36731215e-02  -4.69539436e-02
  -4.02347657e-02  -3.35155878e-02  -2.67964099e-02  -2.00772320e-02
  -1.33580541e-02  -6.63887625e-03   8.03016374e-05]
-0.932204
1.53651
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235559 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56717
Epoch 1, cost is  2.52008
Epoch 2, cost is  2.48623
Epoch 3, cost is  2.45591
Epoch 4, cost is  2.42439
Training took 0.158990 minutes
Weight histogram
[2753 2041 1996 1530 1452 1215 1195 1143 1889 3011] [ -6.50108978e-02  -5.85017779e-02  -5.19926579e-02  -4.54835380e-02
  -3.89744180e-02  -3.24652981e-02  -2.59561781e-02  -1.94470582e-02
  -1.29379383e-02  -6.42881831e-03   8.03016374e-05]
[4143 1003 1019 1127 1307 1478 1790 1914 2178 2266] [ -6.50108978e-02  -5.85017779e-02  -5.19926579e-02  -4.54835380e-02
  -3.89744180e-02  -3.24652981e-02  -2.59561781e-02  -1.94470582e-02
  -1.29379383e-02  -6.42881831e-03   8.03016374e-05]
-0.895509
1.35618
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8621
Epoch 1, cost is  6.78582
Epoch 2, cost is  6.72171
Epoch 3, cost is  6.65988
Epoch 4, cost is  6.5966
Training took 0.122350 minutes
Weight histogram
[   93   119   450 12757  1188   615   387   264   180   147] [ -8.75947252e-03  -7.88835787e-03  -7.01724321e-03  -6.14612856e-03
  -5.27501390e-03  -4.40389925e-03  -3.53278460e-03  -2.66166994e-03
  -1.79055529e-03  -9.19440636e-04  -4.83259828e-05]
[10122  3953  1020   363   144   130   123   113   118   114] [ -8.75947252e-03  -7.88835787e-03  -7.01724321e-03  -6.14612856e-03
  -5.27501390e-03  -4.40389925e-03  -3.53278460e-03  -2.66166994e-03
  -1.79055529e-03  -9.19440636e-04  -4.83259828e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083817 minutes
Epoch 0
Fine tuning took 0.081941 minutes
Epoch 0
Fine tuning took 0.083673 minutes
{'zero': {0: [0.24384236453201971, 0.018472906403940888, 0.29926108374384236, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.36330049261083741, 0.72906403940886699, 0.57512315270935965, 0.66379310344827591], 5: [0.39285714285714285, 0.25246305418719212, 0.12561576354679804, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24384236453201971, 0.022167487684729065, 0.30172413793103448, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.36330049261083741, 0.73768472906403937, 0.56403940886699511, 0.6711822660098522], 5: [0.39285714285714285, 0.24014778325123154, 0.13423645320197045, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24384236453201971, 0.02832512315270936, 0.33620689655172414, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.36330049261083741, 0.71182266009852213, 0.55541871921182262, 0.64532019704433496], 5: [0.39285714285714285, 0.25985221674876846, 0.10837438423645321, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24384236453201971, 0.022167487684729065, 0.35714285714285715, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.36330049261083741, 0.7142857142857143, 0.52093596059113301, 0.66625615763546797], 5: [0.39285714285714285, 0.26354679802955666, 0.12192118226600986, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235164 minutes
Weight histogram
[ 145  463  640  527  915 2542 2735 4644 3283  306] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
[ 143  220  337  555  836  853 1961 2525 5332 3438] [ -1.55507252e-04   9.75020084e-06   1.75007654e-04   3.40265107e-04
   5.05522560e-04   6.70780013e-04   8.36037466e-04   1.00129492e-03
   1.16655237e-03   1.33180983e-03   1.49706728e-03]
-1.07829
1.04983
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42766
Epoch 1, cost is  2.38587
Epoch 2, cost is  2.3515
Epoch 3, cost is  2.32336
Epoch 4, cost is  2.30116
Training took 0.161447 minutes
Weight histogram
[2835 2299 2077 1620 1374 1300 1099 1155 1085 1356] [ -6.71114773e-02  -6.03922994e-02  -5.36731215e-02  -4.69539436e-02
  -4.02347657e-02  -3.35155878e-02  -2.67964099e-02  -2.00772320e-02
  -1.33580541e-02  -6.63887625e-03   8.03016374e-05]
[1992  970  987 1148 1333 1504 1799 1972 2166 2329] [ -6.71114773e-02  -6.03922994e-02  -5.36731215e-02  -4.69539436e-02
  -4.02347657e-02  -3.35155878e-02  -2.67964099e-02  -2.00772320e-02
  -1.33580541e-02  -6.63887625e-03   8.03016374e-05]
-0.932204
1.53651
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234934 minutes
Weight histogram
[ 180  612 1121 1127 1103 2651 4868 4384 1826  353] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
[ 303  422  657 1082 1441  851 1866 2706 4496 4401] [ -1.55507252e-04  -1.31789144e-05   1.29149423e-04   2.71477761e-04
   4.13806099e-04   5.56134437e-04   6.98462775e-04   8.40791113e-04
   9.83119450e-04   1.12544779e-03   1.26777613e-03]
-1.53057
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56717
Epoch 1, cost is  2.52008
Epoch 2, cost is  2.48623
Epoch 3, cost is  2.45591
Epoch 4, cost is  2.42439
Training took 0.160048 minutes
Weight histogram
[2753 2041 1996 1530 1452 1215 1195 1143 1889 3011] [ -6.50108978e-02  -5.85017779e-02  -5.19926579e-02  -4.54835380e-02
  -3.89744180e-02  -3.24652981e-02  -2.59561781e-02  -1.94470582e-02
  -1.29379383e-02  -6.42881831e-03   8.03016374e-05]
[4143 1003 1019 1127 1307 1478 1790 1914 2178 2266] [ -6.50108978e-02  -5.85017779e-02  -5.19926579e-02  -4.54835380e-02
  -3.89744180e-02  -3.24652981e-02  -2.59561781e-02  -1.94470582e-02
  -1.29379383e-02  -6.42881831e-03   8.03016374e-05]
-0.895509
1.35618
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80654
Epoch 1, cost is  6.71245
Epoch 2, cost is  6.64117
Epoch 3, cost is  6.57368
Epoch 4, cost is  6.50548
Training took 0.181548 minutes
Weight histogram
[ 162  200  258 8600 4633 1044  557  347  230  169] [ -1.00328391e-02  -9.03324239e-03  -8.03364564e-03  -7.03404889e-03
  -6.03445214e-03  -5.03485539e-03  -4.03525864e-03  -3.03566189e-03
  -2.03606514e-03  -1.03646839e-03  -3.68716392e-05]
[8713 3819 2163  493  280  142  142  142  149  157] [ -1.00328391e-02  -9.03324239e-03  -8.03364564e-03  -7.03404889e-03
  -6.03445214e-03  -5.03485539e-03  -4.03525864e-03  -3.03566189e-03
  -2.03606514e-03  -1.03646839e-03  -3.68716392e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089059 minutes
Epoch 0
Fine tuning took 0.089041 minutes
Epoch 0
Fine tuning took 0.088129 minutes
{'zero': {0: [0.25985221674876846, 0.036945812807881777, 0.41748768472906406, 0.30172413793103448], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40024630541871919, 0.68103448275862066, 0.48152709359605911, 0.50985221674876846], 5: [0.33990147783251229, 0.28201970443349755, 0.10098522167487685, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25985221674876846, 0.035714285714285712, 0.43472906403940886, 0.30541871921182268], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40024630541871919, 0.70073891625615758, 0.44334975369458129, 0.53940886699507384], 5: [0.33990147783251229, 0.26354679802955666, 0.12192118226600986, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25985221674876846, 0.049261083743842367, 0.43842364532019706, 0.30911330049261082], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40024630541871919, 0.67733990147783252, 0.4642857142857143, 0.52093596059113301], 5: [0.33990147783251229, 0.27339901477832512, 0.097290640394088676, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25985221674876846, 0.027093596059113302, 0.43842364532019706, 0.29802955665024633], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40024630541871919, 0.71059113300492616, 0.45566502463054187, 0.53940886699507384], 5: [0.33990147783251229, 0.26231527093596058, 0.10591133004926108, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236031 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.50616
Epoch 1, cost is  5.10646
Epoch 2, cost is  4.99866
Epoch 3, cost is  4.95903
Epoch 4, cost is  4.95672
Training took 0.160737 minutes
Weight histogram
[2433 1854 1784 2004 2009 2044 1972 2096 1743  286] [-0.77500618 -0.69783177 -0.62065737 -0.54348296 -0.46630856 -0.38913416
 -0.31195975 -0.23478535 -0.15761095 -0.08043654 -0.00326214]
[ 554 1352 1888 1902 1970 2138 2037 2010 2125 2249] [-0.77500618 -0.69783177 -0.62065737 -0.54348296 -0.46630856 -0.38913416
 -0.31195975 -0.23478535 -0.15761095 -0.08043654 -0.00326214]
-29.1
35.8954
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235821 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.68434
Epoch 1, cost is  5.27205
Epoch 2, cost is  5.15472
Epoch 3, cost is  5.10511
Epoch 4, cost is  5.09019
Training took 0.161206 minutes
Weight histogram
[2189 1814 2025 1942 2010 1998 2017 2176 3317  762] [-0.82163024 -0.73979343 -0.65795662 -0.57611981 -0.494283   -0.41244619
 -0.33060938 -0.24877257 -0.16693576 -0.08509895 -0.00326214]
[1144 2764 1947 1956 1932 2015 2059 2135 2043 2255] [-0.82163024 -0.73979343 -0.65795662 -0.57611981 -0.494283   -0.41244619
 -0.33060938 -0.24877257 -0.16693576 -0.08509895 -0.00326214]
-23.9134
26.1079
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.63987
Epoch 1, cost is  3.95795
Epoch 2, cost is  4.27037
Epoch 3, cost is  4.69553
Epoch 4, cost is  5.22427
Training took 0.102884 minutes
Weight histogram
[1804 3139 3058 3313 2724 1492 1078  685  363  569] [-0.31483021 -0.28362204 -0.25241386 -0.22120569 -0.18999751 -0.15878934
 -0.12758117 -0.09637299 -0.06516482 -0.03395664 -0.00274847]
[ 908  923 1228 1641 1959 2229 2242 2359 2510 2226] [-0.31483021 -0.28362204 -0.25241386 -0.22120569 -0.18999751 -0.15878934
 -0.12758117 -0.09637299 -0.06516482 -0.03395664 -0.00274847]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.079469 minutes
Epoch 0
Fine tuning took 0.080259 minutes
Epoch 0
Fine tuning took 0.080884 minutes
{'zero': {0: [0.23645320197044334, 0.2105911330049261, 0.22906403940886699, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.64408866995073888, 0.68596059113300489, 0.6785714285714286], 5: [0.1206896551724138, 0.14532019704433496, 0.084975369458128072, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23645320197044334, 0.28694581280788178, 0.17980295566502463, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.6219211822660099, 0.69827586206896552, 0.59729064039408863], 5: [0.1206896551724138, 0.091133004926108374, 0.12192118226600986, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23645320197044334, 0.2894088669950739, 0.29064039408866993, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.6145320197044335, 0.58251231527093594, 0.54556650246305416], 5: [0.1206896551724138, 0.096059113300492605, 0.1268472906403941, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23645320197044334, 0.20689655172413793, 0.18472906403940886, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.72660098522167482, 0.72783251231527091, 0.59113300492610843], 5: [0.1206896551724138, 0.066502463054187194, 0.087438423645320201, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236076 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.50616
Epoch 1, cost is  5.10646
Epoch 2, cost is  4.99866
Epoch 3, cost is  4.95903
Epoch 4, cost is  4.95672
Training took 0.160800 minutes
Weight histogram
[2433 1854 1784 2004 2009 2044 1972 2096 1743  286] [-0.77500618 -0.69783177 -0.62065737 -0.54348296 -0.46630856 -0.38913416
 -0.31195975 -0.23478535 -0.15761095 -0.08043654 -0.00326214]
[ 554 1352 1888 1902 1970 2138 2037 2010 2125 2249] [-0.77500618 -0.69783177 -0.62065737 -0.54348296 -0.46630856 -0.38913416
 -0.31195975 -0.23478535 -0.15761095 -0.08043654 -0.00326214]
-29.1
35.8954
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234141 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.68434
Epoch 1, cost is  5.27205
Epoch 2, cost is  5.15472
Epoch 3, cost is  5.10511
Epoch 4, cost is  5.09019
Training took 0.160745 minutes
Weight histogram
[2189 1814 2025 1942 2010 1998 2017 2176 3317  762] [-0.82163024 -0.73979343 -0.65795662 -0.57611981 -0.494283   -0.41244619
 -0.33060938 -0.24877257 -0.16693576 -0.08509895 -0.00326214]
[1144 2764 1947 1956 1932 2015 2059 2135 2043 2255] [-0.82163024 -0.73979343 -0.65795662 -0.57611981 -0.494283   -0.41244619
 -0.33060938 -0.24877257 -0.16693576 -0.08509895 -0.00326214]
-23.9134
26.1079
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.23653
Epoch 1, cost is  3.33821
Epoch 2, cost is  3.49102
Epoch 3, cost is  3.78625
Epoch 4, cost is  4.15975
Training took 0.123520 minutes
Weight histogram
[1472 2959 2879 2864 2550 2169 1483  804  381  664] [-0.27973735 -0.25203353 -0.2243297  -0.19662588 -0.16892205 -0.14121822
 -0.1135144  -0.08581057 -0.05810675 -0.03040292 -0.00269909]
[ 913  958 1379 1640 1949 2118 2316 2355 2308 2289] [-0.27973735 -0.25203353 -0.2243297  -0.19662588 -0.16892205 -0.14121822
 -0.1135144  -0.08581057 -0.05810675 -0.03040292 -0.00269909]
-9.51929
11.2219
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081588 minutes
Epoch 0
Fine tuning took 0.082912 minutes
Epoch 0
Fine tuning took 0.083601 minutes
{'zero': {0: [0.21921182266009853, 0.35344827586206895, 0.29187192118226601, 0.40517241379310343], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.46921182266009853, 0.56403940886699511, 0.33743842364532017], 5: [0.088669950738916259, 0.17733990147783252, 0.14408866995073891, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21921182266009853, 0.27216748768472904, 0.22044334975369459, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.62684729064039413, 0.66133004926108374, 0.64162561576354682], 5: [0.088669950738916259, 0.10098522167487685, 0.11822660098522167, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21921182266009853, 0.25246305418719212, 0.26108374384236455, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.64408866995073888, 0.62315270935960587, 0.60344827586206895], 5: [0.088669950738916259, 0.10344827586206896, 0.11576354679802955, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21921182266009853, 0.23645320197044334, 0.20073891625615764, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.69827586206896552, 0.68103448275862066, 0.67610837438423643], 5: [0.088669950738916259, 0.065270935960591137, 0.11822660098522167, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234754 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.50616
Epoch 1, cost is  5.10646
Epoch 2, cost is  4.99866
Epoch 3, cost is  4.95903
Epoch 4, cost is  4.95672
Training took 0.161448 minutes
Weight histogram
[2433 1854 1784 2004 2009 2044 1972 2096 1743  286] [-0.77500618 -0.69783177 -0.62065737 -0.54348296 -0.46630856 -0.38913416
 -0.31195975 -0.23478535 -0.15761095 -0.08043654 -0.00326214]
[ 554 1352 1888 1902 1970 2138 2037 2010 2125 2249] [-0.77500618 -0.69783177 -0.62065737 -0.54348296 -0.46630856 -0.38913416
 -0.31195975 -0.23478535 -0.15761095 -0.08043654 -0.00326214]
-29.1
35.8954
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.233994 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.68434
Epoch 1, cost is  5.27205
Epoch 2, cost is  5.15472
Epoch 3, cost is  5.10511
Epoch 4, cost is  5.09019
Training took 0.159553 minutes
Weight histogram
[2189 1814 2025 1942 2010 1998 2017 2176 3317  762] [-0.82163024 -0.73979343 -0.65795662 -0.57611981 -0.494283   -0.41244619
 -0.33060938 -0.24877257 -0.16693576 -0.08509895 -0.00326214]
[1144 2764 1947 1956 1932 2015 2059 2135 2043 2255] [-0.82163024 -0.73979343 -0.65795662 -0.57611981 -0.494283   -0.41244619
 -0.33060938 -0.24877257 -0.16693576 -0.08509895 -0.00326214]
-23.9134
26.1079
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.73065
Epoch 1, cost is  2.40831
Epoch 2, cost is  2.31027
Epoch 3, cost is  2.39785
Epoch 4, cost is  2.55392
Training took 0.181332 minutes
Weight histogram
[2211 2986 2757 2651 2469 1901 1375  718  495  662] [-0.18797739 -0.16944944 -0.15092148 -0.13239353 -0.11386558 -0.09533763
 -0.07680968 -0.05828172 -0.03975377 -0.02122582 -0.00269787]
[ 991  937 1365 1705 1925 2120 2250 2311 2480 2141] [-0.18797739 -0.16944944 -0.15092148 -0.13239353 -0.11386558 -0.09533763
 -0.07680968 -0.05828172 -0.03975377 -0.02122582 -0.00269787]
-6.41529
7.90691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088717 minutes
Epoch 0
Fine tuning took 0.087848 minutes
Epoch 0
Fine tuning took 0.087811 minutes
{'zero': {0: [0.16009852216748768, 0.21798029556650247, 0.28201970443349755, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.72536945812807885, 0.68472906403940892, 0.73891625615763545], 5: [0.086206896551724144, 0.056650246305418719, 0.033251231527093597, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16009852216748768, 0.20320197044334976, 0.19211822660098521, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.69827586206896552, 0.71798029556650245, 0.61945812807881773], 5: [0.086206896551724144, 0.098522167487684734, 0.089901477832512317, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16009852216748768, 0.18719211822660098, 0.20566502463054187, 0.25], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.72290640394088668, 0.68103448275862066, 0.5923645320197044], 5: [0.086206896551724144, 0.089901477832512317, 0.11330049261083744, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16009852216748768, 0.21182266009852216, 0.2105911330049261, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.69088669950738912, 0.68349753694581283, 0.59359605911330049], 5: [0.086206896551724144, 0.097290640394088676, 0.10591133004926108, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234765 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.73521
Epoch 1, cost is  1.66044
Epoch 2, cost is  1.63311
Epoch 3, cost is  1.6202
Epoch 4, cost is  1.61071
Training took 0.160277 minutes
Weight histogram
[3637 2591 2500 2771 2251 1761 1275  744  364  331] [-0.17211185 -0.15492302 -0.13773419 -0.12054536 -0.10335654 -0.08616771
 -0.06897888 -0.05179005 -0.03460122 -0.01741239 -0.00022356]
[ 554  714 1123 1449 1707 1981 2353 2647 2659 3038] [-0.17211185 -0.15492302 -0.13773419 -0.12054536 -0.10335654 -0.08616771
 -0.06897888 -0.05179005 -0.03460122 -0.01741239 -0.00022356]
-5.10674
7.15348
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235742 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.77673
Epoch 1, cost is  1.68928
Epoch 2, cost is  1.66363
Epoch 3, cost is  1.64617
Epoch 4, cost is  1.63567
Training took 0.158733 minutes
Weight histogram
[3496 2544 2804 2666 2021 1775 1896 1528  798  722] [-0.17516589 -0.15767166 -0.14017742 -0.12268319 -0.10518896 -0.08769472
 -0.07020049 -0.05270626 -0.03521202 -0.01771779 -0.00022356]
[1150 1485 1823 1416 1690 1977 2338 2595 2775 3001] [-0.17516589 -0.15767166 -0.14017742 -0.12268319 -0.10518896 -0.08769472
 -0.07020049 -0.05270626 -0.03521202 -0.01771779 -0.00022356]
-4.7838
6.11332
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.59043
Epoch 1, cost is  5.52858
Epoch 2, cost is  4.82771
Epoch 3, cost is  4.49807
Epoch 4, cost is  4.27998
Training took 0.103025 minutes
Weight histogram
[ 603 2647 2313 2194 1498 1468 1193 1123 1341 3845] [-0.10474743 -0.09429817 -0.0838489  -0.07339964 -0.06295038 -0.05250112
 -0.04205185 -0.03160259 -0.02115333 -0.01070407 -0.00025481]
[3366 1216 1157 1313 1492 1621 1801 1983 2117 2159] [-0.10474743 -0.09429817 -0.0838489  -0.07339964 -0.06295038 -0.05250112
 -0.04205185 -0.03160259 -0.02115333 -0.01070407 -0.00025481]
-1.55839
3.39939
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081468 minutes
Epoch 0
Fine tuning took 0.081342 minutes
Epoch 0
Fine tuning took 0.082656 minutes
{'zero': {0: [0.39039408866995073, 0.19334975369458129, 0.24507389162561577, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.29926108374384236, 0.60591133004926112, 0.57635467980295563, 0.65147783251231528], 5: [0.31034482758620691, 0.20073891625615764, 0.17857142857142858, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.39039408866995073, 0.17733990147783252, 0.21428571428571427, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.29926108374384236, 0.5923645320197044, 0.59113300492610843, 0.61822660098522164], 5: [0.31034482758620691, 0.23029556650246305, 0.19458128078817735, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.39039408866995073, 0.18472906403940886, 0.21798029556650247, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.29926108374384236, 0.58866995073891626, 0.61945812807881773, 0.64162561576354682], 5: [0.31034482758620691, 0.22660098522167488, 0.1625615763546798, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.39039408866995073, 0.19088669950738915, 0.18719211822660098, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.29926108374384236, 0.5923645320197044, 0.5923645320197044, 0.6280788177339901], 5: [0.31034482758620691, 0.21674876847290642, 0.22044334975369459, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234165 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.73521
Epoch 1, cost is  1.66044
Epoch 2, cost is  1.63311
Epoch 3, cost is  1.6202
Epoch 4, cost is  1.61071
Training took 0.159562 minutes
Weight histogram
[3637 2591 2500 2771 2251 1761 1275  744  364  331] [-0.17211185 -0.15492302 -0.13773419 -0.12054536 -0.10335654 -0.08616771
 -0.06897888 -0.05179005 -0.03460122 -0.01741239 -0.00022356]
[ 554  714 1123 1449 1707 1981 2353 2647 2659 3038] [-0.17211185 -0.15492302 -0.13773419 -0.12054536 -0.10335654 -0.08616771
 -0.06897888 -0.05179005 -0.03460122 -0.01741239 -0.00022356]
-5.10674
7.15348
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235466 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.77673
Epoch 1, cost is  1.68928
Epoch 2, cost is  1.66363
Epoch 3, cost is  1.64617
Epoch 4, cost is  1.63567
Training took 0.160702 minutes
Weight histogram
[3496 2544 2804 2666 2021 1775 1896 1528  798  722] [-0.17516589 -0.15767166 -0.14017742 -0.12268319 -0.10518896 -0.08769472
 -0.07020049 -0.05270626 -0.03521202 -0.01771779 -0.00022356]
[1150 1485 1823 1416 1690 1977 2338 2595 2775 3001] [-0.17516589 -0.15767166 -0.14017742 -0.12268319 -0.10518896 -0.08769472
 -0.07020049 -0.05270626 -0.03521202 -0.01771779 -0.00022356]
-4.7838
6.11332
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55362
Epoch 1, cost is  5.32215
Epoch 2, cost is  4.36969
Epoch 3, cost is  3.94414
Epoch 4, cost is  3.68485
Training took 0.123121 minutes
Weight histogram
[ 907 2578 2302 2005 1612 1300 1235 1177 1674 3435] [-0.0834422  -0.07512478 -0.06680736 -0.05848995 -0.05017253 -0.04185512
 -0.0335377  -0.02522029 -0.01690287 -0.00858545 -0.00026804]
[3450 1242 1179 1233 1412 1600 1729 1968 2052 2360] [-0.0834422  -0.07512478 -0.06680736 -0.05848995 -0.05017253 -0.04185512
 -0.0335377  -0.02522029 -0.01690287 -0.00858545 -0.00026804]
-1.45535
2.70075
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083872 minutes
Epoch 0
Fine tuning took 0.083987 minutes
Epoch 0
Fine tuning took 0.084696 minutes
{'zero': {0: [0.35221674876847292, 0.25615763546798032, 0.3288177339901478, 0.26970443349753692], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.55295566502463056, 0.54556650246305416, 0.59852216748768472], 5: [0.22044334975369459, 0.19088669950738915, 0.12561576354679804, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.35221674876847292, 0.12931034482758622, 0.27586206896551724, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.64162561576354682, 0.52093596059113301, 0.67241379310344829], 5: [0.22044334975369459, 0.22906403940886699, 0.20320197044334976, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.35221674876847292, 0.17610837438423646, 0.29310344827586204, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.63669950738916259, 0.51231527093596063, 0.65640394088669951], 5: [0.22044334975369459, 0.18719211822660098, 0.19458128078817735, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.35221674876847292, 0.13054187192118227, 0.2857142857142857, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.64532019704433496, 0.54679802955665024, 0.67733990147783252], 5: [0.22044334975369459, 0.22413793103448276, 0.16748768472906403, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235124 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.73521
Epoch 1, cost is  1.66044
Epoch 2, cost is  1.63311
Epoch 3, cost is  1.6202
Epoch 4, cost is  1.61071
Training took 0.162523 minutes
Weight histogram
[3637 2591 2500 2771 2251 1761 1275  744  364  331] [-0.17211185 -0.15492302 -0.13773419 -0.12054536 -0.10335654 -0.08616771
 -0.06897888 -0.05179005 -0.03460122 -0.01741239 -0.00022356]
[ 554  714 1123 1449 1707 1981 2353 2647 2659 3038] [-0.17211185 -0.15492302 -0.13773419 -0.12054536 -0.10335654 -0.08616771
 -0.06897888 -0.05179005 -0.03460122 -0.01741239 -0.00022356]
-5.10674
7.15348
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235147 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.77673
Epoch 1, cost is  1.68928
Epoch 2, cost is  1.66363
Epoch 3, cost is  1.64617
Epoch 4, cost is  1.63567
Training took 0.159084 minutes
Weight histogram
[3496 2544 2804 2666 2021 1775 1896 1528  798  722] [-0.17516589 -0.15767166 -0.14017742 -0.12268319 -0.10518896 -0.08769472
 -0.07020049 -0.05270626 -0.03521202 -0.01771779 -0.00022356]
[1150 1485 1823 1416 1690 1977 2338 2595 2775 3001] [-0.17516589 -0.15767166 -0.14017742 -0.12268319 -0.10518896 -0.08769472
 -0.07020049 -0.05270626 -0.03521202 -0.01771779 -0.00022356]
-4.7838
6.11332
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46964
Epoch 1, cost is  5.07513
Epoch 2, cost is  3.83045
Epoch 3, cost is  3.24232
Epoch 4, cost is  2.8957
Training took 0.180897 minutes
Weight histogram
[ 985 3161 2160 1841 1641 1486 1111 1249 2941 1650] [-0.05459121 -0.04915854 -0.04372587 -0.0382932  -0.03286053 -0.02742787
 -0.0219952  -0.01656253 -0.01112986 -0.00569719 -0.00026452]
[3611 1236 1172 1263 1338 1510 1715 1942 2201 2237] [-0.05459121 -0.04915854 -0.04372587 -0.0382932  -0.03286053 -0.02742787
 -0.0219952  -0.01656253 -0.01112986 -0.00569719 -0.00026452]
-1.11031
2.02951
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089847 minutes
Epoch 0
Fine tuning took 0.088811 minutes
Epoch 0
Fine tuning took 0.089257 minutes
{'zero': {0: [0.23152709359605911, 0.32019704433497537, 0.25985221674876846, 0.31773399014778325], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.45197044334975367, 0.59113300492610843, 0.52955665024630538], 5: [0.15886699507389163, 0.22783251231527094, 0.14901477832512317, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23152709359605911, 0.32142857142857145, 0.22167487684729065, 0.28694581280788178], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.52832512315270941, 0.62561576354679804, 0.5923645320197044], 5: [0.15886699507389163, 0.15024630541871922, 0.15270935960591134, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23152709359605911, 0.3251231527093596, 0.19950738916256158, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.49384236453201968, 0.63300492610837433, 0.59852216748768472], 5: [0.15886699507389163, 0.18103448275862069, 0.16748768472906403, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23152709359605911, 0.3288177339901478, 0.21551724137931033, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.51477832512315269, 0.63177339901477836, 0.55788177339901479], 5: [0.15886699507389163, 0.15640394088669951, 0.15270935960591134, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235989 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.29979
Epoch 1, cost is  2.25281
Epoch 2, cost is  2.22293
Epoch 3, cost is  2.20087
Epoch 4, cost is  2.17632
Training took 0.161159 minutes
Weight histogram
[3344 2687 2196 1923 1734 1364 1163 1223 1152 1439] [ -7.13587999e-02  -6.42148898e-02  -5.70709796e-02  -4.99270695e-02
  -4.27831593e-02  -3.56392491e-02  -2.84953390e-02  -2.13514288e-02
  -1.42075187e-02  -7.06360852e-03   8.03016374e-05]
[2074 1024 1096 1270 1502 1750 2001 2247 2422 2839] [ -7.13587999e-02  -6.42148898e-02  -5.70709796e-02  -4.99270695e-02
  -4.27831593e-02  -3.56392491e-02  -2.84953390e-02  -2.13514288e-02
  -1.42075187e-02  -7.06360852e-03   8.03016374e-05]
-1.01819
1.60215
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234645 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.3777
Epoch 1, cost is  2.33583
Epoch 2, cost is  2.30796
Epoch 3, cost is  2.28228
Epoch 4, cost is  2.25989
Training took 0.158902 minutes
Weight histogram
[3103 2617 2111 1958 1544 1329 1279 1263 1862 3184] [ -6.95405900e-02  -6.25785009e-02  -5.56164117e-02  -4.86543225e-02
  -4.16922334e-02  -3.47301442e-02  -2.77680550e-02  -2.08059659e-02
  -1.38438767e-02  -6.88178753e-03   8.03016374e-05]
[4254 1035 1123 1253 1475 1757 1980 2231 2398 2744] [ -6.95405900e-02  -6.25785009e-02  -5.56164117e-02  -4.86543225e-02
  -4.16922334e-02  -3.47301442e-02  -2.77680550e-02  -2.08059659e-02
  -1.38438767e-02  -6.88178753e-03   8.03016374e-05]
-0.949445
1.49094
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8808
Epoch 1, cost is  6.81124
Epoch 2, cost is  6.74894
Epoch 3, cost is  6.6882
Epoch 4, cost is  6.62743
Training took 0.102664 minutes
Weight histogram
[  92 7901 7448 1007  596  395  284  209  154  139] [ -7.25918217e-03  -6.53581815e-03  -5.81245413e-03  -5.08909011e-03
  -4.36572609e-03  -3.64236208e-03  -2.91899806e-03  -2.19563404e-03
  -1.47227002e-03  -7.48906005e-04  -2.55419873e-05]
[12420  4238   652   252   145   118   107   102   100    91] [ -7.25918217e-03  -6.53581815e-03  -5.81245413e-03  -5.08909011e-03
  -4.36572609e-03  -3.64236208e-03  -2.91899806e-03  -2.19563404e-03
  -1.47227002e-03  -7.48906005e-04  -2.55419873e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080011 minutes
Epoch 0
Fine tuning took 0.082291 minutes
Epoch 0
Fine tuning took 0.081985 minutes
{'zero': {0: [0.049261083743842367, 0.50369458128078815, 0.018472906403940888, 0.0012315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.25738916256157635, 0.85221674876847286, 0.73522167487684731], 5: [0.35221674876847292, 0.23891625615763548, 0.12931034482758622, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.049261083743842367, 0.48029556650246308, 0.032019704433497539, 0.0024630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.23645320197044334, 0.85960591133004927, 0.72906403940886699], 5: [0.35221674876847292, 0.28325123152709358, 0.10837438423645321, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.049261083743842367, 0.52216748768472909, 0.024630541871921183, 0.0036945812807881772], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.26600985221674878, 0.82389162561576357, 0.73029556650246308], 5: [0.35221674876847292, 0.21182266009852216, 0.15147783251231528, 0.26600985221674878], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.049261083743842367, 0.50369458128078815, 0.02832512315270936, 0.0073891625615763543], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.23891625615763548, 0.83620689655172409, 0.70935960591133007], 5: [0.35221674876847292, 0.25738916256157635, 0.1354679802955665, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236778 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.29979
Epoch 1, cost is  2.25281
Epoch 2, cost is  2.22293
Epoch 3, cost is  2.20087
Epoch 4, cost is  2.17632
Training took 0.159975 minutes
Weight histogram
[3344 2687 2196 1923 1734 1364 1163 1223 1152 1439] [ -7.13587999e-02  -6.42148898e-02  -5.70709796e-02  -4.99270695e-02
  -4.27831593e-02  -3.56392491e-02  -2.84953390e-02  -2.13514288e-02
  -1.42075187e-02  -7.06360852e-03   8.03016374e-05]
[2074 1024 1096 1270 1502 1750 2001 2247 2422 2839] [ -7.13587999e-02  -6.42148898e-02  -5.70709796e-02  -4.99270695e-02
  -4.27831593e-02  -3.56392491e-02  -2.84953390e-02  -2.13514288e-02
  -1.42075187e-02  -7.06360852e-03   8.03016374e-05]
-1.01819
1.60215
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234569 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.3777
Epoch 1, cost is  2.33583
Epoch 2, cost is  2.30796
Epoch 3, cost is  2.28228
Epoch 4, cost is  2.25989
Training took 0.160314 minutes
Weight histogram
[3103 2617 2111 1958 1544 1329 1279 1263 1862 3184] [ -6.95405900e-02  -6.25785009e-02  -5.56164117e-02  -4.86543225e-02
  -4.16922334e-02  -3.47301442e-02  -2.77680550e-02  -2.08059659e-02
  -1.38438767e-02  -6.88178753e-03   8.03016374e-05]
[4254 1035 1123 1253 1475 1757 1980 2231 2398 2744] [ -6.95405900e-02  -6.25785009e-02  -5.56164117e-02  -4.86543225e-02
  -4.16922334e-02  -3.47301442e-02  -2.77680550e-02  -2.08059659e-02
  -1.38438767e-02  -6.88178753e-03   8.03016374e-05]
-0.949445
1.49094
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8618
Epoch 1, cost is  6.78508
Epoch 2, cost is  6.72059
Epoch 3, cost is  6.65818
Epoch 4, cost is  6.5943
Training took 0.122389 minutes
Weight histogram
[   93   119   450 14391  1364   700   439   299   204   166] [ -8.75947252e-03  -7.88835787e-03  -7.01724321e-03  -6.14612856e-03
  -5.27501390e-03  -4.40389925e-03  -3.53278460e-03  -2.66166994e-03
  -1.79055529e-03  -9.19440636e-04  -4.83259828e-05]
[11460  4486  1174   363   144   130   123   113   118   114] [ -8.75947252e-03  -7.88835787e-03  -7.01724321e-03  -6.14612856e-03
  -5.27501390e-03  -4.40389925e-03  -3.53278460e-03  -2.66166994e-03
  -1.79055529e-03  -9.19440636e-04  -4.83259828e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081845 minutes
Epoch 0
Fine tuning took 0.082595 minutes
Epoch 0
Fine tuning took 0.083187 minutes
{'zero': {0: [0.044334975369458129, 0.44704433497536944, 0.17733990147783252, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.17733990147783252, 0.49261083743842365, 0.65024630541871919], 5: [0.35098522167487683, 0.37561576354679804, 0.33004926108374383, 0.32266009852216748], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.044334975369458129, 0.46674876847290642, 0.16502463054187191, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.17241379310344829, 0.48768472906403942, 0.67241379310344829], 5: [0.35098522167487683, 0.3608374384236453, 0.34729064039408869, 0.31034482758620691], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.044334975369458129, 0.45320197044334976, 0.17733990147783252, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.19458128078817735, 0.49261083743842365, 0.60837438423645318], 5: [0.35098522167487683, 0.35221674876847292, 0.33004926108374383, 0.37192118226600984], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.044334975369458129, 0.46798029556650245, 0.15886699507389163, 0.01600985221674877], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.1625615763546798, 0.47783251231527096, 0.62438423645320196], 5: [0.35098522167487683, 0.36945812807881773, 0.36330049261083741, 0.35960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237180 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 5819 2542  282] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 144  223  338  598  824  891 1990 2580 5872 4765] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.29979
Epoch 1, cost is  2.25281
Epoch 2, cost is  2.22293
Epoch 3, cost is  2.20087
Epoch 4, cost is  2.17632
Training took 0.159465 minutes
Weight histogram
[3344 2687 2196 1923 1734 1364 1163 1223 1152 1439] [ -7.13587999e-02  -6.42148898e-02  -5.70709796e-02  -4.99270695e-02
  -4.27831593e-02  -3.56392491e-02  -2.84953390e-02  -2.13514288e-02
  -1.42075187e-02  -7.06360852e-03   8.03016374e-05]
[2074 1024 1096 1270 1502 1750 2001 2247 2422 2839] [ -7.13587999e-02  -6.42148898e-02  -5.70709796e-02  -4.99270695e-02
  -4.27831593e-02  -3.56392491e-02  -2.84953390e-02  -2.13514288e-02
  -1.42075187e-02  -7.06360852e-03   8.03016374e-05]
-1.01819
1.60215
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234243 minutes
Weight histogram
[ 203  670 1189 1151 1336 3284 5536 3800 2591  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 305  429  688 1083 1445  835 2016 2708 4718 6023] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.3777
Epoch 1, cost is  2.33583
Epoch 2, cost is  2.30796
Epoch 3, cost is  2.28228
Epoch 4, cost is  2.25989
Training took 0.159218 minutes
Weight histogram
[3103 2617 2111 1958 1544 1329 1279 1263 1862 3184] [ -6.95405900e-02  -6.25785009e-02  -5.56164117e-02  -4.86543225e-02
  -4.16922334e-02  -3.47301442e-02  -2.77680550e-02  -2.08059659e-02
  -1.38438767e-02  -6.88178753e-03   8.03016374e-05]
[4254 1035 1123 1253 1475 1757 1980 2231 2398 2744] [ -6.95405900e-02  -6.25785009e-02  -5.56164117e-02  -4.86543225e-02
  -4.16922334e-02  -3.47301442e-02  -2.77680550e-02  -2.08059659e-02
  -1.38438767e-02  -6.88178753e-03   8.03016374e-05]
-0.949445
1.49094
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80606
Epoch 1, cost is  6.71168
Epoch 2, cost is  6.63944
Epoch 3, cost is  6.57063
Epoch 4, cost is  6.50216
Training took 0.180129 minutes
Weight histogram
[ 162  200  258 8891 6042 1194  633  393  261  191] [ -1.00328391e-02  -9.03324239e-03  -8.03364564e-03  -7.03404889e-03
  -6.03445214e-03  -5.03485539e-03  -4.03525864e-03  -3.03566189e-03
  -2.03606514e-03  -1.03646839e-03  -3.68716392e-05]
[9865 4336 2519  493  280  142  142  142  149  157] [ -1.00328391e-02  -9.03324239e-03  -8.03364564e-03  -7.03404889e-03
  -6.03445214e-03  -5.03485539e-03  -4.03525864e-03  -3.03566189e-03
  -2.03606514e-03  -1.03646839e-03  -3.68716392e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089584 minutes
Epoch 0
Fine tuning took 0.087650 minutes
Epoch 0
Fine tuning took 0.087933 minutes
{'zero': {0: [0.051724137931034482, 0.66009852216748766, 0.086206896551724144, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.21182266009852216, 0.90394088669950734, 0.94211822660098521], 5: [0.32758620689655171, 0.12807881773399016, 0.009852216748768473, 0.034482758620689655], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.051724137931034482, 0.63423645320197042, 0.10960591133004927, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.23399014778325122, 0.87068965517241381, 0.93472906403940892], 5: [0.32758620689655171, 0.13177339901477833, 0.019704433497536946, 0.038177339901477834], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.051724137931034482, 0.66256157635467983, 0.10960591133004927, 0.029556650246305417], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.21921182266009853, 0.87931034482758619, 0.95443349753694584], 5: [0.32758620689655171, 0.11822660098522167, 0.011083743842364532, 0.01600985221674877], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.051724137931034482, 0.6354679802955665, 0.10098522167487685, 0.024630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.23029556650246305, 0.88423645320197042, 0.94704433497536944], 5: [0.32758620689655171, 0.13423645320197045, 0.014778325123152709, 0.02832512315270936], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234707 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.0248
Epoch 1, cost is  5.62168
Epoch 2, cost is  5.50317
Epoch 3, cost is  5.45879
Epoch 4, cost is  5.45034
Training took 0.159881 minutes
Weight histogram
[2209 2744 1983 2077 2104 2228 2218 2300 1999  388] [-0.85410202 -0.76901803 -0.68393404 -0.59885005 -0.51376607 -0.42868208
 -0.34359809 -0.2585141  -0.17343012 -0.08834613 -0.00326214]
[ 638 1610 2075 2072 2225 2292 2178 2278 2412 2470] [-0.85410202 -0.76901803 -0.68393404 -0.59885005 -0.51376607 -0.42868208
 -0.34359809 -0.2585141  -0.17343012 -0.08834613 -0.00326214]
-33.3339
43.8977
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235322 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.03575
Epoch 1, cost is  5.56774
Epoch 2, cost is  5.41807
Epoch 3, cost is  5.34182
Epoch 4, cost is  5.32092
Training took 0.159989 minutes
Weight histogram
[2269 2387 1862 2218 2000 2380 2128 2455 3585  991] [-0.90594512 -0.81567682 -0.72540853 -0.63514023 -0.54487193 -0.45460363
 -0.36433533 -0.27406704 -0.18379874 -0.09353044 -0.00326214]
[1305 2920 2147 2093 2116 2218 2275 2255 2368 2578] [-0.90594512 -0.81567682 -0.72540853 -0.63514023 -0.54487193 -0.45460363
 -0.36433533 -0.27406704 -0.18379874 -0.09353044 -0.00326214]
-26.0041
27.5827
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.64076
Epoch 1, cost is  3.94807
Epoch 2, cost is  4.23411
Epoch 3, cost is  4.67467
Epoch 4, cost is  5.12788
Training took 0.104597 minutes
Weight histogram
[1848 3552 3423 3706 3048 1671 1215  755  404  628] [-0.31483021 -0.28362204 -0.25241386 -0.22120569 -0.18999751 -0.15878934
 -0.12758117 -0.09637299 -0.06516482 -0.03395664 -0.00274847]
[1007 1028 1365 1822 2173 2478 2482 2616 2794 2485] [-0.31483021 -0.28362204 -0.25241386 -0.22120569 -0.18999751 -0.15878934
 -0.12758117 -0.09637299 -0.06516482 -0.03395664 -0.00274847]
-13.1132
14.5457
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081276 minutes
Epoch 0
Fine tuning took 0.080416 minutes
Epoch 0
Fine tuning took 0.081602 minutes
{'zero': {0: [0.44088669950738918, 0.24630541871921183, 0.14655172413793102, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.6711822660098522, 0.71182266009852213, 0.71059113300492616], 5: [0.10344827586206896, 0.082512315270935957, 0.14162561576354679, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.44088669950738918, 0.26108374384236455, 0.24507389162561577, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.6428571428571429, 0.66502463054187189, 0.67610837438423643], 5: [0.10344827586206896, 0.096059113300492605, 0.089901477832512317, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.44088669950738918, 0.25738916256157635, 0.22413793103448276, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.65147783251231528, 0.67241379310344829, 0.66379310344827591], 5: [0.10344827586206896, 0.091133004926108374, 0.10344827586206896, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.44088669950738918, 0.25738916256157635, 0.2413793103448276, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.66625615763546797, 0.68842364532019706, 0.66995073891625612], 5: [0.10344827586206896, 0.076354679802955669, 0.070197044334975367, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235419 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.0248
Epoch 1, cost is  5.62168
Epoch 2, cost is  5.50317
Epoch 3, cost is  5.45879
Epoch 4, cost is  5.45034
Training took 0.161991 minutes
Weight histogram
[2209 2744 1983 2077 2104 2228 2218 2300 1999  388] [-0.85410202 -0.76901803 -0.68393404 -0.59885005 -0.51376607 -0.42868208
 -0.34359809 -0.2585141  -0.17343012 -0.08834613 -0.00326214]
[ 638 1610 2075 2072 2225 2292 2178 2278 2412 2470] [-0.85410202 -0.76901803 -0.68393404 -0.59885005 -0.51376607 -0.42868208
 -0.34359809 -0.2585141  -0.17343012 -0.08834613 -0.00326214]
-33.3339
43.8977
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234703 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.03575
Epoch 1, cost is  5.56774
Epoch 2, cost is  5.41807
Epoch 3, cost is  5.34182
Epoch 4, cost is  5.32092
Training took 0.160853 minutes
Weight histogram
[2269 2387 1862 2218 2000 2380 2128 2455 3585  991] [-0.90594512 -0.81567682 -0.72540853 -0.63514023 -0.54487193 -0.45460363
 -0.36433533 -0.27406704 -0.18379874 -0.09353044 -0.00326214]
[1305 2920 2147 2093 2116 2218 2275 2255 2368 2578] [-0.90594512 -0.81567682 -0.72540853 -0.63514023 -0.54487193 -0.45460363
 -0.36433533 -0.27406704 -0.18379874 -0.09353044 -0.00326214]
-26.0041
27.5827
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.2246
Epoch 1, cost is  3.32322
Epoch 2, cost is  3.4703
Epoch 3, cost is  3.80206
Epoch 4, cost is  4.16122
Training took 0.125509 minutes
Weight histogram
[1574 3301 3176 3205 2849 2424 1659  901  425  736] [-0.27973735 -0.25203353 -0.2243297  -0.19662588 -0.16892205 -0.14121822
 -0.1135144  -0.08581057 -0.05810675 -0.03040292 -0.00269909]
[1013 1064 1532 1824 2162 2354 2574 2615 2563 2549] [-0.27973735 -0.25203353 -0.2243297  -0.19662588 -0.16892205 -0.14121822
 -0.1135144  -0.08581057 -0.05810675 -0.03040292 -0.00269909]
-9.51929
11.2219
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082170 minutes
Epoch 0
Fine tuning took 0.082855 minutes
Epoch 0
Fine tuning took 0.081680 minutes
{'zero': {0: [0.20812807881773399, 0.23029556650246305, 0.20566502463054187, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.52339901477832518, 0.58990147783251234, 0.3854679802955665], 5: [0.043103448275862072, 0.24630541871921183, 0.20443349753694581, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20812807881773399, 0.2229064039408867, 0.2105911330049261, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.70073891625615758, 0.69704433497536944, 0.68965517241379315], 5: [0.043103448275862072, 0.076354679802955669, 0.092364532019704432, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20812807881773399, 0.24507389162561577, 0.19458128078817735, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.66009852216748766, 0.70812807881773399, 0.71059113300492616], 5: [0.043103448275862072, 0.094827586206896547, 0.097290640394088676, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20812807881773399, 0.25492610837438423, 0.24014778325123154, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.68719211822660098, 0.68472906403940892, 0.72167487684729059], 5: [0.043103448275862072, 0.057881773399014777, 0.075123152709359611, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234770 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.0248
Epoch 1, cost is  5.62168
Epoch 2, cost is  5.50317
Epoch 3, cost is  5.45879
Epoch 4, cost is  5.45034
Training took 0.161410 minutes
Weight histogram
[2209 2744 1983 2077 2104 2228 2218 2300 1999  388] [-0.85410202 -0.76901803 -0.68393404 -0.59885005 -0.51376607 -0.42868208
 -0.34359809 -0.2585141  -0.17343012 -0.08834613 -0.00326214]
[ 638 1610 2075 2072 2225 2292 2178 2278 2412 2470] [-0.85410202 -0.76901803 -0.68393404 -0.59885005 -0.51376607 -0.42868208
 -0.34359809 -0.2585141  -0.17343012 -0.08834613 -0.00326214]
-33.3339
43.8977
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235065 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.03575
Epoch 1, cost is  5.56774
Epoch 2, cost is  5.41807
Epoch 3, cost is  5.34182
Epoch 4, cost is  5.32092
Training took 0.160747 minutes
Weight histogram
[2269 2387 1862 2218 2000 2380 2128 2455 3585  991] [-0.90594512 -0.81567682 -0.72540853 -0.63514023 -0.54487193 -0.45460363
 -0.36433533 -0.27406704 -0.18379874 -0.09353044 -0.00326214]
[1305 2920 2147 2093 2116 2218 2275 2255 2368 2578] [-0.90594512 -0.81567682 -0.72540853 -0.63514023 -0.54487193 -0.45460363
 -0.36433533 -0.27406704 -0.18379874 -0.09353044 -0.00326214]
-26.0041
27.5827
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.73803
Epoch 1, cost is  2.41289
Epoch 2, cost is  2.31904
Epoch 3, cost is  2.40742
Epoch 4, cost is  2.56309
Training took 0.181837 minutes
Weight histogram
[2436 3343 3039 2931 2772 2113 1527  804  552  733] [-0.18797739 -0.16944944 -0.15092148 -0.13239353 -0.11386558 -0.09533763
 -0.07680968 -0.05828172 -0.03975377 -0.02122582 -0.00269787]
[1100 1048 1527 1900 2141 2361 2496 2567 2755 2355] [-0.18797739 -0.16944944 -0.15092148 -0.13239353 -0.11386558 -0.09533763
 -0.07680968 -0.05828172 -0.03975377 -0.02122582 -0.00269787]
-6.41529
7.90691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088206 minutes
Epoch 0
Fine tuning took 0.088586 minutes
Epoch 0
Fine tuning took 0.087081 minutes
{'zero': {0: [0.18349753694581281, 0.44950738916256155, 0.29556650246305421, 0.68596059113300489], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.53078817733990147, 0.69827586206896552, 0.30788177339901479], 5: [0.075123152709359611, 0.019704433497536946, 0.0061576354679802959, 0.0061576354679802959], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18349753694581281, 0.26970443349753692, 0.23645320197044334, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.63054187192118227, 0.66133004926108374, 0.59975369458128081], 5: [0.075123152709359611, 0.099753694581280791, 0.10221674876847291, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18349753694581281, 0.24630541871921183, 0.21674876847290642, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.66748768472906406, 0.66502463054187189, 0.66625615763546797], 5: [0.075123152709359611, 0.086206896551724144, 0.11822660098522167, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18349753694581281, 0.33251231527093594, 0.24753694581280788, 0.33128078817733991], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.55295566502463056, 0.63054187192118227, 0.51600985221674878], 5: [0.075123152709359611, 0.1145320197044335, 0.12192118226600986, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235241 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.76441
Epoch 1, cost is  1.6847
Epoch 2, cost is  1.66017
Epoch 3, cost is  1.64561
Epoch 4, cost is  1.6391
Training took 0.159498 minutes
Weight histogram
[3977 2861 3104 2453 2701 1904 1617  868  411  354] [-0.18271747 -0.16446808 -0.14621869 -0.1279693  -0.10971991 -0.09147051
 -0.07322112 -0.05497173 -0.03672234 -0.01847295 -0.00022356]
[ 587  792 1265 1622 1866 2247 2672 2760 3025 3414] [-0.18271747 -0.16446808 -0.14621869 -0.1279693  -0.10971991 -0.09147051
 -0.07322112 -0.05497173 -0.03672234 -0.01847295 -0.00022356]
-5.58413
8.14333
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235698 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84887
Epoch 1, cost is  1.76201
Epoch 2, cost is  1.731
Epoch 3, cost is  1.71278
Epoch 4, cost is  1.701
Training took 0.160404 minutes
Weight histogram
[3715 2933 2985 2652 2350 2128 1908 1949  884  771] [-0.1878697  -0.16910508 -0.15034047 -0.13157586 -0.11281124 -0.09404663
 -0.07528201 -0.0565174  -0.03775279 -0.01898817 -0.00022356]
[1219 1651 1830 1589 1827 2260 2594 2883 2989 3433] [-0.1878697  -0.16910508 -0.15034047 -0.13157586 -0.11281124 -0.09404663
 -0.07528201 -0.0565174  -0.03775279 -0.01898817 -0.00022356]
-5.1577
6.61173
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.588
Epoch 1, cost is  5.5211
Epoch 2, cost is  4.82522
Epoch 3, cost is  4.49158
Epoch 4, cost is  4.26727
Training took 0.103010 minutes
Weight histogram
[ 603 2943 2582 2476 1665 1648 1335 1248 1517 4233] [-0.10474743 -0.09429806 -0.08384868 -0.07339931 -0.06294994 -0.05250057
 -0.0420512  -0.03160182 -0.02115245 -0.01070308 -0.00025371]
[3712 1357 1282 1464 1660 1802 2005 2205 2350 2413] [-0.10474743 -0.09429806 -0.08384868 -0.07339931 -0.06294994 -0.05250057
 -0.0420512  -0.03160182 -0.02115245 -0.01070308 -0.00025371]
-1.55839
3.39939
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083316 minutes
Epoch 0
Fine tuning took 0.083225 minutes
Epoch 0
Fine tuning took 0.081803 minutes
{'zero': {0: [0.52093596059113301, 0.27586206896551724, 0.25123152709359609, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35714285714285715, 0.54064039408866993, 0.61945812807881773, 0.66625615763546797], 5: [0.12192118226600986, 0.18349753694581281, 0.12931034482758622, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.52093596059113301, 0.24630541871921183, 0.2376847290640394, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35714285714285715, 0.56403940886699511, 0.62315270935960587, 0.61206896551724133], 5: [0.12192118226600986, 0.18965517241379309, 0.13916256157635468, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.52093596059113301, 0.26724137931034481, 0.24261083743842365, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35714285714285715, 0.53694581280788178, 0.62438423645320196, 0.65517241379310343], 5: [0.12192118226600986, 0.19581280788177341, 0.13300492610837439, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.52093596059113301, 0.23275862068965517, 0.23645320197044334, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.35714285714285715, 0.56896551724137934, 0.61699507389162567, 0.65517241379310343], 5: [0.12192118226600986, 0.19827586206896552, 0.14655172413793102, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234561 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.76441
Epoch 1, cost is  1.6847
Epoch 2, cost is  1.66017
Epoch 3, cost is  1.64561
Epoch 4, cost is  1.6391
Training took 0.161438 minutes
Weight histogram
[3977 2861 3104 2453 2701 1904 1617  868  411  354] [-0.18271747 -0.16446808 -0.14621869 -0.1279693  -0.10971991 -0.09147051
 -0.07322112 -0.05497173 -0.03672234 -0.01847295 -0.00022356]
[ 587  792 1265 1622 1866 2247 2672 2760 3025 3414] [-0.18271747 -0.16446808 -0.14621869 -0.1279693  -0.10971991 -0.09147051
 -0.07322112 -0.05497173 -0.03672234 -0.01847295 -0.00022356]
-5.58413
8.14333
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234309 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84887
Epoch 1, cost is  1.76201
Epoch 2, cost is  1.731
Epoch 3, cost is  1.71278
Epoch 4, cost is  1.701
Training took 0.160326 minutes
Weight histogram
[3715 2933 2985 2652 2350 2128 1908 1949  884  771] [-0.1878697  -0.16910508 -0.15034047 -0.13157586 -0.11281124 -0.09404663
 -0.07528201 -0.0565174  -0.03775279 -0.01898817 -0.00022356]
[1219 1651 1830 1589 1827 2260 2594 2883 2989 3433] [-0.1878697  -0.16910508 -0.15034047 -0.13157586 -0.11281124 -0.09404663
 -0.07528201 -0.0565174  -0.03775279 -0.01898817 -0.00022356]
-5.1577
6.61173
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55027
Epoch 1, cost is  5.30942
Epoch 2, cost is  4.36863
Epoch 3, cost is  3.94447
Epoch 4, cost is  3.69159
Training took 0.123701 minutes
Weight histogram
[ 936 2882 2580 2250 1812 1439 1383 1309 1858 3801] [-0.0834422  -0.07512467 -0.06680715 -0.05848963 -0.0501721  -0.04185458
 -0.03353706 -0.02521953 -0.01690201 -0.00858449 -0.00026696]
[3810 1383 1309 1375 1567 1783 1922 2194 2282 2625] [-0.0834422  -0.07512467 -0.06680715 -0.05848963 -0.0501721  -0.04185458
 -0.03353706 -0.02521953 -0.01690201 -0.00858449 -0.00026696]
-1.45535
2.70075
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084358 minutes
Epoch 0
Fine tuning took 0.084862 minutes
Epoch 0
Fine tuning took 0.083479 minutes
{'zero': {0: [0.3854679802955665, 0.27955665024630544, 0.24384236453201971, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.61699507389162567, 0.66995073891625612, 0.73275862068965514], 5: [0.12931034482758622, 0.10344827586206896, 0.086206896551724144, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3854679802955665, 0.15024630541871922, 0.12438423645320197, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.66133004926108374, 0.7426108374384236, 0.74753694581280783], 5: [0.12931034482758622, 0.18842364532019704, 0.13300492610837439, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3854679802955665, 0.17241379310344829, 0.14408866995073891, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.65886699507389157, 0.72290640394088668, 0.75123152709359609], 5: [0.12931034482758622, 0.16871921182266009, 0.13300492610837439, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3854679802955665, 0.13177339901477833, 0.094827586206896547, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.67733990147783252, 0.76847290640394084, 0.77339901477832518], 5: [0.12931034482758622, 0.19088669950738915, 0.13669950738916256, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235394 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.76441
Epoch 1, cost is  1.6847
Epoch 2, cost is  1.66017
Epoch 3, cost is  1.64561
Epoch 4, cost is  1.6391
Training took 0.159907 minutes
Weight histogram
[3977 2861 3104 2453 2701 1904 1617  868  411  354] [-0.18271747 -0.16446808 -0.14621869 -0.1279693  -0.10971991 -0.09147051
 -0.07322112 -0.05497173 -0.03672234 -0.01847295 -0.00022356]
[ 587  792 1265 1622 1866 2247 2672 2760 3025 3414] [-0.18271747 -0.16446808 -0.14621869 -0.1279693  -0.10971991 -0.09147051
 -0.07322112 -0.05497173 -0.03672234 -0.01847295 -0.00022356]
-5.58413
8.14333
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235754 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.84887
Epoch 1, cost is  1.76201
Epoch 2, cost is  1.731
Epoch 3, cost is  1.71278
Epoch 4, cost is  1.701
Training took 0.158634 minutes
Weight histogram
[3715 2933 2985 2652 2350 2128 1908 1949  884  771] [-0.1878697  -0.16910508 -0.15034047 -0.13157586 -0.11281124 -0.09404663
 -0.07528201 -0.0565174  -0.03775279 -0.01898817 -0.00022356]
[1219 1651 1830 1589 1827 2260 2594 2883 2989 3433] [-0.1878697  -0.16910508 -0.15034047 -0.13157586 -0.11281124 -0.09404663
 -0.07528201 -0.0565174  -0.03775279 -0.01898817 -0.00022356]
-5.1577
6.61173
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.4666
Epoch 1, cost is  5.06458
Epoch 2, cost is  3.83426
Epoch 3, cost is  3.24916
Epoch 4, cost is  2.90189
Training took 0.181340 minutes
Weight histogram
[1009 3567 2396 2065 1831 1660 1241 1391 3217 1873] [-0.05459121 -0.04915823 -0.04372525 -0.03829227 -0.03285928 -0.0274263
 -0.02199332 -0.01656034 -0.01112735 -0.00569437 -0.00026139]
[3992 1377 1306 1406 1489 1683 1910 2162 2449 2476] [-0.05459121 -0.04915823 -0.04372525 -0.03829227 -0.03285928 -0.0274263
 -0.02199332 -0.01656034 -0.01112735 -0.00569437 -0.00026139]
-1.13726
2.02951
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089262 minutes
Epoch 0
Fine tuning took 0.090318 minutes
Epoch 0
Fine tuning took 0.088546 minutes
{'zero': {0: [0.33743842364532017, 0.39778325123152708, 0.25985221674876846, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.51231527093596063, 0.61822660098522164, 0.51231527093596063], 5: [0.10714285714285714, 0.089901477832512317, 0.12192118226600986, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33743842364532017, 0.37807881773399016, 0.21921182266009853, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.53448275862068961, 0.67733990147783252, 0.56773399014778325], 5: [0.10714285714285714, 0.087438423645320201, 0.10344827586206896, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33743842364532017, 0.36330049261083741, 0.20935960591133004, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.54433497536945807, 0.67364532019704437, 0.55788177339901479], 5: [0.10714285714285714, 0.092364532019704432, 0.11699507389162561, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33743842364532017, 0.40024630541871919, 0.24507389162561577, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.53325123152709364, 0.6576354679802956, 0.58866995073891626], 5: [0.10714285714285714, 0.066502463054187194, 0.097290640394088676, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235248 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.15585
Epoch 1, cost is  2.12205
Epoch 2, cost is  2.09635
Epoch 3, cost is  2.07922
Epoch 4, cost is  2.05995
Training took 0.160538 minutes
Weight histogram
[3827 3061 2550 2270 1784 1472 1261 1280 1248 1497] [ -7.55022988e-02  -6.79440387e-02  -6.03857787e-02  -5.28275186e-02
  -4.52692586e-02  -3.77109986e-02  -3.01527385e-02  -2.25944785e-02
  -1.50362184e-02  -7.47795840e-03   8.03016374e-05]
[2142 1089 1190 1400 1662 1981 2230 2473 2843 3240] [ -7.55022988e-02  -6.79440387e-02  -6.03857787e-02  -5.28275186e-02
  -4.52692586e-02  -3.77109986e-02  -3.01527385e-02  -2.25944785e-02
  -1.50362184e-02  -7.47795840e-03   8.03016374e-05]
-1.0809
1.68681
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234738 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.26821
Epoch 1, cost is  2.22579
Epoch 2, cost is  2.19993
Epoch 3, cost is  2.17722
Epoch 4, cost is  2.15299
Training took 0.161442 minutes
Weight histogram
[3758 2919 2346 2209 1675 1425 1433 1320 1859 3331] [ -7.35811442e-02  -6.62149996e-02  -5.88488550e-02  -5.14827105e-02
  -4.41165659e-02  -3.67504213e-02  -2.93842767e-02  -2.20181321e-02
  -1.46519875e-02  -7.28584295e-03   8.03016374e-05]
[4330 1103 1220 1380 1656 1999 2187 2529 2778 3093] [ -7.35811442e-02  -6.62149996e-02  -5.88488550e-02  -5.14827105e-02
  -4.41165659e-02  -3.67504213e-02  -2.93842767e-02  -2.20181321e-02
  -1.46519875e-02  -7.28584295e-03   8.03016374e-05]
-1.00797
1.62504
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.88111
Epoch 1, cost is  6.81208
Epoch 2, cost is  6.7504
Epoch 3, cost is  6.6899
Epoch 4, cost is  6.62964
Training took 0.106401 minutes
Weight histogram
[  92 7909 9120 1140  668  443  317  234  172  155] [ -7.25918217e-03  -6.53574905e-03  -5.81231593e-03  -5.08888281e-03
  -4.36544970e-03  -3.64201658e-03  -2.91858346e-03  -2.19515035e-03
  -1.47171723e-03  -7.48284112e-04  -2.48509950e-05]
[13858  4791   686   252   145   118   107   102   100    91] [ -7.25918217e-03  -6.53574905e-03  -5.81231593e-03  -5.08888281e-03
  -4.36544970e-03  -3.64201658e-03  -2.91858346e-03  -2.19515035e-03
  -1.47171723e-03  -7.48284112e-04  -2.48509950e-05]
-0.0843226
0.157114
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081733 minutes
Epoch 0
Fine tuning took 0.081516 minutes
Epoch 0
Fine tuning took 0.081871 minutes
{'zero': {0: [0.19704433497536947, 0.077586206896551727, 0.012315270935960592, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.13916256157635468, 0.90147783251231528, 0.87068965517241381, 0.96798029556650245], 5: [0.66379310344827591, 0.020935960591133004, 0.11699507389162561, 0.0061576354679802959], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19704433497536947, 0.099753694581280791, 0.0086206896551724137, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.13916256157635468, 0.8719211822660099, 0.84729064039408863, 0.96798029556650245], 5: [0.66379310344827591, 0.02832512315270936, 0.14408866995073891, 0.0049261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19704433497536947, 0.089901477832512317, 0.013546798029556651, 0.024630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.13916256157635468, 0.88669950738916259, 0.85098522167487689, 0.96551724137931039], 5: [0.66379310344827591, 0.023399014778325122, 0.1354679802955665, 0.009852216748768473], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19704433497536947, 0.094827586206896547, 0.018472906403940888, 0.034482758620689655], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.13916256157635468, 0.88300492610837433, 0.82512315270935965, 0.96059113300492616], 5: [0.66379310344827591, 0.022167487684729065, 0.15640394088669951, 0.0049261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235414 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.15585
Epoch 1, cost is  2.12205
Epoch 2, cost is  2.09635
Epoch 3, cost is  2.07922
Epoch 4, cost is  2.05995
Training took 0.160705 minutes
Weight histogram
[3827 3061 2550 2270 1784 1472 1261 1280 1248 1497] [ -7.55022988e-02  -6.79440387e-02  -6.03857787e-02  -5.28275186e-02
  -4.52692586e-02  -3.77109986e-02  -3.01527385e-02  -2.25944785e-02
  -1.50362184e-02  -7.47795840e-03   8.03016374e-05]
[2142 1089 1190 1400 1662 1981 2230 2473 2843 3240] [ -7.55022988e-02  -6.79440387e-02  -6.03857787e-02  -5.28275186e-02
  -4.52692586e-02  -3.77109986e-02  -3.01527385e-02  -2.25944785e-02
  -1.50362184e-02  -7.47795840e-03   8.03016374e-05]
-1.0809
1.68681
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234563 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.26821
Epoch 1, cost is  2.22579
Epoch 2, cost is  2.19993
Epoch 3, cost is  2.17722
Epoch 4, cost is  2.15299
Training took 0.160170 minutes
Weight histogram
[3758 2919 2346 2209 1675 1425 1433 1320 1859 3331] [ -7.35811442e-02  -6.62149996e-02  -5.88488550e-02  -5.14827105e-02
  -4.41165659e-02  -3.67504213e-02  -2.93842767e-02  -2.20181321e-02
  -1.46519875e-02  -7.28584295e-03   8.03016374e-05]
[4330 1103 1220 1380 1656 1999 2187 2529 2778 3093] [ -7.35811442e-02  -6.62149996e-02  -5.88488550e-02  -5.14827105e-02
  -4.41165659e-02  -3.67504213e-02  -2.93842767e-02  -2.20181321e-02
  -1.46519875e-02  -7.28584295e-03   8.03016374e-05]
-1.00797
1.62504
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.86237
Epoch 1, cost is  6.78643
Epoch 2, cost is  6.72224
Epoch 3, cost is  6.66024
Epoch 4, cost is  6.59693
Training took 0.123109 minutes
Weight histogram
[   93   119   450 16020  1544   785   491   335   228   185] [ -8.75947252e-03  -7.88831318e-03  -7.01715384e-03  -6.14599451e-03
  -5.27483517e-03  -4.40367583e-03  -3.53251649e-03  -2.66135716e-03
  -1.79019782e-03  -9.19038482e-04  -4.78791444e-05]
[12787  5017  1341   363   144   130   123   113   118   114] [ -8.75947252e-03  -7.88831318e-03  -7.01715384e-03  -6.14599451e-03
  -5.27483517e-03  -4.40367583e-03  -3.53251649e-03  -2.66135716e-03
  -1.79019782e-03  -9.19038482e-04  -4.78791444e-05]
-0.0803888
0.161167
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082092 minutes
Epoch 0
Fine tuning took 0.083091 minutes
Epoch 0
Fine tuning took 0.082261 minutes
{'zero': {0: [0.20073891625615764, 0.25615763546798032, 0.01600985221674877, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.1625615763546798, 0.63177339901477836, 0.94211822660098521, 0.85221674876847286], 5: [0.63669950738916259, 0.11206896551724138, 0.041871921182266007, 0.017241379310344827], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20073891625615764, 0.2894088669950739, 0.01600985221674877, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.1625615763546798, 0.59482758620689657, 0.95566502463054193, 0.79926108374384242], 5: [0.63669950738916259, 0.11576354679802955, 0.02832512315270936, 0.022167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20073891625615764, 0.23891625615763548, 0.022167487684729065, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.1625615763546798, 0.63423645320197042, 0.93103448275862066, 0.83743842364532017], 5: [0.63669950738916259, 0.1268472906403941, 0.046798029556650245, 0.022167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20073891625615764, 0.27463054187192121, 0.012315270935960592, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.1625615763546798, 0.6219211822660099, 0.94704433497536944, 0.8423645320197044], 5: [0.63669950738916259, 0.10344827586206896, 0.04064039408866995, 0.014778325123152709], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234968 minutes
Weight histogram
[ 167  510  693  528 1345 2861 3478 6222 4004  442] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
[ 146  231  362  596  851  949 2129 3155 5979 5852] [ -1.55507252e-04   1.93888889e-05   1.94285030e-04   3.69181171e-04
   5.44077312e-04   7.18973453e-04   8.93869594e-04   1.06876574e-03
   1.24366188e-03   1.41855802e-03   1.59345416e-03]
-1.20288
1.10943
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.15585
Epoch 1, cost is  2.12205
Epoch 2, cost is  2.09635
Epoch 3, cost is  2.07922
Epoch 4, cost is  2.05995
Training took 0.160374 minutes
Weight histogram
[3827 3061 2550 2270 1784 1472 1261 1280 1248 1497] [ -7.55022988e-02  -6.79440387e-02  -6.03857787e-02  -5.28275186e-02
  -4.52692586e-02  -3.77109986e-02  -3.01527385e-02  -2.25944785e-02
  -1.50362184e-02  -7.47795840e-03   8.03016374e-05]
[2142 1089 1190 1400 1662 1981 2230 2473 2843 3240] [ -7.55022988e-02  -6.79440387e-02  -6.03857787e-02  -5.28275186e-02
  -4.52692586e-02  -3.77109986e-02  -3.01527385e-02  -2.25944785e-02
  -1.50362184e-02  -7.47795840e-03   8.03016374e-05]
-1.0809
1.68681
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235701 minutes
Weight histogram
[ 203  670 1189 1151 1368 3909 6451 4238 2606  490] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
[ 309  446  687 1092 1443  848 2157 2701 5251 7341] [ -1.55507252e-04  -6.71196176e-06   1.42083329e-04   2.90878619e-04
   4.39673910e-04   5.88469200e-04   7.37264490e-04   8.86059781e-04
   1.03485507e-03   1.18365036e-03   1.33244565e-03]
-1.59647
0.888362
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.26821
Epoch 1, cost is  2.22579
Epoch 2, cost is  2.19993
Epoch 3, cost is  2.17722
Epoch 4, cost is  2.15299
Training took 0.159971 minutes
Weight histogram
[3758 2919 2346 2209 1675 1425 1433 1320 1859 3331] [ -7.35811442e-02  -6.62149996e-02  -5.88488550e-02  -5.14827105e-02
  -4.41165659e-02  -3.67504213e-02  -2.93842767e-02  -2.20181321e-02
  -1.46519875e-02  -7.28584295e-03   8.03016374e-05]
[4330 1103 1220 1380 1656 1999 2187 2529 2778 3093] [ -7.35811442e-02  -6.62149996e-02  -5.88488550e-02  -5.14827105e-02
  -4.41165659e-02  -3.67504213e-02  -2.93842767e-02  -2.20181321e-02
  -1.46519875e-02  -7.28584295e-03   8.03016374e-05]
-1.00797
1.62504
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN7/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80692
Epoch 1, cost is  6.71308
Epoch 2, cost is  6.64135
Epoch 3, cost is  6.57336
Epoch 4, cost is  6.50507
Training took 0.181544 minutes
Weight histogram
[ 162  200  258 8936 7693 1347  710  439  292  213] [ -1.00328391e-02  -9.03317603e-03  -8.03351291e-03  -7.03384979e-03
  -6.03418668e-03  -5.03452356e-03  -4.03486045e-03  -3.03519733e-03
  -2.03553422e-03  -1.03587110e-03  -3.62079845e-05]
[11009  4848  2888   493   280   142   142   142   149   157] [ -1.00328391e-02  -9.03317603e-03  -8.03351291e-03  -7.03384979e-03
  -6.03418668e-03  -5.03452356e-03  -4.03486045e-03  -3.03519733e-03
  -2.03553422e-03  -1.03587110e-03  -3.62079845e-05]
-0.0692447
0.112672
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087985 minutes
Epoch 0
Fine tuning took 0.089036 minutes
Epoch 0
Fine tuning took 0.088332 minutes
{'zero': {0: [0.19334975369458129, 0.12561576354679804, 0.0061576354679802959, 0.04064039408866995], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.17364532019704434, 0.79679802955665024, 0.8854679802955665, 0.95812807881773399], 5: [0.63300492610837433, 0.077586206896551727, 0.10837438423645321, 0.0012315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19334975369458129, 0.10960591133004927, 0.0012315270935960591, 0.035714285714285712], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.17364532019704434, 0.80541871921182262, 0.8780788177339901, 0.96182266009852213], 5: [0.63300492610837433, 0.084975369458128072, 0.1206896551724138, 0.0024630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19334975369458129, 0.10098522167487685, 0.0061576354679802959, 0.035714285714285712], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.17364532019704434, 0.80788177339901479, 0.86330049261083741, 0.96182266009852213], 5: [0.63300492610837433, 0.091133004926108374, 0.13054187192118227, 0.0024630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19334975369458129, 0.11206896551724138, 0.0036945812807881772, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.17364532019704434, 0.80418719211822665, 0.86206896551724133, 0.97167487684729059], 5: [0.63300492610837433, 0.083743842364532015, 0.13423645320197045, 0.0012315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}}
