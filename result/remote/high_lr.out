Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentADBN3
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184864 minutes
Weight histogram
[ 41 118 175 255 359 380 371 194 107  25] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
[ 82  80  89 114 139 158 208 269 362 524] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.2128
Epoch 1, cost is  1.9837
Epoch 2, cost is  1.91421
Epoch 3, cost is  1.96077
Epoch 4, cost is  2.062
Training took 0.127484 minutes
Weight histogram
[379 404 330 270 219 190 113  53  39  28] [-0.14346196 -0.12934407 -0.11522619 -0.1011083  -0.08699042 -0.07287254
 -0.05875465 -0.04463677 -0.03051888 -0.016401   -0.00228311]
[ 71  93 125 166 190 218 255 278 306 323] [-0.14346196 -0.12934407 -0.11522619 -0.1011083  -0.08699042 -0.07287254
 -0.05875465 -0.04463677 -0.03051888 -0.016401   -0.00228311]
-4.67161
5.54493
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.186180 minutes
Weight histogram
[ 64 225 471 669 920 738 558 258 122  25] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
[168 164 186 235 291 312 480 556 828 830] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.99399
Epoch 1, cost is  3.17997
Epoch 2, cost is  3.38858
Epoch 3, cost is  3.73678
Epoch 4, cost is  4.1774
Training took 0.097132 minutes
Weight histogram
[308 292 269 306 805 822 609 368 162 109] [-0.2442887  -0.22006783 -0.19584697 -0.17162611 -0.14740524 -0.12318438
 -0.09896351 -0.07474265 -0.05052179 -0.02630092 -0.00208006]
[242 370 547 672 786 420 243 249 251 270] [-0.2442887  -0.22006783 -0.19584697 -0.17162611 -0.14740524 -0.12318438
 -0.09896351 -0.07474265 -0.05052179 -0.02630092 -0.00208006]
-6.8542
7.10174
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  4.853
Epoch 1, cost is  4.18982
Epoch 2, cost is  4.49809
Epoch 3, cost is  4.94169
Epoch 4, cost is  5.39047
Training took 0.093246 minutes
Weight histogram
[325 333 306 327 288 166 120  59  52  49] [-0.34673315 -0.31230815 -0.27788314 -0.24345814 -0.20903313 -0.17460812
 -0.14018312 -0.10575811 -0.07133311 -0.0369081  -0.00248309]
[ 99 110 137 189 210 244 233 245 268 290] [-0.34673315 -0.31230815 -0.27788314 -0.24345814 -0.20903313 -0.17460812
 -0.14018312 -0.10575811 -0.07133311 -0.0369081  -0.00248309]
-12.8856
19.2011
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047722 minutes
Epoch 0
Fine tuning took 0.048332 minutes
Epoch 0
Fine tuning took 0.047210 minutes
{'zero': {0: [0.2536945812807882, 0.2019704433497537, 0.18596059113300492, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.70935960591133007, 0.67610837438423643, 0.6785714285714286], 5: [0.14285714285714285, 0.088669950738916259, 0.13793103448275862, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2536945812807882, 0.19334975369458129, 0.21428571428571427, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.74014778325123154, 0.71059113300492616, 0.7068965517241379], 5: [0.14285714285714285, 0.066502463054187194, 0.075123152709359611, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2536945812807882, 0.17733990147783252, 0.19827586206896552, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.74384236453201968, 0.71921182266009853, 0.72906403940886699], 5: [0.14285714285714285, 0.078817733990147784, 0.082512315270935957, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2536945812807882, 0.18719211822660098, 0.18472906403940886, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60344827586206895, 0.73399014778325122, 0.71551724137931039, 0.72044334975369462], 5: [0.14285714285714285, 0.078817733990147784, 0.099753694581280791, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185545 minutes
Weight histogram
[ 41 118 175 255 359 380 371 194 107  25] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
[ 82  80  89 114 139 158 208 269 362 524] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.2128
Epoch 1, cost is  1.9837
Epoch 2, cost is  1.91421
Epoch 3, cost is  1.96077
Epoch 4, cost is  2.062
Training took 0.125782 minutes
Weight histogram
[379 404 330 270 219 190 113  53  39  28] [-0.14346196 -0.12934407 -0.11522619 -0.1011083  -0.08699042 -0.07287254
 -0.05875465 -0.04463677 -0.03051888 -0.016401   -0.00228311]
[ 71  93 125 166 190 218 255 278 306 323] [-0.14346196 -0.12934407 -0.11522619 -0.1011083  -0.08699042 -0.07287254
 -0.05875465 -0.04463677 -0.03051888 -0.016401   -0.00228311]
-4.67161
5.54493
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.186695 minutes
Weight histogram
[ 64 225 471 669 920 738 558 258 122  25] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
[168 164 186 235 291 312 480 556 828 830] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.99399
Epoch 1, cost is  3.17997
Epoch 2, cost is  3.38858
Epoch 3, cost is  3.73678
Epoch 4, cost is  4.1774
Training took 0.094810 minutes
Weight histogram
[308 292 269 306 805 822 609 368 162 109] [-0.2442887  -0.22006783 -0.19584697 -0.17162611 -0.14740524 -0.12318438
 -0.09896351 -0.07474265 -0.05052179 -0.02630092 -0.00208006]
[242 370 547 672 786 420 243 249 251 270] [-0.2442887  -0.22006783 -0.19584697 -0.17162611 -0.14740524 -0.12318438
 -0.09896351 -0.07474265 -0.05052179 -0.02630092 -0.00208006]
-6.8542
7.10174
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.46547
Epoch 1, cost is  3.58412
Epoch 2, cost is  3.71781
Epoch 3, cost is  4.08235
Epoch 4, cost is  4.49458
Training took 0.109167 minutes
Weight histogram
[256 350 284 273 298 227 144  84  55  54] [-0.28246394 -0.25446822 -0.22647251 -0.19847679 -0.17048108 -0.14248536
 -0.11448965 -0.08649393 -0.05849822 -0.0305025  -0.00250679]
[105 117 160 175 214 231 247 251 259 266] [-0.28246394 -0.25446822 -0.22647251 -0.19847679 -0.17048108 -0.14248536
 -0.11448965 -0.08649393 -0.05849822 -0.0305025  -0.00250679]
-9.20221
10.92
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049449 minutes
Epoch 0
Fine tuning took 0.048089 minutes
Epoch 0
Fine tuning took 0.050274 minutes
{'zero': {0: [0.34359605911330049, 0.37561576354679804, 0.28817733990147781, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.43965517241379309, 0.47413793103448276, 0.53817733990147787], 5: [0.091133004926108374, 0.18472906403940886, 0.2376847290640394, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.34359605911330049, 0.18842364532019704, 0.20812807881773399, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.72660098522167482, 0.69211822660098521, 0.70197044334975367], 5: [0.091133004926108374, 0.084975369458128072, 0.099753694581280791, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.34359605911330049, 0.23645320197044334, 0.19581280788177341, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.66502463054187189, 0.68349753694581283, 0.72044334975369462], 5: [0.091133004926108374, 0.098522167487684734, 0.1206896551724138, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.34359605911330049, 0.19334975369458129, 0.19458128078817735, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.72906403940886699, 0.70812807881773399, 0.66133004926108374], 5: [0.091133004926108374, 0.077586206896551727, 0.097290640394088676, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185394 minutes
Weight histogram
[ 41 118 175 255 359 380 371 194 107  25] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
[ 82  80  89 114 139 158 208 269 362 524] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.2128
Epoch 1, cost is  1.9837
Epoch 2, cost is  1.91421
Epoch 3, cost is  1.96077
Epoch 4, cost is  2.062
Training took 0.126873 minutes
Weight histogram
[379 404 330 270 219 190 113  53  39  28] [-0.14346196 -0.12934407 -0.11522619 -0.1011083  -0.08699042 -0.07287254
 -0.05875465 -0.04463677 -0.03051888 -0.016401   -0.00228311]
[ 71  93 125 166 190 218 255 278 306 323] [-0.14346196 -0.12934407 -0.11522619 -0.1011083  -0.08699042 -0.07287254
 -0.05875465 -0.04463677 -0.03051888 -0.016401   -0.00228311]
-4.67161
5.54493
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.186128 minutes
Weight histogram
[ 64 225 471 669 920 738 558 258 122  25] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
[168 164 186 235 291 312 480 556 828 830] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.99399
Epoch 1, cost is  3.17997
Epoch 2, cost is  3.38858
Epoch 3, cost is  3.73678
Epoch 4, cost is  4.1774
Training took 0.094380 minutes
Weight histogram
[308 292 269 306 805 822 609 368 162 109] [-0.2442887  -0.22006783 -0.19584697 -0.17162611 -0.14740524 -0.12318438
 -0.09896351 -0.07474265 -0.05052179 -0.02630092 -0.00208006]
[242 370 547 672 786 420 243 249 251 270] [-0.2442887  -0.22006783 -0.19584697 -0.17162611 -0.14740524 -0.12318438
 -0.09896351 -0.07474265 -0.05052179 -0.02630092 -0.00208006]
-6.8542
7.10174
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.86782
Epoch 1, cost is  2.37253
Epoch 2, cost is  2.28229
Epoch 3, cost is  2.43808
Epoch 4, cost is  2.58034
Training took 0.150785 minutes
Weight histogram
[284 355 297 298 216 221 159  87  63  45] [-0.18131831 -0.16343773 -0.14555714 -0.12767655 -0.10979596 -0.09191538
 -0.07403479 -0.0561542  -0.03827362 -0.02039303 -0.00251244]
[115 104 138 172 196 238 246 259 274 283] [-0.18131831 -0.16343773 -0.14555714 -0.12767655 -0.10979596 -0.09191538
 -0.07403479 -0.0561542  -0.03827362 -0.02039303 -0.00251244]
-6.57355
8.06472
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052957 minutes
Epoch 0
Fine tuning took 0.053366 minutes
Epoch 0
Fine tuning took 0.053629 minutes
{'zero': {0: [0.21921182266009853, 0.26108374384236455, 0.23891625615763548, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6711822660098522, 0.61699507389162567, 0.60221674876847286, 0.66748768472906406], 5: [0.10960591133004927, 0.12192118226600986, 0.15886699507389163, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21921182266009853, 0.15640394088669951, 0.1625615763546798, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6711822660098522, 0.7573891625615764, 0.71921182266009853, 0.73399014778325122], 5: [0.10960591133004927, 0.086206896551724144, 0.11822660098522167, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21921182266009853, 0.20935960591133004, 0.19211822660098521, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6711822660098522, 0.68472906403940892, 0.69827586206896552, 0.7068965517241379], 5: [0.10960591133004927, 0.10591133004926108, 0.10960591133004927, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21921182266009853, 0.19334975369458129, 0.18103448275862069, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6711822660098522, 0.72536945812807885, 0.66995073891625612, 0.73152709359605916], 5: [0.10960591133004927, 0.081280788177339899, 0.14901477832512317, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184163 minutes
Weight histogram
[ 41 118 175 255 359 380 371 194 107  25] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
[ 82  80  89 114 139 158 208 269 362 524] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.2128
Epoch 1, cost is  1.9837
Epoch 2, cost is  1.91421
Epoch 3, cost is  1.96077
Epoch 4, cost is  2.062
Training took 0.125683 minutes
Weight histogram
[379 404 330 270 219 190 113  53  39  28] [-0.14346196 -0.12934407 -0.11522619 -0.1011083  -0.08699042 -0.07287254
 -0.05875465 -0.04463677 -0.03051888 -0.016401   -0.00228311]
[ 71  93 125 166 190 218 255 278 306 323] [-0.14346196 -0.12934407 -0.11522619 -0.1011083  -0.08699042 -0.07287254
 -0.05875465 -0.04463677 -0.03051888 -0.016401   -0.00228311]
-4.67161
5.54493
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.187070 minutes
Weight histogram
[ 64 225 471 669 920 738 558 258 122  25] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
[168 164 186 235 291 312 480 556 828 830] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.99399
Epoch 1, cost is  3.17997
Epoch 2, cost is  3.38858
Epoch 3, cost is  3.73678
Epoch 4, cost is  4.1774
Training took 0.095751 minutes
Weight histogram
[308 292 269 306 805 822 609 368 162 109] [-0.2442887  -0.22006783 -0.19584697 -0.17162611 -0.14740524 -0.12318438
 -0.09896351 -0.07474265 -0.05052179 -0.02630092 -0.00208006]
[242 370 547 672 786 420 243 249 251 270] [-0.2442887  -0.22006783 -0.19584697 -0.17162611 -0.14740524 -0.12318438
 -0.09896351 -0.07474265 -0.05052179 -0.02630092 -0.00208006]
-6.8542
7.10174
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  3.53949
Epoch 1, cost is  1.78086
Epoch 2, cost is  1.52957
Epoch 3, cost is  1.49743
Epoch 4, cost is  1.50299
Training took 0.213782 minutes
Weight histogram
[382 336 323 287 219 186 117  85  57  33] [-0.11639518 -0.10500863 -0.09362209 -0.08223555 -0.070849   -0.05946246
 -0.04807592 -0.03668937 -0.02530283 -0.01391629 -0.00252974]
[110  89 118 154 182 226 253 280 299 314] [-0.11639518 -0.10500863 -0.09362209 -0.08223555 -0.070849   -0.05946246
 -0.04807592 -0.03668937 -0.02530283 -0.01391629 -0.00252974]
-5.05279
6.70743
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.056026 minutes
Epoch 0
Fine tuning took 0.058050 minutes
Epoch 0
Fine tuning took 0.056559 minutes
{'zero': {0: [0.23645320197044334, 0.25738916256157635, 0.27093596059113301, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.59113300492610843, 0.58497536945812811, 0.68103448275862066], 5: [0.11576354679802955, 0.15147783251231528, 0.14408866995073891, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23645320197044334, 0.20073891625615764, 0.17980295566502463, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.70073891625615758, 0.70073891625615758, 0.7931034482758621], 5: [0.11576354679802955, 0.098522167487684734, 0.11945812807881774, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23645320197044334, 0.16995073891625614, 0.14778325123152711, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.73399014778325122, 0.72783251231527091, 0.75246305418719217], 5: [0.11576354679802955, 0.096059113300492605, 0.12438423645320197, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23645320197044334, 0.21921182266009853, 0.17980295566502463, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.63300492610837433, 0.68596059113300489, 0.71674876847290636], 5: [0.11576354679802955, 0.14778325123152711, 0.13423645320197045, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184315 minutes
Weight histogram
[ 41 118 175 255 359 380 371 194 107  25] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
[ 82  80  89 114 139 158 208 269 362 524] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.69896
Epoch 1, cost is  3.93093
Epoch 2, cost is  4.65459
Epoch 3, cost is  5.58006
Epoch 4, cost is  6.3114
Training took 0.127230 minutes
Weight histogram
[277 331 235 241 257 223 218 160  57  26] [-0.60722256 -0.5476628  -0.48810304 -0.42854328 -0.36898351 -0.30942375
 -0.24986399 -0.19030423 -0.13074447 -0.07118471 -0.01162495]
[ 72 137 170 203 213 230 239 235 267 259] [-0.60722256 -0.5476628  -0.48810304 -0.42854328 -0.36898351 -0.30942375
 -0.24986399 -0.19030423 -0.13074447 -0.07118471 -0.01162495]
-21.2053
24.1437
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185919 minutes
Weight histogram
[ 64 225 471 669 920 738 558 258 122  25] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
[168 164 186 235 291 312 480 556 828 830] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.86367
Epoch 1, cost is  6.72653
Epoch 2, cost is  9.34637
Epoch 3, cost is  12.1064
Epoch 4, cost is  14.7453
Training took 0.096365 minutes
Weight histogram
[206 208 247 219 608 747 682 613 431  89] [-1.04088497 -0.93785328 -0.83482158 -0.73178989 -0.62875819 -0.5257265
 -0.42269481 -0.31966311 -0.21663142 -0.11359972 -0.01056803]
[319 573 666 689 714 218 221 210 215 225] [-1.04088497 -0.93785328 -0.83482158 -0.73178989 -0.62875819 -0.5257265
 -0.42269481 -0.31966311 -0.21663142 -0.11359972 -0.01056803]
-33.4854
34.3223
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  4.31593
Epoch 1, cost is  5.16692
Epoch 2, cost is  6.43051
Epoch 3, cost is  8.12511
Epoch 4, cost is  9.91625
Training took 0.092823 minutes
Weight histogram
[229 272 261 266 418 318 246   8   2   5] [-0.73570311 -0.66302196 -0.5903408  -0.51765965 -0.4449785  -0.37229735
 -0.29961619 -0.22693504 -0.15425389 -0.08157273 -0.00889158]
[119 207 238 233 221 219 197 193 209 189] [-0.73570311 -0.66302196 -0.5903408  -0.51765965 -0.4449785  -0.37229735
 -0.29961619 -0.22693504 -0.15425389 -0.08157273 -0.00889158]
-40.3752
44.8515
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046544 minutes
Epoch 0
Fine tuning took 0.048392 minutes
Epoch 0
Fine tuning took 0.048284 minutes
{'zero': {0: [0.24014778325123154, 0.23275862068965517, 0.21428571428571427, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.64655172413793105, 0.54433497536945807, 0.54802955665024633], 5: [0.1625615763546798, 0.1206896551724138, 0.2413793103448276, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24014778325123154, 0.25, 0.21182266009852216, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.61699507389162567, 0.53940886699507384, 0.50862068965517238], 5: [0.1625615763546798, 0.13300492610837439, 0.24876847290640394, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24014778325123154, 0.2536945812807882, 0.22167487684729065, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.63793103448275867, 0.55295566502463056, 0.49753694581280788], 5: [0.1625615763546798, 0.10837438423645321, 0.22536945812807882, 0.26231527093596058], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24014778325123154, 0.23275862068965517, 0.22906403940886699, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.65886699507389157, 0.55911330049261088, 0.53201970443349755], 5: [0.1625615763546798, 0.10837438423645321, 0.21182266009852216, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184430 minutes
Weight histogram
[ 41 118 175 255 359 380 371 194 107  25] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
[ 82  80  89 114 139 158 208 269 362 524] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.69896
Epoch 1, cost is  3.93093
Epoch 2, cost is  4.65459
Epoch 3, cost is  5.58006
Epoch 4, cost is  6.3114
Training took 0.129103 minutes
Weight histogram
[277 331 235 241 257 223 218 160  57  26] [-0.60722256 -0.5476628  -0.48810304 -0.42854328 -0.36898351 -0.30942375
 -0.24986399 -0.19030423 -0.13074447 -0.07118471 -0.01162495]
[ 72 137 170 203 213 230 239 235 267 259] [-0.60722256 -0.5476628  -0.48810304 -0.42854328 -0.36898351 -0.30942375
 -0.24986399 -0.19030423 -0.13074447 -0.07118471 -0.01162495]
-21.2053
24.1437
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185709 minutes
Weight histogram
[ 64 225 471 669 920 738 558 258 122  25] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
[168 164 186 235 291 312 480 556 828 830] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.86367
Epoch 1, cost is  6.72653
Epoch 2, cost is  9.34637
Epoch 3, cost is  12.1064
Epoch 4, cost is  14.7453
Training took 0.094390 minutes
Weight histogram
[206 208 247 219 608 747 682 613 431  89] [-1.04088497 -0.93785328 -0.83482158 -0.73178989 -0.62875819 -0.5257265
 -0.42269481 -0.31966311 -0.21663142 -0.11359972 -0.01056803]
[319 573 666 689 714 218 221 210 215 225] [-1.04088497 -0.93785328 -0.83482158 -0.73178989 -0.62875819 -0.5257265
 -0.42269481 -0.31966311 -0.21663142 -0.11359972 -0.01056803]
-33.4854
34.3223
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.30616
Epoch 1, cost is  5.43005
Epoch 2, cost is  7.16312
Epoch 3, cost is  8.83997
Epoch 4, cost is  10.8135
Training took 0.107401 minutes
Weight histogram
[243 350 407 490 374 152   3   1   0   5] [-0.48420089 -0.43713156 -0.39006223 -0.3429929  -0.29592356 -0.24885423
 -0.2017849  -0.15471557 -0.10764623 -0.0605769  -0.01350757]
[ 96 198 206 233 240 216 231 222 198 185] [-0.48420089 -0.43713156 -0.39006223 -0.3429929  -0.29592356 -0.24885423
 -0.2017849  -0.15471557 -0.10764623 -0.0605769  -0.01350757]
-65.1493
48.5874
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046864 minutes
Epoch 0
Fine tuning took 0.048392 minutes
Epoch 0
Fine tuning took 0.049193 minutes
{'zero': {0: [0.081280788177339899, 0.3460591133004926, 0.24261083743842365, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.5357142857142857, 0.60098522167487689, 0.59605911330049266], 5: [0.2894088669950739, 0.11822660098522167, 0.15640394088669951, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.081280788177339899, 0.30541871921182268, 0.28078817733990147, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.55911330049261088, 0.55541871921182262, 0.54556650246305416], 5: [0.2894088669950739, 0.1354679802955665, 0.16379310344827586, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.081280788177339899, 0.31403940886699505, 0.30049261083743845, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.53817733990147787, 0.50985221674876846, 0.5714285714285714], 5: [0.2894088669950739, 0.14778325123152711, 0.18965517241379309, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.081280788177339899, 0.30418719211822659, 0.27709359605911332, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.54926108374384242, 0.51600985221674878, 0.57389162561576357], 5: [0.2894088669950739, 0.14655172413793102, 0.20689655172413793, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.187482 minutes
Weight histogram
[ 41 118 175 255 359 380 371 194 107  25] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
[ 82  80  89 114 139 158 208 269 362 524] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.69896
Epoch 1, cost is  3.93093
Epoch 2, cost is  4.65459
Epoch 3, cost is  5.58006
Epoch 4, cost is  6.3114
Training took 0.128354 minutes
Weight histogram
[277 331 235 241 257 223 218 160  57  26] [-0.60722256 -0.5476628  -0.48810304 -0.42854328 -0.36898351 -0.30942375
 -0.24986399 -0.19030423 -0.13074447 -0.07118471 -0.01162495]
[ 72 137 170 203 213 230 239 235 267 259] [-0.60722256 -0.5476628  -0.48810304 -0.42854328 -0.36898351 -0.30942375
 -0.24986399 -0.19030423 -0.13074447 -0.07118471 -0.01162495]
-21.2053
24.1437
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185004 minutes
Weight histogram
[ 64 225 471 669 920 738 558 258 122  25] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
[168 164 186 235 291 312 480 556 828 830] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.86367
Epoch 1, cost is  6.72653
Epoch 2, cost is  9.34637
Epoch 3, cost is  12.1064
Epoch 4, cost is  14.7453
Training took 0.095770 minutes
Weight histogram
[206 208 247 219 608 747 682 613 431  89] [-1.04088497 -0.93785328 -0.83482158 -0.73178989 -0.62875819 -0.5257265
 -0.42269481 -0.31966311 -0.21663142 -0.11359972 -0.01056803]
[319 573 666 689 714 218 221 210 215 225] [-1.04088497 -0.93785328 -0.83482158 -0.73178989 -0.62875819 -0.5257265
 -0.42269481 -0.31966311 -0.21663142 -0.11359972 -0.01056803]
-33.4854
34.3223
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  2.79969
Epoch 1, cost is  2.66132
Epoch 2, cost is  3.07186
Epoch 3, cost is  3.55858
Epoch 4, cost is  3.9799
Training took 0.151188 minutes
Weight histogram
[270 250 297 244 260 230 218 167  60  29] [-0.64533269 -0.58243336 -0.51953402 -0.45663469 -0.39373535 -0.33083601
 -0.26793668 -0.20503734 -0.14213801 -0.07923867 -0.01633934]
[ 88 143 191 209 220 228 228 234 241 243] [-0.64533269 -0.58243336 -0.51953402 -0.45663469 -0.39373535 -0.33083601
 -0.26793668 -0.20503734 -0.14213801 -0.07923867 -0.01633934]
-28.0475
30.922
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050857 minutes
Epoch 0
Fine tuning took 0.052718 minutes
Epoch 0
Fine tuning took 0.053388 minutes
{'zero': {0: [0.14778325123152711, 0.065270935960591137, 0.21305418719211822, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78078817733990147, 0.87931034482758619, 0.71674876847290636, 0.72536945812807885], 5: [0.071428571428571425, 0.055418719211822662, 0.070197044334975367, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14778325123152711, 0.20935960591133004, 0.21182266009852216, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78078817733990147, 0.69458128078817738, 0.68965517241379315, 0.6145320197044335], 5: [0.071428571428571425, 0.096059113300492605, 0.098522167487684734, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14778325123152711, 0.20443349753694581, 0.20566502463054187, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78078817733990147, 0.71305418719211822, 0.69704433497536944, 0.67610837438423643], 5: [0.071428571428571425, 0.082512315270935957, 0.097290640394088676, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14778325123152711, 0.23029556650246305, 0.19211822660098521, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78078817733990147, 0.71182266009852213, 0.70443349753694584, 0.60098522167487689], 5: [0.071428571428571425, 0.057881773399014777, 0.10344827586206896, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184492 minutes
Weight histogram
[ 41 118 175 255 359 380 371 194 107  25] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
[ 82  80  89 114 139 158 208 269 362 524] [ -2.39681889e-04  -8.04937416e-05   7.86944060e-05   2.37882553e-04
   3.97070701e-04   5.56258849e-04   7.15446996e-04   8.74635144e-04
   1.03382329e-03   1.19301144e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.69896
Epoch 1, cost is  3.93093
Epoch 2, cost is  4.65459
Epoch 3, cost is  5.58006
Epoch 4, cost is  6.3114
Training took 0.126781 minutes
Weight histogram
[277 331 235 241 257 223 218 160  57  26] [-0.60722256 -0.5476628  -0.48810304 -0.42854328 -0.36898351 -0.30942375
 -0.24986399 -0.19030423 -0.13074447 -0.07118471 -0.01162495]
[ 72 137 170 203 213 230 239 235 267 259] [-0.60722256 -0.5476628  -0.48810304 -0.42854328 -0.36898351 -0.30942375
 -0.24986399 -0.19030423 -0.13074447 -0.07118471 -0.01162495]
-21.2053
24.1437
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185405 minutes
Weight histogram
[ 64 225 471 669 920 738 558 258 122  25] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
[168 164 186 235 291 312 480 556 828 830] [ -2.42434355e-04  -8.29709606e-05   7.64924334e-05   2.35955828e-04
   3.95419222e-04   5.54882616e-04   7.14346010e-04   8.73809404e-04
   1.03327280e-03   1.19273619e-03   1.35219959e-03]
-0.789259
0.587886
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.86367
Epoch 1, cost is  6.72653
Epoch 2, cost is  9.34637
Epoch 3, cost is  12.1064
Epoch 4, cost is  14.7453
Training took 0.097057 minutes
Weight histogram
[206 208 247 219 608 747 682 613 431  89] [-1.04088497 -0.93785328 -0.83482158 -0.73178989 -0.62875819 -0.5257265
 -0.42269481 -0.31966311 -0.21663142 -0.11359972 -0.01056803]
[319 573 666 689 714 218 221 210 215 225] [-1.04088497 -0.93785328 -0.83482158 -0.73178989 -0.62875819 -0.5257265
 -0.42269481 -0.31966311 -0.21663142 -0.11359972 -0.01056803]
-33.4854
34.3223
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  2.45344
Epoch 1, cost is  1.89761
Epoch 2, cost is  1.95457
Epoch 3, cost is  2.09851
Epoch 4, cost is  2.2702
Training took 0.214769 minutes
Weight histogram
[323 326 316 279 269 211 150 107  30  14] [-0.39344838 -0.35572608 -0.31800377 -0.28028147 -0.24255916 -0.20483686
 -0.16711455 -0.12939225 -0.09166994 -0.05394764 -0.01622533]
[ 77 113 156 186 212 235 246 258 274 268] [-0.39344838 -0.35572608 -0.31800377 -0.28028147 -0.24255916 -0.20483686
 -0.16711455 -0.12939225 -0.09166994 -0.05394764 -0.01622533]
-18.0254
20.4979
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.055795 minutes
Epoch 0
Fine tuning took 0.056983 minutes
Epoch 0
Fine tuning took 0.057900 minutes
{'zero': {0: [0.13793103448275862, 0.065270935960591137, 0.14532019704433496, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.84975369458128081, 0.75615763546798032, 0.79187192118226601], 5: [0.10098522167487685, 0.084975369458128072, 0.098522167487684734, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13793103448275862, 0.19950738916256158, 0.16748768472906403, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.6785714285714286, 0.74137931034482762, 0.69211822660098521], 5: [0.10098522167487685, 0.12192118226600986, 0.091133004926108374, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13793103448275862, 0.19211822660098521, 0.17241379310344829, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.69704433497536944, 0.72044334975369462, 0.66871921182266014], 5: [0.10098522167487685, 0.11083743842364532, 0.10714285714285714, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13793103448275862, 0.21798029556650247, 0.16379310344827586, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.65886699507389157, 0.7573891625615764, 0.70197044334975367], 5: [0.10098522167487685, 0.12315270935960591, 0.078817733990147784, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149941 minutes
Weight histogram
[ 64 180 216 441 476 471 617 859 615 111] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 107  101  140  184  200  337  431  709  705 1136] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.764093
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.17791
Epoch 1, cost is  2.08836
Epoch 2, cost is  2.11986
Epoch 3, cost is  2.18256
Epoch 4, cost is  2.25583
Training took 0.116804 minutes
Weight histogram
[669 798 751 578 387 373 243 159  49  43] [-0.19266392 -0.17362584 -0.15458776 -0.13554968 -0.1165116  -0.09747352
 -0.07843544 -0.05939736 -0.04035928 -0.0213212  -0.00228311]
[115 179 265 323 396 457 556 587 581 591] [-0.19266392 -0.17362584 -0.15458776 -0.13554968 -0.1165116  -0.09747352
 -0.07843544 -0.05939736 -0.04035928 -0.0213212  -0.00228311]
-7.00993
9.73579
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150679 minutes
Weight histogram
[  82  294  557  886  989  782  831 1088  471   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 214  206  288  373  452  623  968 1051  716 1184] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.09931
0.683543
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.55717
Epoch 1, cost is  4.727
Epoch 2, cost is  5.05522
Epoch 3, cost is  5.41232
Epoch 4, cost is  5.79742
Training took 0.087400 minutes
Weight histogram
[ 518  527  555  511  502  452 1134 1146  559  171] [-0.39140192 -0.35246973 -0.31353754 -0.27460536 -0.23567317 -0.19674099
 -0.1578088  -0.11887661 -0.07994443 -0.04101224 -0.00208006]
[ 481  935 1287  530  423  459  507  489  482  482] [-0.39140192 -0.35246973 -0.31353754 -0.27460536 -0.23567317 -0.19674099
 -0.1578088  -0.11887661 -0.07994443 -0.04101224 -0.00208006]
-12.0799
14.0227
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.88564
Epoch 1, cost is  4.29211
Epoch 2, cost is  4.62362
Epoch 3, cost is  5.07753
Epoch 4, cost is  5.56782
Training took 0.092953 minutes
Weight histogram
[621 665 634 630 595 347 240 122  98  98] [-0.34673315 -0.31229662 -0.27786008 -0.24342354 -0.20898701 -0.17455047
 -0.14011393 -0.1056774  -0.07124086 -0.03680432 -0.00236779]
[193 223 283 385 430 486 473 494 526 557] [-0.34673315 -0.31229662 -0.27786008 -0.24342354 -0.20898701 -0.17455047
 -0.14011393 -0.1056774  -0.07124086 -0.03680432 -0.00236779]
-14.1499
19.2011
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048394 minutes
Epoch 0
Fine tuning took 0.046179 minutes
Epoch 0
Fine tuning took 0.046925 minutes
{'zero': {0: [0.24630541871921183, 0.27339901477832512, 0.17364532019704434, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44334975369458129, 0.55665024630541871, 0.65270935960591137, 0.61083743842364535], 5: [0.31034482758620691, 0.16995073891625614, 0.17364532019704434, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24630541871921183, 0.20566502463054187, 0.20073891625615764, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44334975369458129, 0.68472906403940892, 0.67980295566502458, 0.65640394088669951], 5: [0.31034482758620691, 0.10960591133004927, 0.11945812807881774, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24630541871921183, 0.20073891625615764, 0.16133004926108374, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44334975369458129, 0.65394088669950734, 0.69334975369458129, 0.60098522167487689], 5: [0.31034482758620691, 0.14532019704433496, 0.14532019704433496, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24630541871921183, 0.16133004926108374, 0.19827586206896552, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44334975369458129, 0.70443349753694584, 0.69704433497536944, 0.63669950738916259], 5: [0.31034482758620691, 0.13423645320197045, 0.10467980295566502, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149170 minutes
Weight histogram
[ 64 180 216 441 476 471 617 859 615 111] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 107  101  140  184  200  337  431  709  705 1136] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.764093
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.17791
Epoch 1, cost is  2.08836
Epoch 2, cost is  2.11986
Epoch 3, cost is  2.18256
Epoch 4, cost is  2.25583
Training took 0.116022 minutes
Weight histogram
[669 798 751 578 387 373 243 159  49  43] [-0.19266392 -0.17362584 -0.15458776 -0.13554968 -0.1165116  -0.09747352
 -0.07843544 -0.05939736 -0.04035928 -0.0213212  -0.00228311]
[115 179 265 323 396 457 556 587 581 591] [-0.19266392 -0.17362584 -0.15458776 -0.13554968 -0.1165116  -0.09747352
 -0.07843544 -0.05939736 -0.04035928 -0.0213212  -0.00228311]
-7.00993
9.73579
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150536 minutes
Weight histogram
[  82  294  557  886  989  782  831 1088  471   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 214  206  288  373  452  623  968 1051  716 1184] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.09931
0.683543
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.55717
Epoch 1, cost is  4.727
Epoch 2, cost is  5.05522
Epoch 3, cost is  5.41232
Epoch 4, cost is  5.79742
Training took 0.089876 minutes
Weight histogram
[ 518  527  555  511  502  452 1134 1146  559  171] [-0.39140192 -0.35246973 -0.31353754 -0.27460536 -0.23567317 -0.19674099
 -0.1578088  -0.11887661 -0.07994443 -0.04101224 -0.00208006]
[ 481  935 1287  530  423  459  507  489  482  482] [-0.39140192 -0.35246973 -0.31353754 -0.27460536 -0.23567317 -0.19674099
 -0.1578088  -0.11887661 -0.07994443 -0.04101224 -0.00208006]
-12.0799
14.0227
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.48086
Epoch 1, cost is  3.63312
Epoch 2, cost is  3.78548
Epoch 3, cost is  4.25321
Epoch 4, cost is  4.71402
Training took 0.108807 minutes
Weight histogram
[538 666 578 528 598 457 300 167 110 108] [-0.28246394 -0.25445759 -0.22645125 -0.19844491 -0.17043856 -0.14243222
 -0.11442587 -0.08641953 -0.05841319 -0.03040684 -0.0024005 ]
[209 251 330 373 423 484 498 518 522 442] [-0.28246394 -0.25445759 -0.22645125 -0.19844491 -0.17043856 -0.14243222
 -0.11442587 -0.08641953 -0.05841319 -0.03040684 -0.0024005 ]
-9.80263
12.4229
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048047 minutes
Epoch 0
Fine tuning took 0.047651 minutes
Epoch 0
Fine tuning took 0.047656 minutes
{'zero': {0: [0.21428571428571427, 0.22413793103448276, 0.21551724137931033, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5923645320197044, 0.65517241379310343, 0.68965517241379315, 0.67241379310344829], 5: [0.19334975369458129, 0.1206896551724138, 0.094827586206896547, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21428571428571427, 0.19211822660098521, 0.14285714285714285, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5923645320197044, 0.71059113300492616, 0.75369458128078815, 0.7426108374384236], 5: [0.19334975369458129, 0.097290640394088676, 0.10344827586206896, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21428571428571427, 0.16502463054187191, 0.15517241379310345, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5923645320197044, 0.72167487684729059, 0.76231527093596063, 0.72290640394088668], 5: [0.19334975369458129, 0.11330049261083744, 0.082512315270935957, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21428571428571427, 0.19088669950738915, 0.14285714285714285, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5923645320197044, 0.73275862068965514, 0.78694581280788178, 0.72167487684729059], 5: [0.19334975369458129, 0.076354679802955669, 0.070197044334975367, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150191 minutes
Weight histogram
[ 64 180 216 441 476 471 617 859 615 111] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 107  101  140  184  200  337  431  709  705 1136] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.764093
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.17791
Epoch 1, cost is  2.08836
Epoch 2, cost is  2.11986
Epoch 3, cost is  2.18256
Epoch 4, cost is  2.25583
Training took 0.117953 minutes
Weight histogram
[669 798 751 578 387 373 243 159  49  43] [-0.19266392 -0.17362584 -0.15458776 -0.13554968 -0.1165116  -0.09747352
 -0.07843544 -0.05939736 -0.04035928 -0.0213212  -0.00228311]
[115 179 265 323 396 457 556 587 581 591] [-0.19266392 -0.17362584 -0.15458776 -0.13554968 -0.1165116  -0.09747352
 -0.07843544 -0.05939736 -0.04035928 -0.0213212  -0.00228311]
-7.00993
9.73579
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151067 minutes
Weight histogram
[  82  294  557  886  989  782  831 1088  471   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 214  206  288  373  452  623  968 1051  716 1184] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.09931
0.683543
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.55717
Epoch 1, cost is  4.727
Epoch 2, cost is  5.05522
Epoch 3, cost is  5.41232
Epoch 4, cost is  5.79742
Training took 0.089241 minutes
Weight histogram
[ 518  527  555  511  502  452 1134 1146  559  171] [-0.39140192 -0.35246973 -0.31353754 -0.27460536 -0.23567317 -0.19674099
 -0.1578088  -0.11887661 -0.07994443 -0.04101224 -0.00208006]
[ 481  935 1287  530  423  459  507  489  482  482] [-0.39140192 -0.35246973 -0.31353754 -0.27460536 -0.23567317 -0.19674099
 -0.1578088  -0.11887661 -0.07994443 -0.04101224 -0.00208006]
-12.0799
14.0227
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.92538
Epoch 1, cost is  2.48873
Epoch 2, cost is  2.42181
Epoch 3, cost is  2.56351
Epoch 4, cost is  2.69653
Training took 0.150200 minutes
Weight histogram
[572 701 570 603 444 446 323 177 124  90] [-0.18184739 -0.16390181 -0.14595622 -0.12801063 -0.11006504 -0.09211945
 -0.07417387 -0.05622828 -0.03828269 -0.0203371  -0.00239151]
[232 215 285 354 404 477 499 515 557 512] [-0.18184739 -0.16390181 -0.14595622 -0.12801063 -0.11006504 -0.09211945
 -0.07417387 -0.05622828 -0.03828269 -0.0203371  -0.00239151]
-7.39972
8.40553
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051479 minutes
Epoch 0
Fine tuning took 0.052035 minutes
Epoch 0
Fine tuning took 0.052257 minutes
{'zero': {0: [0.23522167487684728, 0.1268472906403941, 0.15270935960591134, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.77216748768472909, 0.76354679802955661, 0.77093596059113301], 5: [0.13054187192118227, 0.10098522167487685, 0.083743842364532015, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23522167487684728, 0.13793103448275862, 0.17857142857142858, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.7573891625615764, 0.7426108374384236, 0.73152709359605916], 5: [0.13054187192118227, 0.10467980295566502, 0.078817733990147784, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23522167487684728, 0.16625615763546797, 0.1539408866995074, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.72290640394088668, 0.73768472906403937, 0.73275862068965514], 5: [0.13054187192118227, 0.11083743842364532, 0.10837438423645321, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23522167487684728, 0.087438423645320201, 0.15640394088669951, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.83497536945812811, 0.77955665024630538, 0.7931034482758621], 5: [0.13054187192118227, 0.077586206896551727, 0.064039408866995079, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151071 minutes
Weight histogram
[ 64 180 216 441 476 471 617 859 615 111] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 107  101  140  184  200  337  431  709  705 1136] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.764093
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.17791
Epoch 1, cost is  2.08836
Epoch 2, cost is  2.11986
Epoch 3, cost is  2.18256
Epoch 4, cost is  2.25583
Training took 0.115863 minutes
Weight histogram
[669 798 751 578 387 373 243 159  49  43] [-0.19266392 -0.17362584 -0.15458776 -0.13554968 -0.1165116  -0.09747352
 -0.07843544 -0.05939736 -0.04035928 -0.0213212  -0.00228311]
[115 179 265 323 396 457 556 587 581 591] [-0.19266392 -0.17362584 -0.15458776 -0.13554968 -0.1165116  -0.09747352
 -0.07843544 -0.05939736 -0.04035928 -0.0213212  -0.00228311]
-7.00993
9.73579
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150867 minutes
Weight histogram
[  82  294  557  886  989  782  831 1088  471   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 214  206  288  373  452  623  968 1051  716 1184] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.09931
0.683543
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.55717
Epoch 1, cost is  4.727
Epoch 2, cost is  5.05522
Epoch 3, cost is  5.41232
Epoch 4, cost is  5.79742
Training took 0.090461 minutes
Weight histogram
[ 518  527  555  511  502  452 1134 1146  559  171] [-0.39140192 -0.35246973 -0.31353754 -0.27460536 -0.23567317 -0.19674099
 -0.1578088  -0.11887661 -0.07994443 -0.04101224 -0.00208006]
[ 481  935 1287  530  423  459  507  489  482  482] [-0.39140192 -0.35246973 -0.31353754 -0.27460536 -0.23567317 -0.19674099
 -0.1578088  -0.11887661 -0.07994443 -0.04101224 -0.00208006]
-12.0799
14.0227
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.6228
Epoch 1, cost is  1.87683
Epoch 2, cost is  1.60822
Epoch 3, cost is  1.56739
Epoch 4, cost is  1.57326
Training took 0.213553 minutes
Weight histogram
[743 663 654 590 434 377 241 168 114  66] [-0.11641122 -0.10501463 -0.09361805 -0.08222147 -0.07082489 -0.05942831
 -0.04803173 -0.03663515 -0.02523856 -0.01384198 -0.0024454 ]
[222 186 245 324 376 470 516 564 599 548] [-0.11641122 -0.10501463 -0.09361805 -0.08222147 -0.07082489 -0.05942831
 -0.04803173 -0.03663515 -0.02523856 -0.01384198 -0.0024454 ]
-6.67311
6.92695
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.057247 minutes
Epoch 0
Fine tuning took 0.055879 minutes
Epoch 0
Fine tuning took 0.056238 minutes
{'zero': {0: [0.22660098522167488, 0.16871921182266009, 0.20812807881773399, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.75369458128078815, 0.67487684729064035, 0.64532019704433496], 5: [0.13423645320197045, 0.077586206896551727, 0.11699507389162561, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22660098522167488, 0.10344827586206896, 0.16133004926108374, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.79064039408866993, 0.74630541871921185, 0.73891625615763545], 5: [0.13423645320197045, 0.10591133004926108, 0.092364532019704432, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22660098522167488, 0.13054187192118227, 0.14408866995073891, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.76354679802955661, 0.74384236453201968, 0.75492610837438423], 5: [0.13423645320197045, 0.10591133004926108, 0.11206896551724138, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22660098522167488, 0.11945812807881774, 0.14901477832512317, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.75862068965517238, 0.75369458128078815, 0.73768472906403937], 5: [0.13423645320197045, 0.12192118226600986, 0.097290640394088676, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149858 minutes
Weight histogram
[ 64 180 216 441 476 471 617 859 615 111] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 107  101  140  184  200  337  431  709  705 1136] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.764093
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.90887
Epoch 1, cost is  6.90535
Epoch 2, cost is  7.38218
Epoch 3, cost is  7.93706
Epoch 4, cost is  8.55859
Training took 0.116283 minutes
Weight histogram
[546 529 642 544 454 345 397 333 215  45] [-0.92809814 -0.83645082 -0.7448035  -0.65315619 -0.56150887 -0.46986155
 -0.37821423 -0.28656691 -0.19491959 -0.10327227 -0.01162495]
[159 295 348 394 409 445 543 493 488 476] [-0.92809814 -0.83645082 -0.7448035  -0.65315619 -0.56150887 -0.46986155
 -0.37821423 -0.28656691 -0.19491959 -0.10327227 -0.01162495]
-34.3473
35.7069
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150148 minutes
Weight histogram
[  82  294  557  886  989  782  831 1088  471   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 214  206  288  373  452  623  968 1051  716 1184] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.09931
0.683543
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  16.7814
Epoch 1, cost is  18.3571
Epoch 2, cost is  20.5407
Epoch 3, cost is  22.8906
Epoch 4, cost is  25.3238
Training took 0.089946 minutes
Weight histogram
[ 421  434  434  460  483  418  655 1217 1163  390] [-1.85727489 -1.6726042  -1.48793352 -1.30326283 -1.11859215 -0.93392146
 -0.74925077 -0.56458009 -0.3799094  -0.19523871 -0.01056803]
[ 846 1306  985  422  418  449  426  413  406  404] [-1.85727489 -1.6726042  -1.48793352 -1.30326283 -1.11859215 -0.93392146
 -0.74925077 -0.56458009 -0.3799094  -0.19523871 -0.01056803]
-67.242
75.5227
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.59668
Epoch 1, cost is  5.03358
Epoch 2, cost is  5.71424
Epoch 3, cost is  6.57578
Epoch 4, cost is  7.53465
Training took 0.094878 minutes
Weight histogram
[ 230  273  259  416 1351  955  544    9    3   10] [-0.73570311 -0.66296286 -0.5902226  -0.51748235 -0.44474209 -0.37200184
 -0.29926158 -0.22652133 -0.15378107 -0.08104082 -0.00830057]
[236 480 522 583 583 633 422 193 209 189] [-0.73570311 -0.66296286 -0.5902226  -0.51748235 -0.44474209 -0.37200184
 -0.29926158 -0.22652133 -0.15378107 -0.08104082 -0.00830057]
-59.0772
57.1348
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044770 minutes
Epoch 0
Fine tuning took 0.046877 minutes
Epoch 0
Fine tuning took 0.045096 minutes
{'zero': {0: [0.32389162561576357, 0.25862068965517243, 0.23399014778325122, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.56034482758620685, 0.61083743842364535, 0.56157635467980294], 5: [0.089901477832512317, 0.18103448275862069, 0.15517241379310345, 0.29433497536945813], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.32389162561576357, 0.27832512315270935, 0.33990147783251229, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.5431034482758621, 0.50862068965517238, 0.53078817733990147], 5: [0.089901477832512317, 0.17857142857142858, 0.15147783251231528, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.32389162561576357, 0.29926108374384236, 0.31773399014778325, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.56650246305418717, 0.53940886699507384, 0.51354679802955661], 5: [0.089901477832512317, 0.13423645320197045, 0.14285714285714285, 0.26724137931034481], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.32389162561576357, 0.2857142857142857, 0.29310344827586204, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.56650246305418717, 0.54187192118226601, 0.51724137931034486], 5: [0.089901477832512317, 0.14778325123152711, 0.16502463054187191, 0.29433497536945813], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150327 minutes
Weight histogram
[ 64 180 216 441 476 471 617 859 615 111] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 107  101  140  184  200  337  431  709  705 1136] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.764093
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.90887
Epoch 1, cost is  6.90535
Epoch 2, cost is  7.38218
Epoch 3, cost is  7.93706
Epoch 4, cost is  8.55859
Training took 0.115970 minutes
Weight histogram
[546 529 642 544 454 345 397 333 215  45] [-0.92809814 -0.83645082 -0.7448035  -0.65315619 -0.56150887 -0.46986155
 -0.37821423 -0.28656691 -0.19491959 -0.10327227 -0.01162495]
[159 295 348 394 409 445 543 493 488 476] [-0.92809814 -0.83645082 -0.7448035  -0.65315619 -0.56150887 -0.46986155
 -0.37821423 -0.28656691 -0.19491959 -0.10327227 -0.01162495]
-34.3473
35.7069
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150467 minutes
Weight histogram
[  82  294  557  886  989  782  831 1088  471   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 214  206  288  373  452  623  968 1051  716 1184] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.09931
0.683543
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  16.7814
Epoch 1, cost is  18.3571
Epoch 2, cost is  20.5407
Epoch 3, cost is  22.8906
Epoch 4, cost is  25.3238
Training took 0.089137 minutes
Weight histogram
[ 421  434  434  460  483  418  655 1217 1163  390] [-1.85727489 -1.6726042  -1.48793352 -1.30326283 -1.11859215 -0.93392146
 -0.74925077 -0.56458009 -0.3799094  -0.19523871 -0.01056803]
[ 846 1306  985  422  418  449  426  413  406  404] [-1.85727489 -1.6726042  -1.48793352 -1.30326283 -1.11859215 -0.93392146
 -0.74925077 -0.56458009 -0.3799094  -0.19523871 -0.01056803]
-67.242
75.5227
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.2312
Epoch 1, cost is  5.28686
Epoch 2, cost is  6.67389
Epoch 3, cost is  8.29504
Epoch 4, cost is  9.92751
Training took 0.109131 minutes
Weight histogram
[235 214 206 377 784 940 936 334  13  11] [-0.75476682 -0.6806409  -0.60651497 -0.53238905 -0.45826312 -0.3841372
 -0.31001127 -0.23588535 -0.16175942 -0.08763349 -0.01350757]
[280 487 527 519 524 518 476 303 200 216] [-0.75476682 -0.6806409  -0.60651497 -0.53238905 -0.45826312 -0.3841372
 -0.31001127 -0.23588535 -0.16175942 -0.08763349 -0.01350757]
-65.1493
48.5874
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046723 minutes
Epoch 0
Fine tuning took 0.047814 minutes
Epoch 0
Fine tuning took 0.047416 minutes
{'zero': {0: [0.40517241379310343, 0.27463054187192121, 0.33374384236453203, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.59113300492610843, 0.51724137931034486, 0.62315270935960587], 5: [0.060344827586206899, 0.13423645320197045, 0.14901477832512317, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.40517241379310343, 0.27093596059113301, 0.37192118226600984, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.57266009852216748, 0.44827586206896552, 0.5714285714285714], 5: [0.060344827586206899, 0.15640394088669951, 0.17980295566502463, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.40517241379310343, 0.28448275862068967, 0.39408866995073893, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.53940886699507384, 0.44334975369458129, 0.56280788177339902], 5: [0.060344827586206899, 0.17610837438423646, 0.1625615763546798, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.40517241379310343, 0.25985221674876846, 0.36699507389162561, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.56280788177339902, 0.47536945812807879, 0.59113300492610843], 5: [0.060344827586206899, 0.17733990147783252, 0.15763546798029557, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150198 minutes
Weight histogram
[ 64 180 216 441 476 471 617 859 615 111] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 107  101  140  184  200  337  431  709  705 1136] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.764093
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.90887
Epoch 1, cost is  6.90535
Epoch 2, cost is  7.38218
Epoch 3, cost is  7.93706
Epoch 4, cost is  8.55859
Training took 0.117553 minutes
Weight histogram
[546 529 642 544 454 345 397 333 215  45] [-0.92809814 -0.83645082 -0.7448035  -0.65315619 -0.56150887 -0.46986155
 -0.37821423 -0.28656691 -0.19491959 -0.10327227 -0.01162495]
[159 295 348 394 409 445 543 493 488 476] [-0.92809814 -0.83645082 -0.7448035  -0.65315619 -0.56150887 -0.46986155
 -0.37821423 -0.28656691 -0.19491959 -0.10327227 -0.01162495]
-34.3473
35.7069
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150142 minutes
Weight histogram
[  82  294  557  886  989  782  831 1088  471   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 214  206  288  373  452  623  968 1051  716 1184] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.09931
0.683543
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  16.7814
Epoch 1, cost is  18.3571
Epoch 2, cost is  20.5407
Epoch 3, cost is  22.8906
Epoch 4, cost is  25.3238
Training took 0.088533 minutes
Weight histogram
[ 421  434  434  460  483  418  655 1217 1163  390] [-1.85727489 -1.6726042  -1.48793352 -1.30326283 -1.11859215 -0.93392146
 -0.74925077 -0.56458009 -0.3799094  -0.19523871 -0.01056803]
[ 846 1306  985  422  418  449  426  413  406  404] [-1.85727489 -1.6726042  -1.48793352 -1.30326283 -1.11859215 -0.93392146
 -0.74925077 -0.56458009 -0.3799094  -0.19523871 -0.01056803]
-67.242
75.5227
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.34528
Epoch 1, cost is  3.6062
Epoch 2, cost is  4.4263
Epoch 3, cost is  5.33012
Epoch 4, cost is  6.16769
Training took 0.150777 minutes
Weight histogram
[325 511 607 508 559 524 503 382  94  37] [-0.64533269 -0.58237255 -0.51941241 -0.45645227 -0.39349213 -0.33053199
 -0.26757185 -0.20461171 -0.14165157 -0.07869143 -0.01573128]
[192 318 422 439 467 461 470 482 477 322] [-0.64533269 -0.58237255 -0.51941241 -0.45645227 -0.39349213 -0.33053199
 -0.26757185 -0.20461171 -0.14165157 -0.07869143 -0.01573128]
-33.9971
36.2665
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049888 minutes
Epoch 0
Fine tuning took 0.050346 minutes
Epoch 0
Fine tuning took 0.050285 minutes
{'zero': {0: [0.15763546798029557, 0.31403940886699505, 0.26354679802955666, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.61822660098522164, 0.65640394088669951, 0.59605911330049266], 5: [0.087438423645320201, 0.067733990147783252, 0.080049261083743842, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.2376847290640394, 0.19581280788177341, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.69827586206896552, 0.70073891625615758, 0.73152709359605916], 5: [0.087438423645320201, 0.064039408866995079, 0.10344827586206896, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.21182266009852216, 0.21428571428571427, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.7068965517241379, 0.68719211822660098, 0.72660098522167482], 5: [0.087438423645320201, 0.081280788177339899, 0.098522167487684734, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.21305418719211822, 0.18103448275862069, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.69334975369458129, 0.70812807881773399, 0.64655172413793105], 5: [0.087438423645320201, 0.093596059113300489, 0.11083743842364532, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150311 minutes
Weight histogram
[ 64 180 216 441 476 471 617 859 615 111] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 107  101  140  184  200  337  431  709  705 1136] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.764093
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.90887
Epoch 1, cost is  6.90535
Epoch 2, cost is  7.38218
Epoch 3, cost is  7.93706
Epoch 4, cost is  8.55859
Training took 0.116642 minutes
Weight histogram
[546 529 642 544 454 345 397 333 215  45] [-0.92809814 -0.83645082 -0.7448035  -0.65315619 -0.56150887 -0.46986155
 -0.37821423 -0.28656691 -0.19491959 -0.10327227 -0.01162495]
[159 295 348 394 409 445 543 493 488 476] [-0.92809814 -0.83645082 -0.7448035  -0.65315619 -0.56150887 -0.46986155
 -0.37821423 -0.28656691 -0.19491959 -0.10327227 -0.01162495]
-34.3473
35.7069
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149785 minutes
Weight histogram
[  82  294  557  886  989  782  831 1088  471   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 214  206  288  373  452  623  968 1051  716 1184] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.09931
0.683543
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  16.7814
Epoch 1, cost is  18.3571
Epoch 2, cost is  20.5407
Epoch 3, cost is  22.8906
Epoch 4, cost is  25.3238
Training took 0.087929 minutes
Weight histogram
[ 421  434  434  460  483  418  655 1217 1163  390] [-1.85727489 -1.6726042  -1.48793352 -1.30326283 -1.11859215 -0.93392146
 -0.74925077 -0.56458009 -0.3799094  -0.19523871 -0.01056803]
[ 846 1306  985  422  418  449  426  413  406  404] [-1.85727489 -1.6726042  -1.48793352 -1.30326283 -1.11859215 -0.93392146
 -0.74925077 -0.56458009 -0.3799094  -0.19523871 -0.01056803]
-67.242
75.5227
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  2.68154
Epoch 1, cost is  2.00605
Epoch 2, cost is  2.04529
Epoch 3, cost is  2.21482
Epoch 4, cost is  2.36038
Training took 0.213959 minutes
Weight histogram
[601 659 615 589 539 420 322 203  75  27] [-0.39945662 -0.36109716 -0.3227377  -0.28437825 -0.24601879 -0.20765933
 -0.16929987 -0.13094041 -0.09258096 -0.0542215  -0.01586204]
[162 233 323 386 437 480 502 523 542 462] [-0.39945662 -0.36109716 -0.3227377  -0.28437825 -0.24601879 -0.20765933
 -0.16929987 -0.13094041 -0.09258096 -0.0542215  -0.01586204]
-25.1671
21.306
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.055776 minutes
Epoch 0
Fine tuning took 0.055406 minutes
Epoch 0
Fine tuning took 0.056012 minutes
{'zero': {0: [0.14901477832512317, 0.33743842364532017, 0.25985221674876846, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.4963054187192118, 0.49507389162561577, 0.59605911330049266], 5: [0.078817733990147784, 0.16625615763546797, 0.24507389162561577, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14901477832512317, 0.30541871921182268, 0.26477832512315269, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.61083743842364535, 0.63916256157635465, 0.69704433497536944], 5: [0.078817733990147784, 0.083743842364532015, 0.096059113300492605, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14901477832512317, 0.26477832512315269, 0.25, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.64162561576354682, 0.6219211822660099, 0.66995073891625612], 5: [0.078817733990147784, 0.093596059113300489, 0.12807881773399016, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14901477832512317, 0.35591133004926107, 0.30665024630541871, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.56157635467980294, 0.57635467980295563, 0.64039408866995073], 5: [0.078817733990147784, 0.082512315270935957, 0.11699507389162561, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150498 minutes
Weight histogram
[  64  180  216  441  476  529  992 1835 1162  180] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 117  118  170  214  279  417  695  826 1354 1885] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.771986
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.88307
Epoch 1, cost is  2.77784
Epoch 2, cost is  2.82289
Epoch 3, cost is  2.90506
Epoch 4, cost is  3.00338
Training took 0.118058 minutes
Weight histogram
[ 809  786  515 1063 1053  740  556  330  162   61] [-0.26920885 -0.24251628 -0.2158237  -0.18913113 -0.16243856 -0.13574598
 -0.10905341 -0.08236084 -0.05566826 -0.02897569 -0.00228311]
[176 314 449 573 733 811 813 754 727 725] [-0.26920885 -0.24251628 -0.2158237  -0.18913113 -0.16243856 -0.13574598
 -0.10905341 -0.08236084 -0.05566826 -0.02897569 -0.00228311]
-10.4837
14.2422
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149265 minutes
Weight histogram
[  82  294  557  886  989 1020 1746 1835  596   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 234  243  343  417  617  870 1255  807 1317 1997] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.10857
0.785184
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.91244
Epoch 1, cost is  6.90206
Epoch 2, cost is  7.20415
Epoch 3, cost is  7.57728
Epoch 4, cost is  7.98977
Training took 0.088942 minutes
Weight histogram
[ 728  746  605  736  796  732  644 1463 1305  345] [-0.54985702 -0.49507932 -0.44030163 -0.38552393 -0.33074624 -0.27596854
 -0.22119084 -0.16641315 -0.11163545 -0.05685775 -0.00208006]
[ 835 1671  847  622  723  694  691  699  666  652] [-0.54985702 -0.49507932 -0.44030163 -0.38552393 -0.33074624 -0.27596854
 -0.22119084 -0.16641315 -0.11163545 -0.05685775 -0.00208006]
-17.9488
20.6501
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.92337
Epoch 1, cost is  4.35109
Epoch 2, cost is  4.7323
Epoch 3, cost is  5.15283
Epoch 4, cost is  5.65317
Training took 0.094463 minutes
Weight histogram
[910 998 940 939 891 557 362 185 147 146] [-0.34779999 -0.31325043 -0.27870087 -0.24415131 -0.20960175 -0.17505219
 -0.14050263 -0.10595307 -0.07140351 -0.03685395 -0.00230439]
[286 338 436 573 647 727 721 744 787 816] [-0.34779999 -0.31325043 -0.27870087 -0.24415131 -0.20960175 -0.17505219
 -0.14050263 -0.10595307 -0.07140351 -0.03685395 -0.00230439]
-14.1499
19.9534
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048417 minutes
Epoch 0
Fine tuning took 0.048517 minutes
Epoch 0
Fine tuning took 0.046419 minutes
{'zero': {0: [0.21921182266009853, 0.19950738916256158, 0.16009852216748768, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50246305418719217, 0.63300492610837433, 0.6219211822660099, 0.6576354679802956], 5: [0.27832512315270935, 0.16748768472906403, 0.21798029556650247, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21921182266009853, 0.23152709359605911, 0.17610837438423646, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50246305418719217, 0.60591133004926112, 0.6071428571428571, 0.66133004926108374], 5: [0.27832512315270935, 0.1625615763546798, 0.21674876847290642, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21921182266009853, 0.23152709359605911, 0.14162561576354679, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50246305418719217, 0.63177339901477836, 0.63177339901477836, 0.6576354679802956], 5: [0.27832512315270935, 0.13669950738916256, 0.22660098522167488, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21921182266009853, 0.21182266009852216, 0.15517241379310345, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50246305418719217, 0.63300492610837433, 0.61576354679802958, 0.62438423645320196], 5: [0.27832512315270935, 0.15517241379310345, 0.22906403940886699, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149919 minutes
Weight histogram
[  64  180  216  441  476  529  992 1835 1162  180] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 117  118  170  214  279  417  695  826 1354 1885] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.771986
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.88307
Epoch 1, cost is  2.77784
Epoch 2, cost is  2.82289
Epoch 3, cost is  2.90506
Epoch 4, cost is  3.00338
Training took 0.116769 minutes
Weight histogram
[ 809  786  515 1063 1053  740  556  330  162   61] [-0.26920885 -0.24251628 -0.2158237  -0.18913113 -0.16243856 -0.13574598
 -0.10905341 -0.08236084 -0.05566826 -0.02897569 -0.00228311]
[176 314 449 573 733 811 813 754 727 725] [-0.26920885 -0.24251628 -0.2158237  -0.18913113 -0.16243856 -0.13574598
 -0.10905341 -0.08236084 -0.05566826 -0.02897569 -0.00228311]
-10.4837
14.2422
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149132 minutes
Weight histogram
[  82  294  557  886  989 1020 1746 1835  596   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 234  243  343  417  617  870 1255  807 1317 1997] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.10857
0.785184
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.91244
Epoch 1, cost is  6.90206
Epoch 2, cost is  7.20415
Epoch 3, cost is  7.57728
Epoch 4, cost is  7.98977
Training took 0.090768 minutes
Weight histogram
[ 728  746  605  736  796  732  644 1463 1305  345] [-0.54985702 -0.49507932 -0.44030163 -0.38552393 -0.33074624 -0.27596854
 -0.22119084 -0.16641315 -0.11163545 -0.05685775 -0.00208006]
[ 835 1671  847  622  723  694  691  699  666  652] [-0.54985702 -0.49507932 -0.44030163 -0.38552393 -0.33074624 -0.27596854
 -0.22119084 -0.16641315 -0.11163545 -0.05685775 -0.00208006]
-17.9488
20.6501
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.50421
Epoch 1, cost is  3.70634
Epoch 2, cost is  3.85951
Epoch 3, cost is  4.25937
Epoch 4, cost is  4.68851
Training took 0.109021 minutes
Weight histogram
[782 965 892 827 868 705 457 252 165 162] [-0.28246394 -0.25445185 -0.22643976 -0.19842767 -0.17041559 -0.1424035
 -0.11439141 -0.08637932 -0.05836723 -0.03035515 -0.00234306]
[311 373 492 557 631 722 748 771 779 691] [-0.28246394 -0.25445185 -0.22643976 -0.19842767 -0.17041559 -0.1424035
 -0.11439141 -0.08637932 -0.05836723 -0.03035515 -0.00234306]
-9.80263
13.4142
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048361 minutes
Epoch 0
Fine tuning took 0.050464 minutes
Epoch 0
Fine tuning took 0.050880 minutes
{'zero': {0: [0.17980295566502463, 0.27339901477832512, 0.25123152709359609, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.56773399014778325, 0.57266009852216748, 0.63054187192118227], 5: [0.18596059113300492, 0.15886699507389163, 0.17610837438423646, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17980295566502463, 0.14532019704433496, 0.12807881773399016, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.65640394088669951, 0.66748768472906406, 0.66871921182266014], 5: [0.18596059113300492, 0.19827586206896552, 0.20443349753694581, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17980295566502463, 0.19211822660098521, 0.17241379310344829, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.65640394088669951, 0.61945812807881773, 0.66133004926108374], 5: [0.18596059113300492, 0.15147783251231528, 0.20812807881773399, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17980295566502463, 0.11206896551724138, 0.10344827586206896, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.68472906403940892, 0.69458128078817738, 0.6576354679802956], 5: [0.18596059113300492, 0.20320197044334976, 0.2019704433497537, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150218 minutes
Weight histogram
[  64  180  216  441  476  529  992 1835 1162  180] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 117  118  170  214  279  417  695  826 1354 1885] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.771986
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.88307
Epoch 1, cost is  2.77784
Epoch 2, cost is  2.82289
Epoch 3, cost is  2.90506
Epoch 4, cost is  3.00338
Training took 0.118566 minutes
Weight histogram
[ 809  786  515 1063 1053  740  556  330  162   61] [-0.26920885 -0.24251628 -0.2158237  -0.18913113 -0.16243856 -0.13574598
 -0.10905341 -0.08236084 -0.05566826 -0.02897569 -0.00228311]
[176 314 449 573 733 811 813 754 727 725] [-0.26920885 -0.24251628 -0.2158237  -0.18913113 -0.16243856 -0.13574598
 -0.10905341 -0.08236084 -0.05566826 -0.02897569 -0.00228311]
-10.4837
14.2422
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150481 minutes
Weight histogram
[  82  294  557  886  989 1020 1746 1835  596   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 234  243  343  417  617  870 1255  807 1317 1997] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.10857
0.785184
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.91244
Epoch 1, cost is  6.90206
Epoch 2, cost is  7.20415
Epoch 3, cost is  7.57728
Epoch 4, cost is  7.98977
Training took 0.088915 minutes
Weight histogram
[ 728  746  605  736  796  732  644 1463 1305  345] [-0.54985702 -0.49507932 -0.44030163 -0.38552393 -0.33074624 -0.27596854
 -0.22119084 -0.16641315 -0.11163545 -0.05685775 -0.00208006]
[ 835 1671  847  622  723  694  691  699  666  652] [-0.54985702 -0.49507932 -0.44030163 -0.38552393 -0.33074624 -0.27596854
 -0.22119084 -0.16641315 -0.11163545 -0.05685775 -0.00208006]
-17.9488
20.6501
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.9395
Epoch 1, cost is  2.53128
Epoch 2, cost is  2.47129
Epoch 3, cost is  2.62737
Epoch 4, cost is  2.77797
Training took 0.149531 minutes
Weight histogram
[ 854 1050  836  909  689  660  491  266  185  135] [-0.18184739 -0.16389569 -0.14594398 -0.12799228 -0.11004058 -0.09208887
 -0.07413717 -0.05618546 -0.03823376 -0.02028205 -0.00233035]
[343 326 425 532 606 711 746 772 826 788] [-0.18184739 -0.16389569 -0.14594398 -0.12799228 -0.11004058 -0.09208887
 -0.07413717 -0.05618546 -0.03823376 -0.02028205 -0.00233035]
-7.39972
9.17823
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051489 minutes
Epoch 0
Fine tuning took 0.052640 minutes
Epoch 0
Fine tuning took 0.053451 minutes
{'zero': {0: [0.21551724137931033, 0.42980295566502463, 0.29556650246305421, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.36945812807881773, 0.40640394088669951, 0.47167487684729065], 5: [0.21305418719211822, 0.20073891625615764, 0.29802955665024633, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21551724137931033, 0.17364532019704434, 0.16995073891625614, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.67733990147783252, 0.68226600985221675, 0.6785714285714286], 5: [0.21305418719211822, 0.14901477832512317, 0.14778325123152711, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21551724137931033, 0.18719211822660098, 0.18226600985221675, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.67733990147783252, 0.6428571428571429, 0.61822660098522164], 5: [0.21305418719211822, 0.1354679802955665, 0.1748768472906404, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21551724137931033, 0.30049261083743845, 0.21551724137931033, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.54187192118226601, 0.51600985221674878, 0.56896551724137934], 5: [0.21305418719211822, 0.15763546798029557, 0.26847290640394089, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.152923 minutes
Weight histogram
[  64  180  216  441  476  529  992 1835 1162  180] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 117  118  170  214  279  417  695  826 1354 1885] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.771986
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.88307
Epoch 1, cost is  2.77784
Epoch 2, cost is  2.82289
Epoch 3, cost is  2.90506
Epoch 4, cost is  3.00338
Training took 0.119606 minutes
Weight histogram
[ 809  786  515 1063 1053  740  556  330  162   61] [-0.26920885 -0.24251628 -0.2158237  -0.18913113 -0.16243856 -0.13574598
 -0.10905341 -0.08236084 -0.05566826 -0.02897569 -0.00228311]
[176 314 449 573 733 811 813 754 727 725] [-0.26920885 -0.24251628 -0.2158237  -0.18913113 -0.16243856 -0.13574598
 -0.10905341 -0.08236084 -0.05566826 -0.02897569 -0.00228311]
-10.4837
14.2422
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150148 minutes
Weight histogram
[  82  294  557  886  989 1020 1746 1835  596   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 234  243  343  417  617  870 1255  807 1317 1997] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.10857
0.785184
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.91244
Epoch 1, cost is  6.90206
Epoch 2, cost is  7.20415
Epoch 3, cost is  7.57728
Epoch 4, cost is  7.98977
Training took 0.087385 minutes
Weight histogram
[ 728  746  605  736  796  732  644 1463 1305  345] [-0.54985702 -0.49507932 -0.44030163 -0.38552393 -0.33074624 -0.27596854
 -0.22119084 -0.16641315 -0.11163545 -0.05685775 -0.00208006]
[ 835 1671  847  622  723  694  691  699  666  652] [-0.54985702 -0.49507932 -0.44030163 -0.38552393 -0.33074624 -0.27596854
 -0.22119084 -0.16641315 -0.11163545 -0.05685775 -0.00208006]
-17.9488
20.6501
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.62977
Epoch 1, cost is  1.91055
Epoch 2, cost is  1.65178
Epoch 3, cost is  1.60823
Epoch 4, cost is  1.61225
Training took 0.213080 minutes
Weight histogram
[1085 1012  987  872  661  571  363  256  170   98] [-0.11641122 -0.10500313 -0.09359504 -0.08218695 -0.07077886 -0.05937077
 -0.04796268 -0.03655459 -0.0251465  -0.01373841 -0.00233032]
[337 283 379 496 588 711 785 842 904 750] [-0.11641122 -0.10500313 -0.09359504 -0.08218695 -0.07077886 -0.05937077
 -0.04796268 -0.03655459 -0.0251465  -0.01373841 -0.00233032]
-6.67311
6.92695
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.056690 minutes
Epoch 0
Fine tuning took 0.057402 minutes
Epoch 0
Fine tuning took 0.058445 minutes
{'zero': {0: [0.12561576354679804, 0.22044334975369459, 0.11576354679802955, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76600985221674878, 0.63177339901477836, 0.62315270935960587, 0.66995073891625612], 5: [0.10837438423645321, 0.14778325123152711, 0.26108374384236455, 0.22783251231527094], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12561576354679804, 0.15517241379310345, 0.11822660098522167, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76600985221674878, 0.70935960591133007, 0.7142857142857143, 0.66871921182266014], 5: [0.10837438423645321, 0.1354679802955665, 0.16748768472906403, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12561576354679804, 0.13793103448275862, 0.14162561576354679, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76600985221674878, 0.7142857142857143, 0.69458128078817738, 0.67610837438423643], 5: [0.10837438423645321, 0.14778325123152711, 0.16379310344827586, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12561576354679804, 0.19581280788177341, 0.15147783251231528, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76600985221674878, 0.66256157635467983, 0.66133004926108374, 0.67733990147783252], 5: [0.10837438423645321, 0.14162561576354679, 0.18719211822660098, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151267 minutes
Weight histogram
[  64  180  216  441  476  529  992 1835 1162  180] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 117  118  170  214  279  417  695  826 1354 1885] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.771986
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  10.6186
Epoch 1, cost is  10.1769
Epoch 2, cost is  10.4653
Epoch 3, cost is  10.954
Epoch 4, cost is  11.502
Training took 0.114587 minutes
Weight histogram
[712 767 538 682 823 825 600 570 448 110] [-1.3382616  -1.20559794 -1.07293427 -0.94027061 -0.80760694 -0.67494328
 -0.54227961 -0.40961595 -0.27695228 -0.14428862 -0.01162495]
[278 477 562 616 756 711 690 675 666 644] [-1.3382616  -1.20559794 -1.07293427 -0.94027061 -0.80760694 -0.67494328
 -0.54227961 -0.40961595 -0.27695228 -0.14428862 -0.01162495]
-49.5877
57.885
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150410 minutes
Weight histogram
[  82  294  557  886  989 1020 1746 1835  596   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 234  243  343  417  617  870 1255  807 1317 1997] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.10857
0.785184
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  31.0794
Epoch 1, cost is  31.3811
Epoch 2, cost is  33.2914
Epoch 3, cost is  35.5231
Epoch 4, cost is  37.8584
Training took 0.085695 minutes
Weight histogram
[ 668  655  632  583  659  683  605  913 1802  900] [-2.69943786 -2.43055087 -2.16166389 -1.89277691 -1.62388993 -1.35500294
 -1.08611596 -0.81722898 -0.54834199 -0.27945501 -0.01056803]
[1438 1669  613  639  620  600  614  664  630  613] [-2.69943786 -2.43055087 -2.16166389 -1.89277691 -1.62388993 -1.35500294
 -1.08611596 -0.81722898 -0.54834199 -0.27945501 -0.01056803]
-94.987
110.865
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.68199
Epoch 1, cost is  5.15152
Epoch 2, cost is  5.7187
Epoch 3, cost is  6.35844
Epoch 4, cost is  6.87947
Training took 0.094847 minutes
Weight histogram
[ 230  273  259  432 2097 2043  712   10    4   15] [-0.73570311 -0.66296286 -0.5902226  -0.51748235 -0.44474209 -0.37200184
 -0.29926158 -0.22652133 -0.15378107 -0.08104082 -0.00830057]
[363 760 859 945 979 962 616 193 209 189] [-0.73570311 -0.66296286 -0.5902226  -0.51748235 -0.44474209 -0.37200184
 -0.29926158 -0.22652133 -0.15378107 -0.08104082 -0.00830057]
-65.292
57.1348
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045837 minutes
Epoch 0
Fine tuning took 0.044739 minutes
Epoch 0
Fine tuning took 0.046747 minutes
{'zero': {0: [0.47290640394088668, 0.33620689655172414, 0.28325123152709358, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.12438423645320197, 0.44334975369458129, 0.5357142857142857, 0.56157635467980294], 5: [0.40270935960591131, 0.22044334975369459, 0.18103448275862069, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.47290640394088668, 0.31896551724137934, 0.30172413793103448, 0.28078817733990147], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.12438423645320197, 0.47783251231527096, 0.51600985221674878, 0.53817733990147787], 5: [0.40270935960591131, 0.20320197044334976, 0.18226600985221675, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.47290640394088668, 0.32389162561576357, 0.27586206896551724, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.12438423645320197, 0.47660098522167488, 0.53694581280788178, 0.54679802955665024], 5: [0.40270935960591131, 0.19950738916256158, 0.18719211822660098, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.47290640394088668, 0.31773399014778325, 0.31650246305418717, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.12438423645320197, 0.49507389162561577, 0.50615763546798032, 0.52832512315270941], 5: [0.40270935960591131, 0.18719211822660098, 0.17733990147783252, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149655 minutes
Weight histogram
[  64  180  216  441  476  529  992 1835 1162  180] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 117  118  170  214  279  417  695  826 1354 1885] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.771986
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  10.6186
Epoch 1, cost is  10.1769
Epoch 2, cost is  10.4653
Epoch 3, cost is  10.954
Epoch 4, cost is  11.502
Training took 0.114344 minutes
Weight histogram
[712 767 538 682 823 825 600 570 448 110] [-1.3382616  -1.20559794 -1.07293427 -0.94027061 -0.80760694 -0.67494328
 -0.54227961 -0.40961595 -0.27695228 -0.14428862 -0.01162495]
[278 477 562 616 756 711 690 675 666 644] [-1.3382616  -1.20559794 -1.07293427 -0.94027061 -0.80760694 -0.67494328
 -0.54227961 -0.40961595 -0.27695228 -0.14428862 -0.01162495]
-49.5877
57.885
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150065 minutes
Weight histogram
[  82  294  557  886  989 1020 1746 1835  596   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 234  243  343  417  617  870 1255  807 1317 1997] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.10857
0.785184
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  31.0794
Epoch 1, cost is  31.3811
Epoch 2, cost is  33.2914
Epoch 3, cost is  35.5231
Epoch 4, cost is  37.8584
Training took 0.085323 minutes
Weight histogram
[ 668  655  632  583  659  683  605  913 1802  900] [-2.69943786 -2.43055087 -2.16166389 -1.89277691 -1.62388993 -1.35500294
 -1.08611596 -0.81722898 -0.54834199 -0.27945501 -0.01056803]
[1438 1669  613  639  620  600  614  664  630  613] [-2.69943786 -2.43055087 -2.16166389 -1.89277691 -1.62388993 -1.35500294
 -1.08611596 -0.81722898 -0.54834199 -0.27945501 -0.01056803]
-94.987
110.865
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.33873
Epoch 1, cost is  5.77987
Epoch 2, cost is  7.72676
Epoch 3, cost is  9.6995
Epoch 4, cost is  11.7516
Training took 0.107800 minutes
Weight histogram
[ 235  214  485  748 1210 1457 1301  393   15   17] [-0.75476682 -0.6806409  -0.60651497 -0.53238905 -0.45826312 -0.3841372
 -0.31001127 -0.23588535 -0.16175942 -0.08763349 -0.01350757]
[379 706 794 775 791 771 700 517 426 216] [-0.75476682 -0.6806409  -0.60651497 -0.53238905 -0.45826312 -0.3841372
 -0.31001127 -0.23588535 -0.16175942 -0.08763349 -0.01350757]
-65.1493
48.5874
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048211 minutes
Epoch 0
Fine tuning took 0.047410 minutes
Epoch 0
Fine tuning took 0.047309 minutes
{'zero': {0: [0.29802955665024633, 0.24014778325123154, 0.21798029556650247, 0.31403940886699505], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52216748768472909, 0.55788177339901479, 0.68965517241379315, 0.58497536945812811], 5: [0.17980295566502463, 0.2019704433497537, 0.092364532019704432, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29802955665024633, 0.20566502463054187, 0.21551724137931033, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52216748768472909, 0.67610837438423643, 0.69211822660098521, 0.66379310344827591], 5: [0.17980295566502463, 0.11822660098522167, 0.092364532019704432, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29802955665024633, 0.19581280788177341, 0.19088669950738915, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52216748768472909, 0.68719211822660098, 0.70073891625615758, 0.64162561576354682], 5: [0.17980295566502463, 0.11699507389162561, 0.10837438423645321, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29802955665024633, 0.18596059113300492, 0.19827586206896552, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52216748768472909, 0.70443349753694584, 0.67980295566502458, 0.6785714285714286], 5: [0.17980295566502463, 0.10960591133004927, 0.12192118226600986, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149939 minutes
Weight histogram
[  64  180  216  441  476  529  992 1835 1162  180] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 117  118  170  214  279  417  695  826 1354 1885] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.771986
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  10.6186
Epoch 1, cost is  10.1769
Epoch 2, cost is  10.4653
Epoch 3, cost is  10.954
Epoch 4, cost is  11.502
Training took 0.114417 minutes
Weight histogram
[712 767 538 682 823 825 600 570 448 110] [-1.3382616  -1.20559794 -1.07293427 -0.94027061 -0.80760694 -0.67494328
 -0.54227961 -0.40961595 -0.27695228 -0.14428862 -0.01162495]
[278 477 562 616 756 711 690 675 666 644] [-1.3382616  -1.20559794 -1.07293427 -0.94027061 -0.80760694 -0.67494328
 -0.54227961 -0.40961595 -0.27695228 -0.14428862 -0.01162495]
-49.5877
57.885
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149947 minutes
Weight histogram
[  82  294  557  886  989 1020 1746 1835  596   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 234  243  343  417  617  870 1255  807 1317 1997] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.10857
0.785184
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  31.0794
Epoch 1, cost is  31.3811
Epoch 2, cost is  33.2914
Epoch 3, cost is  35.5231
Epoch 4, cost is  37.8584
Training took 0.087931 minutes
Weight histogram
[ 668  655  632  583  659  683  605  913 1802  900] [-2.69943786 -2.43055087 -2.16166389 -1.89277691 -1.62388993 -1.35500294
 -1.08611596 -0.81722898 -0.54834199 -0.27945501 -0.01056803]
[1438 1669  613  639  620  600  614  664  630  613] [-2.69943786 -2.43055087 -2.16166389 -1.89277691 -1.62388993 -1.35500294
 -1.08611596 -0.81722898 -0.54834199 -0.27945501 -0.01056803]
-94.987
110.865
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.63581
Epoch 1, cost is  3.93276
Epoch 2, cost is  4.73779
Epoch 3, cost is  5.61985
Epoch 4, cost is  6.56547
Training took 0.151203 minutes
Weight histogram
[483 792 866 784 842 814 763 581 106  44] [-0.64533269 -0.58234556 -0.51935843 -0.4563713  -0.39338416 -0.33039703
 -0.2674099  -0.20442277 -0.14143563 -0.0784485  -0.01546137]
[301 502 635 663 687 684 688 702 695 518] [-0.64533269 -0.58234556 -0.51935843 -0.4563713  -0.39338416 -0.33039703
 -0.2674099  -0.20442277 -0.14143563 -0.0784485  -0.01546137]
-35.0402
42.436
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049029 minutes
Epoch 0
Fine tuning took 0.051081 minutes
Epoch 0
Fine tuning took 0.049150 minutes
{'zero': {0: [0.2376847290640394, 0.19581280788177341, 0.42857142857142855, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.52955665024630538, 0.52832512315270941, 0.57635467980295563], 5: [0.18226600985221675, 0.27463054187192121, 0.043103448275862072, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2376847290640394, 0.23029556650246305, 0.24630541871921183, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.63916256157635465, 0.66748768472906406, 0.59113300492610843], 5: [0.18226600985221675, 0.13054187192118227, 0.086206896551724144, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2376847290640394, 0.20566502463054187, 0.2376847290640394, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.69088669950738912, 0.63423645320197042, 0.63054187192118227], 5: [0.18226600985221675, 0.10344827586206896, 0.12807881773399016, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2376847290640394, 0.31650246305418717, 0.28694581280788178, 0.42241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.51724137931034486, 0.6071428571428571, 0.45320197044334976], 5: [0.18226600985221675, 0.16625615763546797, 0.10591133004926108, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150409 minutes
Weight histogram
[  64  180  216  441  476  529  992 1835 1162  180] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
[ 117  118  170  214  279  417  695  826 1354 1885] [ -2.39681889e-04  -4.49233310e-05   1.49835227e-04   3.44593785e-04
   5.39352343e-04   7.34110901e-04   9.28869459e-04   1.12362802e-03
   1.31838658e-03   1.51314513e-03   1.70790369e-03]
-1.13845
0.771986
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  10.6186
Epoch 1, cost is  10.1769
Epoch 2, cost is  10.4653
Epoch 3, cost is  10.954
Epoch 4, cost is  11.502
Training took 0.115162 minutes
Weight histogram
[712 767 538 682 823 825 600 570 448 110] [-1.3382616  -1.20559794 -1.07293427 -0.94027061 -0.80760694 -0.67494328
 -0.54227961 -0.40961595 -0.27695228 -0.14428862 -0.01162495]
[278 477 562 616 756 711 690 675 666 644] [-1.3382616  -1.20559794 -1.07293427 -0.94027061 -0.80760694 -0.67494328
 -0.54227961 -0.40961595 -0.27695228 -0.14428862 -0.01162495]
-49.5877
57.885
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149747 minutes
Weight histogram
[  82  294  557  886  989 1020 1746 1835  596   95] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
[ 234  243  343  417  617  870 1255  807 1317 1997] [ -2.42434355e-04  -6.52131188e-05   1.12008117e-04   2.89229353e-04
   4.66450589e-04   6.43671825e-04   8.20893061e-04   9.98114297e-04
   1.17533553e-03   1.35255677e-03   1.52977800e-03]
-1.10857
0.785184
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  31.0794
Epoch 1, cost is  31.3811
Epoch 2, cost is  33.2914
Epoch 3, cost is  35.5231
Epoch 4, cost is  37.8584
Training took 0.088571 minutes
Weight histogram
[ 668  655  632  583  659  683  605  913 1802  900] [-2.69943786 -2.43055087 -2.16166389 -1.89277691 -1.62388993 -1.35500294
 -1.08611596 -0.81722898 -0.54834199 -0.27945501 -0.01056803]
[1438 1669  613  639  620  600  614  664  630  613] [-2.69943786 -2.43055087 -2.16166389 -1.89277691 -1.62388993 -1.35500294
 -1.08611596 -0.81722898 -0.54834199 -0.27945501 -0.01056803]
-94.987
110.865
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  2.75359
Epoch 1, cost is  2.26518
Epoch 2, cost is  2.32079
Epoch 3, cost is  2.4731
Epoch 4, cost is  2.64617
Training took 0.215106 minutes
Weight histogram
[700 987 932 889 861 696 525 329 101  55] [-0.41190013 -0.37225633 -0.33261253 -0.29296872 -0.25332492 -0.21368112
 -0.17403732 -0.13439351 -0.09474971 -0.05510591 -0.01546211]
[257 380 521 618 688 733 786 801 838 453] [-0.41190013 -0.37225633 -0.33261253 -0.29296872 -0.25332492 -0.21368112
 -0.17403732 -0.13439351 -0.09474971 -0.05510591 -0.01546211]
-25.1671
29.4731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.056120 minutes
Epoch 0
Fine tuning took 0.056617 minutes
Epoch 0
Fine tuning took 0.056083 minutes
{'zero': {0: [0.18472906403940886, 0.29310344827586204, 0.3608374384236453, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.63423645320197042, 0.58990147783251234, 0.58497536945812811], 5: [0.071428571428571425, 0.072660098522167482, 0.049261083743842367, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18472906403940886, 0.27216748768472904, 0.25, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.63916256157635465, 0.67364532019704437, 0.6785714285714286], 5: [0.071428571428571425, 0.088669950738916259, 0.076354679802955669, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18472906403940886, 0.25985221674876846, 0.2229064039408867, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.64408866995073888, 0.69458128078817738, 0.68596059113300489], 5: [0.071428571428571425, 0.096059113300492605, 0.082512315270935957, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18472906403940886, 0.30541871921182268, 0.24384236453201971, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.57266009852216748, 0.6711822660098522, 0.65640394088669951], 5: [0.071428571428571425, 0.12192118226600986, 0.084975369458128072, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151276 minutes
Weight histogram
[  89  245  443  549  647 1500 2305 1369  769  184] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
[ 123  131  179  238  346  499  817  982 1830 2955] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
-1.2276
0.855174
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81736
Epoch 1, cost is  3.59782
Epoch 2, cost is  3.609
Epoch 3, cost is  3.66059
Epoch 4, cost is  3.74314
Training took 0.117340 minutes
Weight histogram
[1110  911 1018  870  977 1356  912  599  264   83] [-0.33641014 -0.30299743 -0.26958473 -0.23617203 -0.20275933 -0.16934662
 -0.13593392 -0.10252122 -0.06910852 -0.03569582 -0.00228311]
[ 243  463  678  902 1023 1002  919  918  985  967] [-0.33641014 -0.30299743 -0.26958473 -0.23617203 -0.20275933 -0.16934662
 -0.13593392 -0.10252122 -0.06910852 -0.03569582 -0.00228311]
-16.038
19.858
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149246 minutes
Weight histogram
[ 106  399  715 1156 1023 2036 2668 1488  478   56] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
[ 244  265  375  448  729 1052 1169  923 1838 3082] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
-1.28146
0.884157
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.69869
Epoch 1, cost is  9.56139
Epoch 2, cost is  9.83273
Epoch 3, cost is  10.2068
Epoch 4, cost is  10.6394
Training took 0.090040 minutes
Weight histogram
[ 908  937  824  914  864  974 1016  823 2246  619] [-0.71921438 -0.64750095 -0.57578752 -0.50407408 -0.43236065 -0.36064722
 -0.28893379 -0.21722035 -0.14550692 -0.07379349 -0.00208006]
[1295 1850  808  947  913  920  882  870  832  808] [-0.71921438 -0.64750095 -0.57578752 -0.50407408 -0.43236065 -0.36064722
 -0.28893379 -0.21722035 -0.14550692 -0.07379349 -0.00208006]
-27.5061
27.1594
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.93463
Epoch 1, cost is  4.39483
Epoch 2, cost is  4.72764
Epoch 3, cost is  5.24662
Epoch 4, cost is  5.70068
Training took 0.093700 minutes
Weight histogram
[1168 1337 1265 1251 1166  793  484  250  193  193] [-0.34779999 -0.31324755 -0.27869512 -0.24414269 -0.20959025 -0.17503782
 -0.14048538 -0.10593295 -0.07138052 -0.03682808 -0.00227565]
[ 375  453  583  764  864  972  963 1009 1048 1069] [-0.34779999 -0.31324755 -0.27869512 -0.24414269 -0.20959025 -0.17503782
 -0.14048538 -0.10593295 -0.07138052 -0.03682808 -0.00227565]
-15.2706
19.9534
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046147 minutes
Epoch 0
Fine tuning took 0.048111 minutes
Epoch 0
Fine tuning took 0.046682 minutes
{'zero': {0: [0.45443349753694579, 0.35714285714285715, 0.3645320197044335, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.54802955665024633, 0.50123152709359609, 0.53817733990147787], 5: [0.11699507389162561, 0.094827586206896547, 0.13423645320197045, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.45443349753694579, 0.34729064039408869, 0.36576354679802958, 0.34359605911330049], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.55049261083743839, 0.54433497536945807, 0.53694581280788178], 5: [0.11699507389162561, 0.10221674876847291, 0.089901477832512317, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.45443349753694579, 0.37807881773399016, 0.35591133004926107, 0.32142857142857145], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.54926108374384242, 0.54187192118226601, 0.55665024630541871], 5: [0.11699507389162561, 0.072660098522167482, 0.10221674876847291, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.45443349753694579, 0.34975369458128081, 0.35837438423645318, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.55295566502463056, 0.54433497536945807, 0.54433497536945807], 5: [0.11699507389162561, 0.097290640394088676, 0.097290640394088676, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149176 minutes
Weight histogram
[  89  245  443  549  647 1500 2305 1369  769  184] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
[ 123  131  179  238  346  499  817  982 1830 2955] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
-1.2276
0.855174
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81736
Epoch 1, cost is  3.59782
Epoch 2, cost is  3.609
Epoch 3, cost is  3.66059
Epoch 4, cost is  3.74314
Training took 0.117938 minutes
Weight histogram
[1110  911 1018  870  977 1356  912  599  264   83] [-0.33641014 -0.30299743 -0.26958473 -0.23617203 -0.20275933 -0.16934662
 -0.13593392 -0.10252122 -0.06910852 -0.03569582 -0.00228311]
[ 243  463  678  902 1023 1002  919  918  985  967] [-0.33641014 -0.30299743 -0.26958473 -0.23617203 -0.20275933 -0.16934662
 -0.13593392 -0.10252122 -0.06910852 -0.03569582 -0.00228311]
-16.038
19.858
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151495 minutes
Weight histogram
[ 106  399  715 1156 1023 2036 2668 1488  478   56] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
[ 244  265  375  448  729 1052 1169  923 1838 3082] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
-1.28146
0.884157
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.69869
Epoch 1, cost is  9.56139
Epoch 2, cost is  9.83273
Epoch 3, cost is  10.2068
Epoch 4, cost is  10.6394
Training took 0.091014 minutes
Weight histogram
[ 908  937  824  914  864  974 1016  823 2246  619] [-0.71921438 -0.64750095 -0.57578752 -0.50407408 -0.43236065 -0.36064722
 -0.28893379 -0.21722035 -0.14550692 -0.07379349 -0.00208006]
[1295 1850  808  947  913  920  882  870  832  808] [-0.71921438 -0.64750095 -0.57578752 -0.50407408 -0.43236065 -0.36064722
 -0.28893379 -0.21722035 -0.14550692 -0.07379349 -0.00208006]
-27.5061
27.1594
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.52712
Epoch 1, cost is  3.74922
Epoch 2, cost is  3.93527
Epoch 3, cost is  4.36203
Epoch 4, cost is  4.86931
Training took 0.108973 minutes
Weight histogram
[1024 1269 1194 1123 1151  958  620  326  220  215] [-0.28246394 -0.25444985 -0.22643575 -0.19842166 -0.17040757 -0.14239347
 -0.11437938 -0.08636529 -0.05835119 -0.0303371  -0.00232301]
[ 412  492  655  740  839  961 1003 1031 1041  926] [-0.28246394 -0.25444985 -0.22643575 -0.19842166 -0.17040757 -0.14239347
 -0.11437938 -0.08636529 -0.05835119 -0.0303371  -0.00232301]
-9.80263
13.4142
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046874 minutes
Epoch 0
Fine tuning took 0.048177 minutes
Epoch 0
Fine tuning took 0.049618 minutes
{'zero': {0: [0.41256157635467983, 0.33620689655172414, 0.45689655172413796, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48152709359605911, 0.55665024630541871, 0.42857142857142855, 0.50615763546798032], 5: [0.10591133004926108, 0.10714285714285714, 0.1145320197044335, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.41256157635467983, 0.2857142857142857, 0.28448275862068967, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48152709359605911, 0.58743842364532017, 0.58866995073891626, 0.6576354679802956], 5: [0.10591133004926108, 0.1268472906403941, 0.1268472906403941, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.41256157635467983, 0.30295566502463056, 0.29187192118226601, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48152709359605911, 0.61083743842364535, 0.6071428571428571, 0.60344827586206895], 5: [0.10591133004926108, 0.086206896551724144, 0.10098522167487685, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.41256157635467983, 0.23645320197044334, 0.26231527093596058, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48152709359605911, 0.66502463054187189, 0.59975369458128081, 0.66379310344827591], 5: [0.10591133004926108, 0.098522167487684734, 0.13793103448275862, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148571 minutes
Weight histogram
[  89  245  443  549  647 1500 2305 1369  769  184] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
[ 123  131  179  238  346  499  817  982 1830 2955] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
-1.2276
0.855174
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81736
Epoch 1, cost is  3.59782
Epoch 2, cost is  3.609
Epoch 3, cost is  3.66059
Epoch 4, cost is  3.74314
Training took 0.117237 minutes
Weight histogram
[1110  911 1018  870  977 1356  912  599  264   83] [-0.33641014 -0.30299743 -0.26958473 -0.23617203 -0.20275933 -0.16934662
 -0.13593392 -0.10252122 -0.06910852 -0.03569582 -0.00228311]
[ 243  463  678  902 1023 1002  919  918  985  967] [-0.33641014 -0.30299743 -0.26958473 -0.23617203 -0.20275933 -0.16934662
 -0.13593392 -0.10252122 -0.06910852 -0.03569582 -0.00228311]
-16.038
19.858
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150619 minutes
Weight histogram
[ 106  399  715 1156 1023 2036 2668 1488  478   56] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
[ 244  265  375  448  729 1052 1169  923 1838 3082] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
-1.28146
0.884157
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.69869
Epoch 1, cost is  9.56139
Epoch 2, cost is  9.83273
Epoch 3, cost is  10.2068
Epoch 4, cost is  10.6394
Training took 0.088820 minutes
Weight histogram
[ 908  937  824  914  864  974 1016  823 2246  619] [-0.71921438 -0.64750095 -0.57578752 -0.50407408 -0.43236065 -0.36064722
 -0.28893379 -0.21722035 -0.14550692 -0.07379349 -0.00208006]
[1295 1850  808  947  913  920  882  870  832  808] [-0.71921438 -0.64750095 -0.57578752 -0.50407408 -0.43236065 -0.36064722
 -0.28893379 -0.21722035 -0.14550692 -0.07379349 -0.00208006]
-27.5061
27.1594
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.95429
Epoch 1, cost is  2.57207
Epoch 2, cost is  2.49821
Epoch 3, cost is  2.66343
Epoch 4, cost is  2.82026
Training took 0.150550 minutes
Weight histogram
[1115 1384 1137 1215  940  873  650  366  240  180] [-0.18184739 -0.16389356 -0.14593972 -0.12798588 -0.11003205 -0.09207821
 -0.07412438 -0.05617054 -0.0382167  -0.02026287 -0.00230903]
[ 456  443  577  725  819  952 1002 1028 1100  998] [-0.18184739 -0.16389356 -0.14593972 -0.12798588 -0.11003205 -0.09207821
 -0.07412438 -0.05617054 -0.0382167  -0.02026287 -0.00230903]
-7.39972
9.17823
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052614 minutes
Epoch 0
Fine tuning took 0.051249 minutes
Epoch 0
Fine tuning took 0.054125 minutes
{'zero': {0: [0.27093596059113301, 0.4605911330049261, 0.34359605911330049, 0.37807881773399016], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.44458128078817732, 0.55788177339901479, 0.56773399014778325], 5: [0.098522167487684734, 0.094827586206896547, 0.098522167487684734, 0.054187192118226604], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27093596059113301, 0.28325123152709358, 0.23522167487684728, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.61699507389162567, 0.69334975369458129, 0.6428571428571429], 5: [0.098522167487684734, 0.099753694581280791, 0.071428571428571425, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27093596059113301, 0.31527093596059114, 0.24384236453201971, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.59482758620689657, 0.64655172413793105, 0.63423645320197042], 5: [0.098522167487684734, 0.089901477832512317, 0.10960591133004927, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27093596059113301, 0.32266009852216748, 0.24507389162561577, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.59729064039408863, 0.66379310344827591, 0.6219211822660099], 5: [0.098522167487684734, 0.080049261083743842, 0.091133004926108374, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149718 minutes
Weight histogram
[  89  245  443  549  647 1500 2305 1369  769  184] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
[ 123  131  179  238  346  499  817  982 1830 2955] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
-1.2276
0.855174
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.81736
Epoch 1, cost is  3.59782
Epoch 2, cost is  3.609
Epoch 3, cost is  3.66059
Epoch 4, cost is  3.74314
Training took 0.117889 minutes
Weight histogram
[1110  911 1018  870  977 1356  912  599  264   83] [-0.33641014 -0.30299743 -0.26958473 -0.23617203 -0.20275933 -0.16934662
 -0.13593392 -0.10252122 -0.06910852 -0.03569582 -0.00228311]
[ 243  463  678  902 1023 1002  919  918  985  967] [-0.33641014 -0.30299743 -0.26958473 -0.23617203 -0.20275933 -0.16934662
 -0.13593392 -0.10252122 -0.06910852 -0.03569582 -0.00228311]
-16.038
19.858
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150854 minutes
Weight histogram
[ 106  399  715 1156 1023 2036 2668 1488  478   56] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
[ 244  265  375  448  729 1052 1169  923 1838 3082] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
-1.28146
0.884157
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.69869
Epoch 1, cost is  9.56139
Epoch 2, cost is  9.83273
Epoch 3, cost is  10.2068
Epoch 4, cost is  10.6394
Training took 0.087700 minutes
Weight histogram
[ 908  937  824  914  864  974 1016  823 2246  619] [-0.71921438 -0.64750095 -0.57578752 -0.50407408 -0.43236065 -0.36064722
 -0.28893379 -0.21722035 -0.14550692 -0.07379349 -0.00208006]
[1295 1850  808  947  913  920  882  870  832  808] [-0.71921438 -0.64750095 -0.57578752 -0.50407408 -0.43236065 -0.36064722
 -0.28893379 -0.21722035 -0.14550692 -0.07379349 -0.00208006]
-27.5061
27.1594
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.64236
Epoch 1, cost is  1.94189
Epoch 2, cost is  1.67558
Epoch 3, cost is  1.63344
Epoch 4, cost is  1.65348
Training took 0.212638 minutes
Weight histogram
[1394 1359 1331 1165  876  785  491  345  224  130] [-0.11641122 -0.10500191 -0.09359261 -0.0821833  -0.070774   -0.05936469
 -0.04795539 -0.03654608 -0.02513678 -0.01372747 -0.00231817]
[ 448  381  511  667  788  950 1048 1115 1199  993] [-0.11641122 -0.10500191 -0.09359261 -0.0821833  -0.070774   -0.05936469
 -0.04795539 -0.03654608 -0.02513678 -0.01372747 -0.00231817]
-6.67311
7.03904
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.055574 minutes
Epoch 0
Fine tuning took 0.056646 minutes
Epoch 0
Fine tuning took 0.056561 minutes
{'zero': {0: [0.16379310344827586, 0.24507389162561577, 0.33374384236453203, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.6354679802955665, 0.60344827586206895, 0.68842364532019706], 5: [0.098522167487684734, 0.11945812807881774, 0.062807881773399021, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16379310344827586, 0.23029556650246305, 0.23029556650246305, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.62931034482758619, 0.66133004926108374, 0.68226600985221675], 5: [0.098522167487684734, 0.14039408866995073, 0.10837438423645321, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16379310344827586, 0.21798029556650247, 0.2229064039408867, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.65517241379310343, 0.66009852216748766, 0.6576354679802956], 5: [0.098522167487684734, 0.1268472906403941, 0.11699507389162561, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16379310344827586, 0.22413793103448276, 0.26354679802955666, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.6576354679802956, 0.66009852216748766, 0.76108374384236455], 5: [0.098522167487684734, 0.11822660098522167, 0.076354679802955669, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150101 minutes
Weight histogram
[  89  245  443  549  647 1500 2305 1369  769  184] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
[ 123  131  179  238  346  499  817  982 1830 2955] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
-1.2276
0.855174
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  15.0257
Epoch 1, cost is  14.0654
Epoch 2, cost is  14.1421
Epoch 3, cost is  14.4207
Epoch 4, cost is  14.8403
Training took 0.113175 minutes
Weight histogram
[ 915  959  767  910  769 1059 1015  811  670  225] [-1.74919903 -1.57544162 -1.40168422 -1.22792681 -1.0541694  -0.88041199
 -0.70665458 -0.53289718 -0.35913977 -0.18538236 -0.01162495]
[411 692 787 965 922 891 866 859 860 847] [-1.74919903 -1.57544162 -1.40168422 -1.22792681 -1.0541694  -0.88041199
 -0.70665458 -0.53289718 -0.35913977 -0.18538236 -0.01162495]
-71.2848
85.4943
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150701 minutes
Weight histogram
[ 106  399  715 1156 1023 2036 2668 1488  478   56] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
[ 244  265  375  448  729 1052 1169  923 1838 3082] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
-1.28146
0.884157
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  47.418
Epoch 1, cost is  46.699
Epoch 2, cost is  48.1814
Epoch 3, cost is  50.3399
Epoch 4, cost is  52.7026
Training took 0.088105 minutes
Weight histogram
[ 800  824  706  842  884  858  886  902 1871 1552] [-3.69937372 -3.33049315 -2.96161258 -2.59273201 -2.22385144 -1.85497088
 -1.48609031 -1.11720974 -0.74832917 -0.3794486  -0.01056803]
[2100 1428  847  826  795  890  841  824  794  780] [-3.69937372 -3.33049315 -2.96161258 -2.59273201 -2.22385144 -1.85497088
 -1.48609031 -1.11720974 -0.74832917 -0.3794486  -0.01056803]
-125.731
135.665
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.75619
Epoch 1, cost is  5.24093
Epoch 2, cost is  5.70913
Epoch 3, cost is  6.01885
Epoch 4, cost is  6.66993
Training took 0.094557 minutes
Weight histogram
[ 231  273  258  433 2491 3499  879   11    5   20] [-0.73570311 -0.66290082 -0.59009854 -0.51729625 -0.44449396 -0.37169168
 -0.29888939 -0.2260871  -0.15328482 -0.08048253 -0.00768024]
[ 505 1090 1247 1375 1364 1312  616  193  209  189] [-0.73570311 -0.66290082 -0.59009854 -0.51729625 -0.44449396 -0.37169168
 -0.29888939 -0.2260871  -0.15328482 -0.08048253 -0.00768024]
-66.4183
61.5511
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046163 minutes
Epoch 0
Fine tuning took 0.046261 minutes
Epoch 0
Fine tuning took 0.045649 minutes
{'zero': {0: [0.18596059113300492, 0.26354679802955666, 0.26108374384236455, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.58128078817733986, 0.59729064039408863, 0.61083743842364535], 5: [0.1748768472906404, 0.15517241379310345, 0.14162561576354679, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18596059113300492, 0.25, 0.26108374384236455, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.58990147783251234, 0.60344827586206895, 0.61576354679802958], 5: [0.1748768472906404, 0.16009852216748768, 0.1354679802955665, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18596059113300492, 0.25246305418719212, 0.26108374384236455, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.61206896551724133, 0.59975369458128081, 0.61576354679802958], 5: [0.1748768472906404, 0.1354679802955665, 0.13916256157635468, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18596059113300492, 0.2376847290640394, 0.24753694581280788, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.62931034482758619, 0.59482758620689657, 0.61822660098522164], 5: [0.1748768472906404, 0.13300492610837439, 0.15763546798029557, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148400 minutes
Weight histogram
[  89  245  443  549  647 1500 2305 1369  769  184] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
[ 123  131  179  238  346  499  817  982 1830 2955] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
-1.2276
0.855174
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  15.0257
Epoch 1, cost is  14.0654
Epoch 2, cost is  14.1421
Epoch 3, cost is  14.4207
Epoch 4, cost is  14.8403
Training took 0.114195 minutes
Weight histogram
[ 915  959  767  910  769 1059 1015  811  670  225] [-1.74919903 -1.57544162 -1.40168422 -1.22792681 -1.0541694  -0.88041199
 -0.70665458 -0.53289718 -0.35913977 -0.18538236 -0.01162495]
[411 692 787 965 922 891 866 859 860 847] [-1.74919903 -1.57544162 -1.40168422 -1.22792681 -1.0541694  -0.88041199
 -0.70665458 -0.53289718 -0.35913977 -0.18538236 -0.01162495]
-71.2848
85.4943
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148429 minutes
Weight histogram
[ 106  399  715 1156 1023 2036 2668 1488  478   56] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
[ 244  265  375  448  729 1052 1169  923 1838 3082] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
-1.28146
0.884157
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  47.418
Epoch 1, cost is  46.699
Epoch 2, cost is  48.1814
Epoch 3, cost is  50.3399
Epoch 4, cost is  52.7026
Training took 0.088693 minutes
Weight histogram
[ 800  824  706  842  884  858  886  902 1871 1552] [-3.69937372 -3.33049315 -2.96161258 -2.59273201 -2.22385144 -1.85497088
 -1.48609031 -1.11720974 -0.74832917 -0.3794486  -0.01056803]
[2100 1428  847  826  795  890  841  824  794  780] [-3.69937372 -3.33049315 -2.96161258 -2.59273201 -2.22385144 -1.85497088
 -1.48609031 -1.11720974 -0.74832917 -0.3794486  -0.01056803]
-125.731
135.665
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.23279
Epoch 1, cost is  5.23712
Epoch 2, cost is  6.69812
Epoch 3, cost is  8.37352
Epoch 4, cost is  10.188
Training took 0.111177 minutes
Weight histogram
[ 207  417  501  839 1161 1710 2000 1067  173   25] [-0.86838162 -0.78289421 -0.69740681 -0.6119194  -0.526432   -0.44094459
 -0.35545719 -0.26996978 -0.18448238 -0.09899497 -0.01350757]
[ 553  991 1098 1090 1065 1037  883  688  466  229] [-0.86838162 -0.78289421 -0.69740681 -0.6119194  -0.526432   -0.44094459
 -0.35545719 -0.26996978 -0.18448238 -0.09899497 -0.01350757]
-65.1493
50.5944
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046377 minutes
Epoch 0
Fine tuning took 0.044976 minutes
Epoch 0
Fine tuning took 0.045807 minutes
{'zero': {0: [0.084975369458128072, 0.28694581280788178, 0.34975369458128081, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.6280788177339901, 0.5357142857142857, 0.67364532019704437], 5: [0.29433497536945813, 0.084975369458128072, 0.1145320197044335, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.084975369458128072, 0.27955665024630544, 0.34113300492610837, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.6071428571428571, 0.52463054187192115, 0.61945812807881773], 5: [0.29433497536945813, 0.11330049261083744, 0.13423645320197045, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.084975369458128072, 0.30418719211822659, 0.3608374384236453, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.57512315270935965, 0.52339901477832518, 0.65517241379310343], 5: [0.29433497536945813, 0.1206896551724138, 0.11576354679802955, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.084975369458128072, 0.32019704433497537, 0.33251231527093594, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.56403940886699511, 0.5431034482758621, 0.6428571428571429], 5: [0.29433497536945813, 0.11576354679802955, 0.12438423645320197, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151683 minutes
Weight histogram
[  89  245  443  549  647 1500 2305 1369  769  184] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
[ 123  131  179  238  346  499  817  982 1830 2955] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
-1.2276
0.855174
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  15.0257
Epoch 1, cost is  14.0654
Epoch 2, cost is  14.1421
Epoch 3, cost is  14.4207
Epoch 4, cost is  14.8403
Training took 0.112094 minutes
Weight histogram
[ 915  959  767  910  769 1059 1015  811  670  225] [-1.74919903 -1.57544162 -1.40168422 -1.22792681 -1.0541694  -0.88041199
 -0.70665458 -0.53289718 -0.35913977 -0.18538236 -0.01162495]
[411 692 787 965 922 891 866 859 860 847] [-1.74919903 -1.57544162 -1.40168422 -1.22792681 -1.0541694  -0.88041199
 -0.70665458 -0.53289718 -0.35913977 -0.18538236 -0.01162495]
-71.2848
85.4943
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151597 minutes
Weight histogram
[ 106  399  715 1156 1023 2036 2668 1488  478   56] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
[ 244  265  375  448  729 1052 1169  923 1838 3082] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
-1.28146
0.884157
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  47.418
Epoch 1, cost is  46.699
Epoch 2, cost is  48.1814
Epoch 3, cost is  50.3399
Epoch 4, cost is  52.7026
Training took 0.087686 minutes
Weight histogram
[ 800  824  706  842  884  858  886  902 1871 1552] [-3.69937372 -3.33049315 -2.96161258 -2.59273201 -2.22385144 -1.85497088
 -1.48609031 -1.11720974 -0.74832917 -0.3794486  -0.01056803]
[2100 1428  847  826  795  890  841  824  794  780] [-3.69937372 -3.33049315 -2.96161258 -2.59273201 -2.22385144 -1.85497088
 -1.48609031 -1.11720974 -0.74832917 -0.3794486  -0.01056803]
-125.731
135.665
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.94579
Epoch 1, cost is  4.57546
Epoch 2, cost is  5.64235
Epoch 3, cost is  6.82149
Epoch 4, cost is  7.79809
Training took 0.149400 minutes
Weight histogram
[ 572 1072 1151 1058 1138 1131 1046  766  115   51] [-0.64533269 -0.58232617 -0.51931965 -0.45631313 -0.39330661 -0.33030008
 -0.26729356 -0.20428704 -0.14128052 -0.07827399 -0.01526747]
[405 685 851 883 908 896 900 918 914 740] [-0.64533269 -0.58232617 -0.51931965 -0.45631313 -0.39330661 -0.33030008
 -0.26729356 -0.20428704 -0.14128052 -0.07827399 -0.01526747]
-44.2971
42.436
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050952 minutes
Epoch 0
Fine tuning took 0.049453 minutes
Epoch 0
Fine tuning took 0.051661 minutes
{'zero': {0: [0.30418719211822659, 0.48275862068965519, 0.26724137931034481, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.35714285714285715, 0.63300492610837433, 0.58620689655172409], 5: [0.10960591133004927, 0.16009852216748768, 0.099753694581280791, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30418719211822659, 0.17980295566502463, 0.19704433497536947, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.69334975369458129, 0.66502463054187189, 0.65640394088669951], 5: [0.10960591133004927, 0.1268472906403941, 0.13793103448275862, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30418719211822659, 0.16995073891625614, 0.17857142857142858, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.67980295566502458, 0.68842364532019706, 0.66256157635467983], 5: [0.10960591133004927, 0.15024630541871922, 0.13300492610837439, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30418719211822659, 0.17857142857142858, 0.19950738916256158, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58620689655172409, 0.70812807881773399, 0.70566502463054193, 0.65517241379310343], 5: [0.10960591133004927, 0.11330049261083744, 0.094827586206896547, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150323 minutes
Weight histogram
[  89  245  443  549  647 1500 2305 1369  769  184] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
[ 123  131  179  238  346  499  817  982 1830 2955] [ -2.39681889e-04  -9.46779619e-07   2.37788330e-04   4.76523439e-04
   7.15258549e-04   9.53993658e-04   1.19272877e-03   1.43146388e-03
   1.67019899e-03   1.90893410e-03   2.14766921e-03]
-1.2276
0.855174
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  15.0257
Epoch 1, cost is  14.0654
Epoch 2, cost is  14.1421
Epoch 3, cost is  14.4207
Epoch 4, cost is  14.8403
Training took 0.112801 minutes
Weight histogram
[ 915  959  767  910  769 1059 1015  811  670  225] [-1.74919903 -1.57544162 -1.40168422 -1.22792681 -1.0541694  -0.88041199
 -0.70665458 -0.53289718 -0.35913977 -0.18538236 -0.01162495]
[411 692 787 965 922 891 866 859 860 847] [-1.74919903 -1.57544162 -1.40168422 -1.22792681 -1.0541694  -0.88041199
 -0.70665458 -0.53289718 -0.35913977 -0.18538236 -0.01162495]
-71.2848
85.4943
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149412 minutes
Weight histogram
[ 106  399  715 1156 1023 2036 2668 1488  478   56] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
[ 244  265  375  448  729 1052 1169  923 1838 3082] [ -2.42434355e-04  -4.20857017e-05   1.58262951e-04   3.58611604e-04
   5.58960257e-04   7.59308910e-04   9.59657563e-04   1.16000622e-03
   1.36035487e-03   1.56070352e-03   1.76105218e-03]
-1.28146
0.884157
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  47.418
Epoch 1, cost is  46.699
Epoch 2, cost is  48.1814
Epoch 3, cost is  50.3399
Epoch 4, cost is  52.7026
Training took 0.086368 minutes
Weight histogram
[ 800  824  706  842  884  858  886  902 1871 1552] [-3.69937372 -3.33049315 -2.96161258 -2.59273201 -2.22385144 -1.85497088
 -1.48609031 -1.11720974 -0.74832917 -0.3794486  -0.01056803]
[2100 1428  847  826  795  890  841  824  794  780] [-3.69937372 -3.33049315 -2.96161258 -2.59273201 -2.22385144 -1.85497088
 -1.48609031 -1.11720974 -0.74832917 -0.3794486  -0.01056803]
-125.731
135.665
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  2.92156
Epoch 1, cost is  2.50277
Epoch 2, cost is  2.63307
Epoch 3, cost is  2.8763
Epoch 4, cost is  3.14764
Training took 0.215907 minutes
Weight histogram
[ 999 1281 1247 1166 1139  944  705  423  118   78] [-0.41190013 -0.37224556 -0.332591   -0.29293643 -0.25328186 -0.21362729
 -0.17397272 -0.13431816 -0.09466359 -0.05500902 -0.01535445]
[ 354  536  722  855  943  992 1051 1082 1081  484] [-0.41190013 -0.37224556 -0.332591   -0.29293643 -0.25328186 -0.21362729
 -0.17397272 -0.13431816 -0.09466359 -0.05500902 -0.01535445]
-25.1671
29.4731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.056571 minutes
Epoch 0
Fine tuning took 0.053587 minutes
Epoch 0
Fine tuning took 0.054142 minutes
{'zero': {0: [0.17118226600985223, 0.51231527093596063, 0.32635467980295568, 0.39532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.4039408866995074, 0.49507389162561577, 0.40147783251231528], 5: [0.097290640394088676, 0.083743842364532015, 0.17857142857142858, 0.20320197044334976], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17118226600985223, 0.15270935960591134, 0.18472906403940886, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.70935960591133007, 0.65270935960591137, 0.65024630541871919], 5: [0.097290640394088676, 0.13793103448275862, 0.1625615763546798, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17118226600985223, 0.18349753694581281, 0.19088669950738915, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.70320197044334976, 0.6785714285714286, 0.66748768472906406], 5: [0.097290640394088676, 0.11330049261083744, 0.13054187192118227, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17118226600985223, 0.17364532019704434, 0.21921182266009853, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.6354679802955665, 0.57389162561576357, 0.56157635467980294], 5: [0.097290640394088676, 0.19088669950738915, 0.20689655172413793, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151741 minutes
Weight histogram
[ 111  269  542  662  928 2426 1903 1814 1346  124] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 126  135  188  249  383  575  811 1094 1990 4574] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59962
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.66448
Epoch 1, cost is  4.33154
Epoch 2, cost is  4.28984
Epoch 3, cost is  4.3169
Epoch 4, cost is  4.37488
Training took 0.116399 minutes
Weight histogram
[1303 1199 1287 1067 1075 1266 1492  882  447  107] [-0.40343115 -0.36331634 -0.32320154 -0.28308674 -0.24297193 -0.20285713
 -0.16274233 -0.12262752 -0.08251272 -0.04239792 -0.00228311]
[ 316  638  921 1222 1208 1100 1131 1169 1213 1207] [-0.40343115 -0.36331634 -0.32320154 -0.28308674 -0.24297193 -0.20285713
 -0.16274233 -0.12262752 -0.08251272 -0.04239792 -0.00228311]
-17.5059
22.7596
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151251 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 2839 1664  888   96] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 247  275  383  459  744 1171 1092 1012 1896 4871] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.905
Epoch 1, cost is  12.3455
Epoch 2, cost is  12.413
Epoch 3, cost is  12.637
Epoch 4, cost is  12.9233
Training took 0.088897 minutes
Weight histogram
[1272  850 1035 1156 1123 1090 1275 1087 2251 1011] [-0.88842249 -0.79978825 -0.711154   -0.62251976 -0.53388552 -0.44525127
 -0.35661703 -0.26798279 -0.17934854 -0.0907143  -0.00208006]
[1745 1728 1075 1110 1109 1058 1037  988 1158 1142] [-0.88842249 -0.79978825 -0.711154   -0.62251976 -0.53388552 -0.44525127
 -0.35661703 -0.26798279 -0.17934854 -0.0907143  -0.00208006]
-32.9395
31.289
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.97651
Epoch 1, cost is  4.43035
Epoch 2, cost is  4.79831
Epoch 3, cost is  5.32812
Epoch 4, cost is  5.84739
Training took 0.093987 minutes
Weight histogram
[1365 1671 1618 1568 1466 1036  606  318  238  239] [-0.34779999 -0.31324491 -0.27868984 -0.24413476 -0.20957969 -0.17502461
 -0.14046954 -0.10591446 -0.07135939 -0.03680431 -0.00224924]
[ 466  571  738  956 1086 1216 1216 1275 1320 1281] [-0.34779999 -0.31324491 -0.27868984 -0.24413476 -0.20957969 -0.17502461
 -0.14046954 -0.10591446 -0.07135939 -0.03680431 -0.00224924]
-18.5101
19.9534
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046933 minutes
Epoch 0
Fine tuning took 0.047429 minutes
Epoch 0
Fine tuning took 0.047531 minutes
{'zero': {0: [0.45320197044334976, 0.35098522167487683, 0.30541871921182268, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41502463054187194, 0.48152709359605911, 0.51724137931034486, 0.47906403940886699], 5: [0.13177339901477833, 0.16748768472906403, 0.17733990147783252, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.45320197044334976, 0.37561576354679804, 0.30049261083743845, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41502463054187194, 0.51231527093596063, 0.51970443349753692, 0.51724137931034486], 5: [0.13177339901477833, 0.11206896551724138, 0.17980295566502463, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.45320197044334976, 0.39408866995073893, 0.31034482758620691, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41502463054187194, 0.45073891625615764, 0.53078817733990147, 0.51724137931034486], 5: [0.13177339901477833, 0.15517241379310345, 0.15886699507389163, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.45320197044334976, 0.38054187192118227, 0.29556650246305421, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41502463054187194, 0.47906403940886699, 0.52832512315270941, 0.53201970443349755], 5: [0.13177339901477833, 0.14039408866995073, 0.17610837438423646, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150285 minutes
Weight histogram
[ 111  269  542  662  928 2426 1903 1814 1346  124] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 126  135  188  249  383  575  811 1094 1990 4574] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59962
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.66448
Epoch 1, cost is  4.33154
Epoch 2, cost is  4.28984
Epoch 3, cost is  4.3169
Epoch 4, cost is  4.37488
Training took 0.118468 minutes
Weight histogram
[1303 1199 1287 1067 1075 1266 1492  882  447  107] [-0.40343115 -0.36331634 -0.32320154 -0.28308674 -0.24297193 -0.20285713
 -0.16274233 -0.12262752 -0.08251272 -0.04239792 -0.00228311]
[ 316  638  921 1222 1208 1100 1131 1169 1213 1207] [-0.40343115 -0.36331634 -0.32320154 -0.28308674 -0.24297193 -0.20285713
 -0.16274233 -0.12262752 -0.08251272 -0.04239792 -0.00228311]
-17.5059
22.7596
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149927 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 2839 1664  888   96] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 247  275  383  459  744 1171 1092 1012 1896 4871] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.905
Epoch 1, cost is  12.3455
Epoch 2, cost is  12.413
Epoch 3, cost is  12.637
Epoch 4, cost is  12.9233
Training tWARNING (theano.gof.cmodule): A module that was loaded by this ModuleCache can no longer be read from file /homes/js3611/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/tmpJAdoOH/08209d8457594474b6919aac94eba7e6.so... this could lead to problems.
WARNING (theano.gof.cmodule): Removing key file /homes/js3611/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/tmpJAdoOH/key.pkl because the corresponding module is gone from the file system.
ook 0.087296 minutes
Weight histogram
[1272  850 1035 1156 1123 1090 1275 1087 2251 1011] [-0.88842249 -0.79978825 -0.711154   -0.62251976 -0.53388552 -0.44525127
 -0.35661703 -0.26798279 -0.17934854 -0.0907143  -0.00208006]
[1745 1728 1075 1110 1109 1058 1037  988 1158 1142] [-0.88842249 -0.79978825 -0.711154   -0.62251976 -0.53388552 -0.44525127
 -0.35661703 -0.26798279 -0.17934854 -0.0907143  -0.00208006]
-32.9395
31.289
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.53933
Epoch 1, cost is  3.74742
Epoch 2, cost is  3.95286
Epoch 3, cost is  4.3519
Epoch 4, cost is  4.80808
Training took 0.109355 minutes
Weight histogram
[1259 1545 1491 1415 1442 1219  800  410  280  264] [-0.28246394 -0.25444985 -0.22643575 -0.19842166 -0.17040757 -0.14239347
 -0.11437938 -0.08636529 -0.05835119 -0.0303371  -0.00232301]
[ 513  613  824  920 1051 1199 1258 1287 1303 1157] [-0.28246394 -0.25444985 -0.22643575 -0.19842166 -0.17040757 -0.14239347
 -0.11437938 -0.08636529 -0.05835119 -0.0303371  -0.00232301]
-9.80263
13.4142
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048910 minutes
Epoch 0
Fine tuning took 0.047075 minutes
Epoch 0
Fine tuning took 0.049204 minutes
{'zero': {0: [0.23399014778325122, 0.30295566502463056, 0.3645320197044335, 0.37684729064039407], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.50246305418719217, 0.39162561576354682, 0.45689655172413796], 5: [0.081280788177339899, 0.19458128078817735, 0.24384236453201971, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23399014778325122, 0.30665024630541871, 0.23152709359605911, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.60467980295566504, 0.5923645320197044, 0.57758620689655171], 5: [0.081280788177339899, 0.088669950738916259, 0.17610837438423646, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23399014778325122, 0.27586206896551724, 0.2857142857142857, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.6219211822660099, 0.52955665024630538, 0.61206896551724133], 5: [0.081280788177339899, 0.10221674876847291, 0.18472906403940886, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23399014778325122, 0.30911330049261082, 0.20073891625615764, 0.32389162561576357], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.59113300492610843, 0.61699507389162567, 0.55911330049261088], 5: [0.081280788177339899, 0.099753694581280791, 0.18226600985221675, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149195 minutes
Weight histogram
[ 111  269  542  662  928 2426 1903 1814 1346  124] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 126  135  188  249  383  575  811 1094 1990 4574] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59962
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.66448
Epoch 1, cost is  4.33154
Epoch 2, cost is  4.28984
Epoch 3, cost is  4.3169
Epoch 4, cost is  4.37488
Training took 0.117760 minutes
Weight histogram
[1303 1199 1287 1067 1075 1266 1492  882  447  107] [-0.40343115 -0.36331634 -0.32320154 -0.28308674 -0.24297193 -0.20285713
 -0.16274233 -0.12262752 -0.08251272 -0.04239792 -0.00228311]
[ 316  638  921 1222 1208 1100 1131 1169 1213 1207] [-0.40343115 -0.36331634 -0.32320154 -0.28308674 -0.24297193 -0.20285713
 -0.16274233 -0.12262752 -0.08251272 -0.04239792 -0.00228311]
-17.5059
22.7596
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151120 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 2839 1664  888   96] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 247  275  383  459  744 1171 1092 1012 1896 4871] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.905
Epoch 1, cost is  12.3455
Epoch 2, cost is  12.413
Epoch 3, cost is  12.637
Epoch 4, cost is  12.9233
Training took 0.089893 minutes
Weight histogram
[1272  850 1035 1156 1123 1090 1275 1087 2251 1011] [-0.88842249 -0.79978825 -0.711154   -0.62251976 -0.53388552 -0.44525127
 -0.35661703 -0.26798279 -0.17934854 -0.0907143  -0.00208006]
[1745 1728 1075 1110 1109 1058 1037  988 1158 1142] [-0.88842249 -0.79978825 -0.711154   -0.62251976 -0.53388552 -0.44525127
 -0.35661703 -0.26798279 -0.17934854 -0.0907143  -0.00208006]
-32.9395
31.289
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.99737
Epoch 1, cost is  2.63561
Epoch 2, cost is  2.57994
Epoch 3, cost is  2.73103
Epoch 4, cost is  2.88789
Training took 0.148827 minutes
Weight histogram
[1362 1712 1443 1484 1230 1090  818  457  304  225] [-0.18184739 -0.16389113 -0.14593486 -0.12797859 -0.11002233 -0.09206606
 -0.07410979 -0.05615353 -0.03819726 -0.02024099 -0.00228473]
[ 567  558  721  908 1027 1189 1246 1284 1371 1254] [-0.18184739 -0.16389113 -0.14593486 -0.12797859 -0.11002233 -0.09206606
 -0.07410979 -0.05615353 -0.03819726 -0.02024099 -0.00228473]
-7.41958
9.17823
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.053526 minutes
Epoch 0
Fine tuning took 0.052649 minutes
Epoch 0
Fine tuning took 0.051371 minutes
{'zero': {0: [0.18719211822660098, 0.37931034482758619, 0.24384236453201971, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.29802955665024633, 0.44950738916256155, 0.33866995073891626], 5: [0.18226600985221675, 0.32266009852216748, 0.30665024630541871, 0.43472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18719211822660098, 0.23645320197044334, 0.20443349753694581, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.59852216748768472, 0.62438423645320196, 0.64655172413793105], 5: [0.18226600985221675, 0.16502463054187191, 0.17118226600985223, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18719211822660098, 0.23399014778325122, 0.24384236453201971, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.59113300492610843, 0.57512315270935965, 0.56650246305418717], 5: [0.18226600985221675, 0.1748768472906404, 0.18103448275862069, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18719211822660098, 0.28201970443349755, 0.19088669950738915, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.45197044334975367, 0.6145320197044335, 0.52463054187192115], 5: [0.18226600985221675, 0.26600985221674878, 0.19458128078817735, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149035 minutes
Weight histogram
[ 111  269  542  662  928 2426 1903 1814 1346  124] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 126  135  188  249  383  575  811 1094 1990 4574] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59962
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.66448
Epoch 1, cost is  4.33154
Epoch 2, cost is  4.28984
Epoch 3, cost is  4.3169
Epoch 4, cost is  4.37488
Training took 0.117722 minutes
Weight histogram
[1303 1199 1287 1067 1075 1266 1492  882  447  107] [-0.40343115 -0.36331634 -0.32320154 -0.28308674 -0.24297193 -0.20285713
 -0.16274233 -0.12262752 -0.08251272 -0.04239792 -0.00228311]
[ 316  638  921 1222 1208 1100 1131 1169 1213 1207] [-0.40343115 -0.36331634 -0.32320154 -0.28308674 -0.24297193 -0.20285713
 -0.16274233 -0.12262752 -0.08251272 -0.04239792 -0.00228311]
-17.5059
22.7596
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149898 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 2839 1664  888   96] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 247  275  383  459  744 1171 1092 1012 1896 4871] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.905
Epoch 1, cost is  12.3455
Epoch 2, cost is  12.413
Epoch 3, cost is  12.637
Epoch 4, cost is  12.9233
Training took 0.087548 minutes
Weight histogram
[1272  850 1035 1156 1123 1090 1275 1087 2251 1011] [-0.88842249 -0.79978825 -0.711154   -0.62251976 -0.53388552 -0.44525127
 -0.35661703 -0.26798279 -0.17934854 -0.0907143  -0.00208006]
[1745 1728 1075 1110 1109 1058 1037  988 1158 1142] [-0.88842249 -0.79978825 -0.711154   -0.62251976 -0.53388552 -0.44525127
 -0.35661703 -0.26798279 -0.17934854 -0.0907143  -0.00208006]
-32.9395
31.289
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.68565
Epoch 1, cost is  1.97626
Epoch 2, cost is  1.70537
Epoch 3, cost is  1.66033
Epoch 4, cost is  1.65628
Training took 0.212458 minutes
Weight histogram
[1717 1705 1633 1485 1090  989  630  435  279  162] [-0.11641122 -0.10499865 -0.09358609 -0.08217352 -0.07076095 -0.05934839
 -0.04793582 -0.03652326 -0.02511069 -0.01369813 -0.00228556]
[ 560  484  646  838  993 1191 1304 1390 1495 1224] [-0.11641122 -0.10499865 -0.09358609 -0.08217352 -0.07076095 -0.05934839
 -0.04793582 -0.03652326 -0.02511069 -0.01369813 -0.00228556]
-6.7849
7.03904
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.056030 minutes
Epoch 0
Fine tuning took 0.055616 minutes
Epoch 0
Fine tuning took 0.055672 minutes
{'zero': {0: [0.17610837438423646, 0.37438423645320196, 0.17980295566502463, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65270935960591137, 0.41502463054187194, 0.59359605911330049, 0.50862068965517238], 5: [0.17118226600985223, 0.2105911330049261, 0.22660098522167488, 0.26600985221674878], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17610837438423646, 0.19458128078817735, 0.16133004926108374, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65270935960591137, 0.6576354679802956, 0.6428571428571429, 0.60467980295566504], 5: [0.17118226600985223, 0.14778325123152711, 0.19581280788177341, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17610837438423646, 0.21674876847290642, 0.1625615763546798, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65270935960591137, 0.6219211822660099, 0.66379310344827591, 0.63669950738916259], 5: [0.17118226600985223, 0.16133004926108374, 0.17364532019704434, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17610837438423646, 0.14408866995073891, 0.1354679802955665, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65270935960591137, 0.70073891625615758, 0.7142857142857143, 0.63669950738916259], 5: [0.17118226600985223, 0.15517241379310345, 0.15024630541871922, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150384 minutes
Weight histogram
[ 111  269  542  662  928 2426 1903 1814 1346  124] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 126  135  188  249  383  575  811 1094 1990 4574] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59962
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  19.6594
Epoch 1, cost is  18.2927
Epoch 2, cost is  18.2274
Epoch 3, cost is  18.4838
Epoch 4, cost is  18.8611
Training took 0.113167 minutes
Weight histogram
[1211  906 1211  981 1173  952 1358 1092  864  377] [-2.15010142 -1.93625378 -1.72240613 -1.50855848 -1.29471083 -1.08086319
 -0.86701554 -0.65316789 -0.43932025 -0.2254726  -0.01162495]
[ 558  883 1118 1131 1096 1057 1058 1041 1115 1068] [-2.15010142 -1.93625378 -1.72240613 -1.50855848 -1.29471083 -1.08086319
 -0.86701554 -0.65316789 -0.43932025 -0.2254726  -0.01162495]
-90.5238
97.4335
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148769 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 2839 1664  888   96] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 247  275  383  459  744 1171 1092 1012 1896 4871] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  59.2978
Epoch 1, cost is  57.4604
Epoch 2, cost is  58.1962
Epoch 3, cost is  59.5667
Epoch 4, cost is  61.2854
Training took 0.086557 minutes
Weight histogram
[1162  878 1032  952  996 1076 1082 1148 1739 2085] [-4.59711981 -4.13846463 -3.67980945 -3.22115427 -2.7624991  -2.30384392
 -1.84518874 -1.38653356 -0.92787838 -0.46922321 -0.01056803]
[2714 1161 1036  968 1069 1010  993  947 1145 1107] [-4.59711981 -4.13846463 -3.67980945 -3.22115427 -2.7624991  -2.30384392
 -1.84518874 -1.38653356 -0.92787838 -0.46922321 -0.01056803]
-161.38
174.958
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.76561
Epoch 1, cost is  4.74803
Epoch 2, cost is  4.73045
Epoch 3, cost is  4.87309
Epoch 4, cost is  5.22218
Training took 0.094153 minutes
Weight histogram
[ 231  273  258  433 2491 5342 1054   12    6   25] [-0.73570311 -0.66290082 -0.59009854 -0.51729625 -0.44449396 -0.37169168
 -0.29888939 -0.2260871  -0.15328482 -0.08048253 -0.00768024]
[ 660 1715 2139 1728 1364 1312  616  193  209  189] [-0.73570311 -0.66290082 -0.59009854 -0.51729625 -0.44449396 -0.37169168
 -0.29888939 -0.2260871  -0.15328482 -0.08048253 -0.00768024]
-66.4183
61.5511
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043649 minutes
Epoch 0
Fine tuning took 0.044633 minutes
Epoch 0
Fine tuning took 0.044444 minutes
{'zero': {0: [0.67364532019704437, 0.32758620689655171, 0.35344827586206895, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.21182266009852216, 0.47536945812807879, 0.43226600985221675, 0.5357142857142857], 5: [0.1145320197044335, 0.19704433497536947, 0.21428571428571427, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.67364532019704437, 0.33004926108374383, 0.36699507389162561, 0.31896551724137934], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.21182266009852216, 0.44458128078817732, 0.41256157635467983, 0.51108374384236455], 5: [0.1145320197044335, 0.22536945812807882, 0.22044334975369459, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.67364532019704437, 0.35221674876847292, 0.36699507389162561, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.21182266009852216, 0.43103448275862066, 0.40763546798029554, 0.52709359605911332], 5: [0.1145320197044335, 0.21674876847290642, 0.22536945812807882, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.67364532019704437, 0.33497536945812806, 0.35221674876847292, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.21182266009852216, 0.43472906403940886, 0.3891625615763547, 0.52216748768472909], 5: [0.1145320197044335, 0.23029556650246305, 0.25862068965517243, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150619 minutes
Weight histogram
[ 111  269  542  662  928 2426 1903 1814 1346  124] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 126  135  188  249  383  575  811 1094 1990 4574] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59962
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  19.6594
Epoch 1, cost is  18.2927
Epoch 2, cost is  18.2274
Epoch 3, cost is  18.4838
Epoch 4, cost is  18.8611
Training took 0.111208 minutes
Weight histogram
[1211  906 1211  981 1173  952 1358 1092  864  377] [-2.15010142 -1.93625378 -1.72240613 -1.50855848 -1.29471083 -1.08086319
 -0.86701554 -0.65316789 -0.43932025 -0.2254726  -0.01162495]
[ 558  883 1118 1131 1096 1057 1058 1041 1115 1068] [-2.15010142 -1.93625378 -1.72240613 -1.50855848 -1.29471083 -1.08086319
 -0.86701554 -0.65316789 -0.43932025 -0.2254726  -0.01162495]
-90.5238
97.4335
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150104 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 2839 1664  888   96] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 247  275  383  459  744 1171 1092 1012 1896 4871] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  59.2978
Epoch 1, cost is  57.4604
Epoch 2, cost is  58.1962
Epoch 3, cost is  59.5667
Epoch 4, cost is  61.2854
Training took 0.086780 minutes
Weight histogram
[1162  878 1032  952  996 1076 1082 1148 1739 2085] [-4.59711981 -4.13846463 -3.67980945 -3.22115427 -2.7624991  -2.30384392
 -1.84518874 -1.38653356 -0.92787838 -0.46922321 -0.01056803]
[2714 1161 1036  968 1069 1010  993  947 1145 1107] [-4.59711981 -4.13846463 -3.67980945 -3.22115427 -2.7624991  -2.30384392
 -1.84518874 -1.38653356 -0.92787838 -0.46922321 -0.01056803]
-161.38
174.958
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.49283
Epoch 1, cost is  6.07404
Epoch 2, cost is  8.15291
Epoch 3, cost is  10.5963
Epoch 4, cost is  13.073
Training took 0.107610 minutes
Weight histogram
[ 207  417  586 1250 1546 2140 2400 1361  186   32] [-0.86838162 -0.78289421 -0.69740681 -0.6119194  -0.526432   -0.44094459
 -0.35545719 -0.26996978 -0.18448238 -0.09899497 -0.01350757]
[ 663 1215 1336 1336 1310 1270 1127  918  705  245] [-0.86838162 -0.78289421 -0.69740681 -0.6119194  -0.526432   -0.44094459
 -0.35545719 -0.26996978 -0.18448238 -0.09899497 -0.01350757]
-65.1493
54.9559
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044547 minutes
Epoch 0
Fine tuning took 0.047698 minutes
Epoch 0
Fine tuning took 0.047126 minutes
{'zero': {0: [0.31773399014778325, 0.39408866995073893, 0.33620689655172414, 0.29064039408866993], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49753694581280788, 0.40640394088669951, 0.42364532019704432, 0.36330049261083741], 5: [0.18472906403940886, 0.19950738916256158, 0.24014778325123154, 0.3460591133004926], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31773399014778325, 0.19950738916256158, 0.28448275862068967, 0.29433497536945813], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49753694581280788, 0.61945812807881773, 0.53448275862068961, 0.50492610837438423], 5: [0.18472906403940886, 0.18103448275862069, 0.18103448275862069, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31773399014778325, 0.25246305418719212, 0.28817733990147781, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49753694581280788, 0.57019704433497542, 0.5357142857142857, 0.53078817733990147], 5: [0.18472906403940886, 0.17733990147783252, 0.17610837438423646, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31773399014778325, 0.2019704433497537, 0.29310344827586204, 0.31650246305418717], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49753694581280788, 0.58374384236453203, 0.53078817733990147, 0.44950738916256155], 5: [0.18472906403940886, 0.21428571428571427, 0.17610837438423646, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149941 minutes
Weight histogram
[ 111  269  542  662  928 2426 1903 1814 1346  124] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 126  135  188  249  383  575  811 1094 1990 4574] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59962
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  19.6594
Epoch 1, cost is  18.2927
Epoch 2, cost is  18.2274
Epoch 3, cost is  18.4838
Epoch 4, cost is  18.8611
Training took 0.112672 minutes
Weight histogram
[1211  906 1211  981 1173  952 1358 1092  864  377] [-2.15010142 -1.93625378 -1.72240613 -1.50855848 -1.29471083 -1.08086319
 -0.86701554 -0.65316789 -0.43932025 -0.2254726  -0.01162495]
[ 558  883 1118 1131 1096 1057 1058 1041 1115 1068] [-2.15010142 -1.93625378 -1.72240613 -1.50855848 -1.29471083 -1.08086319
 -0.86701554 -0.65316789 -0.43932025 -0.2254726  -0.01162495]
-90.5238
97.4335
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149100 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 2839 1664  888   96] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 247  275  383  459  744 1171 1092 1012 1896 4871] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  59.2978
Epoch 1, cost is  57.4604
Epoch 2, cost is  58.1962
Epoch 3, cost is  59.5667
Epoch 4, cost is  61.2854
Training took 0.085490 minutes
Weight histogram
[1162  878 1032  952  996 1076 1082 1148 1739 2085] [-4.59711981 -4.13846463 -3.67980945 -3.22115427 -2.7624991  -2.30384392
 -1.84518874 -1.38653356 -0.92787838 -0.46922321 -0.01056803]
[2714 1161 1036  968 1069 1010  993  947 1145 1107] [-4.59711981 -4.13846463 -3.67980945 -3.22115427 -2.7624991  -2.30384392
 -1.84518874 -1.38653356 -0.92787838 -0.46922321 -0.01056803]
-161.38
174.958
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.98928
Epoch 1, cost is  4.37755
Epoch 2, cost is  5.29205
Epoch 3, cost is  6.35162
Epoch 4, cost is  7.39127
Training took 0.149797 minutes
Weight histogram
[ 679 1335 1420 1345 1390 1445 1342  980  130   59] [-0.64533269 -0.58230482 -0.51927694 -0.45624907 -0.39322119 -0.33019332
 -0.26716544 -0.20413756 -0.14110969 -0.07808181 -0.01505394]
[ 522  880 1061 1102 1127 1114 1107 1134 1129  949] [-0.64533269 -0.58230482 -0.51927694 -0.45624907 -0.39322119 -0.33019332
 -0.26716544 -0.20413756 -0.14110969 -0.07808181 -0.01505394]
-44.2971
42.436
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048410 minutes
Epoch 0
Fine tuning took 0.050670 minutes
Epoch 0
Fine tuning took 0.049378 minutes
{'zero': {0: [0.25, 0.40270935960591131, 0.29187192118226601, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69950738916256161, 0.37068965517241381, 0.53201970443349755, 0.50492610837438423], 5: [0.050492610837438424, 0.22660098522167488, 0.17610837438423646, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25, 0.25, 0.19950738916256158, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69950738916256161, 0.66748768472906406, 0.67980295566502458, 0.76477832512315269], 5: [0.050492610837438424, 0.082512315270935957, 0.1206896551724138, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25, 0.27832512315270935, 0.20812807881773399, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69950738916256161, 0.60221674876847286, 0.61822660098522164, 0.67733990147783252], 5: [0.050492610837438424, 0.11945812807881774, 0.17364532019704434, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25, 0.17610837438423646, 0.14408866995073891, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69950738916256161, 0.76108374384236455, 0.80172413793103448, 0.86206896551724133], 5: [0.050492610837438424, 0.062807881773399021, 0.054187192118226604, 0.038177339901477834], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.152572 minutes
Weight histogram
[ 111  269  542  662  928 2426 1903 1814 1346  124] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 126  135  188  249  383  575  811 1094 1990 4574] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59962
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  19.6594
Epoch 1, cost is  18.2927
Epoch 2, cost is  18.2274
Epoch 3, cost is  18.4838
Epoch 4, cost is  18.8611
Training took 0.111054 minutes
Weight histogram
[1211  906 1211  981 1173  952 1358 1092  864  377] [-2.15010142 -1.93625378 -1.72240613 -1.50855848 -1.29471083 -1.08086319
 -0.86701554 -0.65316789 -0.43932025 -0.2254726  -0.01162495]
[ 558  883 1118 1131 1096 1057 1058 1041 1115 1068] [-2.15010142 -1.93625378 -1.72240613 -1.50855848 -1.29471083 -1.08086319
 -0.86701554 -0.65316789 -0.43932025 -0.2254726  -0.01162495]
-90.5238
97.4335
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150873 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 2839 1664  888   96] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 247  275  383  459  744 1171 1092 1012 1896 4871] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  59.2978
Epoch 1, cost is  57.4604
Epoch 2, cost is  58.1962
Epoch 3, cost is  59.5667
Epoch 4, cost is  61.2854
Training took 0.083848 minutes
Weight histogram
[1162  878 1032  952  996 1076 1082 1148 1739 2085] [-4.59711981 -4.13846463 -3.67980945 -3.22115427 -2.7624991  -2.30384392
 -1.84518874 -1.38653356 -0.92787838 -0.46922321 -0.01056803]
[2714 1161 1036  968 1069 1010  993  947 1145 1107] [-4.59711981 -4.13846463 -3.67980945 -3.22115427 -2.7624991  -2.30384392
 -1.84518874 -1.38653356 -0.92787838 -0.46922321 -0.01056803]
-161.38
174.958
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  2.65464
Epoch 1, cost is  2.13419
Epoch 2, cost is  2.21062
Epoch 3, cost is  2.39651
Epoch 4, cost is  2.60401
Training took 0.215452 minutes
Weight histogram
[1197 1560 1628 1462 1412 1180  884  535  157  110] [-0.41777006 -0.37750391 -0.33723777 -0.29697162 -0.25670547 -0.21643933
 -0.17617318 -0.13590703 -0.09564089 -0.05537474 -0.0151086 ]
[ 440  663  905 1061 1176 1234 1314 1345 1351  636] [-0.41777006 -0.37750391 -0.33723777 -0.29697162 -0.25670547 -0.21643933
 -0.17617318 -0.13590703 -0.09564089 -0.05537474 -0.0151086 ]
-25.1671
29.4731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.055525 minutes
Epoch 0
Fine tuning took 0.054150 minutes
Epoch 0
Fine tuning took 0.053609 minutes
{'zero': {0: [0.23891625615763548, 0.22167487684729065, 0.41379310344827586, 0.24630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65640394088669951, 0.37561576354679804, 0.40024630541871919, 0.28078817733990147], 5: [0.10467980295566502, 0.40270935960591131, 0.18596059113300492, 0.47290640394088668], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23891625615763548, 0.19950738916256158, 0.21305418719211822, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65640394088669951, 0.63177339901477836, 0.57266009852216748, 0.58743842364532017], 5: [0.10467980295566502, 0.16871921182266009, 0.21428571428571427, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23891625615763548, 0.2536945812807882, 0.24630541871921183, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65640394088669951, 0.59482758620689657, 0.56034482758620685, 0.57512315270935965], 5: [0.10467980295566502, 0.15147783251231528, 0.19334975369458129, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23891625615763548, 0.22783251231527094, 0.22167487684729065, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65640394088669951, 0.59729064039408863, 0.55911330049261088, 0.59852216748768472], 5: [0.10467980295566502, 0.1748768472906404, 0.21921182266009853, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150944 minutes
Weight histogram
[ 111  269  542  662  928 2426 1934 2413 2570  295] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 129  139  199  249  395  648  857 1212 2643 5679] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.84229
Epoch 1, cost is  4.46228
Epoch 2, cost is  4.39106
Epoch 3, cost is  4.39415
Epoch 4, cost is  4.42427
Training took 0.117971 minutes
Weight histogram
[1792 1154 1355 1552 1256 1093 1919 1215  670  144] [-0.46830544 -0.42170321 -0.37510097 -0.32849874 -0.28189651 -0.23529428
 -0.18869204 -0.14208981 -0.09548758 -0.04888535 -0.00228311]
[ 392  807 1223 1406 1293 1289 1333 1400 1465 1542] [-0.46830544 -0.42170321 -0.37510097 -0.32849874 -0.28189651 -0.23529428
 -0.18869204 -0.14208981 -0.09548758 -0.04888535 -0.00228311]
-20.1057
26.8378
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149579 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 3039 2711 1591  171] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 252  286  391  491  781 1268 1010 1141 2101 6454] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  14.9038
Epoch 1, cost is  14.255
Epoch 2, cost is  14.2955
Epoch 3, cost is  14.4983
Epoch 4, cost is  14.779
Training took 0.089546 minutes
Weight histogram
[1458  946 1458 1106 1311 1391 1329 1492 2116 1568] [-1.07321537 -0.96610183 -0.8589883  -0.75187477 -0.64476124 -0.53764771
 -0.43053418 -0.32342065 -0.21630712 -0.10919359 -0.00208006]
[2264 1555 1324 1309 1257 1222 1155 1412 1360 1317] [-1.07321537 -0.96610183 -0.8589883  -0.75187477 -0.64476124 -0.53764771
 -0.43053418 -0.32342065 -0.21630712 -0.10919359 -0.00208006]
-37.4886
35.5424
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.01321
Epoch 1, cost is  4.5509
Epoch 2, cost is  4.89229
Epoch 3, cost is  5.36064
Epoch 4, cost is  5.80265
Training took 0.093816 minutes
Weight histogram
[1503 2050 1970 1851 1811 1283  728  385  283  286] [-0.34779999 -0.31324491 -0.27868984 -0.24413476 -0.20957969 -0.17502461
 -0.14046954 -0.10591446 -0.07135939 -0.03680431 -0.00224924]
[ 557  688  898 1166 1322 1463 1480 1543 1610 1423] [-0.34779999 -0.31324491 -0.27868984 -0.24413476 -0.20957969 -0.17502461
 -0.14046954 -0.10591446 -0.07135939 -0.03680431 -0.00224924]
-20.323
19.9534
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047772 minutes
Epoch 0
Fine tuning took 0.045888 minutes
Epoch 0
Fine tuning took 0.045980 minutes
{'zero': {0: [0.50123152709359609, 0.30049261083743845, 0.31157635467980294, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.46305418719211822, 0.47413793103448276, 0.44581280788177341], 5: [0.17610837438423646, 0.23645320197044334, 0.21428571428571427, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.50123152709359609, 0.33374384236453203, 0.30541871921182268, 0.30788177339901479], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.46921182266009853, 0.50862068965517238, 0.45443349753694579], 5: [0.17610837438423646, 0.19704433497536947, 0.18596059113300492, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.50123152709359609, 0.33374384236453203, 0.28448275862068967, 0.29802955665024633], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.47660098522167488, 0.51231527093596063, 0.47167487684729065], 5: [0.17610837438423646, 0.18965517241379309, 0.20320197044334976, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.50123152709359609, 0.32635467980295568, 0.31650246305418717, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.44581280788177341, 0.48275862068965519, 0.45197044334975367], 5: [0.17610837438423646, 0.22783251231527094, 0.20073891625615764, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149692 minutes
Weight histogram
[ 111  269  542  662  928 2426 1934 2413 2570  295] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 129  139  199  249  395  648  857 1212 2643 5679] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.84229
Epoch 1, cost is  4.46228
Epoch 2, cost is  4.39106
Epoch 3, cost is  4.39415
Epoch 4, cost is  4.42427
Training took 0.115799 minutes
Weight histogram
[1792 1154 1355 1552 1256 1093 1919 1215  670  144] [-0.46830544 -0.42170321 -0.37510097 -0.32849874 -0.28189651 -0.23529428
 -0.18869204 -0.14208981 -0.09548758 -0.04888535 -0.00228311]
[ 392  807 1223 1406 1293 1289 1333 1400 1465 1542] [-0.46830544 -0.42170321 -0.37510097 -0.32849874 -0.28189651 -0.23529428
 -0.18869204 -0.14208981 -0.09548758 -0.04888535 -0.00228311]
-20.1057
26.8378
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149614 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 3039 2711 1591  171] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 252  286  391  491  781 1268 1010 1141 2101 6454] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  14.9038
Epoch 1, cost is  14.255
Epoch 2, cost is  14.2955
Epoch 3, cost is  14.4983
Epoch 4, cost is  14.779
Training took 0.087102 minutes
Weight histogram
[1458  946 1458 1106 1311 1391 1329 1492 2116 1568] [-1.07321537 -0.96610183 -0.8589883  -0.75187477 -0.64476124 -0.53764771
 -0.43053418 -0.32342065 -0.21630712 -0.10919359 -0.00208006]
[2264 1555 1324 1309 1257 1222 1155 1412 1360 1317] [-1.07321537 -0.96610183 -0.8589883  -0.75187477 -0.64476124 -0.53764771
 -0.43053418 -0.32342065 -0.21630712 -0.10919359 -0.00208006]
-37.4886
35.5424
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.56912
Epoch 1, cost is  3.79577
Epoch 2, cost is  4.00125
Epoch 3, cost is  4.37539
Epoch 4, cost is  4.82249
Training took 0.110810 minutes
Weight histogram
[1496 1817 1814 1702 1708 1486  980  494  342  311] [-0.28246394 -0.25444951 -0.22643509 -0.19842067 -0.17040624 -0.14239182
 -0.1143774  -0.08636297 -0.05834855 -0.03033413 -0.0023197 ]
[ 615  736  997 1104 1258 1438 1510 1542 1567 1383] [-0.28246394 -0.25444951 -0.22643509 -0.19842067 -0.17040624 -0.14239182
 -0.1143774  -0.08636297 -0.05834855 -0.03033413 -0.0023197 ]
-10.2874
13.4142
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048979 minutes
Epoch 0
Fine tuning took 0.046361 minutes
Epoch 0
Fine tuning took 0.047851 minutes
{'zero': {0: [0.25985221674876846, 0.29064039408866993, 0.17733990147783252, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.43965517241379309, 0.56403940886699511, 0.60221674876847286], 5: [0.1748768472906404, 0.26970443349753692, 0.25862068965517243, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25985221674876846, 0.26847290640394089, 0.2105911330049261, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.57266009852216748, 0.6354679802955665, 0.61206896551724133], 5: [0.1748768472906404, 0.15886699507389163, 0.1539408866995074, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25985221674876846, 0.24014778325123154, 0.24261083743842365, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.58497536945812811, 0.6354679802955665, 0.63793103448275867], 5: [0.1748768472906404, 0.1748768472906404, 0.12192118226600986, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25985221674876846, 0.2019704433497537, 0.19458128078817735, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.64039408866995073, 0.68349753694581283, 0.61699507389162567], 5: [0.1748768472906404, 0.15763546798029557, 0.12192118226600986, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150027 minutes
Weight histogram
[ 111  269  542  662  928 2426 1934 2413 2570  295] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 129  139  199  249  395  648  857 1212 2643 5679] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.84229
Epoch 1, cost is  4.46228
Epoch 2, cost is  4.39106
Epoch 3, cost is  4.39415
Epoch 4, cost is  4.42427
Training took 0.116909 minutes
Weight histogram
[1792 1154 1355 1552 1256 1093 1919 1215  670  144] [-0.46830544 -0.42170321 -0.37510097 -0.32849874 -0.28189651 -0.23529428
 -0.18869204 -0.14208981 -0.09548758 -0.04888535 -0.00228311]
[ 392  807 1223 1406 1293 1289 1333 1400 1465 1542] [-0.46830544 -0.42170321 -0.37510097 -0.32849874 -0.28189651 -0.23529428
 -0.18869204 -0.14208981 -0.09548758 -0.04888535 -0.00228311]
-20.1057
26.8378
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149181 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 3039 2711 1591  171] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 252  286  391  491  781 1268 1010 1141 2101 6454] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  14.9038
Epoch 1, cost is  14.255
Epoch 2, cost is  14.2955
Epoch 3, cost is  14.4983
Epoch 4, cost is  14.779
Training took 0.087279 minutes
Weight histogram
[1458  946 1458 1106 1311 1391 1329 1492 2116 1568] [-1.07321537 -0.96610183 -0.8589883  -0.75187477 -0.64476124 -0.53764771
 -0.43053418 -0.32342065 -0.21630712 -0.10919359 -0.00208006]
[2264 1555 1324 1309 1257 1222 1155 1412 1360 1317] [-1.07321537 -0.96610183 -0.8589883  -0.75187477 -0.64476124 -0.53764771
 -0.43053418 -0.32342065 -0.21630712 -0.10919359 -0.00208006]
-37.4886
35.5424
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.02548
Epoch 1, cost is  2.66461
Epoch 2, cost is  2.59018
Epoch 3, cost is  2.75305
Epoch 4, cost is  2.92725
Training took 0.149800 minutes
Weight histogram
[1591 2046 1743 1783 1493 1321  978  556  369  270] [-0.18184739 -0.16389113 -0.14593486 -0.12797859 -0.11002233 -0.09206606
 -0.07410979 -0.05615353 -0.03819726 -0.02024099 -0.00228473]
[ 680  673  866 1089 1234 1427 1492 1540 1642 1507] [-0.18184739 -0.16389113 -0.14593486 -0.12797859 -0.11002233 -0.09206606
 -0.07410979 -0.05615353 -0.03819726 -0.02024099 -0.00228473]
-7.41958
9.17823
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052437 minutes
Epoch 0
Fine tuning took 0.050961 minutes
Epoch 0
Fine tuning took 0.050509 minutes
{'zero': {0: [0.27216748768472904, 0.23029556650246305, 0.21551724137931033, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.50369458128078815, 0.52586206896551724, 0.6428571428571429], 5: [0.16009852216748768, 0.26600985221674878, 0.25862068965517243, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27216748768472904, 0.26108374384236455, 0.19458128078817735, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.59113300492610843, 0.63669950738916259, 0.60960591133004927], 5: [0.16009852216748768, 0.14778325123152711, 0.16871921182266009, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27216748768472904, 0.26108374384236455, 0.20566502463054187, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.58743842364532017, 0.65024630541871919, 0.6071428571428571], 5: [0.16009852216748768, 0.15147783251231528, 0.14408866995073891, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27216748768472904, 0.25492610837438423, 0.17980295566502463, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.5788177339901478, 0.64778325123152714, 0.61576354679802958], 5: [0.16009852216748768, 0.16625615763546797, 0.17241379310344829, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150196 minutes
Weight histogram
[ 111  269  542  662  928 2426 1934 2413 2570  295] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 129  139  199  249  395  648  857 1212 2643 5679] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.84229
Epoch 1, cost is  4.46228
Epoch 2, cost is  4.39106
Epoch 3, cost is  4.39415
Epoch 4, cost is  4.42427
Training took 0.116041 minutes
Weight histogram
[1792 1154 1355 1552 1256 1093 1919 1215  670  144] [-0.46830544 -0.42170321 -0.37510097 -0.32849874 -0.28189651 -0.23529428
 -0.18869204 -0.14208981 -0.09548758 -0.04888535 -0.00228311]
[ 392  807 1223 1406 1293 1289 1333 1400 1465 1542] [-0.46830544 -0.42170321 -0.37510097 -0.32849874 -0.28189651 -0.23529428
 -0.18869204 -0.14208981 -0.09548758 -0.04888535 -0.00228311]
-20.1057
26.8378
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149167 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 3039 2711 1591  171] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 252  286  391  491  781 1268 1010 1141 2101 6454] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  14.9038
Epoch 1, cost is  14.255
Epoch 2, cost is  14.2955
Epoch 3, cost is  14.4983
Epoch 4, cost is  14.779
Training took 0.087093 minutes
Weight histogram
[1458  946 1458 1106 1311 1391 1329 1492 2116 1568] [-1.07321537 -0.96610183 -0.8589883  -0.75187477 -0.64476124 -0.53764771
 -0.43053418 -0.32342065 -0.21630712 -0.10919359 -0.00208006]
[2264 1555 1324 1309 1257 1222 1155 1412 1360 1317] [-1.07321537 -0.96610183 -0.8589883  -0.75187477 -0.64476124 -0.53764771
 -0.43053418 -0.32342065 -0.21630712 -0.10919359 -0.00208006]
-37.4886
35.5424
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.70816
Epoch 1, cost is  1.99336
Epoch 2, cost is  1.72644
Epoch 3, cost is  1.67391
Epoch 4, cost is  1.67066
Training took 0.212322 minutes
Weight histogram
[2030 2052 1948 1794 1320 1183  766  526  337  194] [-0.11641122 -0.10499865 -0.09358609 -0.08217352 -0.07076095 -0.05934839
 -0.04793582 -0.03652326 -0.02511069 -0.01369813 -0.00228556]
[ 673  583  780 1011 1191 1429 1552 1663 1781 1487] [-0.11641122 -0.10499865 -0.09358609 -0.08217352 -0.07076095 -0.05934839
 -0.04793582 -0.03652326 -0.02511069 -0.01369813 -0.00228556]
-6.7849
7.03904
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.057231 minutes
Epoch 0
Fine tuning took 0.054685 minutes
Epoch 0
Fine tuning took 0.056282 minutes
{'zero': {0: [0.19827586206896552, 0.25985221674876846, 0.16748768472906403, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.6071428571428571, 0.68719211822660098, 0.68596059113300489], 5: [0.12438423645320197, 0.13300492610837439, 0.14532019704433496, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19827586206896552, 0.16133004926108374, 0.1539408866995074, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.71921182266009853, 0.72783251231527091, 0.73522167487684731], 5: [0.12438423645320197, 0.11945812807881774, 0.11822660098522167, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19827586206896552, 0.21182266009852216, 0.13793103448275862, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.6219211822660099, 0.72044334975369462, 0.66625615763546797], 5: [0.12438423645320197, 0.16625615763546797, 0.14162561576354679, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19827586206896552, 0.19334975369458129, 0.1625615763546798, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.68103448275862066, 0.71921182266009853, 0.71182266009852213], 5: [0.12438423645320197, 0.12561576354679804, 0.11822660098522167, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150677 minutes
Weight histogram
[ 111  269  542  662  928 2426 1934 2413 2570  295] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 129  139  199  249  395  648  857 1212 2643 5679] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.6978
Epoch 1, cost is  20.9558
Epoch 2, cost is  20.6387
Epoch 3, cost is  20.6557
Epoch 4, cost is  20.7797
Training took 0.111444 minutes
Weight histogram
[1514 1116 1325 1252 1149 1428 1229 1525 1076  536] [-2.55799508 -2.30335807 -2.04872106 -1.79408404 -1.53944703 -1.28481002
 -1.030173   -0.77553599 -0.52089898 -0.26626196 -0.01162495]
[ 691 1105 1361 1302 1246 1243 1242 1291 1297 1372] [-2.55799508 -2.30335807 -2.04872106 -1.79408404 -1.53944703 -1.28481002
 -1.030173   -0.77553599 -0.52089898 -0.26626196 -0.01162495]
-100.51
103.517
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149920 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 3039 2711 1591  171] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 252  286  391  491  781 1268 1010 1141 2101 6454] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  73.4864
Epoch 1, cost is  70.3396
Epoch 2, cost is  70.6103
Epoch 3, cost is  71.6968
Epoch 4, cost is  73.1078
Training took 0.083717 minutes
Weight histogram
[1412 1132 1363 1063 1094 1242 1273 1352 1519 2725] [-5.45318794 -4.90892595 -4.36466396 -3.82040197 -3.27613998 -2.73187799
 -2.18761599 -1.643354   -1.09909201 -0.55483002 -0.01056803]
[3056 1194 1180 1213 1207 1166 1127 1368 1349 1315] [-5.45318794 -4.90892595 -4.36466396 -3.82040197 -3.27613998 -2.73187799
 -2.18761599 -1.643354   -1.09909201 -0.55483002 -0.01056803]
-190.034
194.04
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.87219
Epoch 1, cost is  6.42949
Epoch 2, cost is  7.90802
Epoch 3, cost is  9.38171
Epoch 4, cost is  10.918
Training took 0.091886 minutes
Weight histogram
[ 231  359  571  754 2979 5874 1314   30    8   30] [-0.73570311 -0.66290082 -0.59009854 -0.51729625 -0.44449396 -0.37169168
 -0.29888939 -0.2260871  -0.15328482 -0.08048253 -0.00768024]
[ 733 1886 2345 1969 1594 1559  812  406  455  391] [-0.73570311 -0.66290082 -0.59009854 -0.51729625 -0.44449396 -0.37169168
 -0.29888939 -0.2260871  -0.15328482 -0.08048253 -0.00768024]
-66.4183
62.6716
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044466 minutes
Epoch 0
Fine tuning took 0.044493 minutes
Epoch 0
Fine tuning took 0.046217 minutes
{'zero': {0: [0.42364532019704432, 0.27709359605911332, 0.22536945812807882, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.26108374384236455, 0.51724137931034486, 0.61083743842364535, 0.64408866995073888], 5: [0.31527093596059114, 0.20566502463054187, 0.16379310344827586, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.42364532019704432, 0.25246305418719212, 0.24384236453201971, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.26108374384236455, 0.52832512315270941, 0.60591133004926112, 0.64778325123152714], 5: [0.31527093596059114, 0.21921182266009853, 0.15024630541871922, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.42364532019704432, 0.30172413793103448, 0.25123152709359609, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.26108374384236455, 0.4963054187192118, 0.60221674876847286, 0.65886699507389157], 5: [0.31527093596059114, 0.2019704433497537, 0.14655172413793102, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.42364532019704432, 0.28078817733990147, 0.26354679802955666, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.26108374384236455, 0.49507389162561577, 0.58497536945812811, 0.65270935960591137], 5: [0.31527093596059114, 0.22413793103448276, 0.15147783251231528, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150215 minutes
Weight histogram
[ 111  269  542  662  928 2426 1934 2413 2570  295] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 129  139  199  249  395  648  857 1212 2643 5679] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.6978
Epoch 1, cost is  20.9558
Epoch 2, cost is  20.6387
Epoch 3, cost is  20.6557
Epoch 4, cost is  20.7797
Training took 0.110668 minutes
Weight histogram
[1514 1116 1325 1252 1149 1428 1229 1525 1076  536] [-2.55799508 -2.30335807 -2.04872106 -1.79408404 -1.53944703 -1.28481002
 -1.030173   -0.77553599 -0.52089898 -0.26626196 -0.01162495]
[ 691 1105 1361 1302 1246 1243 1242 1291 1297 1372] [-2.55799508 -2.30335807 -2.04872106 -1.79408404 -1.53944703 -1.28481002
 -1.030173   -0.77553599 -0.52089898 -0.26626196 -0.01162495]
-100.51
103.517
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149168 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 3039 2711 1591  171] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 252  286  391  491  781 1268 1010 1141 2101 6454] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  73.4864
Epoch 1, cost is  70.3396
Epoch 2, cost is  70.6103
Epoch 3, cost is  71.6968
Epoch 4, cost is  73.1078
Training took 0.083691 minutes
Weight histogram
[1412 1132 1363 1063 1094 1242 1273 1352 1519 2725] [-5.45318794 -4.90892595 -4.36466396 -3.82040197 -3.27613998 -2.73187799
 -2.18761599 -1.643354   -1.09909201 -0.55483002 -0.01056803]
[3056 1194 1180 1213 1207 1166 1127 1368 1349 1315] [-5.45318794 -4.90892595 -4.36466396 -3.82040197 -3.27613998 -2.73187799
 -2.18761599 -1.643354   -1.09909201 -0.55483002 -0.01056803]
-190.034
194.04
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.70671
Epoch 1, cost is  6.18555
Epoch 2, cost is  8.00583
Epoch 3, cost is  10.1294
Epoch 4, cost is  12.1041
Training took 0.107863 minutes
Weight histogram
[ 207  417  587 1257 2003 2697 3079 1673  192   38] [-0.86838162 -0.78284284 -0.69730405 -0.61176527 -0.52622649 -0.44068771
 -0.35514892 -0.26961014 -0.18407136 -0.09853258 -0.01299379]
[ 802 1505 1669 1646 1625 1550 1443  960  705  245] [-0.86838162 -0.78284284 -0.69730405 -0.61176527 -0.52622649 -0.44068771
 -0.35514892 -0.26961014 -0.18407136 -0.09853258 -0.01299379]
-65.1493
54.9559
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047481 minutes
Epoch 0
Fine tuning took 0.046313 minutes
Epoch 0
Fine tuning took 0.045767 minutes
{'zero': {0: [0.32019704433497537, 0.22413793103448276, 0.30665024630541871, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47660098522167488, 0.65147783251231528, 0.59482758620689657, 0.5714285714285714], 5: [0.20320197044334976, 0.12438423645320197, 0.098522167487684734, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.32019704433497537, 0.21798029556650247, 0.29802955665024633, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47660098522167488, 0.65394088669950734, 0.5714285714285714, 0.5923645320197044], 5: [0.20320197044334976, 0.12807881773399016, 0.13054187192118227, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.32019704433497537, 0.22906403940886699, 0.29802955665024633, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47660098522167488, 0.64408866995073888, 0.55172413793103448, 0.57389162561576357], 5: [0.20320197044334976, 0.1268472906403941, 0.15024630541871922, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.32019704433497537, 0.17980295566502463, 0.31280788177339902, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47660098522167488, 0.69827586206896552, 0.56034482758620685, 0.57389162561576357], 5: [0.20320197044334976, 0.12192118226600986, 0.1268472906403941, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151696 minutes
Weight histogram
[ 111  269  542  662  928 2426 1934 2413 2570  295] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 129  139  199  249  395  648  857 1212 2643 5679] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.6978
Epoch 1, cost is  20.9558
Epoch 2, cost is  20.6387
Epoch 3, cost is  20.6557
Epoch 4, cost is  20.7797
Training took 0.110443 minutes
Weight histogram
[1514 1116 1325 1252 1149 1428 1229 1525 1076  536] [-2.55799508 -2.30335807 -2.04872106 -1.79408404 -1.53944703 -1.28481002
 -1.030173   -0.77553599 -0.52089898 -0.26626196 -0.01162495]
[ 691 1105 1361 1302 1246 1243 1242 1291 1297 1372] [-2.55799508 -2.30335807 -2.04872106 -1.79408404 -1.53944703 -1.28481002
 -1.030173   -0.77553599 -0.52089898 -0.26626196 -0.01162495]
-100.51
103.517
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149845 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 3039 2711 1591  171] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 252  286  391  491  781 1268 1010 1141 2101 6454] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  73.4864
Epoch 1, cost is  70.3396
Epoch 2, cost is  70.6103
Epoch 3, cost is  71.6968
Epoch 4, cost is  73.1078
Training took 0.086324 minutes
Weight histogram
[1412 1132 1363 1063 1094 1242 1273 1352 1519 2725] [-5.45318794 -4.90892595 -4.36466396 -3.82040197 -3.27613998 -2.73187799
 -2.18761599 -1.643354   -1.09909201 -0.55483002 -0.01056803]
[3056 1194 1180 1213 1207 1166 1127 1368 1349 1315] [-5.45318794 -4.90892595 -4.36466396 -3.82040197 -3.27613998 -2.73187799
 -2.18761599 -1.643354   -1.09909201 -0.55483002 -0.01056803]
-190.034
194.04
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.76464
Epoch 1, cost is  4.12489
Epoch 2, cost is  4.93659
Epoch 3, cost is  5.87886
Epoch 4, cost is  6.81855
Training took 0.151450 minutes
Weight histogram
[ 921 1586 1662 1577 1654 1670 1606 1226  182   66] [-0.64533269 -0.58226508 -0.51919748 -0.45612987 -0.39306226 -0.32999465
 -0.26692704 -0.20385943 -0.14079182 -0.07772421 -0.0146566 ]
[ 673 1147 1340 1375 1409 1393 1368 1427 1373  645] [-0.64533269 -0.58226508 -0.51919748 -0.45612987 -0.39306226 -0.32999465
 -0.26692704 -0.20385943 -0.14079182 -0.07772421 -0.0146566 ]
-44.2971
42.436
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048295 minutes
Epoch 0
Fine tuning took 0.048879 minutes
Epoch 0
Fine tuning took 0.049337 minutes
{'zero': {0: [0.16748768472906403, 0.33866995073891626, 0.23891625615763548, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.38054187192118227, 0.49014778325123154, 0.46305418719211822], 5: [0.22660098522167488, 0.28078817733990147, 0.27093596059113301, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16748768472906403, 0.18965517241379309, 0.19088669950738915, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.68596059113300489, 0.66379310344827591, 0.70443349753694584], 5: [0.22660098522167488, 0.12438423645320197, 0.14532019704433496, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16748768472906403, 0.18226600985221675, 0.19211822660098521, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.72167487684729059, 0.67610837438423643, 0.69704433497536944], 5: [0.22660098522167488, 0.096059113300492605, 0.13177339901477833, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16748768472906403, 0.20073891625615764, 0.18965517241379309, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.69334975369458129, 0.7068965517241379, 0.71798029556650245], 5: [0.22660098522167488, 0.10591133004926108, 0.10344827586206896, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149283 minutes
Weight histogram
[ 111  269  542  662  928 2426 1934 2413 2570  295] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 129  139  199  249  395  648  857 1212 2643 5679] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.05992
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.6978
Epoch 1, cost is  20.9558
Epoch 2, cost is  20.6387
Epoch 3, cost is  20.6557
Epoch 4, cost is  20.7797
Training took 0.113279 minutes
Weight histogram
[1514 1116 1325 1252 1149 1428 1229 1525 1076  536] [-2.55799508 -2.30335807 -2.04872106 -1.79408404 -1.53944703 -1.28481002
 -1.030173   -0.77553599 -0.52089898 -0.26626196 -0.01162495]
[ 691 1105 1361 1302 1246 1243 1242 1291 1297 1372] [-2.55799508 -2.30335807 -2.04872106 -1.79408404 -1.53944703 -1.28481002
 -1.030173   -0.77553599 -0.52089898 -0.26626196 -0.01162495]
-100.51
103.517
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148592 minutes
Weight histogram
[ 121  483  866 1228 1328 2637 3039 2711 1591  171] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
[ 252  286  391  491  781 1268 1010 1141 2101 6454] [ -2.42434355e-04  -2.66615098e-05   1.89111335e-04   4.04884180e-04
   6.20657025e-04   8.36429870e-04   1.05220272e-03   1.26797556e-03
   1.48374841e-03   1.69952125e-03   1.91529409e-03]
-1.58406
0.890599
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  73.4864
Epoch 1, cost is  70.3396
Epoch 2, cost is  70.6103
Epoch 3, cost is  71.6968
Epoch 4, cost is  73.1078
Training took 0.083442 minutes
Weight histogram
[1412 1132 1363 1063 1094 1242 1273 1352 1519 2725] [-5.45318794 -4.90892595 -4.36466396 -3.82040197 -3.27613998 -2.73187799
 -2.18761599 -1.643354   -1.09909201 -0.55483002 -0.01056803]
[3056 1194 1180 1213 1207 1166 1127 1368 1349 1315] [-5.45318794 -4.90892595 -4.36466396 -3.82040197 -3.27613998 -2.73187799
 -2.18761599 -1.643354   -1.09909201 -0.55483002 -0.01056803]
-190.034
194.04
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  2.8182
Epoch 1, cost is  2.24167
Epoch 2, cost is  2.28123
Epoch 3, cost is  2.40348
Epoch 4, cost is  2.62585
Training took 0.215839 minutes
Weight histogram
[1303 1857 1977 1789 1670 1435 1103  666  206  144] [-0.42494649 -0.38392782 -0.34290916 -0.30189049 -0.26087183 -0.21985317
 -0.1788345  -0.13781584 -0.09679717 -0.05577851 -0.01475985]
[ 522  791 1074 1264 1397 1469 1567 1605 1622  839] [-0.42494649 -0.38392782 -0.34290916 -0.30189049 -0.26087183 -0.21985317
 -0.1788345  -0.13781584 -0.09679717 -0.05577851 -0.01475985]
-27.8845
29.4731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.055121 minutes
Epoch 0
Fine tuning took 0.053204 minutes
Epoch 0
Fine tuning took 0.055312 minutes
{'zero': {0: [0.20320197044334976, 0.29802955665024633, 0.19704433497536947, 0.34236453201970446], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68103448275862066, 0.50369458128078815, 0.41502463054187194, 0.50369458128078815], 5: [0.11576354679802955, 0.19827586206896552, 0.38793103448275862, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20320197044334976, 0.21921182266009853, 0.15147783251231528, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68103448275862066, 0.6354679802955665, 0.66133004926108374, 0.61576354679802958], 5: [0.11576354679802955, 0.14532019704433496, 0.18719211822660098, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20320197044334976, 0.19950738916256158, 0.17857142857142858, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68103448275862066, 0.68719211822660098, 0.63669950738916259, 0.67610837438423643], 5: [0.11576354679802955, 0.11330049261083744, 0.18472906403940886, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20320197044334976, 0.20812807881773399, 0.17364532019704434, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68103448275862066, 0.61699507389162567, 0.59482758620689657, 0.66009852216748766], 5: [0.11576354679802955, 0.1748768472906404, 0.23152709359605911, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149731 minutes
Weight histogram
[ 111  269  542  673 1026 2933 3012 2738 2574  297] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 131  143  203  252  414  702  869 1437 2810 7214] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.93649
Epoch 1, cost is  5.49151
Epoch 2, cost is  5.38872
Epoch 3, cost is  5.37808
Epoch 4, cost is  5.40323
Training took 0.117347 minutes
Weight histogram
[1710 1485 1552 1579 1725 1524 1602 1857  916  225] [-0.54593605 -0.49157075 -0.43720546 -0.38284017 -0.32847487 -0.27410958
 -0.21974429 -0.16537899 -0.1110137  -0.05664841 -0.00228311]
[ 487 1013 1534 1560 1448 1539 1595 1696 1721 1582] [-0.54593605 -0.49157075 -0.43720546 -0.38284017 -0.32847487 -0.27410958
 -0.21974429 -0.16537899 -0.1110137  -0.05664841 -0.00228311]
-22.2371
27.8155
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148174 minutes
Weight histogram
[ 142  596 1071 1269 2093 3307 3262 3337 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  297  414  553  843 1409  891 1448 2995 7091] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.916799
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.4606
Epoch 1, cost is  16.6837
Epoch 2, cost is  16.651
Epoch 3, cost is  16.7984
Epoch 4, cost is  17.0447
Training took 0.090108 minutes
Weight histogram
[1759 1319 1125 1736 1254 1583 1558 1725 1962 2179] [-1.24574614 -1.12137953 -0.99701292 -0.87264631 -0.7482797  -0.6239131
 -0.49954649 -0.37517988 -0.25081327 -0.12644666 -0.00208006]
[2832 1374 1518 1486 1417 1336 1589 1565 1554 1529] [-1.24574614 -1.12137953 -0.99701292 -0.87264631 -0.7482797  -0.6239131
 -0.49954649 -0.37517988 -0.25081327 -0.12644666 -0.00208006]
-45.3128
38.7991
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.99082
Epoch 1, cost is  4.45452
Epoch 2, cost is  4.79323
Epoch 3, cost is  5.27976
Epoch 4, cost is  5.78752
Training took 0.093102 minutes
Weight histogram
[1764 2362 2325 2099 2127 1534  850  452  330  332] [-0.34779999 -0.31324491 -0.27868984 -0.24413476 -0.20957969 -0.17502461
 -0.14046954 -0.10591446 -0.07135939 -0.03680431 -0.00224924]
[ 646  804 1047 1365 1537 1711 1731 1807 1888 1639] [-0.34779999 -0.31324491 -0.27868984 -0.24413476 -0.20957969 -0.17502461
 -0.14046954 -0.10591446 -0.07135939 -0.03680431 -0.00224924]
-20.8546
19.9534
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045900 minutes
Epoch 0
Fine tuning took 0.047014 minutes
Epoch 0
Fine tuning took 0.045714 minutes
{'zero': {0: [0.37684729064039407, 0.33497536945812806, 0.39408866995073893, 0.27709359605911332], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.49507389162561577, 0.41133004926108374, 0.55049261083743839], 5: [0.13177339901477833, 0.16995073891625614, 0.19458128078817735, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37684729064039407, 0.34359605911330049, 0.36206896551724138, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.49137931034482757, 0.44334975369458129, 0.54187192118226601], 5: [0.13177339901477833, 0.16502463054187191, 0.19458128078817735, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37684729064039407, 0.33128078817733991, 0.36330049261083741, 0.27709359605911332], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.52586206896551724, 0.44581280788177341, 0.52216748768472909], 5: [0.13177339901477833, 0.14285714285714285, 0.19088669950738915, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37684729064039407, 0.32142857142857145, 0.35591133004926107, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.52955665024630538, 0.44704433497536944, 0.54679802955665024], 5: [0.13177339901477833, 0.14901477832512317, 0.19704433497536947, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.155535 minutes
Weight histogram
[ 111  269  542  673 1026 2933 3012 2738 2574  297] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 131  143  203  252  414  702  869 1437 2810 7214] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.93649
Epoch 1, cost is  5.49151
Epoch 2, cost is  5.38872
Epoch 3, cost is  5.37808
Epoch 4, cost is  5.40323
Training took 0.117987 minutes
Weight histogram
[1710 1485 1552 1579 1725 1524 1602 1857  916  225] [-0.54593605 -0.49157075 -0.43720546 -0.38284017 -0.32847487 -0.27410958
 -0.21974429 -0.16537899 -0.1110137  -0.05664841 -0.00228311]
[ 487 1013 1534 1560 1448 1539 1595 1696 1721 1582] [-0.54593605 -0.49157075 -0.43720546 -0.38284017 -0.32847487 -0.27410958
 -0.21974429 -0.16537899 -0.1110137  -0.05664841 -0.00228311]
-22.2371
27.8155
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148659 minutes
Weight histogram
[ 142  596 1071 1269 2093 3307 3262 3337 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  297  414  553  843 1409  891 1448 2995 7091] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.916799
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.4606
Epoch 1, cost is  16.6837
Epoch 2, cost is  16.651
Epoch 3, cost is  16.7984
Epoch 4, cost is  17.0447
Training took 0.089964 minutes
Weight histogram
[1759 1319 1125 1736 1254 1583 1558 1725 1962 2179] [-1.24574614 -1.12137953 -0.99701292 -0.87264631 -0.7482797  -0.6239131
 -0.49954649 -0.37517988 -0.25081327 -0.12644666 -0.00208006]
[2832 1374 1518 1486 1417 1336 1589 1565 1554 1529] [-1.24574614 -1.12137953 -0.99701292 -0.87264631 -0.7482797  -0.6239131
 -0.49954649 -0.37517988 -0.25081327 -0.12644666 -0.00208006]
-45.3128
38.7991
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.56571
Epoch 1, cost is  3.7807
Epoch 2, cost is  4.00281
Epoch 3, cost is  4.40845
Epoch 4, cost is  4.93228
Training took 0.107801 minutes
Weight histogram
[1725 2092 2141 1969 1988 1768 1152  577  402  361] [-0.28246394 -0.25444691 -0.22642989 -0.19841286 -0.17039584 -0.14237881
 -0.11436179 -0.08634476 -0.05832774 -0.03031071 -0.00229369]
[ 715  859 1167 1288 1467 1674 1765 1798 1828 1614] [-0.28246394 -0.25444691 -0.22642989 -0.19841286 -0.17039584 -0.14237881
 -0.11436179 -0.08634476 -0.05832774 -0.03031071 -0.00229369]
-10.2874
13.4142
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045974 minutes
Epoch 0
Fine tuning took 0.046687 minutes
Epoch 0
Fine tuning took 0.048679 minutes
{'zero': {0: [0.27586206896551724, 0.15763546798029557, 0.19088669950738915, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.61699507389162567, 0.61206896551724133, 0.64778325123152714], 5: [0.20320197044334976, 0.22536945812807882, 0.19704433497536947, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27586206896551724, 0.27093596059113301, 0.26108374384236455, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.56034482758620685, 0.59975369458128081, 0.63916256157635465], 5: [0.20320197044334976, 0.16871921182266009, 0.13916256157635468, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27586206896551724, 0.25492610837438423, 0.23399014778325122, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.57758620689655171, 0.60591133004926112, 0.63177339901477836], 5: [0.20320197044334976, 0.16748768472906403, 0.16009852216748768, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27586206896551724, 0.2536945812807882, 0.24630541871921183, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.55911330049261088, 0.60960591133004927, 0.66748768472906406], 5: [0.20320197044334976, 0.18719211822660098, 0.14408866995073891, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149685 minutes
Weight histogram
[ 111  269  542  673 1026 2933 3012 2738 2574  297] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 131  143  203  252  414  702  869 1437 2810 7214] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.93649
Epoch 1, cost is  5.49151
Epoch 2, cost is  5.38872
Epoch 3, cost is  5.37808
Epoch 4, cost is  5.40323
Training took 0.118198 minutes
Weight histogram
[1710 1485 1552 1579 1725 1524 1602 1857  916  225] [-0.54593605 -0.49157075 -0.43720546 -0.38284017 -0.32847487 -0.27410958
 -0.21974429 -0.16537899 -0.1110137  -0.05664841 -0.00228311]
[ 487 1013 1534 1560 1448 1539 1595 1696 1721 1582] [-0.54593605 -0.49157075 -0.43720546 -0.38284017 -0.32847487 -0.27410958
 -0.21974429 -0.16537899 -0.1110137  -0.05664841 -0.00228311]
-22.2371
27.8155
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148651 minutes
Weight histogram
[ 142  596 1071 1269 2093 3307 3262 3337 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  297  414  553  843 1409  891 1448 2995 7091] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.916799
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.4606
Epoch 1, cost is  16.6837
Epoch 2, cost is  16.651
Epoch 3, cost is  16.7984
Epoch 4, cost is  17.0447
Training took 0.091247 minutes
Weight histogram
[1759 1319 1125 1736 1254 1583 1558 1725 1962 2179] [-1.24574614 -1.12137953 -0.99701292 -0.87264631 -0.7482797  -0.6239131
 -0.49954649 -0.37517988 -0.25081327 -0.12644666 -0.00208006]
[2832 1374 1518 1486 1417 1336 1589 1565 1554 1529] [-1.24574614 -1.12137953 -0.99701292 -0.87264631 -0.7482797  -0.6239131
 -0.49954649 -0.37517988 -0.25081327 -0.12644666 -0.00208006]
-45.3128
38.7991
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.02396
Epoch 1, cost is  2.68428
Epoch 2, cost is  2.62501
Epoch 3, cost is  2.81365
Epoch 4, cost is  2.97785
Training took 0.148825 minutes
Weight histogram
[1868 2348 2060 2072 1744 1536 1146  652  435  314] [-0.18184739 -0.16389047 -0.14593355 -0.12797662 -0.1100197  -0.09206277
 -0.07410585 -0.05614893 -0.038192   -0.02023508 -0.00227816]
[ 793  788 1017 1273 1444 1663 1739 1799 1913 1746] [-0.18184739 -0.16389047 -0.14593355 -0.12797662 -0.1100197  -0.09206277
 -0.07410585 -0.05614893 -0.038192   -0.02023508 -0.00227816]
-7.41958
9.17823
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052015 minutes
Epoch 0
Fine tuning took 0.051062 minutes
Epoch 0
Fine tuning took 0.050494 minutes
{'zero': {0: [0.20320197044334976, 0.31896551724137934, 0.24384236453201971, 0.37438423645320196], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.32758620689655171, 0.60098522167487689, 0.4605911330049261], 5: [0.16748768472906403, 0.35344827586206895, 0.15517241379310345, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20320197044334976, 0.21182266009852216, 0.21182266009852216, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.56157635467980294, 0.60344827586206895, 0.56896551724137934], 5: [0.16748768472906403, 0.22660098522167488, 0.18472906403940886, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20320197044334976, 0.25, 0.25123152709359609, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.53201970443349755, 0.59975369458128081, 0.57635467980295563], 5: [0.16748768472906403, 0.21798029556650247, 0.14901477832512317, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20320197044334976, 0.26477832512315269, 0.19704433497536947, 0.30295566502463056], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.44088669950738918, 0.66133004926108374, 0.53817733990147787], 5: [0.16748768472906403, 0.29433497536945813, 0.14162561576354679, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.152015 minutes
Weight histogram
[ 111  269  542  673 1026 2933 3012 2738 2574  297] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 131  143  203  252  414  702  869 1437 2810 7214] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.93649
Epoch 1, cost is  5.49151
Epoch 2, cost is  5.38872
Epoch 3, cost is  5.37808
Epoch 4, cost is  5.40323
Training took 0.116897 minutes
Weight histogram
[1710 1485 1552 1579 1725 1524 1602 1857  916  225] [-0.54593605 -0.49157075 -0.43720546 -0.38284017 -0.32847487 -0.27410958
 -0.21974429 -0.16537899 -0.1110137  -0.05664841 -0.00228311]
[ 487 1013 1534 1560 1448 1539 1595 1696 1721 1582] [-0.54593605 -0.49157075 -0.43720546 -0.38284017 -0.32847487 -0.27410958
 -0.21974429 -0.16537899 -0.1110137  -0.05664841 -0.00228311]
-22.2371
27.8155
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151261 minutes
Weight histogram
[ 142  596 1071 1269 2093 3307 3262 3337 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  297  414  553  843 1409  891 1448 2995 7091] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.916799
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.4606
Epoch 1, cost is  16.6837
Epoch 2, cost is  16.651
Epoch 3, cost is  16.7984
Epoch 4, cost is  17.0447
Training took 0.086748 minutes
Weight histogram
[1759 1319 1125 1736 1254 1583 1558 1725 1962 2179] [-1.24574614 -1.12137953 -0.99701292 -0.87264631 -0.7482797  -0.6239131
 -0.49954649 -0.37517988 -0.25081327 -0.12644666 -0.00208006]
[2832 1374 1518 1486 1417 1336 1589 1565 1554 1529] [-1.24574614 -1.12137953 -0.99701292 -0.87264631 -0.7482797  -0.6239131
 -0.49954649 -0.37517988 -0.25081327 -0.12644666 -0.00208006]
-45.3128
38.7991
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.71649
Epoch 1, cost is  2.01746
Epoch 2, cost is  1.73715
Epoch 3, cost is  1.68991
Epoch 4, cost is  1.69146
Training took 0.213965 minutes
Weight histogram
[2315 2429 2248 2109 1545 1389  907  619  388  226] [-0.11641122 -0.10499749 -0.09358376 -0.08217003 -0.07075631 -0.05934258
 -0.04792885 -0.03651512 -0.0251014  -0.01368767 -0.00227394]
[ 784  683  912 1179 1387 1660 1801 1930 2066 1773] [-0.11641122 -0.10499749 -0.09358376 -0.08217003 -0.07075631 -0.05934258
 -0.04792885 -0.03651512 -0.0251014  -0.01368767 -0.00227394]
-6.7849
7.03904
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.056458 minutes
Epoch 0
Fine tuning took 0.057178 minutes
Epoch 0
Fine tuning took 0.056606 minutes
{'zero': {0: [0.20320197044334976, 0.24261083743842365, 0.25246305418719212, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.65024630541871919, 0.64655172413793105, 0.5431034482758621], 5: [0.1539408866995074, 0.10714285714285714, 0.10098522167487685, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20320197044334976, 0.18965517241379309, 0.22044334975369459, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.64162561576354682, 0.64162561576354682, 0.66009852216748766], 5: [0.1539408866995074, 0.16871921182266009, 0.13793103448275862, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20320197044334976, 0.17610837438423646, 0.18719211822660098, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.68472906403940892, 0.68103448275862066, 0.65270935960591137], 5: [0.1539408866995074, 0.13916256157635468, 0.13177339901477833, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20320197044334976, 0.24876847290640394, 0.21305418719211822, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.58620689655172409, 0.63300492610837433, 0.5788177339901478], 5: [0.1539408866995074, 0.16502463054187191, 0.1539408866995074, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149601 minutes
Weight histogram
[ 111  269  542  673 1026 2933 3012 2738 2574  297] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 131  143  203  252  414  702  869 1437 2810 7214] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  28.259
Epoch 1, cost is  26.2068
Epoch 2, cost is  25.8631
Epoch 3, cost is  25.9345
Epoch 4, cost is  26.1738
Training took 0.111626 minutes
Weight histogram
[1651 1445 1313 1571 1444 1435 1427 1865 1342  682] [-2.98293328 -2.68580245 -2.38867162 -2.09154078 -1.79440995 -1.49727912
 -1.20014828 -0.90301745 -0.60588662 -0.30875578 -0.01162495]
[ 855 1367 1566 1472 1439 1428 1496 1532 1565 1455] [-2.98293328 -2.68580245 -2.38867162 -2.09154078 -1.79440995 -1.49727912
 -1.20014828 -0.90301745 -0.60588662 -0.30875578 -0.01162495]
-109.302
113.629
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149707 minutes
Weight histogram
[ 142  596 1071 1269 2093 3307 3262 3337 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  297  414  553  843 1409  891 1448 2995 7091] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.916799
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  86.1883
Epoch 1, cost is  82.1432
Epoch 2, cost is  82.2035
Epoch 3, cost is  83.1407
Epoch 4, cost is  84.4449
Training took 0.084721 minutes
Weight histogram
[1643 1462 1385 1525 1261 1233 1543 1438 1518 3192] [-6.29906797 -5.67021798 -5.04136798 -4.41251799 -3.783668   -3.154818
 -2.52596801 -1.89711801 -1.26826802 -0.63941802 -0.01056803]
[3240 1391 1332 1441 1357 1295 1574 1560 1541 1469] [-6.29906797 -5.67021798 -5.04136798 -4.41251799 -3.783668   -3.154818
 -2.52596801 -1.89711801 -1.26826802 -0.63941802 -0.01056803]
-235.024
235.76
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.77712
Epoch 1, cost is  6.2595
Epoch 2, cost is  7.76053
Epoch 3, cost is  9.5139
Epoch 4, cost is  11.3139
Training took 0.093407 minutes
Weight histogram
[ 231  684  916 1092 3295 6253 1596   62   11   35] [-0.73570311 -0.66290082 -0.59009854 -0.51729625 -0.44449396 -0.37169168
 -0.29888939 -0.2260871  -0.15328482 -0.08048253 -0.00768024]
[ 827 2188 2504 2208 1864 1706 1020  634  656  568] [-0.73570311 -0.66290082 -0.59009854 -0.51729625 -0.44449396 -0.37169168
 -0.29888939 -0.2260871  -0.15328482 -0.08048253 -0.00768024]
-66.4183
62.6716
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044682 minutes
Epoch 0
Fine tuning took 0.043321 minutes
Epoch 0
Fine tuning took 0.046028 minutes
{'zero': {0: [0.35467980295566504, 0.22783251231527094, 0.34975369458128081, 0.35467980295566504], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40270935960591131, 0.56280788177339902, 0.49753694581280788, 0.45812807881773399], 5: [0.24261083743842365, 0.20935960591133004, 0.15270935960591134, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.35467980295566504, 0.24876847290640394, 0.33620689655172414, 0.31650246305418717], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40270935960591131, 0.51231527093596063, 0.50615763546798032, 0.4642857142857143], 5: [0.24261083743842365, 0.23891625615763548, 0.15763546798029557, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.35467980295566504, 0.22906403940886699, 0.32389162561576357, 0.3251231527093596], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40270935960591131, 0.54064039408866993, 0.52339901477832518, 0.46798029556650245], 5: [0.24261083743842365, 0.23029556650246305, 0.15270935960591134, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.35467980295566504, 0.20443349753694581, 0.38793103448275862, 0.29433497536945813], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40270935960591131, 0.56280788177339902, 0.47167487684729065, 0.49753694581280788], 5: [0.24261083743842365, 0.23275862068965517, 0.14039408866995073, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150666 minutes
Weight histogram
[ 111  269  542  673 1026 2933 3012 2738 2574  297] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 131  143  203  252  414  702  869 1437 2810 7214] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  28.259
Epoch 1, cost is  26.2068
Epoch 2, cost is  25.8631
Epoch 3, cost is  25.9345
Epoch 4, cost is  26.1738
Training took 0.110418 minutes
Weight histogram
[1651 1445 1313 1571 1444 1435 1427 1865 1342  682] [-2.98293328 -2.68580245 -2.38867162 -2.09154078 -1.79440995 -1.49727912
 -1.20014828 -0.90301745 -0.60588662 -0.30875578 -0.01162495]
[ 855 1367 1566 1472 1439 1428 1496 1532 1565 1455] [-2.98293328 -2.68580245 -2.38867162 -2.09154078 -1.79440995 -1.49727912
 -1.20014828 -0.90301745 -0.60588662 -0.30875578 -0.01162495]
-109.302
113.629
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148608 minutes
Weight histogram
[ 142  596 1071 1269 2093 3307 3262 3337 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  297  414  553  843 1409  891 1448 2995 7091] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.916799
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  86.1883
Epoch 1, cost is  82.1432
Epoch 2, cost is  82.2035
Epoch 3, cost is  83.1407
Epoch 4, cost is  84.4449
Training took 0.086678 minutes
Weight histogram
[1643 1462 1385 1525 1261 1233 1543 1438 1518 3192] [-6.29906797 -5.67021798 -5.04136798 -4.41251799 -3.783668   -3.154818
 -2.52596801 -1.89711801 -1.26826802 -0.63941802 -0.01056803]
[3240 1391 1332 1441 1357 1295 1574 1560 1541 1469] [-6.29906797 -5.67021798 -5.04136798 -4.41251799 -3.783668   -3.154818
 -2.52596801 -1.89711801 -1.26826802 -0.63941802 -0.01056803]
-235.024
235.76
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.43925
Epoch 1, cost is  5.95699
Epoch 2, cost is  7.83079
Epoch 3, cost is  9.81955
Epoch 4, cost is  11.9511
Training took 0.108142 minutes
Weight histogram
[ 297  656  846 1494 2265 2991 3359 1975  247   45] [-0.86838162 -0.78280756 -0.6972335  -0.61165945 -0.52608539 -0.44051133
 -0.35493727 -0.26936321 -0.18378916 -0.0982151  -0.01264104]
[ 934 1746 1914 1898 1892 1769 1658 1135  852  377] [-0.86838162 -0.78280756 -0.6972335  -0.61165945 -0.52608539 -0.44051133
 -0.35493727 -0.26936321 -0.18378916 -0.0982151  -0.01264104]
-75.5509
72.5006
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046388 minutes
Epoch 0
Fine tuning took 0.047012 minutes
Epoch 0
Fine tuning took 0.045283 minutes
{'zero': {0: [0.18842364532019704, 0.27093596059113301, 0.28448275862068967, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5788177339901478, 0.46798029556650245, 0.53694581280788178, 0.51600985221674878], 5: [0.23275862068965517, 0.26108374384236455, 0.17857142857142858, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18842364532019704, 0.27463054187192121, 0.2857142857142857, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5788177339901478, 0.45689655172413796, 0.53448275862068961, 0.52216748768472909], 5: [0.23275862068965517, 0.26847290640394089, 0.17980295566502463, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18842364532019704, 0.28325123152709358, 0.29433497536945813, 0.29064039408866993], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5788177339901478, 0.44211822660098521, 0.52832512315270941, 0.49261083743842365], 5: [0.23275862068965517, 0.27463054187192121, 0.17733990147783252, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18842364532019704, 0.24876847290640394, 0.27709359605911332, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5788177339901478, 0.49753694581280788, 0.52586206896551724, 0.50123152709359609], 5: [0.23275862068965517, 0.2536945812807882, 0.19704433497536947, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149564 minutes
Weight histogram
[ 111  269  542  673 1026 2933 3012 2738 2574  297] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 131  143  203  252  414  702  869 1437 2810 7214] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  28.259
Epoch 1, cost is  26.2068
Epoch 2, cost is  25.8631
Epoch 3, cost is  25.9345
Epoch 4, cost is  26.1738
Training took 0.112784 minutes
Weight histogram
[1651 1445 1313 1571 1444 1435 1427 1865 1342  682] [-2.98293328 -2.68580245 -2.38867162 -2.09154078 -1.79440995 -1.49727912
 -1.20014828 -0.90301745 -0.60588662 -0.30875578 -0.01162495]
[ 855 1367 1566 1472 1439 1428 1496 1532 1565 1455] [-2.98293328 -2.68580245 -2.38867162 -2.09154078 -1.79440995 -1.49727912
 -1.20014828 -0.90301745 -0.60588662 -0.30875578 -0.01162495]
-109.302
113.629
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149301 minutes
Weight histogram
[ 142  596 1071 1269 2093 3307 3262 3337 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  297  414  553  843 1409  891 1448 2995 7091] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.916799
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  86.1883
Epoch 1, cost is  82.1432
Epoch 2, cost is  82.2035
Epoch 3, cost is  83.1407
Epoch 4, cost is  84.4449
Training took 0.086947 minutes
Weight histogram
[1643 1462 1385 1525 1261 1233 1543 1438 1518 3192] [-6.29906797 -5.67021798 -5.04136798 -4.41251799 -3.783668   -3.154818
 -2.52596801 -1.89711801 -1.26826802 -0.63941802 -0.01056803]
[3240 1391 1332 1441 1357 1295 1574 1560 1541 1469] [-6.29906797 -5.67021798 -5.04136798 -4.41251799 -3.783668   -3.154818
 -2.52596801 -1.89711801 -1.26826802 -0.63941802 -0.01056803]
-235.024
235.76
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.9734
Epoch 1, cost is  4.58151
Epoch 2, cost is  5.70193
Epoch 3, cost is  6.93669
Epoch 4, cost is  8.13883
Training took 0.149911 minutes
Weight histogram
[1013 1859 1927 1870 1952 1930 1908 1450  193   73] [-0.64533269 -0.58225563 -0.51917856 -0.4561015  -0.39302443 -0.32994736
 -0.2668703  -0.20379323 -0.14071617 -0.0776391  -0.01456203]
[ 792 1348 1558 1592 1638 1608 1581 1649 1587  822] [-0.64533269 -0.58225563 -0.51917856 -0.4561015  -0.39302443 -0.32994736
 -0.2668703  -0.20379323 -0.14071617 -0.0776391  -0.01456203]
-44.2971
43.253
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049422 minutes
Epoch 0
Fine tuning took 0.049871 minutes
Epoch 0
Fine tuning took 0.051571 minutes
{'zero': {0: [0.34359605911330049, 0.15763546798029557, 0.31034482758620691, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54802955665024633, 0.59605911330049266, 0.56896551724137934, 0.62438423645320196], 5: [0.10837438423645321, 0.24630541871921183, 0.1206896551724138, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.34359605911330049, 0.18719211822660098, 0.20566502463054187, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54802955665024633, 0.66625615763546797, 0.70073891625615758, 0.63054187192118227], 5: [0.10837438423645321, 0.14655172413793102, 0.093596059113300489, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.34359605911330049, 0.22167487684729065, 0.22906403940886699, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54802955665024633, 0.65024630541871919, 0.66748768472906406, 0.66379310344827591], 5: [0.10837438423645321, 0.12807881773399016, 0.10344827586206896, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.34359605911330049, 0.22660098522167488, 0.17118226600985223, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54802955665024633, 0.60344827586206895, 0.66748768472906406, 0.67733990147783252], 5: [0.10837438423645321, 0.16995073891625614, 0.16133004926108374, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150710 minutes
Weight histogram
[ 111  269  542  673 1026 2933 3012 2738 2574  297] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
[ 131  143  203  252  414  702  869 1437 2810 7214] [ -2.39681889e-04   2.18629168e-05   2.83407723e-04   5.44952528e-04
   8.06497334e-04   1.06804214e-03   1.32958695e-03   1.59113175e-03
   1.85267656e-03   2.11422136e-03   2.37576617e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  28.259
Epoch 1, cost is  26.2068
Epoch 2, cost is  25.8631
Epoch 3, cost is  25.9345
Epoch 4, cost is  26.1738
Training took 0.109875 minutes
Weight histogram
[1651 1445 1313 1571 1444 1435 1427 1865 1342  682] [-2.98293328 -2.68580245 -2.38867162 -2.09154078 -1.79440995 -1.49727912
 -1.20014828 -0.90301745 -0.60588662 -0.30875578 -0.01162495]
[ 855 1367 1566 1472 1439 1428 1496 1532 1565 1455] [-2.98293328 -2.68580245 -2.38867162 -2.09154078 -1.79440995 -1.49727912
 -1.20014828 -0.90301745 -0.60588662 -0.30875578 -0.01162495]
-109.302
113.629
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148627 minutes
Weight histogram
[ 142  596 1071 1269 2093 3307 3262 3337 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  297  414  553  843 1409  891 1448 2995 7091] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.916799
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  86.1883
Epoch 1, cost is  82.1432
Epoch 2, cost is  82.2035
Epoch 3, cost is  83.1407
Epoch 4, cost is  84.4449
Training took 0.085855 minutes
Weight histogram
[1643 1462 1385 1525 1261 1233 1543 1438 1518 3192] [-6.29906797 -5.67021798 -5.04136798 -4.41251799 -3.783668   -3.154818
 -2.52596801 -1.89711801 -1.26826802 -0.63941802 -0.01056803]
[3240 1391 1332 1441 1357 1295 1574 1560 1541 1469] [-6.29906797 -5.67021798 -5.04136798 -4.41251799 -3.783668   -3.154818
 -2.52596801 -1.89711801 -1.26826802 -0.63941802 -0.01056803]
-235.024
235.76
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.34268
Epoch 1, cost is  2.96459
Epoch 2, cost is  3.16878
Epoch 3, cost is  3.48543
Epoch 4, cost is  3.77018
Training took 0.216828 minutes
Weight histogram
[1543 2154 2248 2084 1938 1686 1310  802  255  155] [-0.42494649 -0.38391494 -0.34288338 -0.30185183 -0.26082028 -0.21978873
 -0.17875718 -0.13772563 -0.09669408 -0.05566253 -0.01463098]
[ 651  993 1334 1540 1708 1777 1884 1923 1767  598] [-0.42494649 -0.38391494 -0.34288338 -0.30185183 -0.26082028 -0.21978873
 -0.17875718 -0.13772563 -0.09669408 -0.05566253 -0.01463098]
-27.8845
29.4731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.054322 minutes
Epoch 0
Fine tuning took 0.053827 minutes
Epoch 0
Fine tuning took 0.054608 minutes
{'zero': {0: [0.18349753694581281, 0.16379310344827586, 0.25123152709359609, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.71305418719211822, 0.59975369458128081, 0.66502463054187189], 5: [0.14162561576354679, 0.12315270935960591, 0.14901477832512317, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18349753694581281, 0.23645320197044334, 0.25, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.62561576354679804, 0.59729064039408863, 0.63177339901477836], 5: [0.14162561576354679, 0.13793103448275862, 0.15270935960591134, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18349753694581281, 0.18596059113300492, 0.24261083743842365, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.67610837438423643, 0.58866995073891626, 0.60098522167487689], 5: [0.14162561576354679, 0.13793103448275862, 0.16871921182266009, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18349753694581281, 0.22660098522167488, 0.27463054187192121, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.63054187192118227, 0.48522167487684731, 0.49753694581280788], 5: [0.14162561576354679, 0.14285714285714285, 0.24014778325123154, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150220 minutes
Weight histogram
[ 115  275  568  675 1170 3197 2869 3083 3483  765] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
[ 135  151  209  298  449  751  963 1780 3937 7527] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.18361
Epoch 1, cost is  6.64738
Epoch 2, cost is  6.54071
Epoch 3, cost is  6.53014
Epoch 4, cost is  6.57917
Training took 0.114860 minutes
Weight histogram
[1896 1672 1723 1851 1823 1790 1611 2310 1221  303] [-0.61959016 -0.55785946 -0.49612875 -0.43439805 -0.37266734 -0.31093664
 -0.24920593 -0.18747523 -0.12574452 -0.06401382 -0.00228311]
[ 601 1267 1834 1689 1714 1792 1910 1943 1783 1667] [-0.61959016 -0.55785946 -0.49612875 -0.43439805 -0.37266734 -0.31093664
 -0.24920593 -0.18747523 -0.12574452 -0.06401382 -0.00228311]
-30.508
36.5834
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148810 minutes
Weight histogram
[ 142  596 1071 1468 2827 4221 3439 3338 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  299  418  557  843 1416  903 1575 2977 8978] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.943999
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.0172
Epoch 1, cost is  19.1102
Epoch 2, cost is  19.0226
Epoch 3, cost is  19.1058
Epoch 4, cost is  19.2946
Training took 0.086244 minutes
Weight histogram
[1752 2141 1406 1451 1598 1720 1810 1759 1809 2779] [-1.39352    -1.254376   -1.11523201 -0.97608802 -0.83694402 -0.69780003
 -0.55865603 -0.41951204 -0.28036805 -0.14122405 -0.00208006]
[3021 1590 1698 1630 1559 1707 1770 1756 1737 1757] [-1.39352    -1.254376   -1.11523201 -0.97608802 -0.83694402 -0.69780003
 -0.55865603 -0.41951204 -0.28036805 -0.14122405 -0.00208006]
-50.2345
43.6034
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.98694
Epoch 1, cost is  4.45929
Epoch 2, cost is  4.85002
Epoch 3, cost is  5.36064
Epoch 4, cost is  5.85739
Training took 0.094978 minutes
Weight histogram
[1983 2689 2666 2388 2455 1776  969  522  375  377] [-0.34779999 -0.31324111 -0.27868223 -0.24412336 -0.20956448 -0.17500561
 -0.14044673 -0.10588785 -0.07132898 -0.0367701  -0.00221122]
[ 735  920 1199 1561 1756 1956 1982 2066 2167 1858] [-0.34779999 -0.31324111 -0.27868223 -0.24412336 -0.20956448 -0.17500561
 -0.14044673 -0.10588785 -0.07132898 -0.0367701  -0.00221122]
-20.8546
19.9534
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045686 minutes
Epoch 0
Fine tuning took 0.045919 minutes
Epoch 0
Fine tuning took 0.046809 minutes
{'zero': {0: [0.30911330049261082, 0.34852216748768472, 0.33128078817733991, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.42733990147783252, 0.46674876847290642, 0.52832512315270941], 5: [0.3682266009852217, 0.22413793103448276, 0.2019704433497537, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30911330049261082, 0.33128078817733991, 0.33743842364532017, 0.28694581280788178], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.45197044334975367, 0.47413793103448276, 0.53940886699507384], 5: [0.3682266009852217, 0.21674876847290642, 0.18842364532019704, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30911330049261082, 0.35098522167487683, 0.32019704433497537, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.44458128078817732, 0.47783251231527096, 0.53817733990147787], 5: [0.3682266009852217, 0.20443349753694581, 0.2019704433497537, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30911330049261082, 0.35344827586206895, 0.36206896551724138, 0.31773399014778325], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.32266009852216748, 0.4642857142857143, 0.44950738916256155, 0.50492610837438423], 5: [0.3682266009852217, 0.18226600985221675, 0.18842364532019704, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149158 minutes
Weight histogram
[ 115  275  568  675 1170 3197 2869 3083 3483  765] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
[ 135  151  209  298  449  751  963 1780 3937 7527] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.18361
Epoch 1, cost is  6.64738
Epoch 2, cost is  6.54071
Epoch 3, cost is  6.53014
Epoch 4, cost is  6.57917
Training took 0.115151 minutes
Weight histogram
[1896 1672 1723 1851 1823 1790 1611 2310 1221  303] [-0.61959016 -0.55785946 -0.49612875 -0.43439805 -0.37266734 -0.31093664
 -0.24920593 -0.18747523 -0.12574452 -0.06401382 -0.00228311]
[ 601 1267 1834 1689 1714 1792 1910 1943 1783 1667] [-0.61959016 -0.55785946 -0.49612875 -0.43439805 -0.37266734 -0.31093664
 -0.24920593 -0.18747523 -0.12574452 -0.06401382 -0.00228311]
-30.508
36.5834
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148951 minutes
Weight histogram
[ 142  596 1071 1468 2827 4221 3439 3338 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  299  418  557  843 1416  903 1575 2977 8978] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.943999
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.0172
Epoch 1, cost is  19.1102
Epoch 2, cost is  19.0226
Epoch 3, cost is  19.1058
Epoch 4, cost is  19.2946
Training took 0.087923 minutes
Weight histogram
[1752 2141 1406 1451 1598 1720 1810 1759 1809 2779] [-1.39352    -1.254376   -1.11523201 -0.97608802 -0.83694402 -0.69780003
 -0.55865603 -0.41951204 -0.28036805 -0.14122405 -0.00208006]
[3021 1590 1698 1630 1559 1707 1770 1756 1737 1757] [-1.39352    -1.254376   -1.11523201 -0.97608802 -0.83694402 -0.69780003
 -0.55865603 -0.41951204 -0.28036805 -0.14122405 -0.00208006]
-50.2345
43.6034
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.56232
Epoch 1, cost is  3.75798
Epoch 2, cost is  3.94019
Epoch 3, cost is  4.36811
Epoch 4, cost is  4.89727
Training took 0.108724 minutes
Weight histogram
[1907 2426 2450 2257 2252 2036 1340  660  464  408] [-0.28246394 -0.25444338 -0.22642283 -0.19840227 -0.17038172 -0.14236116
 -0.1143406  -0.08632005 -0.05829949 -0.03027894 -0.00225838]
[ 816  981 1331 1471 1675 1916 2013 2052 2083 1862] [-0.28246394 -0.25444338 -0.22642283 -0.19840227 -0.17038172 -0.14236116
 -0.1143406  -0.08632005 -0.05829949 -0.03027894 -0.00225838]
-10.2874
13.4142
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048690 minutes
Epoch 0
Fine tuning took 0.046872 minutes
Epoch 0
Fine tuning took 0.048180 minutes
{'zero': {0: [0.21674876847290642, 0.24876847290640394, 0.25862068965517243, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.5923645320197044, 0.53448275862068961, 0.5788177339901478], 5: [0.1539408866995074, 0.15886699507389163, 0.20689655172413793, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21674876847290642, 0.23399014778325122, 0.25, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.60344827586206895, 0.59605911330049266, 0.61576354679802958], 5: [0.1539408866995074, 0.1625615763546798, 0.1539408866995074, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21674876847290642, 0.27832512315270935, 0.29679802955665024, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.58497536945812811, 0.55788177339901479, 0.59605911330049266], 5: [0.1539408866995074, 0.13669950738916256, 0.14532019704433496, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21674876847290642, 0.28694581280788178, 0.26724137931034481, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.56280788177339902, 0.58743842364532017, 0.62068965517241381], 5: [0.1539408866995074, 0.15024630541871922, 0.14532019704433496, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149279 minutes
Weight histogram
[ 115  275  568  675 1170 3197 2869 3083 3483  765] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
[ 135  151  209  298  449  751  963 1780 3937 7527] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.18361
Epoch 1, cost is  6.64738
Epoch 2, cost is  6.54071
Epoch 3, cost is  6.53014
Epoch 4, cost is  6.57917
Training took 0.114955 minutes
Weight histogram
[1896 1672 1723 1851 1823 1790 1611 2310 1221  303] [-0.61959016 -0.55785946 -0.49612875 -0.43439805 -0.37266734 -0.31093664
 -0.24920593 -0.18747523 -0.12574452 -0.06401382 -0.00228311]
[ 601 1267 1834 1689 1714 1792 1910 1943 1783 1667] [-0.61959016 -0.55785946 -0.49612875 -0.43439805 -0.37266734 -0.31093664
 -0.24920593 -0.18747523 -0.12574452 -0.06401382 -0.00228311]
-30.508
36.5834
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148915 minutes
Weight histogram
[ 142  596 1071 1468 2827 4221 3439 3338 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  299  418  557  843 1416  903 1575 2977 8978] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.943999
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.0172
Epoch 1, cost is  19.1102
Epoch 2, cost is  19.0226
Epoch 3, cost is  19.1058
Epoch 4, cost is  19.2946
Training took 0.086132 minutes
Weight histogram
[1752 2141 1406 1451 1598 1720 1810 1759 1809 2779] [-1.39352    -1.254376   -1.11523201 -0.97608802 -0.83694402 -0.69780003
 -0.55865603 -0.41951204 -0.28036805 -0.14122405 -0.00208006]
[3021 1590 1698 1630 1559 1707 1770 1756 1737 1757] [-1.39352    -1.254376   -1.11523201 -0.97608802 -0.83694402 -0.69780003
 -0.55865603 -0.41951204 -0.28036805 -0.14122405 -0.00208006]
-50.2345
43.6034
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.02427
Epoch 1, cost is  2.68905
Epoch 2, cost is  2.61816
Epoch 3, cost is  2.76698
Epoch 4, cost is  2.94709
Training took 0.151895 minutes
Weight histogram
[2125 2655 2393 2366 1991 1748 1321  744  499  358] [-0.18184739 -0.16388522 -0.14592305 -0.12796088 -0.1099987  -0.09203653
 -0.07407436 -0.05611218 -0.03815001 -0.02018784 -0.00222567]
[ 904  903 1162 1457 1651 1898 1983 2051 2179 2012] [-0.18184739 -0.16388522 -0.14592305 -0.12796088 -0.1099987  -0.09203653
 -0.07407436 -0.05611218 -0.03815001 -0.02018784 -0.00222567]
-7.41958
9.17823
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051852 minutes
Epoch 0
Fine tuning took 0.050880 minutes
Epoch 0
Fine tuning took 0.052167 minutes
{'zero': {0: [0.2229064039408867, 0.26847290640394089, 0.25, 0.29802955665024633], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65517241379310343, 0.63423645320197042, 0.68349753694581283, 0.62438423645320196], 5: [0.12192118226600986, 0.097290640394088676, 0.066502463054187194, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2229064039408867, 0.18719211822660098, 0.2105911330049261, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65517241379310343, 0.67980295566502458, 0.66379310344827591, 0.6354679802955665], 5: [0.12192118226600986, 0.13300492610837439, 0.12561576354679804, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2229064039408867, 0.2019704433497537, 0.22413793103448276, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65517241379310343, 0.67610837438423643, 0.65147783251231528, 0.6428571428571429], 5: [0.12192118226600986, 0.12192118226600986, 0.12438423645320197, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2229064039408867, 0.18965517241379309, 0.21921182266009853, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65517241379310343, 0.66748768472906406, 0.67487684729064035, 0.58990147783251234], 5: [0.12192118226600986, 0.14285714285714285, 0.10591133004926108, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149023 minutes
Weight histogram
[ 115  275  568  675 1170 3197 2869 3083 3483  765] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
[ 135  151  209  298  449  751  963 1780 3937 7527] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.18361
Epoch 1, cost is  6.64738
Epoch 2, cost is  6.54071
Epoch 3, cost is  6.53014
Epoch 4, cost is  6.57917
Training took 0.115100 minutes
Weight histogram
[1896 1672 1723 1851 1823 1790 1611 2310 1221  303] [-0.61959016 -0.55785946 -0.49612875 -0.43439805 -0.37266734 -0.31093664
 -0.24920593 -0.18747523 -0.12574452 -0.06401382 -0.00228311]
[ 601 1267 1834 1689 1714 1792 1910 1943 1783 1667] [-0.61959016 -0.55785946 -0.49612875 -0.43439805 -0.37266734 -0.31093664
 -0.24920593 -0.18747523 -0.12574452 -0.06401382 -0.00228311]
-30.508
36.5834
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148828 minutes
Weight histogram
[ 142  596 1071 1468 2827 4221 3439 3338 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  299  418  557  843 1416  903 1575 2977 8978] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.943999
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.0172
Epoch 1, cost is  19.1102
Epoch 2, cost is  19.0226
Epoch 3, cost is  19.1058
Epoch 4, cost is  19.2946
Training took 0.086327 minutes
Weight histogram
[1752 2141 1406 1451 1598 1720 1810 1759 1809 2779] [-1.39352    -1.254376   -1.11523201 -0.97608802 -0.83694402 -0.69780003
 -0.55865603 -0.41951204 -0.28036805 -0.14122405 -0.00208006]
[3021 1590 1698 1630 1559 1707 1770 1756 1737 1757] [-1.39352    -1.254376   -1.11523201 -0.97608802 -0.83694402 -0.69780003
 -0.55865603 -0.41951204 -0.28036805 -0.14122405 -0.00208006]
-50.2345
43.6034
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.71502
Epoch 1, cost is  2.0293
Epoch 2, cost is  1.74482
Epoch 3, cost is  1.70824
Epoch 4, cost is  1.71395
Training took 0.212320 minutes
Weight histogram
[2599 2796 2555 2420 1777 1611 1033  708  444  257] [-0.11641122 -0.10499749 -0.09358376 -0.08217003 -0.07075631 -0.05934258
 -0.04792885 -0.03651512 -0.0251014  -0.01368767 -0.00227394]
[ 898  789 1054 1355 1592 1908 2069 2208 2360 1967] [-0.11641122 -0.10499749 -0.09358376 -0.08217003 -0.07075631 -0.05934258
 -0.04792885 -0.03651512 -0.0251014  -0.01368767 -0.00227394]
-6.7849
7.03904
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.056319 minutes
Epoch 0
Fine tuning took 0.056221 minutes
Epoch 0
Fine tuning took 0.055138 minutes
{'zero': {0: [0.27463054187192121, 0.34975369458128081, 0.27339901477832512, 0.32266009852216748], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59359605911330049, 0.55049261083743839, 0.66625615763546797, 0.62931034482758619], 5: [0.13177339901477833, 0.099753694581280791, 0.060344827586206899, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27463054187192121, 0.24507389162561577, 0.22167487684729065, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59359605911330049, 0.64901477832512311, 0.67980295566502458, 0.69334975369458129], 5: [0.13177339901477833, 0.10591133004926108, 0.098522167487684734, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27463054187192121, 0.24384236453201971, 0.23645320197044334, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59359605911330049, 0.64532019704433496, 0.6428571428571429, 0.65886699507389157], 5: [0.13177339901477833, 0.11083743842364532, 0.1206896551724138, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27463054187192121, 0.3460591133004926, 0.25985221674876846, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59359605911330049, 0.53817733990147787, 0.62438423645320196, 0.67610837438423643], 5: [0.13177339901477833, 0.11576354679802955, 0.11576354679802955, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150682 minutes
Weight histogram
[ 115  275  568  675 1170 3197 2869 3083 3483  765] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
[ 135  151  209  298  449  751  963 1780 3937 7527] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  32.3552
Epoch 1, cost is  29.6806
Epoch 2, cost is  29.0603
Epoch 3, cost is  28.9459
Epoch 4, cost is  29.0746
Training took 0.109593 minutes
Weight histogram
[1675 1448 1740 1605 1707 1822 1709 1822 1786  886] [-3.46302986 -3.11788937 -2.77274888 -2.42760839 -2.0824679  -1.73732741
 -1.39218692 -1.04704642 -0.70190593 -0.35676544 -0.01162495]
[1033 1650 1741 1642 1640 1703 1734 1776 1650 1631] [-3.46302986 -3.11788937 -2.77274888 -2.42760839 -2.0824679  -1.73732741
 -1.39218692 -1.04704642 -0.70190593 -0.35676544 -0.01162495]
-125.125
139.141
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149450 minutes
Weight histogram
[ 142  596 1071 1468 2827 4221 3439 3338 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  299  418  557  843 1416  903 1575 2977 8978] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.943999
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  97.893
Epoch 1, cost is  92.3451
Epoch 2, cost is  91.4577
Epoch 3, cost is  91.7708
Epoch 4, cost is  92.4566
Training took 0.086617 minutes
Weight histogram
[1720 1630 1719 1566 1593 1620 1539 1683 1765 3390] [-7.23401833 -6.5116733  -5.78932827 -5.06698324 -4.34463821 -3.62229318
 -2.89994815 -2.17760312 -1.45525809 -0.73291306 -0.01056803]
[3412 1562 1551 1563 1491 1684 1747 1730 1691 1794] [-7.23401833 -6.5116733  -5.78932827 -5.06698324 -4.34463821 -3.62229318
 -2.89994815 -2.17760312 -1.45525809 -0.73291306 -0.01056803]
-257.032
245.881
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.67018
Epoch 1, cost is  6.3358
Epoch 2, cost is  8.03892
Epoch 3, cost is  9.77538
Epoch 4, cost is  11.3877
Training took 0.093680 minutes
Weight histogram
[ 179  549 1011 1390 1959 5500 5204  348   18   42] [-0.82977349 -0.74756416 -0.66535484 -0.58314551 -0.50093619 -0.41872686
 -0.33651754 -0.25430822 -0.17209889 -0.08988957 -0.00768024]
[1018 2870 2798 2441 2215 1829  937  968  896  228] [-0.82977349 -0.74756416 -0.66535484 -0.58314551 -0.50093619 -0.41872686
 -0.33651754 -0.25430822 -0.17209889 -0.08988957 -0.00768024]
-66.4183
62.6716
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045506 minutes
Epoch 0
Fine tuning took 0.043409 minutes
Epoch 0
Fine tuning took 0.045723 minutes
{'zero': {0: [0.16748768472906403, 0.23275862068965517, 0.21428571428571427, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.14655172413793102, 0.53817733990147787, 0.50862068965517238, 0.56527093596059108], 5: [0.68596059113300489, 0.22906403940886699, 0.27709359605911332, 0.29679802955665024], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16748768472906403, 0.24384236453201971, 0.24014778325123154, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.14655172413793102, 0.51108374384236455, 0.50369458128078815, 0.5431034482758621], 5: [0.68596059113300489, 0.24507389162561577, 0.25615763546798032, 0.29556650246305421], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16748768472906403, 0.26970443349753692, 0.2413793103448276, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.14655172413793102, 0.53448275862068961, 0.51847290640394084, 0.56403940886699511], 5: [0.68596059113300489, 0.19581280788177341, 0.24014778325123154, 0.27586206896551724], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16748768472906403, 0.25985221674876846, 0.20443349753694581, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.14655172413793102, 0.50615763546798032, 0.54802955665024633, 0.53325123152709364], 5: [0.68596059113300489, 0.23399014778325122, 0.24753694581280788, 0.30788177339901479], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150410 minutes
Weight histogram
[ 115  275  568  675 1170 3197 2869 3083 3483  765] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
[ 135  151  209  298  449  751  963 1780 3937 7527] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  32.3552
Epoch 1, cost is  29.6806
Epoch 2, cost is  29.0603
Epoch 3, cost is  28.9459
Epoch 4, cost is  29.0746
Training took 0.109546 minutes
Weight histogram
[1675 1448 1740 1605 1707 1822 1709 1822 1786  886] [-3.46302986 -3.11788937 -2.77274888 -2.42760839 -2.0824679  -1.73732741
 -1.39218692 -1.04704642 -0.70190593 -0.35676544 -0.01162495]
[1033 1650 1741 1642 1640 1703 1734 1776 1650 1631] [-3.46302986 -3.11788937 -2.77274888 -2.42760839 -2.0824679  -1.73732741
 -1.39218692 -1.04704642 -0.70190593 -0.35676544 -0.01162495]
-125.125
139.141
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149017 minutes
Weight histogram
[ 142  596 1071 1468 2827 4221 3439 3338 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  299  418  557  843 1416  903 1575 2977 8978] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.943999
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  97.893
Epoch 1, cost is  92.3451
Epoch 2, cost is  91.4577
Epoch 3, cost is  91.7708
Epoch 4, cost is  92.4566
Training took 0.084787 minutes
Weight histogram
[1720 1630 1719 1566 1593 1620 1539 1683 1765 3390] [-7.23401833 -6.5116733  -5.78932827 -5.06698324 -4.34463821 -3.62229318
 -2.89994815 -2.17760312 -1.45525809 -0.73291306 -0.01056803]
[3412 1562 1551 1563 1491 1684 1747 1730 1691 1794] [-7.23401833 -6.5116733  -5.78932827 -5.06698324 -4.34463821 -3.62229318
 -2.89994815 -2.17760312 -1.45525809 -0.73291306 -0.01056803]
-257.032
245.881
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.7018
Epoch 1, cost is  5.96776
Epoch 2, cost is  7.84508
Epoch 3, cost is  9.78035
Epoch 4, cost is  11.7064
Training took 0.108683 minutes
Weight histogram
[ 297  657  845 1770 2720 3554 3923 2134  249   51] [-0.86838162 -0.7827708  -0.69715998 -0.61154917 -0.52593835 -0.44032753
 -0.35471671 -0.2691059  -0.18349508 -0.09788426 -0.01227344]
[1080 2020 2251 2197 2176 2037 1939 1271  852  377] [-0.86838162 -0.7827708  -0.69715998 -0.61154917 -0.52593835 -0.44032753
 -0.35471671 -0.2691059  -0.18349508 -0.09788426 -0.01227344]
-75.5509
72.5006
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044128 minutes
Epoch 0
Fine tuning took 0.046141 minutes
Epoch 0
Fine tuning took 0.047303 minutes
{'zero': {0: [0.25862068965517243, 0.1268472906403941, 0.14408866995073891, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.75123152709359609, 0.77832512315270941, 0.73768472906403937], 5: [0.22783251231527094, 0.12192118226600986, 0.077586206896551727, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25862068965517243, 0.20566502463054187, 0.25, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.59852216748768472, 0.55418719211822665, 0.69334975369458129], 5: [0.22783251231527094, 0.19581280788177341, 0.19581280788177341, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25862068965517243, 0.22536945812807882, 0.26724137931034481, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.61330049261083741, 0.55418719211822665, 0.66748768472906406], 5: [0.22783251231527094, 0.16133004926108374, 0.17857142857142858, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25862068965517243, 0.19088669950738915, 0.26970443349753692, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.61330049261083741, 0.55049261083743839, 0.68842364532019706], 5: [0.22783251231527094, 0.19581280788177341, 0.17980295566502463, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149767 minutes
Weight histogram
[ 115  275  568  675 1170 3197 2869 3083 3483  765] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
[ 135  151  209  298  449  751  963 1780 3937 7527] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  32.3552
Epoch 1, cost is  29.6806
Epoch 2, cost is  29.0603
Epoch 3, cost is  28.9459
Epoch 4, cost is  29.0746
Training took 0.112292 minutes
Weight histogram
[1675 1448 1740 1605 1707 1822 1709 1822 1786  886] [-3.46302986 -3.11788937 -2.77274888 -2.42760839 -2.0824679  -1.73732741
 -1.39218692 -1.04704642 -0.70190593 -0.35676544 -0.01162495]
[1033 1650 1741 1642 1640 1703 1734 1776 1650 1631] [-3.46302986 -3.11788937 -2.77274888 -2.42760839 -2.0824679  -1.73732741
 -1.39218692 -1.04704642 -0.70190593 -0.35676544 -0.01162495]
-125.125
139.141
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149668 minutes
Weight histogram
[ 142  596 1071 1468 2827 4221 3439 3338 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  299  418  557  843 1416  903 1575 2977 8978] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.943999
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  97.893
Epoch 1, cost is  92.3451
Epoch 2, cost is  91.4577
Epoch 3, cost is  91.7708
Epoch 4, cost is  92.4566
Training took 0.083394 minutes
Weight histogram
[1720 1630 1719 1566 1593 1620 1539 1683 1765 3390] [-7.23401833 -6.5116733  -5.78932827 -5.06698324 -4.34463821 -3.62229318
 -2.89994815 -2.17760312 -1.45525809 -0.73291306 -0.01056803]
[3412 1562 1551 1563 1491 1684 1747 1730 1691 1794] [-7.23401833 -6.5116733  -5.78932827 -5.06698324 -4.34463821 -3.62229318
 -2.89994815 -2.17760312 -1.45525809 -0.73291306 -0.01056803]
-257.032
245.881
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.12829
Epoch 1, cost is  4.95804
Epoch 2, cost is  6.20256
Epoch 3, cost is  7.49317
Epoch 4, cost is  8.67843
Training took 0.150731 minutes
Weight histogram
[1029 2083 2200 2120 2228 2222 2198 1791  249   80] [-0.64533269 -0.58224286 -0.51915303 -0.45606319 -0.39297336 -0.32988353
 -0.26679369 -0.20370386 -0.14061403 -0.07752419 -0.01443436]
[ 903 1559 1772 1810 1856 1819 1798 1865 1798 1020] [-0.64533269 -0.58224286 -0.51915303 -0.45606319 -0.39297336 -0.32988353
 -0.26679369 -0.20370386 -0.14061403 -0.07752419 -0.01443436]
-44.2971
43.253
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049040 minutes
Epoch 0
Fine tuning took 0.050359 minutes
Epoch 0
Fine tuning took 0.050068 minutes
{'zero': {0: [0.2229064039408867, 0.21182266009852216, 0.15147783251231528, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.61945812807881773, 0.66256157635467983, 0.60837438423645318], 5: [0.1625615763546798, 0.16871921182266009, 0.18596059113300492, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2229064039408867, 0.21428571428571427, 0.29310344827586204, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.66009852216748766, 0.54926108374384242, 0.5714285714285714], 5: [0.1625615763546798, 0.12561576354679804, 0.15763546798029557, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2229064039408867, 0.24014778325123154, 0.23645320197044334, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.66009852216748766, 0.6354679802955665, 0.60591133004926112], 5: [0.1625615763546798, 0.099753694581280791, 0.12807881773399016, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2229064039408867, 0.24630541871921183, 0.43965517241379309, 0.45197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.60591133004926112, 0.44704433497536944, 0.40024630541871919], 5: [0.1625615763546798, 0.14778325123152711, 0.11330049261083744, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151366 minutes
Weight histogram
[ 115  275  568  675 1170 3197 2869 3083 3483  765] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
[ 135  151  209  298  449  751  963 1780 3937 7527] [ -2.39681889e-04   2.66588089e-05   2.92999507e-04   5.59340205e-04
   8.25680903e-04   1.09202160e-03   1.35836230e-03   1.62470300e-03
   1.89104369e-03   2.15738439e-03   2.42372509e-03]
-1.59994
1.07537
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  32.3552
Epoch 1, cost is  29.6806
Epoch 2, cost is  29.0603
Epoch 3, cost is  28.9459
Epoch 4, cost is  29.0746
Training took 0.109780 minutes
Weight histogram
[1675 1448 1740 1605 1707 1822 1709 1822 1786  886] [-3.46302986 -3.11788937 -2.77274888 -2.42760839 -2.0824679  -1.73732741
 -1.39218692 -1.04704642 -0.70190593 -0.35676544 -0.01162495]
[1033 1650 1741 1642 1640 1703 1734 1776 1650 1631] [-3.46302986 -3.11788937 -2.77274888 -2.42760839 -2.0824679  -1.73732741
 -1.39218692 -1.04704642 -0.70190593 -0.35676544 -0.01162495]
-125.125
139.141
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149681 minutes
Weight histogram
[ 142  596 1071 1468 2827 4221 3439 3338 1032   91] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[ 259  299  418  557  843 1416  903 1575 2977 8978] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.943999
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  97.893
Epoch 1, cost is  92.3451
Epoch 2, cost is  91.4577
Epoch 3, cost is  91.7708
Epoch 4, cost is  92.4566
Training took 0.085728 minutes
Weight histogram
[1720 1630 1719 1566 1593 1620 1539 1683 1765 3390] [-7.23401833 -6.5116733  -5.78932827 -5.06698324 -4.34463821 -3.62229318
 -2.89994815 -2.17760312 -1.45525809 -0.73291306 -0.01056803]
[3412 1562 1551 1563 1491 1684 1747 1730 1691 1794] [-7.23401833 -6.5116733  -5.78932827 -5.06698324 -4.34463821 -3.62229318
 -2.89994815 -2.17760312 -1.45525809 -0.73291306 -0.01056803]
-257.032
245.881
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  2.92934
Epoch 1, cost is  2.43233
Epoch 2, cost is  2.53699
Epoch 3, cost is  2.75489
Epoch 4, cost is  3.0475
Training took 0.213936 minutes
Weight histogram
[1833 2473 2562 2367 2188 1922 1469  914  299  173] [-0.42516088 -0.38410347 -0.34304606 -0.30198865 -0.26093123 -0.21987382
 -0.17881641 -0.13775899 -0.09670158 -0.05564417 -0.01458676]
[ 740 1126 1515 1754 1937 2021 2147 2177 2033  750] [-0.42516088 -0.38410347 -0.34304606 -0.30198865 -0.26093123 -0.21987382
 -0.17881641 -0.13775899 -0.09670158 -0.05564417 -0.01458676]
-27.8845
29.4731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.054922 minutes
Epoch 0
Fine tuning took 0.054328 minutes
Epoch 0
Fine tuning took 0.054996 minutes
{'zero': {0: [0.14285714285714285, 0.076354679802955669, 0.12438423645320197, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.84113300492610843, 0.79433497536945807, 0.79433497536945807], 5: [0.11330049261083744, 0.082512315270935957, 0.081280788177339899, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14285714285714285, 0.1539408866995074, 0.1748768472906404, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.75862068965517238, 0.68226600985221675, 0.75246305418719217], 5: [0.11330049261083744, 0.087438423645320201, 0.14285714285714285, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14285714285714285, 0.1354679802955665, 0.18719211822660098, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.76724137931034486, 0.68349753694581283, 0.70812807881773399], 5: [0.11330049261083744, 0.097290640394088676, 0.12931034482758622, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14285714285714285, 0.12438423645320197, 0.14778325123152711, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.78325123152709364, 0.68349753694581283, 0.70320197044334976], 5: [0.11330049261083744, 0.092364532019704432, 0.16871921182266009, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150553 minutes
Weight histogram
[ 144  335  705  841 2393 3778 3150 4043 2075  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[ 135  151  209  298  449  751  963 1780 3937 9552] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.64241
Epoch 1, cost is  7.09457
Epoch 2, cost is  6.9452
Epoch 3, cost is  6.87811
Epoch 4, cost is  6.86413
Training took 0.114768 minutes
Weight histogram
[2028 1981 1922 2119 1980 2030 1849 2371 1569  376] [-0.68760663 -0.61907428 -0.55054193 -0.48200958 -0.41347723 -0.34494487
 -0.27641252 -0.20788017 -0.13934782 -0.07081547 -0.00228311]
[ 690 1538 2012 1805 1919 2013 2226 1980 1836 2206] [-0.68760663 -0.61907428 -0.55054193 -0.48200958 -0.41347723 -0.34494487
 -0.27641252 -0.20788017 -0.13934782 -0.07081547 -0.00228311]
-39.6264
39.3643
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149206 minutes
Weight histogram
[ 142  596 1071 1468 2833 4413 4239 4165 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 11003] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.9506
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.9783
Epoch 1, cost is  21.865
Epoch 2, cost is  21.6871
Epoch 3, cost is  21.7267
Epoch 4, cost is  21.886
Training took 0.087213 minutes
Weight histogram
[1968 1792 2219 1744 1790 1679 2026 1949 2056 3027] [-1.57094872 -1.41406185 -1.25717499 -1.10028812 -0.94340125 -0.78651439
 -0.62962752 -0.47274066 -0.31585379 -0.15896692 -0.00208006]
[3183 1808 1881 1780 1749 1983 1944 1939 1957 2026] [-1.57094872 -1.41406185 -1.25717499 -1.10028812 -0.94340125 -0.78651439
 -0.62962752 -0.47274066 -0.31585379 -0.15896692 -0.00208006]
-55.0173
47.896
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.97912
Epoch 1, cost is  4.45163
Epoch 2, cost is  4.7764
Epoch 3, cost is  5.23314
Epoch 4, cost is  5.71317
Training took 0.092014 minutes
Weight histogram
[2278 2989 3009 2713 2725 1987 1091  591  419  423] [-0.34779999 -0.31323836 -0.27867673 -0.2441151  -0.20955347 -0.17499183
 -0.1404302  -0.10586857 -0.07130694 -0.03674531 -0.00218368]
[ 823 1033 1346 1758 1974 2200 2227 2312 2422 2130] [-0.34779999 -0.31323836 -0.27867673 -0.2441151  -0.20955347 -0.17499183
 -0.1404302  -0.10586857 -0.07130694 -0.03674531 -0.00218368]
-20.8546
19.9534
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046541 minutes
Epoch 0
Fine tuning took 0.044991 minutes
Epoch 0
Fine tuning took 0.045127 minutes
{'zero': {0: [0.52463054187192115, 0.32758620689655171, 0.33251231527093594, 0.39039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3682266009852217, 0.49261083743842365, 0.50985221674876846, 0.44088669950738918], 5: [0.10714285714285714, 0.17980295566502463, 0.15763546798029557, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.52463054187192115, 0.31896551724137934, 0.32758620689655171, 0.39778325123152708], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3682266009852217, 0.45812807881773399, 0.47044334975369456, 0.44334975369458129], 5: [0.10714285714285714, 0.2229064039408867, 0.2019704433497537, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.52463054187192115, 0.35221674876847292, 0.34113300492610837, 0.41995073891625617], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3682266009852217, 0.42857142857142855, 0.46674876847290642, 0.41995073891625617], 5: [0.10714285714285714, 0.21921182266009853, 0.19211822660098521, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.52463054187192115, 0.31896551724137934, 0.34236453201970446, 0.36576354679802958], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.3682266009852217, 0.45443349753694579, 0.45812807881773399, 0.44334975369458129], 5: [0.10714285714285714, 0.22660098522167488, 0.19950738916256158, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150566 minutes
Weight histogram
[ 144  335  705  841 2393 3778 3150 4043 2075  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[ 135  151  209  298  449  751  963 1780 3937 9552] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.64241
Epoch 1, cost is  7.09457
Epoch 2, cost is  6.9452
Epoch 3, cost is  6.87811
Epoch 4, cost is  6.86413
Training took 0.115843 minutes
Weight histogram
[2028 1981 1922 2119 1980 2030 1849 2371 1569  376] [-0.68760663 -0.61907428 -0.55054193 -0.48200958 -0.41347723 -0.34494487
 -0.27641252 -0.20788017 -0.13934782 -0.07081547 -0.00228311]
[ 690 1538 2012 1805 1919 2013 2226 1980 1836 2206] [-0.68760663 -0.61907428 -0.55054193 -0.48200958 -0.41347723 -0.34494487
 -0.27641252 -0.20788017 -0.13934782 -0.07081547 -0.00228311]
-39.6264
39.3643
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149963 minutes
Weight histogram
[ 142  596 1071 1468 2833 4413 4239 4165 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 11003] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.9506
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.9783
Epoch 1, cost is  21.865
Epoch 2, cost is  21.6871
Epoch 3, cost is  21.7267
Epoch 4, cost is  21.886
Training took 0.085675 minutes
Weight histogram
[1968 1792 2219 1744 1790 1679 2026 1949 2056 3027] [-1.57094872 -1.41406185 -1.25717499 -1.10028812 -0.94340125 -0.78651439
 -0.62962752 -0.47274066 -0.31585379 -0.15896692 -0.00208006]
[3183 1808 1881 1780 1749 1983 1944 1939 1957 2026] [-1.57094872 -1.41406185 -1.25717499 -1.10028812 -0.94340125 -0.78651439
 -0.62962752 -0.47274066 -0.31585379 -0.15896692 -0.00208006]
-55.0173
47.896
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.55623
Epoch 1, cost is  3.78337
Epoch 2, cost is  4.00096
Epoch 3, cost is  4.40253
Epoch 4, cost is  4.86818
Training took 0.109654 minutes
Weight histogram
[2081 2742 2742 2558 2565 2306 1505  745  521  460] [-0.28246394 -0.2544409  -0.22641787 -0.19839483 -0.1703718  -0.14234876
 -0.11432573 -0.08630269 -0.05827965 -0.03025662 -0.00223358]
[ 916 1101 1496 1656 1884 2158 2273 2312 2352 2077] [-0.28246394 -0.2544409  -0.22641787 -0.19839483 -0.1703718  -0.14234876
 -0.11432573 -0.08630269 -0.05827965 -0.03025662 -0.00223358]
-10.2874
13.4142
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047375 minutes
Epoch 0
Fine tuning took 0.046336 minutes
Epoch 0
Fine tuning took 0.047626 minutes
{'zero': {0: [0.29679802955665024, 0.32142857142857145, 0.22167487684729065, 0.32389162561576357], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.57635467980295563, 0.58004926108374388, 0.56650246305418717], 5: [0.061576354679802957, 0.10221674876847291, 0.19827586206896552, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29679802955665024, 0.27093596059113301, 0.21921182266009853, 0.28078817733990147], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.64655172413793105, 0.67487684729064035, 0.63300492610837433], 5: [0.061576354679802957, 0.082512315270935957, 0.10591133004926108, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29679802955665024, 0.30911330049261082, 0.23645320197044334, 0.30172413793103448], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.58497536945812811, 0.6354679802955665, 0.56403940886699511], 5: [0.061576354679802957, 0.10591133004926108, 0.12807881773399016, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29679802955665024, 0.24384236453201971, 0.20073891625615764, 0.26970443349753692], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.68472906403940892, 0.72906403940886699, 0.67364532019704437], 5: [0.061576354679802957, 0.071428571428571425, 0.070197044334975367, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149065 minutes
Weight histogram
[ 144  335  705  841 2393 3778 3150 4043 2075  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[ 135  151  209  298  449  751  963 1780 3937 9552] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.64241
Epoch 1, cost is  7.09457
Epoch 2, cost is  6.9452
Epoch 3, cost is  6.87811
Epoch 4, cost is  6.86413
Training took 0.116438 minutes
Weight histogram
[2028 1981 1922 2119 1980 2030 1849 2371 1569  376] [-0.68760663 -0.61907428 -0.55054193 -0.48200958 -0.41347723 -0.34494487
 -0.27641252 -0.20788017 -0.13934782 -0.07081547 -0.00228311]
[ 690 1538 2012 1805 1919 2013 2226 1980 1836 2206] [-0.68760663 -0.61907428 -0.55054193 -0.48200958 -0.41347723 -0.34494487
 -0.27641252 -0.20788017 -0.13934782 -0.07081547 -0.00228311]
-39.6264
39.3643
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149996 minutes
Weight histogram
[ 142  596 1071 1468 2833 4413 4239 4165 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 11003] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.9506
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.9783
Epoch 1, cost is  21.865
Epoch 2, cost is  21.6871
Epoch 3, cost is  21.7267
Epoch 4, cost is  21.886
Training took 0.088105 minutes
Weight histogram
[1968 1792 2219 1744 1790 1679 2026 1949 2056 3027] [-1.57094872 -1.41406185 -1.25717499 -1.10028812 -0.94340125 -0.78651439
 -0.62962752 -0.47274066 -0.31585379 -0.15896692 -0.00208006]
[3183 1808 1881 1780 1749 1983 1944 1939 1957 2026] [-1.57094872 -1.41406185 -1.25717499 -1.10028812 -0.94340125 -0.78651439
 -0.62962752 -0.47274066 -0.31585379 -0.15896692 -0.00208006]
-55.0173
47.896
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.01395
Epoch 1, cost is  2.6996
Epoch 2, cost is  2.65276
Epoch 3, cost is  2.80413
Epoch 4, cost is  3.02111
Training took 0.148476 minutes
Weight histogram
[2376 2960 2724 2655 2254 1961 1488  848  558  401] [-0.18184739 -0.16388242 -0.14591744 -0.12795247 -0.10998749 -0.09202251
 -0.07405754 -0.05609256 -0.03812758 -0.02016261 -0.00219763]
[1021 1026 1324 1643 1877 2146 2242 2319 2461 2166] [-0.18184739 -0.16388242 -0.14591744 -0.12795247 -0.10998749 -0.09202251
 -0.07405754 -0.05609256 -0.03812758 -0.02016261 -0.00219763]
-7.56632
9.17823
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049738 minutes
Epoch 0
Fine tuning took 0.052295 minutes
Epoch 0
Fine tuning took 0.050006 minutes
{'zero': {0: [0.29556650246305421, 0.2376847290640394, 0.34729064039408869, 0.35344827586206895], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.64532019704433496, 0.54802955665024633, 0.56403940886699511], 5: [0.21921182266009853, 0.11699507389162561, 0.10467980295566502, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29556650246305421, 0.17857142857142858, 0.24876847290640394, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.72044334975369462, 0.66133004926108374, 0.61330049261083741], 5: [0.21921182266009853, 0.10098522167487685, 0.089901477832512317, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29556650246305421, 0.21921182266009853, 0.27463054187192121, 0.28448275862068967], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.65640394088669951, 0.61206896551724133, 0.58497536945812811], 5: [0.21921182266009853, 0.12438423645320197, 0.11330049261083744, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29556650246305421, 0.16995073891625614, 0.18226600985221675, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48522167487684731, 0.73275862068965514, 0.74384236453201968, 0.64655172413793105], 5: [0.21921182266009853, 0.097290640394088676, 0.073891625615763554, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149042 minutes
Weight histogram
[ 144  335  705  841 2393 3778 3150 4043 2075  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[ 135  151  209  298  449  751  963 1780 3937 9552] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.64241
Epoch 1, cost is  7.09457
Epoch 2, cost is  6.9452
Epoch 3, cost is  6.87811
Epoch 4, cost is  6.86413
Training took 0.116027 minutes
Weight histogram
[2028 1981 1922 2119 1980 2030 1849 2371 1569  376] [-0.68760663 -0.61907428 -0.55054193 -0.48200958 -0.41347723 -0.34494487
 -0.27641252 -0.20788017 -0.13934782 -0.07081547 -0.00228311]
[ 690 1538 2012 1805 1919 2013 2226 1980 1836 2206] [-0.68760663 -0.61907428 -0.55054193 -0.48200958 -0.41347723 -0.34494487
 -0.27641252 -0.20788017 -0.13934782 -0.07081547 -0.00228311]
-39.6264
39.3643
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151475 minutes
Weight histogram
[ 142  596 1071 1468 2833 4413 4239 4165 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 11003] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.9506
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.9783
Epoch 1, cost is  21.865
Epoch 2, cost is  21.6871
Epoch 3, cost is  21.7267
Epoch 4, cost is  21.886
Training took 0.089330 minutes
Weight histogram
[1968 1792 2219 1744 1790 1679 2026 1949 2056 3027] [-1.57094872 -1.41406185 -1.25717499 -1.10028812 -0.94340125 -0.78651439
 -0.62962752 -0.47274066 -0.31585379 -0.15896692 -0.00208006]
[3183 1808 1881 1780 1749 1983 1944 1939 1957 2026] [-1.57094872 -1.41406185 -1.25717499 -1.10028812 -0.94340125 -0.78651439
 -0.62962752 -0.47274066 -0.31585379 -0.15896692 -0.00208006]
-55.0173
47.896
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.72267
Epoch 1, cost is  2.04137
Epoch 2, cost is  1.76381
Epoch 3, cost is  1.73237
Epoch 4, cost is  1.73858
Training took 0.212191 minutes
Weight histogram
[2881 3154 2881 2725 2007 1816 1172  801  499  289] [-0.11641122 -0.10499707 -0.09358293 -0.08216878 -0.07075464 -0.0593405
 -0.04792635 -0.03651221 -0.02509807 -0.01368392 -0.00226978]
[1008  891 1187 1526 1787 2140 2318 2475 2636 2257] [-0.11641122 -0.10499707 -0.09358293 -0.08216878 -0.07075464 -0.0593405
 -0.04792635 -0.03651221 -0.02509807 -0.01368392 -0.00226978]
-6.7849
7.47634
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.054943 minutes
Epoch 0
Fine tuning took 0.055190 minutes
Epoch 0
Fine tuning took 0.056746 minutes
{'zero': {0: [0.32635467980295568, 0.15640394088669951, 0.20689655172413793, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.76231527093596063, 0.73275862068965514, 0.63300492610837433], 5: [0.10098522167487685, 0.081280788177339899, 0.060344827586206899, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.32635467980295568, 0.23275862068965517, 0.2229064039408867, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.66625615763546797, 0.66133004926108374, 0.62561576354679804], 5: [0.10098522167487685, 0.10098522167487685, 0.11576354679802955, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.32635467980295568, 0.23399014778325122, 0.21305418719211822, 0.23645320197044334], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.66502463054187189, 0.65147783251231528, 0.65270935960591137], 5: [0.10098522167487685, 0.10098522167487685, 0.1354679802955665, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.32635467980295568, 0.18226600985221675, 0.21305418719211822, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.74630541871921185, 0.73029556650246308, 0.68103448275862066], 5: [0.10098522167487685, 0.071428571428571425, 0.056650246305418719, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149335 minutes
Weight histogram
[ 144  335  705  841 2393 3778 3150 4043 2075  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[ 135  151  209  298  449  751  963 1780 3937 9552] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  37.4592
Epoch 1, cost is  34.7784
Epoch 2, cost is  34.1102
Epoch 3, cost is  33.9137
Epoch 4, cost is  33.941
Training took 0.113460 minutes
Weight histogram
[1966 1724 1746 1918 1881 1902 1913 1940 2142 1093] [-3.89612794 -3.50767764 -3.11922734 -2.73077704 -2.34232674 -1.95387645
 -1.56542615 -1.17697585 -0.78852555 -0.40007525 -0.01162495]
[1196 1891 1906 1828 1857 1894 1998 1846 1832 1977] [-3.89612794 -3.50767764 -3.11922734 -2.73077704 -2.34232674 -1.95387645
 -1.56542615 -1.17697585 -0.78852555 -0.40007525 -0.01162495]
-141.711
166.155
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148903 minutes
Weight histogram
[ 142  596 1071 1468 2833 4413 4239 4165 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 11003] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.9506
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  111.363
Epoch 1, cost is  106.0
Epoch 2, cost is  105.352
Epoch 3, cost is  105.716
Epoch 4, cost is  106.635
Training took 0.087399 minutes
Weight histogram
[1790 1461 1868 1998 1912 1807 1714 1995 2062 3643] [-8.41815186 -7.57739347 -6.73663509 -5.89587671 -5.05511832 -4.21435994
 -3.37360156 -2.53284318 -1.69208479 -0.85132641 -0.01056803]
[3580 1731 1750 1707 1713 1942 1938 1864 2012 2013] [-8.41815186 -7.57739347 -6.73663509 -5.89587671 -5.05511832 -4.21435994
 -3.37360156 -2.53284318 -1.69208479 -0.85132641 -0.01056803]
-287.08
260.757
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.98887
Epoch 1, cost is  5.64164
Epoch 2, cost is  6.88944
Epoch 3, cost is  8.11608
Epoch 4, cost is  9.2571
Training took 0.093382 minutes
Weight histogram
[ 179  549 1011 1390 2459 6246 5844  480   20   47] [-0.82977349 -0.74756416 -0.66535484 -0.58314551 -0.50093619 -0.41872686
 -0.33651754 -0.25430822 -0.17209889 -0.08988957 -0.00768024]
[1158 3103 3147 2734 2514 2108 1259 1078  896  228] [-0.82977349 -0.74756416 -0.66535484 -0.58314551 -0.50093619 -0.41872686
 -0.33651754 -0.25430822 -0.17209889 -0.08988957 -0.00768024]
-66.4183
62.6716
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045959 minutes
Epoch 0
Fine tuning took 0.043698 minutes
Epoch 0
Fine tuning took 0.045839 minutes
{'zero': {0: [0.14285714285714285, 0.29187192118226601, 0.35098522167487683, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.49137931034482757, 0.41133004926108374, 0.55295566502463056], 5: [0.31773399014778325, 0.21674876847290642, 0.2376847290640394, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14285714285714285, 0.31896551724137934, 0.29679802955665024, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.47167487684729065, 0.43719211822660098, 0.50862068965517238], 5: [0.31773399014778325, 0.20935960591133004, 0.26600985221674878, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14285714285714285, 0.31650246305418717, 0.30418719211822659, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.46305418719211822, 0.44088669950738918, 0.44334975369458129], 5: [0.31773399014778325, 0.22044334975369459, 0.25492610837438423, 0.25], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14285714285714285, 0.33620689655172414, 0.29187192118226601, 0.30418719211822659], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.48399014778325122, 0.44827586206896552, 0.47413793103448276], 5: [0.31773399014778325, 0.17980295566502463, 0.25985221674876846, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151676 minutes
Weight histogram
[ 144  335  705  841 2393 3778 3150 4043 2075  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[ 135  151  209  298  449  751  963 1780 3937 9552] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  37.4592
Epoch 1, cost is  34.7784
Epoch 2, cost is  34.1102
Epoch 3, cost is  33.9137
Epoch 4, cost is  33.941
Training took 0.112376 minutes
Weight histogram
[1966 1724 1746 1918 1881 1902 1913 1940 2142 1093] [-3.89612794 -3.50767764 -3.11922734 -2.73077704 -2.34232674 -1.95387645
 -1.56542615 -1.17697585 -0.78852555 -0.40007525 -0.01162495]
[1196 1891 1906 1828 1857 1894 1998 1846 1832 1977] [-3.89612794 -3.50767764 -3.11922734 -2.73077704 -2.34232674 -1.95387645
 -1.56542615 -1.17697585 -0.78852555 -0.40007525 -0.01162495]
-141.711
166.155
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147902 minutes
Weight histogram
[ 142  596 1071 1468 2833 4413 4239 4165 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 11003] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.9506
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  111.363
Epoch 1, cost is  106.0
Epoch 2, cost is  105.352
Epoch 3, cost is  105.716
Epoch 4, cost is  106.635
Training took 0.084293 minutes
Weight histogram
[1790 1461 1868 1998 1912 1807 1714 1995 2062 3643] [-8.41815186 -7.57739347 -6.73663509 -5.89587671 -5.05511832 -4.21435994
 -3.37360156 -2.53284318 -1.69208479 -0.85132641 -0.01056803]
[3580 1731 1750 1707 1713 1942 1938 1864 2012 2013] [-8.41815186 -7.57739347 -6.73663509 -5.89587671 -5.05511832 -4.21435994
 -3.37360156 -2.53284318 -1.69208479 -0.85132641 -0.01056803]
-287.08
260.757
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.86322
Epoch 1, cost is  6.11403
Epoch 2, cost is  7.99985
Epoch 3, cost is  10.1557
Epoch 4, cost is  12.3205
Training took 0.108211 minutes
Weight histogram
[ 297  657  845 1770 3131 4152 4640 2424  252   57] [-0.86838162 -0.7827708  -0.69715998 -0.61154917 -0.52593835 -0.44032753
 -0.35471671 -0.2691059  -0.18349508 -0.09788426 -0.01227344]
[1244 2279 2574 2499 2494 2327 2231 1348  852  377] [-0.86838162 -0.7827708  -0.69715998 -0.61154917 -0.52593835 -0.44032753
 -0.35471671 -0.2691059  -0.18349508 -0.09788426 -0.01227344]
-75.5509
72.5006
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044335 minutes
Epoch 0
Fine tuning took 0.045612 minutes
Epoch 0
Fine tuning took 0.046564 minutes
{'zero': {0: [0.13793103448275862, 0.24753694581280788, 0.28448275862068967, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47906403940886699, 0.40517241379310343, 0.45197044334975367, 0.57758620689655171], 5: [0.38300492610837439, 0.34729064039408869, 0.26354679802955666, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13793103448275862, 0.26724137931034481, 0.22906403940886699, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47906403940886699, 0.40517241379310343, 0.49507389162561577, 0.50615763546798032], 5: [0.38300492610837439, 0.32758620689655171, 0.27586206896551724, 0.27832512315270935], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13793103448275862, 0.2536945812807882, 0.26231527093596058, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47906403940886699, 0.43719211822660098, 0.49507389162561577, 0.54187192118226601], 5: [0.38300492610837439, 0.30911330049261082, 0.24261083743842365, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13793103448275862, 0.24630541871921183, 0.25862068965517243, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47906403940886699, 0.42610837438423643, 0.46921182266009853, 0.55049261083743839], 5: [0.38300492610837439, 0.32758620689655171, 0.27216748768472904, 0.25123152709359609], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150030 minutes
Weight histogram
[ 144  335  705  841 2393 3778 3150 4043 2075  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[ 135  151  209  298  449  751  963 1780 3937 9552] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  37.4592
Epoch 1, cost is  34.7784
Epoch 2, cost is  34.1102
Epoch 3, cost is  33.9137
Epoch 4, cost is  33.941
Training took 0.110215 minutes
Weight histogram
[1966 1724 1746 1918 1881 1902 1913 1940 2142 1093] [-3.89612794 -3.50767764 -3.11922734 -2.73077704 -2.34232674 -1.95387645
 -1.56542615 -1.17697585 -0.78852555 -0.40007525 -0.01162495]
[1196 1891 1906 1828 1857 1894 1998 1846 1832 1977] [-3.89612794 -3.50767764 -3.11922734 -2.73077704 -2.34232674 -1.95387645
 -1.56542615 -1.17697585 -0.78852555 -0.40007525 -0.01162495]
-141.711
166.155
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151614 minutes
Weight histogram
[ 142  596 1071 1468 2833 4413 4239 4165 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 11003] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.9506
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  111.363
Epoch 1, cost is  106.0
Epoch 2, cost is  105.352
Epoch 3, cost is  105.716
Epoch 4, cost is  106.635
Training took 0.083063 minutes
Weight histogram
[1790 1461 1868 1998 1912 1807 1714 1995 2062 3643] [-8.41815186 -7.57739347 -6.73663509 -5.89587671 -5.05511832 -4.21435994
 -3.37360156 -2.53284318 -1.69208479 -0.85132641 -0.01056803]
[3580 1731 1750 1707 1713 1942 1938 1864 2012 2013] [-8.41815186 -7.57739347 -6.73663509 -5.89587671 -5.05511832 -4.21435994
 -3.37360156 -2.53284318 -1.69208479 -0.85132641 -0.01056803]
-287.08
260.757
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.07448
Epoch 1, cost is  5.10972
Epoch 2, cost is  6.66883
Epoch 3, cost is  8.24905
Epoch 4, cost is  9.84386
Training took 0.150605 minutes
Weight histogram
[1030 2304 2534 2465 2568 2563 2559 1861  254   87] [-0.64533269 -0.58223458 -0.51913646 -0.45603834 -0.39294022 -0.32984211
 -0.26674399 -0.20364587 -0.14054775 -0.07744963 -0.01435152]
[1016 1764 2001 2028 2083 2048 2016 2089 2018 1162] [-0.64533269 -0.58223458 -0.51913646 -0.45603834 -0.39294022 -0.32984211
 -0.26674399 -0.20364587 -0.14054775 -0.07744963 -0.01435152]
-54.2861
43.9049
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050389 minutes
Epoch 0
Fine tuning took 0.050563 minutes
Epoch 0
Fine tuning took 0.051032 minutes
{'zero': {0: [0.18842364532019704, 0.30172413793103448, 0.3645320197044335, 0.32266009852216748], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.53201970443349755, 0.42733990147783252, 0.42364532019704432], 5: [0.20566502463054187, 0.16625615763546797, 0.20812807881773399, 0.2536945812807882], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18842364532019704, 0.28694581280788178, 0.30049261083743845, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.57019704433497542, 0.5073891625615764, 0.56650246305418717], 5: [0.20566502463054187, 0.14285714285714285, 0.19211822660098521, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18842364532019704, 0.27093596059113301, 0.26477832512315269, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.57266009852216748, 0.51477832512315269, 0.55295566502463056], 5: [0.20566502463054187, 0.15640394088669951, 0.22044334975369459, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18842364532019704, 0.27955665024630544, 0.24261083743842365, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.51724137931034486, 0.55418719211822665, 0.58004926108374388], 5: [0.20566502463054187, 0.20320197044334976, 0.20320197044334976, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149079 minutes
Weight histogram
[ 144  335  705  841 2393 3778 3150 4043 2075  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[ 135  151  209  298  449  751  963 1780 3937 9552] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  37.4592
Epoch 1, cost is  34.7784
Epoch 2, cost is  34.1102
Epoch 3, cost is  33.9137
Epoch 4, cost is  33.941
Training took 0.112752 minutes
Weight histogram
[1966 1724 1746 1918 1881 1902 1913 1940 2142 1093] [-3.89612794 -3.50767764 -3.11922734 -2.73077704 -2.34232674 -1.95387645
 -1.56542615 -1.17697585 -0.78852555 -0.40007525 -0.01162495]
[1196 1891 1906 1828 1857 1894 1998 1846 1832 1977] [-3.89612794 -3.50767764 -3.11922734 -2.73077704 -2.34232674 -1.95387645
 -1.56542615 -1.17697585 -0.78852555 -0.40007525 -0.01162495]
-141.711
166.155
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149033 minutes
Weight histogram
[ 142  596 1071 1468 2833 4413 4239 4165 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 11003] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.58406
0.9506
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  111.363
Epoch 1, cost is  106.0
Epoch 2, cost is  105.352
Epoch 3, cost is  105.716
Epoch 4, cost is  106.635
Training took 0.084680 minutes
Weight histogram
[1790 1461 1868 1998 1912 1807 1714 1995 2062 3643] [-8.41815186 -7.57739347 -6.73663509 -5.89587671 -5.05511832 -4.21435994
 -3.37360156 -2.53284318 -1.69208479 -0.85132641 -0.01056803]
[3580 1731 1750 1707 1713 1942 1938 1864 2012 2013] [-8.41815186 -7.57739347 -6.73663509 -5.89587671 -5.05511832 -4.21435994
 -3.37360156 -2.53284318 -1.69208479 -0.85132641 -0.01056803]
-287.08
260.757
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.4868
Epoch 1, cost is  3.13275
Epoch 2, cost is  3.10817
Epoch 3, cost is  3.28781
Epoch 4, cost is  3.44935
Training took 0.216119 minutes
Weight histogram
[1873 2872 2831 2626 2428 2163 1700 1108  443  181] [-0.42930233 -0.38782057 -0.34633881 -0.30485705 -0.26337529 -0.22189353
 -0.18041177 -0.13893001 -0.09744825 -0.05596649 -0.01448473]
[ 850 1285 1722 1966 2164 2257 2408 2436 2245  892] [-0.42930233 -0.38782057 -0.34633881 -0.30485705 -0.26337529 -0.22189353
 -0.18041177 -0.13893001 -0.09744825 -0.05596649 -0.01448473]
-27.8845
29.4731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.053573 minutes
Epoch 0
Fine tuning took 0.055428 minutes
Epoch 0
Fine tuning took 0.054258 minutes
{'zero': {0: [0.17857142857142858, 0.098522167487684734, 0.080049261083743842, 0.068965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.56403940886699511, 0.64408866995073888, 0.61206896551724133], 5: [0.11699507389162561, 0.33743842364532017, 0.27586206896551724, 0.31896551724137934], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17857142857142858, 0.21921182266009853, 0.17118226600985223, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.64039408866995073, 0.58866995073891626, 0.62931034482758619], 5: [0.11699507389162561, 0.14039408866995073, 0.24014778325123154, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17857142857142858, 0.21305418719211822, 0.17857142857142858, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.6354679802955665, 0.6280788177339901, 0.62684729064039413], 5: [0.11699507389162561, 0.15147783251231528, 0.19334975369458129, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17857142857142858, 0.24384236453201971, 0.17733990147783252, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.57512315270935965, 0.63300492610837433, 0.63669950738916259], 5: [0.11699507389162561, 0.18103448275862069, 0.18965517241379309, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149789 minutes
Weight histogram
[ 144  335  705  841 2393 4124 4621 4245 2081  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[  135   151   209   298   449   751   963  1780  3937 11577] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.14193
Epoch 1, cost is  7.52484
Epoch 2, cost is  7.33598
Epoch 3, cost is  7.2796
Epoch 4, cost is  7.25269
Training took 0.118101 minutes
Weight histogram
[2400 2137 2117 2031 2332 2262 2148 2287 2064  472] [-0.7522527  -0.67725574 -0.60225878 -0.52726182 -0.45226486 -0.37726791
 -0.30227095 -0.22727399 -0.15227703 -0.07728007 -0.00228311]
[ 805 1810 2134 2032 2148 2323 2261 2066 2327 2344] [-0.7522527  -0.67725574 -0.60225878 -0.52726182 -0.45226486 -0.37726791
 -0.30227095 -0.22727399 -0.15227703 -0.07728007 -0.00228311]
-42.8803
41.7102
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148391 minutes
Weight histogram
[ 142  596 1071 1492 3172 5368 4839 4272 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 13028] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.62206
0.954873
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  25.7555
Epoch 1, cost is  24.6435
Epoch 2, cost is  24.4971
Epoch 3, cost is  24.5695
Epoch 4, cost is  24.7899
Training took 0.088713 minutes
Weight histogram
[1980 2053 2313 2005 1893 2109 2159 2206 2316 3241] [-1.75325918 -1.57814127 -1.40302336 -1.22790544 -1.05278753 -0.87766962
 -0.70255171 -0.52743379 -0.35231588 -0.17719797 -0.00208006]
[3363 2050 2064 1942 2107 2161 2175 2172 2248 1993] [-1.75325918 -1.57814127 -1.40302336 -1.22790544 -1.05278753 -0.87766962
 -0.70255171 -0.52743379 -0.35231588 -0.17719797 -0.00208006]
-60.9204
54.9023
... retrieved True_rbm_350-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.97743
Epoch 1, cost is  4.51034
Epoch 2, cost is  4.86874
Epoch 3, cost is  5.38878
Epoch 4, cost is  5.90233
Training took 0.092822 minutes
Weight histogram
[2527 3311 3335 3031 3017 2225 1213  658  463  470] [-0.34779999 -0.31323603 -0.27867207 -0.24410811 -0.20954415 -0.17498019
 -0.14041622 -0.10585226 -0.0712883  -0.03672434 -0.00216038]
[ 909 1148 1497 1958 2194 2444 2478 2571 2703 2348] [-0.34779999 -0.31323603 -0.27867207 -0.24410811 -0.20954415 -0.17498019
 -0.14041622 -0.10585226 -0.0712883  -0.03672434 -0.00216038]
-22.6077
19.9534
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044084 minutes
Epoch 0
Fine tuning took 0.044705 minutes
Epoch 0
Fine tuning took 0.045690 minutes
{'zero': {0: [0.40886699507389163, 0.33620689655172414, 0.39532019704433496, 0.3251231527093596], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40024630541871919, 0.45197044334975367, 0.43349753694581283, 0.49507389162561577], 5: [0.19088669950738915, 0.21182266009852216, 0.17118226600985223, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.40886699507389163, 0.38054187192118227, 0.44950738916256155, 0.39408866995073893], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40024630541871919, 0.4605911330049261, 0.41133004926108374, 0.49753694581280788], 5: [0.19088669950738915, 0.15886699507389163, 0.13916256157635468, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.40886699507389163, 0.39532019704433496, 0.41379310344827586, 0.37315270935960593], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40024630541871919, 0.42733990147783252, 0.42610837438423643, 0.48275862068965519], 5: [0.19088669950738915, 0.17733990147783252, 0.16009852216748768, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.40886699507389163, 0.42980295566502463, 0.46674876847290642, 0.39408866995073893], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40024630541871919, 0.42733990147783252, 0.39285714285714285, 0.50985221674876846], 5: [0.19088669950738915, 0.14285714285714285, 0.14039408866995073, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150538 minutes
Weight histogram
[ 144  335  705  841 2393 4124 4621 4245 2081  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[  135   151   209   298   449   751   963  1780  3937 11577] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.14193
Epoch 1, cost is  7.52484
Epoch 2, cost is  7.33598
Epoch 3, cost is  7.2796
Epoch 4, cost is  7.25269
Training took 0.115060 minutes
Weight histogram
[2400 2137 2117 2031 2332 2262 2148 2287 2064  472] [-0.7522527  -0.67725574 -0.60225878 -0.52726182 -0.45226486 -0.37726791
 -0.30227095 -0.22727399 -0.15227703 -0.07728007 -0.00228311]
[ 805 1810 2134 2032 2148 2323 2261 2066 2327 2344] [-0.7522527  -0.67725574 -0.60225878 -0.52726182 -0.45226486 -0.37726791
 -0.30227095 -0.22727399 -0.15227703 -0.07728007 -0.00228311]
-42.8803
41.7102
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149198 minutes
Weight histogram
[ 142  596 1071 1492 3172 5368 4839 4272 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 13028] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.62206
0.954873
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  25.7555
Epoch 1, cost is  24.6435
Epoch 2, cost is  24.4971
Epoch 3, cost is  24.5695
Epoch 4, cost is  24.7899
Training took 0.090185 minutes
Weight histogram
[1980 2053 2313 2005 1893 2109 2159 2206 2316 3241] [-1.75325918 -1.57814127 -1.40302336 -1.22790544 -1.05278753 -0.87766962
 -0.70255171 -0.52743379 -0.35231588 -0.17719797 -0.00208006]
[3363 2050 2064 1942 2107 2161 2175 2172 2248 1993] [-1.75325918 -1.57814127 -1.40302336 -1.22790544 -1.05278753 -0.87766962
 -0.70255171 -0.52743379 -0.35231588 -0.17719797 -0.00208006]
-60.9204
54.9023
... retrieved True_rbm_350-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.5681
Epoch 1, cost is  3.82857
Epoch 2, cost is  4.06951
Epoch 3, cost is  4.50532
Epoch 4, cost is  5.0688
Training took 0.107584 minutes
Weight histogram
[2273 3043 3079 2824 2843 2597 1672  829  577  513] [-0.28246394 -0.25443944 -0.22641494 -0.19839044 -0.17036594 -0.14234144
 -0.11431695 -0.08629245 -0.05826795 -0.03024345 -0.00221895]
[1014 1218 1666 1839 2093 2398 2521 2562 2610 2329] [-0.28246394 -0.25443944 -0.22641494 -0.19839044 -0.17036594 -0.14234144
 -0.11431695 -0.08629245 -0.05826795 -0.03024345 -0.00221895]
-10.4799
13.4142
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048236 minutes
Epoch 0
Fine tuning took 0.047829 minutes
Epoch 0
Fine tuning took 0.045790 minutes
{'zero': {0: [0.31034482758620691, 0.27709359605911332, 0.22044334975369459, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.52586206896551724, 0.61699507389162567, 0.57019704433497542], 5: [0.13916256157635468, 0.19704433497536947, 0.1625615763546798, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31034482758620691, 0.34113300492610837, 0.27955665024630544, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.56896551724137934, 0.61822660098522164, 0.63669950738916259], 5: [0.13916256157635468, 0.089901477832512317, 0.10221674876847291, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31034482758620691, 0.29064039408866993, 0.26354679802955666, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.55295566502463056, 0.6219211822660099, 0.6280788177339901], 5: [0.13916256157635468, 0.15640394088669951, 0.1145320197044335, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31034482758620691, 0.27955665024630544, 0.27463054187192121, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.59605911330049266, 0.62315270935960587, 0.67610837438423643], 5: [0.13916256157635468, 0.12438423645320197, 0.10221674876847291, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150253 minutes
Weight histogram
[ 144  335  705  841 2393 4124 4621 4245 2081  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[  135   151   209   298   449   751   963  1780  3937 11577] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.14193
Epoch 1, cost is  7.52484
Epoch 2, cost is  7.33598
Epoch 3, cost is  7.2796
Epoch 4, cost is  7.25269
Training took 0.117062 minutes
Weight histogram
[2400 2137 2117 2031 2332 2262 2148 2287 2064  472] [-0.7522527  -0.67725574 -0.60225878 -0.52726182 -0.45226486 -0.37726791
 -0.30227095 -0.22727399 -0.15227703 -0.07728007 -0.00228311]
[ 805 1810 2134 2032 2148 2323 2261 2066 2327 2344] [-0.7522527  -0.67725574 -0.60225878 -0.52726182 -0.45226486 -0.37726791
 -0.30227095 -0.22727399 -0.15227703 -0.07728007 -0.00228311]
-42.8803
41.7102
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147763 minutes
Weight histogram
[ 142  596 1071 1492 3172 5368 4839 4272 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 13028] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.62206
0.954873
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  25.7555
Epoch 1, cost is  24.6435
Epoch 2, cost is  24.4971
Epoch 3, cost is  24.5695
Epoch 4, cost is  24.7899
Training took 0.090291 minutes
Weight histogram
[1980 2053 2313 2005 1893 2109 2159 2206 2316 3241] [-1.75325918 -1.57814127 -1.40302336 -1.22790544 -1.05278753 -0.87766962
 -0.70255171 -0.52743379 -0.35231588 -0.17719797 -0.00208006]
[3363 2050 2064 1942 2107 2161 2175 2172 2248 1993] [-1.75325918 -1.57814127 -1.40302336 -1.22790544 -1.05278753 -0.87766962
 -0.70255171 -0.52743379 -0.35231588 -0.17719797 -0.00208006]
-60.9204
54.9023
... retrieved True_rbm_350-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.02218
Epoch 1, cost is  2.70287
Epoch 2, cost is  2.6444
Epoch 3, cost is  2.78876
Epoch 4, cost is  2.98726
Training took 0.149142 minutes
Weight histogram
[2629 3284 3040 2933 2510 2185 1658  945  620  446] [-0.18184739 -0.16388242 -0.14591744 -0.12795247 -0.10998749 -0.09202251
 -0.07405754 -0.05609256 -0.03812758 -0.02016261 -0.00219763]
[1130 1143 1474 1832 2084 2380 2489 2571 2728 2419] [-0.18184739 -0.16388242 -0.14591744 -0.12795247 -0.10998749 -0.09202251
 -0.07405754 -0.05609256 -0.03812758 -0.02016261 -0.00219763]
-7.56632
9.17823
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.051242 minutes
Epoch 0
Fine tuning took 0.050304 minutes
Epoch 0
Fine tuning took 0.050758 minutes
{'zero': {0: [0.27709359605911332, 0.18842364532019704, 0.17118226600985223, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.49014778325123154, 0.44704433497536944, 0.69827586206896552], 5: [0.21305418719211822, 0.32142857142857145, 0.3817733990147783, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27709359605911332, 0.24014778325123154, 0.24876847290640394, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.64408866995073888, 0.64039408866995073, 0.59975369458128081], 5: [0.21305418719211822, 0.11576354679802955, 0.11083743842364532, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27709359605911332, 0.23645320197044334, 0.26477832512315269, 0.23645320197044334], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.60344827586206895, 0.58990147783251234, 0.61822660098522164], 5: [0.21305418719211822, 0.16009852216748768, 0.14532019704433496, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27709359605911332, 0.15517241379310345, 0.22167487684729065, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.71551724137931039, 0.65270935960591137, 0.64408866995073888], 5: [0.21305418719211822, 0.12931034482758622, 0.12561576354679804, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150946 minutes
Weight histogram
[ 144  335  705  841 2393 4124 4621 4245 2081  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[  135   151   209   298   449   751   963  1780  3937 11577] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.14193
Epoch 1, cost is  7.52484
Epoch 2, cost is  7.33598
Epoch 3, cost is  7.2796
Epoch 4, cost is  7.25269
Training took 0.115050 minutes
Weight histogram
[2400 2137 2117 2031 2332 2262 2148 2287 2064  472] [-0.7522527  -0.67725574 -0.60225878 -0.52726182 -0.45226486 -0.37726791
 -0.30227095 -0.22727399 -0.15227703 -0.07728007 -0.00228311]
[ 805 1810 2134 2032 2148 2323 2261 2066 2327 2344] [-0.7522527  -0.67725574 -0.60225878 -0.52726182 -0.45226486 -0.37726791
 -0.30227095 -0.22727399 -0.15227703 -0.07728007 -0.00228311]
-42.8803
41.7102
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147993 minutes
Weight histogram
[ 142  596 1071 1492 3172 5368 4839 4272 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 13028] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.62206
0.954873
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  25.7555
Epoch 1, cost is  24.6435
Epoch 2, cost is  24.4971
Epoch 3, cost is  24.5695
Epoch 4, cost is  24.7899
Training took 0.090234 minutes
Weight histogram
[1980 2053 2313 2005 1893 2109 2159 2206 2316 3241] [-1.75325918 -1.57814127 -1.40302336 -1.22790544 -1.05278753 -0.87766962
 -0.70255171 -0.52743379 -0.35231588 -0.17719797 -0.00208006]
[3363 2050 2064 1942 2107 2161 2175 2172 2248 1993] [-1.75325918 -1.57814127 -1.40302336 -1.22790544 -1.05278753 -0.87766962
 -0.70255171 -0.52743379 -0.35231588 -0.17719797 -0.00208006]
-60.9204
54.9023
... retrieved True_rbm_350-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.72815
Epoch 1, cost is  2.04639
Epoch 2, cost is  1.78075
Epoch 3, cost is  1.73277
Epoch 4, cost is  1.73627
Training took 0.213272 minutes
Weight histogram
[3197 3514 3189 3017 2236 2023 1314  889  554  317] [-0.11641122 -0.10499326 -0.09357531 -0.08215736 -0.0707394  -0.05932145
 -0.0479035  -0.03648555 -0.02506759 -0.01364964 -0.00223169]
[1121  996 1326 1702 1989 2378 2581 2748 2923 2486] [-0.11641122 -0.10499326 -0.09357531 -0.08215736 -0.0707394  -0.05932145
 -0.0479035  -0.03648555 -0.02506759 -0.01364964 -0.00223169]
-6.7849
7.47634
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.055961 minutes
Epoch 0
Fine tuning took 0.055660 minutes
Epoch 0
Fine tuning took 0.054992 minutes
{'zero': {0: [0.18226600985221675, 0.24876847290640394, 0.24384236453201971, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68226600985221675, 0.60837438423645318, 0.58743842364532017, 0.69334975369458129], 5: [0.1354679802955665, 0.14285714285714285, 0.16871921182266009, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18226600985221675, 0.2413793103448276, 0.15147783251231528, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68226600985221675, 0.68349753694581283, 0.76477832512315269, 0.71798029556650245], 5: [0.1354679802955665, 0.075123152709359611, 0.083743842364532015, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18226600985221675, 0.22783251231527094, 0.19827586206896552, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68226600985221675, 0.68596059113300489, 0.68965517241379315, 0.68596059113300489], 5: [0.1354679802955665, 0.086206896551724144, 0.11206896551724138, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18226600985221675, 0.20689655172413793, 0.15147783251231528, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68226600985221675, 0.70566502463054193, 0.76847290640394084, 0.79433497536945807], 5: [0.1354679802955665, 0.087438423645320201, 0.080049261083743842, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150322 minutes
Weight histogram
[ 144  335  705  841 2393 4124 4621 4245 2081  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[  135   151   209   298   449   751   963  1780  3937 11577] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  38.6051
Epoch 1, cost is  35.6703
Epoch 2, cost is  34.7134
Epoch 3, cost is  34.3065
Epoch 4, cost is  34.1635
Training took 0.112079 minutes
Weight histogram
[2426 1917 1736 2021 2106 2156 2094 2149 2424 1221] [-4.25562954 -3.83122908 -3.40682862 -2.98242816 -2.5580277  -2.13362725
 -1.70922679 -1.28482633 -0.86042587 -0.43602541 -0.01162495]
[1338 2105 2048 1984 2055 2133 2091 1985 2136 2375] [-4.25562954 -3.83122908 -3.40682862 -2.98242816 -2.5580277  -2.13362725
 -1.70922679 -1.28482633 -0.86042587 -0.43602541 -0.01162495]
-153.913
171.248
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149461 minutes
Weight histogram
[ 142  596 1071 1492 3172 5368 4839 4272 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 13028] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.62206
0.954873
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  120.221
Epoch 1, cost is  114.85
Epoch 2, cost is  113.946
Epoch 3, cost is  114.133
Epoch 4, cost is  114.977
Training took 0.084161 minutes
Weight histogram
[2233 1729 1833 2239 2077 2033 1973 2079 2236 3843] [-9.25987434 -8.33494371 -7.41001308 -6.48508245 -5.56015182 -4.63522119
 -3.71029055 -2.78535992 -1.86042929 -0.93549866 -0.01056803]
[3757 1900 1942 1852 2042 2137 2093 2186 2219 2147] [-9.25987434 -8.33494371 -7.41001308 -6.48508245 -5.56015182 -4.63522119
 -3.71029055 -2.78535992 -1.86042929 -0.93549866 -0.01056803]
-312.863
305.544
... retrieved True_rbm_350-50_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.00292
Epoch 1, cost is  6.51419
Epoch 2, cost is  8.4168
Epoch 3, cost is  9.9466
Epoch 4, cost is  11.5439
Training took 0.094699 minutes
Weight histogram
[ 179  549 1080 1791 2936 6759 6285  596   23   52] [-0.82977349 -0.74756416 -0.66535484 -0.58314551 -0.50093619 -0.41872686
 -0.33651754 -0.25430822 -0.17209889 -0.08988957 -0.00768024]
[1252 3284 3376 2983 2801 2357 1518 1334 1117  228] [-0.82977349 -0.74756416 -0.66535484 -0.58314551 -0.50093619 -0.41872686
 -0.33651754 -0.25430822 -0.17209889 -0.08988957 -0.00768024]
-66.4183
64.7511
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043025 minutes
Epoch 0
Fine tuning took 0.043229 minutes
Epoch 0
Fine tuning took 0.044165 minutes
{'zero': {0: [0.22167487684729065, 0.40640394088669951, 0.25985221674876846, 0.34975369458128081], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.017241379310344827, 0.34113300492610837, 0.45689655172413796, 0.39778325123152708], 5: [0.76108374384236455, 0.25246305418719212, 0.28325123152709358, 0.25246305418719212], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22167487684729065, 0.36699507389162561, 0.26724137931034481, 0.34729064039408869], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.017241379310344827, 0.35098522167487683, 0.42364532019704432, 0.39039408866995073], 5: [0.76108374384236455, 0.28201970443349755, 0.30911330049261082, 0.26231527093596058], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22167487684729065, 0.35591133004926107, 0.27463054187192121, 0.33128078817733991], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.017241379310344827, 0.34359605911330049, 0.43472906403940886, 0.40517241379310343], 5: [0.76108374384236455, 0.30049261083743845, 0.29064039408866993, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22167487684729065, 0.37315270935960593, 0.28201970443349755, 0.34359605911330049], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.017241379310344827, 0.34359605911330049, 0.42857142857142855, 0.39285714285714285], 5: [0.76108374384236455, 0.28325123152709358, 0.2894088669950739, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149829 minutes
Weight histogram
[ 144  335  705  841 2393 4124 4621 4245 2081  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[  135   151   209   298   449   751   963  1780  3937 11577] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  38.6051
Epoch 1, cost is  35.6703
Epoch 2, cost is  34.7134
Epoch 3, cost is  34.3065
Epoch 4, cost is  34.1635
Training took 0.111214 minutes
Weight histogram
[2426 1917 1736 2021 2106 2156 2094 2149 2424 1221] [-4.25562954 -3.83122908 -3.40682862 -2.98242816 -2.5580277  -2.13362725
 -1.70922679 -1.28482633 -0.86042587 -0.43602541 -0.01162495]
[1338 2105 2048 1984 2055 2133 2091 1985 2136 2375] [-4.25562954 -3.83122908 -3.40682862 -2.98242816 -2.5580277  -2.13362725
 -1.70922679 -1.28482633 -0.86042587 -0.43602541 -0.01162495]
-153.913
171.248
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148305 minutes
Weight histogram
[ 142  596 1071 1492 3172 5368 4839 4272 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 13028] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.62206
0.954873
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  120.221
Epoch 1, cost is  114.85
Epoch 2, cost is  113.946
Epoch 3, cost is  114.133
Epoch 4, cost is  114.977
Training took 0.084153 minutes
Weight histogram
[2233 1729 1833 2239 2077 2033 1973 2079 2236 3843] [-9.25987434 -8.33494371 -7.41001308 -6.48508245 -5.56015182 -4.63522119
 -3.71029055 -2.78535992 -1.86042929 -0.93549866 -0.01056803]
[3757 1900 1942 1852 2042 2137 2093 2186 2219 2147] [-9.25987434 -8.33494371 -7.41001308 -6.48508245 -5.56015182 -4.63522119
 -3.71029055 -2.78535992 -1.86042929 -0.93549866 -0.01056803]
-312.863
305.544
... retrieved True_rbm_350-100_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.73451
Epoch 1, cost is  6.24036
Epoch 2, cost is  8.35071
Epoch 3, cost is  10.3762
Epoch 4, cost is  12.6818
Training took 0.109376 minutes
Weight histogram
[ 312  915 1113 2064 3429 4491 5009 2597  257   63] [-0.86838162 -0.78271029 -0.69703897 -0.61136764 -0.52569631 -0.44002499
 -0.35435366 -0.26868234 -0.18301101 -0.09733968 -0.01166836]
[1353 2500 2825 2739 2723 2552 2453 1553 1064  488] [-0.86838162 -0.78271029 -0.69703897 -0.61136764 -0.52569631 -0.44002499
 -0.35435366 -0.26868234 -0.18301101 -0.09733968 -0.01166836]
-75.5509
72.5006
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047423 minutes
Epoch 0
Fine tuning took 0.045251 minutes
Epoch 0
Fine tuning took 0.045906 minutes
{'zero': {0: [0.37315270935960593, 0.33866995073891626, 0.37192118226600984, 0.38423645320197042], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.46305418719211822, 0.38054187192118227, 0.37561576354679804, 0.41502463054187194], 5: [0.16379310344827586, 0.28078817733990147, 0.25246305418719212, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37315270935960593, 0.37807881773399016, 0.37438423645320196, 0.35344827586206895], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.46305418719211822, 0.34852216748768472, 0.40147783251231528, 0.43226600985221675], 5: [0.16379310344827586, 0.27339901477832512, 0.22413793103448276, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37315270935960593, 0.37068965517241381, 0.3854679802955665, 0.32142857142857145], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.46305418719211822, 0.33990147783251229, 0.35714285714285715, 0.45073891625615764], 5: [0.16379310344827586, 0.2894088669950739, 0.25738916256157635, 0.22783251231527094], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37315270935960593, 0.34236453201970446, 0.32758620689655171, 0.35344827586206895], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.46305418719211822, 0.35221674876847292, 0.42610837438423643, 0.40886699507389163], 5: [0.16379310344827586, 0.30541871921182268, 0.24630541871921183, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150322 minutes
Weight histogram
[ 144  335  705  841 2393 4124 4621 4245 2081  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[  135   151   209   298   449   751   963  1780  3937 11577] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  38.6051
Epoch 1, cost is  35.6703
Epoch 2, cost is  34.7134
Epoch 3, cost is  34.3065
Epoch 4, cost is  34.1635
Training took 0.109357 minutes
Weight histogram
[2426 1917 1736 2021 2106 2156 2094 2149 2424 1221] [-4.25562954 -3.83122908 -3.40682862 -2.98242816 -2.5580277  -2.13362725
 -1.70922679 -1.28482633 -0.86042587 -0.43602541 -0.01162495]
[1338 2105 2048 1984 2055 2133 2091 1985 2136 2375] [-4.25562954 -3.83122908 -3.40682862 -2.98242816 -2.5580277  -2.13362725
 -1.70922679 -1.28482633 -0.86042587 -0.43602541 -0.01162495]
-153.913
171.248
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148065 minutes
Weight histogram
[ 142  596 1071 1492 3172 5368 4839 4272 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 13028] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.62206
0.954873
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  120.221
Epoch 1, cost is  114.85
Epoch 2, cost is  113.946
Epoch 3, cost is  114.133
Epoch 4, cost is  114.977
Training took 0.086676 minutes
Weight histogram
[2233 1729 1833 2239 2077 2033 1973 2079 2236 3843] [-9.25987434 -8.33494371 -7.41001308 -6.48508245 -5.56015182 -4.63522119
 -3.71029055 -2.78535992 -1.86042929 -0.93549866 -0.01056803]
[3757 1900 1942 1852 2042 2137 2093 2186 2219 2147] [-9.25987434 -8.33494371 -7.41001308 -6.48508245 -5.56015182 -4.63522119
 -3.71029055 -2.78535992 -1.86042929 -0.93549866 -0.01056803]
-312.863
305.544
... retrieved True_rbm_350-250_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.25039
Epoch 1, cost is  4.90757
Epoch 2, cost is  5.8979
Epoch 3, cost is  6.94456
Epoch 4, cost is  8.05386
Training took 0.150383 minutes
Weight histogram
[1155 2553 2808 2735 2812 2817 2855 2148  272   95] [-0.64533269 -0.58221722 -0.51910174 -0.45598627 -0.39287079 -0.32975532
 -0.26663984 -0.20352437 -0.1404089  -0.07729342 -0.01417795]
[1134 1971 2221 2238 2299 2256 2230 2299 2232 1370] [-0.64533269 -0.58221722 -0.51910174 -0.45598627 -0.39287079 -0.32975532
 -0.26663984 -0.20352437 -0.1404089  -0.07729342 -0.01417795]
-54.2861
45.7567
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049130 minutes
Epoch 0
Fine tuning took 0.048740 minutes
Epoch 0
Fine tuning took 0.048150 minutes
{'zero': {0: [0.18965517241379309, 0.10221674876847291, 0.10467980295566502, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69458128078817738, 0.82389162561576357, 0.80541871921182262, 0.64901477832512311], 5: [0.11576354679802955, 0.073891625615763554, 0.089901477832512317, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18965517241379309, 0.29064039408866993, 0.21428571428571427, 0.30911330049261082], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69458128078817738, 0.55172413793103448, 0.6145320197044335, 0.48645320197044334], 5: [0.11576354679802955, 0.15763546798029557, 0.17118226600985223, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18965517241379309, 0.25615763546798032, 0.25123152709359609, 0.29064039408866993], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69458128078817738, 0.58128078817733986, 0.60098522167487689, 0.49876847290640391], 5: [0.11576354679802955, 0.1625615763546798, 0.14778325123152711, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18965517241379309, 0.32758620689655171, 0.22044334975369459, 0.31527093596059114], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69458128078817738, 0.51724137931034486, 0.61576354679802958, 0.38793103448275862], 5: [0.11576354679802955, 0.15517241379310345, 0.16379310344827586, 0.29679802955665024], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150361 minutes
Weight histogram
[ 144  335  705  841 2393 4124 4621 4245 2081  761] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
[  135   151   209   298   449   751   963  1780  3937 11577] [ -2.39681889e-04   5.85050584e-05   3.56692006e-04   6.54878953e-04
   9.53065901e-04   1.25125285e-03   1.54943980e-03   1.84762674e-03
   2.14581369e-03   2.44400064e-03   2.74218759e-03]
-1.59994
1.17233
training layer 1, rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  38.6051
Epoch 1, cost is  35.6703
Epoch 2, cost is  34.7134
Epoch 3, cost is  34.3065
Epoch 4, cost is  34.1635
Training took 0.110207 minutes
Weight histogram
[2426 1917 1736 2021 2106 2156 2094 2149 2424 1221] [-4.25562954 -3.83122908 -3.40682862 -2.98242816 -2.5580277  -2.13362725
 -1.70922679 -1.28482633 -0.86042587 -0.43602541 -0.01162495]
[1338 2105 2048 1984 2055 2133 2091 1985 2136 2375] [-4.25562954 -3.83122908 -3.40682862 -2.98242816 -2.5580277  -2.13362725
 -1.70922679 -1.28482633 -0.86042587 -0.43602541 -0.01162495]
-153.913
171.248
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149552 minutes
Weight histogram
[ 142  596 1071 1492 3172 5368 4839 4272 1227   96] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
[  259   299   418   557   843  1416   903  1575  2977 13028] [ -2.42434355e-04  -6.63172978e-06   2.29170895e-04   4.64973520e-04
   7.00776145e-04   9.36578770e-04   1.17238140e-03   1.40818402e-03
   1.64398664e-03   1.87978927e-03   2.11559189e-03]
-1.62206
0.954873
training layer 1, rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.05_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  120.221
Epoch 1, cost is  114.85
Epoch 2, cost is  113.946
Epoch 3, cost is  114.133
Epoch 4, cost is  114.977
Training took 0.084205 minutes
Weight histogram
[2233 1729 1833 2239 2077 2033 1973 2079 2236 3843] [-9.25987434 -8.33494371 -7.41001308 -6.48508245 -5.56015182 -4.63522119
 -3.71029055 -2.78535992 -1.86042929 -0.93549866 -0.01056803]
[3757 1900 1942 1852 2042 2137 2093 2186 2219 2147] [-9.25987434 -8.33494371 -7.41001308 -6.48508245 -5.56015182 -4.63522119
 -3.71029055 -2.78535992 -1.86042929 -0.93549866 -0.01056803]
-312.863
305.544
... retrieved True_rbm_350-500_classical1_batch10_lr0.05_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN3/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.16292
Epoch 1, cost is  2.80264
Epoch 2, cost is  2.99956
Epoch 3, cost is  3.23745
Epoch 4, cost is  3.539
Training took 0.215331 minutes
Weight histogram
[2139 3165 3136 2905 2699 2413 1889 1230  463  211] [-0.42961732 -0.38807293 -0.34652855 -0.30498416 -0.26343978 -0.22189539
 -0.18035101 -0.13880662 -0.09726224 -0.05571785 -0.01417347]
[ 935 1426 1906 2179 2381 2486 2651 2675 2492 1119] [-0.42961732 -0.38807293 -0.34652855 -0.30498416 -0.26343978 -0.22189539
 -0.18035101 -0.13880662 -0.09726224 -0.05571785 -0.01417347]
-27.8845
29.4731
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.054511 minutes
Epoch 0
Fine tuning took 0.053287 minutes
Epoch 0
Fine tuning took 0.054625 minutes
{'zero': {0: [0.15763546798029557, 0.17610837438423646, 0.31773399014778325, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.75, 0.56650246305418717, 0.69334975369458129], 5: [0.10591133004926108, 0.073891625615763554, 0.11576354679802955, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.27339901477832512, 0.19211822660098521, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.60344827586206895, 0.63177339901477836, 0.6145320197044335], 5: [0.10591133004926108, 0.12315270935960591, 0.17610837438423646, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.23399014778325122, 0.19827586206896552, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.62931034482758619, 0.67241379310344829, 0.58374384236453203], 5: [0.10591133004926108, 0.13669950738916256, 0.12931034482758622, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.23275862068965517, 0.1748768472906404, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.58251231527093594, 0.66009852216748766, 0.60221674876847286], 5: [0.10591133004926108, 0.18472906403940886, 0.16502463054187191, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}}
