Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentDBN
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentDBN
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.403787 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.7343
Epoch 1, cost is  6.49799
Epoch 2, cost is  6.26442
Epoch 3, cost is  6.04326
Epoch 4, cost is  5.878
Training took 0.109302 minutes
Weight histogram
[695 845  80 107  61  53  85  60  23  16] [ -7.28369923e-03  -6.55050224e-03  -5.81730525e-03  -5.08410826e-03
  -4.35091127e-03  -3.61771428e-03  -2.88451730e-03  -2.15132031e-03
  -1.41812332e-03  -6.84926326e-04   4.82706637e-05]
[422 256 210 179 156 158 150 155 162 177] [ -7.28369923e-03  -6.55050224e-03  -5.81730525e-03  -5.08410826e-03
  -4.35091127e-03  -3.61771428e-03  -2.88451730e-03  -2.15132031e-03
  -1.41812332e-03  -6.84926326e-04   4.82706637e-05]
-0.485711
0.412906
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.77717
Epoch 1, cost is  6.50461
Epoch 2, cost is  5.95774
Epoch 3, cost is  5.39185
Epoch 4, cost is  4.99261
Training took 0.073001 minutes
Weight histogram
[ 513  702  254  183   96   91 1618  278  221   94] [-0.01890839 -0.01700025 -0.01509212 -0.01318399 -0.01127585 -0.00936772
 -0.00745958 -0.00555145 -0.00364332 -0.00173518  0.00017295]
[1240  525  405  365  346  380  319  143  158  169] [-0.01890839 -0.01700025 -0.01509212 -0.01318399 -0.01127585 -0.00936772
 -0.00745958 -0.00555145 -0.00364332 -0.00173518  0.00017295]
-0.485711
0.412906
fine tuning ...
Epoch 0
Fine tuning took 0.053772 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052833 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052326 minutes
{0: [0.017241379310344827, 0.023399014778325122, 0.024630541871921183, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9568965517241379, 0.95197044334975367, 0.94950738916256161, 0.95443349753694584], 5: [0.025862068965517241, 0.024630541871921183, 0.025862068965517241, 0.019704433497536946], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.402965 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.7343
Epoch 1, cost is  6.49799
Epoch 2, cost is  6.26442
Epoch 3, cost is  6.04326
Epoch 4, cost is  5.878
Training took 0.108082 minutes
Weight histogram
[695 845  80 107  61  53  85  60  23  16] [ -7.28369923e-03  -6.55050224e-03  -5.81730525e-03  -5.08410826e-03
  -4.35091127e-03  -3.61771428e-03  -2.88451730e-03  -2.15132031e-03
  -1.41812332e-03  -6.84926326e-04   4.82706637e-05]
[422 256 210 179 156 158 150 155 162 177] [ -7.28369923e-03  -6.55050224e-03  -5.81730525e-03  -5.08410826e-03
  -4.35091127e-03  -3.61771428e-03  -2.88451730e-03  -2.15132031e-03
  -1.41812332e-03  -6.84926326e-04   4.82706637e-05]
-0.485711
0.412906
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.67806
Epoch 1, cost is  6.14376
Epoch 2, cost is  5.29714
Epoch 3, cost is  4.67261
Epoch 4, cost is  4.24539
Training took 0.089019 minutes
Weight histogram
[ 340  333  308  914 1401  300  178  137   96   43] [-0.01142897 -0.01025848 -0.00908798 -0.00791749 -0.00674699 -0.00557649
 -0.004406   -0.0032355  -0.00206501 -0.00089451  0.00027599]
[1019  433  346  316  299  305  311  314  341  366] [-0.01142897 -0.01025848 -0.00908798 -0.00791749 -0.00674699 -0.00557649
 -0.004406   -0.0032355  -0.00206501 -0.00089451  0.00027599]
-0.485711
0.412906
fine tuning ...
Epoch 0
Fine tuning took 0.053021 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053620 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053712 minutes
{0: [0.020935960591133004, 0.023399014778325122, 0.019704433497536946, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9568965517241379, 0.94827586206896552, 0.96059113300492616, 0.95443349753694584], 5: [0.022167487684729065, 0.02832512315270936, 0.019704433497536946, 0.025862068965517241], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.403535 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.7343
Epoch 1, cost is  6.49799
Epoch 2, cost is  6.26442
Epoch 3, cost is  6.04326
Epoch 4, cost is  5.878
Training took 0.109153 minutes
Weight histogram
[695 845  80 107  61  53  85  60  23  16] [ -7.28369923e-03  -6.55050224e-03  -5.81730525e-03  -5.08410826e-03
  -4.35091127e-03  -3.61771428e-03  -2.88451730e-03  -2.15132031e-03
  -1.41812332e-03  -6.84926326e-04   4.82706637e-05]
[422 256 210 179 156 158 150 155 162 177] [ -7.28369923e-03  -6.55050224e-03  -5.81730525e-03  -5.08410826e-03
  -4.35091127e-03  -3.61771428e-03  -2.88451730e-03  -2.15132031e-03
  -1.41812332e-03  -6.84926326e-04   4.82706637e-05]
-0.485711
0.412906
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.57108
Epoch 1, cost is  5.73233
Epoch 2, cost is  4.77431
Epoch 3, cost is  4.20847
Epoch 4, cost is  3.90472
Training took 0.115109 minutes
Weight histogram
[ 868 1293  361  391  304  487  172   99   45   30] [ -7.56461592e-03  -6.80312993e-03  -6.04164394e-03  -5.28015794e-03
  -4.51867195e-03  -3.75718595e-03  -2.99569996e-03  -2.23421397e-03
  -1.47272797e-03  -7.11241978e-04   5.02440162e-05]
[1034  481  422  416  420  476  305  156  162  178] [ -7.56461592e-03  -6.80312993e-03  -6.04164394e-03  -5.28015794e-03
  -4.51867195e-03  -3.75718595e-03  -2.99569996e-03  -2.23421397e-03
  -1.47272797e-03  -7.11241978e-04   5.02440162e-05]
-0.485711
0.412906
fine tuning ...
Epoch 0
Fine tuning took 0.057894 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057488 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056157 minutes
{0: [0.022167487684729065, 0.024630541871921183, 0.012315270935960592, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.95935960591133007, 0.95320197044334976, 0.96674876847290636, 0.96551724137931039], 5: [0.018472906403940888, 0.022167487684729065, 0.020935960591133004, 0.023399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.401967 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.66668
Epoch 1, cost is  6.22687
Epoch 2, cost is  5.78599
Epoch 3, cost is  5.52781
Epoch 4, cost is  5.3407
Training took 0.154570 minutes
Weight histogram
[ 339  299 1161  118   29   28   21   12   10    8] [ -4.02440410e-03  -3.61806474e-03  -3.21172538e-03  -2.80538602e-03
  -2.39904665e-03  -1.99270729e-03  -1.58636793e-03  -1.18002857e-03
  -7.73689206e-04  -3.67349844e-04   3.89895176e-05]
[500 179 149 150 150 162 173 183 192 187] [ -4.02440410e-03  -3.61806474e-03  -3.21172538e-03  -2.80538602e-03
  -2.39904665e-03  -1.99270729e-03  -1.58636793e-03  -1.18002857e-03
  -7.73689206e-04  -3.67349844e-04   3.89895176e-05]
-0.290863
0.310241
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.74987
Epoch 1, cost is  6.25528
Epoch 2, cost is  5.61143
Epoch 3, cost is  5.2473
Epoch 4, cost is  5.03099
Training took 0.086657 minutes
Weight histogram
[ 193  230  236  279  810  124  838 1205   89   46] [ -1.06374891e-02  -9.56984128e-03  -8.50219341e-03  -7.43454555e-03
  -6.36689768e-03  -5.29924981e-03  -4.23160195e-03  -3.16395408e-03
  -2.09630622e-03  -1.02865835e-03   3.89895176e-05]
[1239  519  519  575  273  149  167  176  203  230] [ -1.06374891e-02  -9.56984128e-03  -8.50219341e-03  -7.43454555e-03
  -6.36689768e-03  -5.29924981e-03  -4.23160195e-03  -3.16395408e-03
  -2.09630622e-03  -1.02865835e-03   3.89895176e-05]
-0.407707
0.38786
fine tuning ...
Epoch 0
Fine tuning took 0.059490 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061769 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060960 minutes
{0: [0.01600985221674877, 0.017241379310344827, 0.019704433497536946, 0.0061576354679802959], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97044334975369462, 0.97167487684729059, 0.96182266009852213, 0.96798029556650245], 5: [0.013546798029556651, 0.011083743842364532, 0.018472906403940888, 0.025862068965517241], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.403798 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.66668
Epoch 1, cost is  6.22687
Epoch 2, cost is  5.78599
Epoch 3, cost is  5.52781
Epoch 4, cost is  5.3407
Training took 0.152222 minutes
Weight histogram
[ 339  299 1161  118   29   28   21   12   10    8] [ -4.02440410e-03  -3.61806474e-03  -3.21172538e-03  -2.80538602e-03
  -2.39904665e-03  -1.99270729e-03  -1.58636793e-03  -1.18002857e-03
  -7.73689206e-04  -3.67349844e-04   3.89895176e-05]
[500 179 149 150 150 162 173 183 192 187] [ -4.02440410e-03  -3.61806474e-03  -3.21172538e-03  -2.80538602e-03
  -2.39904665e-03  -1.99270729e-03  -1.58636793e-03  -1.18002857e-03
  -7.73689206e-04  -3.67349844e-04   3.89895176e-05]
-0.290863
0.310241
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.59937
Epoch 1, cost is  5.70715
Epoch 2, cost is  4.99703
Epoch 3, cost is  4.70658
Epoch 4, cost is  4.54877
Training took 0.113113 minutes
Weight histogram
[ 398  273  258  232 1034 1161  544   77   45   28] [ -7.30624376e-03  -6.57172043e-03  -5.83719710e-03  -5.10267378e-03
  -4.36815045e-03  -3.63362712e-03  -2.89910379e-03  -2.16458047e-03
  -1.43005714e-03  -6.95533810e-04   3.89895176e-05]
[1001  346  307  314  349  378  417  395  254  289] [ -7.30624376e-03  -6.57172043e-03  -5.83719710e-03  -5.10267378e-03
  -4.36815045e-03  -3.63362712e-03  -2.89910379e-03  -2.16458047e-03
  -1.43005714e-03  -6.95533810e-04   3.89895176e-05]
-0.298587
0.310241
fine tuning ...
Epoch 0
Fine tuning took 0.063869 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062721 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062853 minutes
{0: [0.011083743842364532, 0.014778325123152709, 0.01600985221674877, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96798029556650245, 0.97536945812807885, 0.96059113300492616, 0.9568965517241379], 5: [0.020935960591133004, 0.009852216748768473, 0.023399014778325122, 0.020935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.402954 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.66668
Epoch 1, cost is  6.22687
Epoch 2, cost is  5.78599
Epoch 3, cost is  5.52781
Epoch 4, cost is  5.3407
Training took 0.152960 minutes
Weight histogram
[ 339  299 1161  118   29   28   21   12   10    8] [ -4.02440410e-03  -3.61806474e-03  -3.21172538e-03  -2.80538602e-03
  -2.39904665e-03  -1.99270729e-03  -1.58636793e-03  -1.18002857e-03
  -7.73689206e-04  -3.67349844e-04   3.89895176e-05]
[500 179 149 150 150 162 173 183 192 187] [ -4.02440410e-03  -3.61806474e-03  -3.21172538e-03  -2.80538602e-03
  -2.39904665e-03  -1.99270729e-03  -1.58636793e-03  -1.18002857e-03
  -7.73689206e-04  -3.67349844e-04   3.89895176e-05]
-0.290863
0.310241
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.43953
Epoch 1, cost is  5.25494
Epoch 2, cost is  4.63789
Epoch 3, cost is  4.40548
Epoch 4, cost is  4.26974
Training took 0.158779 minutes
Weight histogram
[ 508  600  676 1433  587  106   67   32   23   18] [ -4.57788864e-03  -4.11515195e-03  -3.65241526e-03  -3.18967856e-03
  -2.72694187e-03  -2.26420518e-03  -1.80146848e-03  -1.33873179e-03
  -8.75995096e-04  -4.13258402e-04   4.94782907e-05]
[922 335 297 330 377 456 540 413 193 187] [ -4.57788864e-03  -4.11515195e-03  -3.65241526e-03  -3.18967856e-03
  -2.72694187e-03  -2.26420518e-03  -1.80146848e-03  -1.33873179e-03
  -8.75995096e-04  -4.13258402e-04   4.94782907e-05]
-0.290863
0.310241
fine tuning ...
Epoch 0
Fine tuning took 0.069396 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069623 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068073 minutes
{0: [0.018472906403940888, 0.017241379310344827, 0.020935960591133004, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97044334975369462, 0.96059113300492616, 0.96182266009852213, 0.96182266009852213], 5: [0.011083743842364532, 0.022167487684729065, 0.017241379310344827, 0.023399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.404081 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.56068
Epoch 1, cost is  5.85834
Epoch 2, cost is  5.37892
Epoch 3, cost is  5.08311
Epoch 4, cost is  4.83894
Training took 0.210777 minutes
Weight histogram
[ 311 1051  598   20   13   10    8    5    5    4] [ -2.13161320e-03  -1.91432302e-03  -1.69703284e-03  -1.47974266e-03
  -1.26245248e-03  -1.04516230e-03  -8.27872115e-04  -6.10581934e-04
  -3.93291752e-04  -1.76001570e-04   4.12886111e-05]
[450 166 156 167 173 180 181 177 190 185] [ -2.13161320e-03  -1.91432302e-03  -1.69703284e-03  -1.47974266e-03
  -1.26245248e-03  -1.04516230e-03  -8.27872115e-04  -6.10581934e-04
  -3.93291752e-04  -1.76001570e-04   4.12886111e-05]
-0.288236
0.266373
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.75247
Epoch 1, cost is  6.26251
Epoch 2, cost is  5.87112
Epoch 3, cost is  5.66768
Epoch 4, cost is  5.53566
Training took 0.112250 minutes
Weight histogram
[ 198  226  272  210  210  729  101 2009   68   27] [ -7.32858153e-03  -6.59159452e-03  -5.85460750e-03  -5.11762049e-03
  -4.38063347e-03  -3.64364646e-03  -2.90665945e-03  -2.16967243e-03
  -1.43268542e-03  -6.95698403e-04   4.12886111e-05]
[1156  616  628  510  152  164  185  186  214  239] [ -7.32858153e-03  -6.59159452e-03  -5.85460750e-03  -5.11762049e-03
  -4.38063347e-03  -3.64364646e-03  -2.90665945e-03  -2.16967243e-03
  -1.43268542e-03  -6.95698403e-04   4.12886111e-05]
-0.354087
0.37046
fine tuning ...
Epoch 0
Fine tuning took 0.068834 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069738 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069292 minutes
{0: [0.018472906403940888, 0.012315270935960592, 0.020935960591133004, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.95935960591133007, 0.96305418719211822, 0.95935960591133007, 0.96305418719211822], 5: [0.022167487684729065, 0.024630541871921183, 0.019704433497536946, 0.011083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.402531 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.56068
Epoch 1, cost is  5.85834
Epoch 2, cost is  5.37892
Epoch 3, cost is  5.08311
Epoch 4, cost is  4.83894
Training took 0.208236 minutes
Weight histogram
[ 311 1051  598   20   13   10    8    5    5    4] [ -2.13161320e-03  -1.91432302e-03  -1.69703284e-03  -1.47974266e-03
  -1.26245248e-03  -1.04516230e-03  -8.27872115e-04  -6.10581934e-04
  -3.93291752e-04  -1.76001570e-04   4.12886111e-05]
[450 166 156 167 173 180 181 177 190 185] [ -2.13161320e-03  -1.91432302e-03  -1.69703284e-03  -1.47974266e-03
  -1.26245248e-03  -1.04516230e-03  -8.27872115e-04  -6.10581934e-04
  -3.93291752e-04  -1.76001570e-04   4.12886111e-05]
-0.288236
0.266373
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.60512
Epoch 1, cost is  5.82883
Epoch 2, cost is  5.40215
Epoch 3, cost is  5.21787
Epoch 4, cost is  5.09701
Training took 0.158646 minutes
Weight histogram
[ 411  286  217  256  535  430 1806   58   32   19] [ -5.06430957e-03  -4.55374976e-03  -4.04318994e-03  -3.53263012e-03
  -3.02207030e-03  -2.51151048e-03  -2.00095066e-03  -1.49039084e-03
  -9.79831026e-04  -4.69271207e-04   4.12886111e-05]
[912 359 367 387 414 439 436 215 251 270] [ -5.06430957e-03  -4.55374976e-03  -4.04318994e-03  -3.53263012e-03
  -3.02207030e-03  -2.51151048e-03  -2.00095066e-03  -1.49039084e-03
  -9.79831026e-04  -4.69271207e-04   4.12886111e-05]
-0.351887
0.266373
fine tuning ...
Epoch 0
Fine tuning took 0.074643 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073522 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074310 minutes
{0: [0.018472906403940888, 0.014778325123152709, 0.009852216748768473, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.96551724137931039, 0.97167487684729059, 0.95197044334975367], 5: [0.017241379310344827, 0.019704433497536946, 0.018472906403940888, 0.020935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.402575 minutes
Weight histogram
[  5   7  22 229 491 562 433 217  48  11] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 75  75 100 125 180 162 286 312 288 422] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.515417
0.472872
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.56068
Epoch 1, cost is  5.85834
Epoch 2, cost is  5.37892
Epoch 3, cost is  5.08311
Epoch 4, cost is  4.83894
Training took 0.209886 minutes
Weight histogram
[ 311 1051  598   20   13   10    8    5    5    4] [ -2.13161320e-03  -1.91432302e-03  -1.69703284e-03  -1.47974266e-03
  -1.26245248e-03  -1.04516230e-03  -8.27872115e-04  -6.10581934e-04
  -3.93291752e-04  -1.76001570e-04   4.12886111e-05]
[450 166 156 167 173 180 181 177 190 185] [ -2.13161320e-03  -1.91432302e-03  -1.69703284e-03  -1.47974266e-03
  -1.26245248e-03  -1.04516230e-03  -8.27872115e-04  -6.10581934e-04
  -3.93291752e-04  -1.76001570e-04   4.12886111e-05]
-0.288236
0.266373
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.41203
Epoch 1, cost is  5.46775
Epoch 2, cost is  5.10012
Epoch 3, cost is  4.92163
Epoch 4, cost is  4.79472
Training took 0.217397 minutes
Weight histogram
[ 522  404  309 1253 1428   55   34   22   14    9] [ -3.09556583e-03  -2.78188039e-03  -2.46819494e-03  -2.15450950e-03
  -1.84082405e-03  -1.52713861e-03  -1.21345317e-03  -8.99767721e-04
  -5.86082277e-04  -2.72396833e-04   4.12886111e-05]
[806 311 304 344 386 426 451 481 356 185] [ -3.09556583e-03  -2.78188039e-03  -2.46819494e-03  -2.15450950e-03
  -1.84082405e-03  -1.52713861e-03  -1.21345317e-03  -8.99767721e-04
  -5.86082277e-04  -2.72396833e-04   4.12886111e-05]
-0.288236
0.266373
fine tuning ...
Epoch 0
Fine tuning took 0.078273 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.080111 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079696 minutes
{0: [0.0036945812807881772, 0.018472906403940888, 0.014778325123152709, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97536945812807885, 0.95443349753694584, 0.95935960591133007, 0.93719211822660098], 5: [0.020935960591133004, 0.027093596059113302, 0.025862068965517241, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380549 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.75081
Epoch 1, cost is  5.64441
Epoch 2, cost is  5.56079
Epoch 3, cost is  5.48989
Epoch 4, cost is  5.41971
Training took 0.107096 minutes
Weight histogram
[1126  559  490 1352  134  109   93  106   57   24] [ -1.00638038e-02  -9.05259631e-03  -8.04138887e-03  -7.03018143e-03
  -6.01897399e-03  -5.00776655e-03  -3.99655910e-03  -2.98535166e-03
  -1.97414422e-03  -9.62936778e-04   4.82706637e-05]
[661 383 302 291 328 375 405 419 436 450] [ -1.00638038e-02  -9.05259631e-03  -8.04138887e-03  -7.03018143e-03
  -6.01897399e-03  -5.00776655e-03  -3.99655910e-03  -2.98535166e-03
  -1.97414422e-03  -9.62936778e-04   4.82706637e-05]
-0.667883
0.606744
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.93979
Epoch 1, cost is  4.6826
Epoch 2, cost is  4.49704
Epoch 3, cost is  4.37139
Epoch 4, cost is  4.27185
Training took 0.070659 minutes
Weight histogram
[ 706  921  437  746  672  242  128 1670  361  192] [-0.02688514 -0.02417933 -0.02147352 -0.01876771 -0.0160619  -0.01335609
 -0.01065028 -0.00794448 -0.00523867 -0.00253286  0.00017295]
[1702  726  668  543  290  355  380  404  458  549] [-0.02688514 -0.02417933 -0.02147352 -0.01876771 -0.0160619  -0.01335609
 -0.01065028 -0.00794448 -0.00523867 -0.00253286  0.00017295]
-0.587077
0.525713
fine tuning ...
Epoch 0
Fine tuning took 0.052385 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053426 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052604 minutes
{0: [0.01600985221674877, 0.020935960591133004, 0.013546798029556651, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97044334975369462, 0.96305418719211822, 0.97167487684729059, 0.9568965517241379], 5: [0.013546798029556651, 0.01600985221674877, 0.014778325123152709, 0.017241379310344827], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379743 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.75081
Epoch 1, cost is  5.64441
Epoch 2, cost is  5.56079
Epoch 3, cost is  5.48989
Epoch 4, cost is  5.41971
Training took 0.109177 minutes
Weight histogram
[1126  559  490 1352  134  109   93  106   57   24] [ -1.00638038e-02  -9.05259631e-03  -8.04138887e-03  -7.03018143e-03
  -6.01897399e-03  -5.00776655e-03  -3.99655910e-03  -2.98535166e-03
  -1.97414422e-03  -9.62936778e-04   4.82706637e-05]
[661 383 302 291 328 375 405 419 436 450] [ -1.00638038e-02  -9.05259631e-03  -8.04138887e-03  -7.03018143e-03
  -6.01897399e-03  -5.00776655e-03  -3.99655910e-03  -2.98535166e-03
  -1.97414422e-03  -9.62936778e-04   4.82706637e-05]
-0.667883
0.606744
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.23844
Epoch 1, cost is  4.06715
Epoch 2, cost is  3.97497
Epoch 3, cost is  3.9009
Epoch 4, cost is  3.83008
Training took 0.084916 minutes
Weight histogram
[1089  761  268  455  447 1587  977  233  191   67] [-0.01608267 -0.0144468  -0.01281094 -0.01117507 -0.00953921 -0.00790334
 -0.00626747 -0.00463161 -0.00299574 -0.00135988  0.00027599]
[1264  559  475  462  476  521  508  452  595  763] [-0.01608267 -0.0144468  -0.01281094 -0.01117507 -0.00953921 -0.00790334
 -0.00626747 -0.00463161 -0.00299574 -0.00135988  0.00027599]
-0.485711
0.412906
fine tuning ...
Epoch 0
Fine tuning took 0.054218 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055172 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054048 minutes
{0: [0.014778325123152709, 0.018472906403940888, 0.029556650246305417, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96798029556650245, 0.9568965517241379, 0.9568965517241379, 0.96305418719211822], 5: [0.017241379310344827, 0.024630541871921183, 0.013546798029556651, 0.017241379310344827], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381872 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.75081
Epoch 1, cost is  5.64441
Epoch 2, cost is  5.56079
Epoch 3, cost is  5.48989
Epoch 4, cost is  5.41971
Training took 0.109809 minutes
Weight histogram
[1126  559  490 1352  134  109   93  106   57   24] [ -1.00638038e-02  -9.05259631e-03  -8.04138887e-03  -7.03018143e-03
  -6.01897399e-03  -5.00776655e-03  -3.99655910e-03  -2.98535166e-03
  -1.97414422e-03  -9.62936778e-04   4.82706637e-05]
[661 383 302 291 328 375 405 419 436 450] [ -1.00638038e-02  -9.05259631e-03  -8.04138887e-03  -7.03018143e-03
  -6.01897399e-03  -5.00776655e-03  -3.99655910e-03  -2.98535166e-03
  -1.97414422e-03  -9.62936778e-04   4.82706637e-05]
-0.667883
0.606744
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.98098
Epoch 1, cost is  3.8445
Epoch 2, cost is  3.73339
Epoch 3, cost is  3.62061
Epoch 4, cost is  3.51556
Training took 0.107880 minutes
Weight histogram
[1904  112  952 1373  450  417  552  200   77   38] [ -9.61140264e-03  -8.64523797e-03  -7.67907331e-03  -6.71290864e-03
  -5.74674398e-03  -4.78057931e-03  -3.81441465e-03  -2.84824998e-03
  -1.88208531e-03  -9.15920649e-04   5.02440162e-05]
[1034  481  422  416  420  476  652  789  877  508] [ -9.61140264e-03  -8.64523797e-03  -7.67907331e-03  -6.71290864e-03
  -5.74674398e-03  -4.78057931e-03  -3.81441465e-03  -2.84824998e-03
  -1.88208531e-03  -9.15920649e-04   5.02440162e-05]
-0.485711
0.412906
fine tuning ...
Epoch 0
Fine tuning took 0.056049 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056734 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057294 minutes
{0: [0.009852216748768473, 0.018472906403940888, 0.013546798029556651, 0.018472906403940888], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96674876847290636, 0.96551724137931039, 0.97660098522167482, 0.96305418719211822], 5: [0.023399014778325122, 0.01600985221674877, 0.009852216748768473, 0.018472906403940888], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380363 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.19439
Epoch 1, cost is  5.03785
Epoch 2, cost is  4.9147
Epoch 3, cost is  4.81604
Epoch 4, cost is  4.72346
Training took 0.152810 minutes
Weight histogram
[1526  375  417  569 1031   49   34   24   14   11] [ -5.23845851e-03  -4.71071371e-03  -4.18296891e-03  -3.65522410e-03
  -3.12747930e-03  -2.59973450e-03  -2.07198970e-03  -1.54424489e-03
  -1.01650009e-03  -4.88755286e-04   3.89895176e-05]
[664 287 293 339 359 407 416 415 425 445] [ -5.23845851e-03  -4.71071371e-03  -4.18296891e-03  -3.65522410e-03
  -3.12747930e-03  -2.59973450e-03  -2.07198970e-03  -1.54424489e-03
  -1.01650009e-03  -4.88755286e-04   3.89895176e-05]
-0.496254
0.466189
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.13737
Epoch 1, cost is  5.02954
Epoch 2, cost is  4.95189
Epoch 3, cost is  4.88264
Epoch 4, cost is  4.82239
Training took 0.084337 minutes
Weight histogram
[ 716  623  404  235  308  400 1078  491 1710  110] [ -1.85685344e-02  -1.67077820e-02  -1.48470296e-02  -1.29862772e-02
  -1.11255248e-02  -9.26477244e-03  -7.40402005e-03  -5.54326766e-03
  -3.68251527e-03  -1.82176287e-03   3.89895176e-05]
[1526  793  746  234  262  301  445  550  593  625] [ -1.85685344e-02  -1.67077820e-02  -1.48470296e-02  -1.29862772e-02
  -1.11255248e-02  -9.26477244e-03  -7.40402005e-03  -5.54326766e-03
  -3.68251527e-03  -1.82176287e-03   3.89895176e-05]
-0.530839
0.570165
fine tuning ...
Epoch 0
Fine tuning took 0.059473 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060280 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060127 minutes
{0: [0.009852216748768473, 0.01600985221674877, 0.012315270935960592, 0.0073891625615763543], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96059113300492616, 0.96551724137931039, 0.97290640394088668, 0.96182266009852213], 5: [0.029556650246305417, 0.018472906403940888, 0.014778325123152709, 0.030788177339901478], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379456 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.19439
Epoch 1, cost is  5.03785
Epoch 2, cost is  4.9147
Epoch 3, cost is  4.81604
Epoch 4, cost is  4.72346
Training took 0.152454 minutes
Weight histogram
[1526  375  417  569 1031   49   34   24   14   11] [ -5.23845851e-03  -4.71071371e-03  -4.18296891e-03  -3.65522410e-03
  -3.12747930e-03  -2.59973450e-03  -2.07198970e-03  -1.54424489e-03
  -1.01650009e-03  -4.88755286e-04   3.89895176e-05]
[664 287 293 339 359 407 416 415 425 445] [ -5.23845851e-03  -4.71071371e-03  -4.18296891e-03  -3.65522410e-03
  -3.12747930e-03  -2.59973450e-03  -2.07198970e-03  -1.54424489e-03
  -1.01650009e-03  -4.88755286e-04   3.89895176e-05]
-0.496254
0.466189
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.6534
Epoch 1, cost is  4.53759
Epoch 2, cost is  4.45164
Epoch 3, cost is  4.37105
Epoch 4, cost is  4.28993
Training took 0.110138 minutes
Weight histogram
[1017  634  330  467  404  348 1276 1450  102   47] [ -1.09436028e-02  -9.84534354e-03  -8.74708431e-03  -7.64882508e-03
  -6.55056586e-03  -5.45230663e-03  -4.35404740e-03  -3.25578817e-03
  -2.15752894e-03  -1.05926971e-03   3.89895176e-05]
[1177  460  472  543  620  445  497  598  630  633] [ -1.09436028e-02  -9.84534354e-03  -8.74708431e-03  -7.64882508e-03
  -6.55056586e-03  -5.45230663e-03  -4.35404740e-03  -3.25578817e-03
  -2.15752894e-03  -1.05926971e-03   3.89895176e-05]
-0.369741
0.384465
fine tuning ...
Epoch 0
Fine tuning took 0.063674 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062781 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063706 minutes
{0: [0.018472906403940888, 0.022167487684729065, 0.012315270935960592, 0.013546798029556651], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96182266009852213, 0.9568965517241379, 0.95443349753694584, 0.94950738916256161], 5: [0.019704433497536946, 0.020935960591133004, 0.033251231527093597, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380095 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.19439
Epoch 1, cost is  5.03785
Epoch 2, cost is  4.9147
Epoch 3, cost is  4.81604
Epoch 4, cost is  4.72346
Training took 0.154394 minutes
Weight histogram
[1526  375  417  569 1031   49   34   24   14   11] [ -5.23845851e-03  -4.71071371e-03  -4.18296891e-03  -3.65522410e-03
  -3.12747930e-03  -2.59973450e-03  -2.07198970e-03  -1.54424489e-03
  -1.01650009e-03  -4.88755286e-04   3.89895176e-05]
[664 287 293 339 359 407 416 415 425 445] [ -5.23845851e-03  -4.71071371e-03  -4.18296891e-03  -3.65522410e-03
  -3.12747930e-03  -2.59973450e-03  -2.07198970e-03  -1.54424489e-03
  -1.01650009e-03  -4.88755286e-04   3.89895176e-05]
-0.496254
0.466189
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.39808
Epoch 1, cost is  4.24897
Epoch 2, cost is  4.12226
Epoch 3, cost is  3.99772
Epoch 4, cost is  3.88245
Training took 0.151720 minutes
Weight histogram
[1349  606  251  883 1022 1408  412   81   38   25] [ -6.19898411e-03  -5.57413787e-03  -4.94929163e-03  -4.32444539e-03
  -3.69959915e-03  -3.07475291e-03  -2.44990667e-03  -1.82506043e-03
  -1.20021419e-03  -5.75367949e-04   4.94782907e-05]
[985 369 356 411 517 615 875 803 648 496] [ -6.19898411e-03  -5.57413787e-03  -4.94929163e-03  -4.32444539e-03
  -3.69959915e-03  -3.07475291e-03  -2.44990667e-03  -1.82506043e-03
  -1.20021419e-03  -5.75367949e-04   4.94782907e-05]
-0.334661
0.336241
fine tuning ...
Epoch 0
Fine tuning took 0.068697 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068034 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069324 minutes
{0: [0.02832512315270936, 0.018472906403940888, 0.022167487684729065, 0.020935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93226600985221675, 0.93965517241379315, 0.94088669950738912, 0.93965517241379315], 5: [0.039408866995073892, 0.041871921182266007, 0.036945812807881777, 0.039408866995073892], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380434 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.70956
Epoch 1, cost is  4.53232
Epoch 2, cost is  4.3976
Epoch 3, cost is  4.29083
Epoch 4, cost is  4.20246
Training took 0.208162 minutes
Weight histogram
[1457  499   46  735 1244   27   17   12    7    6] [ -3.09621776e-03  -2.78246712e-03  -2.46871648e-03  -2.15496585e-03
  -1.84121521e-03  -1.52746457e-03  -1.21371394e-03  -8.99963299e-04
  -5.86212662e-04  -2.72462026e-04   4.12886111e-05]
[600 302 329 339 357 386 405 420 440 472] [ -3.09621776e-03  -2.78246712e-03  -2.46871648e-03  -2.15496585e-03
  -1.84121521e-03  -1.52746457e-03  -1.21371394e-03  -8.99963299e-04
  -5.86212662e-04  -2.72462026e-04   4.12886111e-05]
-0.380441
0.383572
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.53021
Epoch 1, cost is  5.43879
Epoch 2, cost is  5.36489
Epoch 3, cost is  5.30493
Epoch 4, cost is  5.24803
Training took 0.107163 minutes
Weight histogram
[ 822  561  399  239  331  424  359  817 2058   65] [ -1.22772595e-02  -1.10454047e-02  -9.81354987e-03  -8.58169506e-03
  -7.34984025e-03  -6.11798544e-03  -4.88613063e-03  -3.65427582e-03
  -2.42242101e-03  -1.19056620e-03   4.12886111e-05]
[1551 1039  462  279  310  380  469  495  530  560] [ -1.22772595e-02  -1.10454047e-02  -9.81354987e-03  -8.58169506e-03
  -7.34984025e-03  -6.11798544e-03  -4.88613063e-03  -3.65427582e-03
  -2.42242101e-03  -1.19056620e-03   4.12886111e-05]
-0.542073
0.540362
fine tuning ...
Epoch 0
Fine tuning took 0.069351 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069400 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068310 minutes
{0: [0.013546798029556651, 0.022167487684729065, 0.014778325123152709, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96921182266009853, 0.95197044334975367, 0.97290640394088668, 0.93596059113300489], 5: [0.017241379310344827, 0.025862068965517241, 0.012315270935960592, 0.041871921182266007], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380690 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.70956
Epoch 1, cost is  4.53232
Epoch 2, cost is  4.3976
Epoch 3, cost is  4.29083
Epoch 4, cost is  4.20246
Training took 0.208681 minutes
Weight histogram
[1457  499   46  735 1244   27   17   12    7    6] [ -3.09621776e-03  -2.78246712e-03  -2.46871648e-03  -2.15496585e-03
  -1.84121521e-03  -1.52746457e-03  -1.21371394e-03  -8.99963299e-04
  -5.86212662e-04  -2.72462026e-04   4.12886111e-05]
[600 302 329 339 357 386 405 420 440 472] [ -3.09621776e-03  -2.78246712e-03  -2.46871648e-03  -2.15496585e-03
  -1.84121521e-03  -1.52746457e-03  -1.21371394e-03  -8.99963299e-04
  -5.86212662e-04  -2.72462026e-04   4.12886111e-05]
-0.380441
0.383572
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.04564
Epoch 1, cost is  4.91701
Epoch 2, cost is  4.81214
Epoch 3, cost is  4.71676
Epoch 4, cost is  4.63741
Training took 0.153619 minutes
Weight histogram
[1477  452  225  464  334  439  691 1898   66   29] [ -7.02385651e-03  -6.31734200e-03  -5.61082749e-03  -4.90431297e-03
  -4.19779846e-03  -3.49128395e-03  -2.78476944e-03  -2.07825493e-03
  -1.37174041e-03  -6.65225901e-04   4.12886111e-05]
[1163  619  687  752  414  474  528  477  474  487] [ -7.02385651e-03  -6.31734200e-03  -5.61082749e-03  -4.90431297e-03
  -4.19779846e-03  -3.49128395e-03  -2.78476944e-03  -2.07825493e-03
  -1.37174041e-03  -6.65225901e-04   4.12886111e-05]
-0.524216
0.39696
fine tuning ...
Epoch 0
Fine tuning took 0.074965 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073772 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074366 minutes
{0: [0.023399014778325122, 0.020935960591133004, 0.032019704433497539, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93719211822660098, 0.94334975369458129, 0.93719211822660098, 0.93596059113300489], 5: [0.039408866995073892, 0.035714285714285712, 0.030788177339901478, 0.04064039408866995], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380805 minutes
Weight histogram
[   5    7   22  229  504  667  922 1195  462   37] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[111 168 226 300 473 603 369 393 587 820] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.760996
0.815157
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.70956
Epoch 1, cost is  4.53232
Epoch 2, cost is  4.3976
Epoch 3, cost is  4.29083
Epoch 4, cost is  4.20246
Training took 0.208506 minutes
Weight histogram
[1457  499   46  735 1244   27   17   12    7    6] [ -3.09621776e-03  -2.78246712e-03  -2.46871648e-03  -2.15496585e-03
  -1.84121521e-03  -1.52746457e-03  -1.21371394e-03  -8.99963299e-04
  -5.86212662e-04  -2.72462026e-04   4.12886111e-05]
[600 302 329 339 357 386 405 420 440 472] [ -3.09621776e-03  -2.78246712e-03  -2.46871648e-03  -2.15496585e-03
  -1.84121521e-03  -1.52746457e-03  -1.21371394e-03  -8.99963299e-04
  -5.86212662e-04  -2.72462026e-04   4.12886111e-05]
-0.380441
0.383572
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.69364
Epoch 1, cost is  4.543
Epoch 2, cost is  4.42739
Epoch 3, cost is  4.32834
Epoch 4, cost is  4.24272
Training took 0.208502 minutes
Weight histogram
[1552  428  528  448  577 2361  104   42   22   13] [ -4.08658292e-03  -3.67379577e-03  -3.26100861e-03  -2.84822146e-03
  -2.43543431e-03  -2.02264716e-03  -1.60986000e-03  -1.19707285e-03
  -7.84285695e-04  -3.71498542e-04   4.12886111e-05]
[964 460 536 625 680 790 618 446 463 493] [ -4.08658292e-03  -3.67379577e-03  -3.26100861e-03  -2.84822146e-03
  -2.43543431e-03  -2.02264716e-03  -1.60986000e-03  -1.19707285e-03
  -7.84285695e-04  -3.71498542e-04   4.12886111e-05]
-0.293899
0.308971
fine tuning ...
Epoch 0
Fine tuning took 0.079013 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079484 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079260 minutes
{0: [0.038177339901477834, 0.038177339901477834, 0.033251231527093597, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.92733990147783252, 0.91995073891625612, 0.92980295566502458, 0.9211822660098522], 5: [0.034482758620689655, 0.041871921182266007, 0.036945812807881777, 0.039408866995073892], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380638 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.29835
Epoch 1, cost is  5.21465
Epoch 2, cost is  5.15907
Epoch 3, cost is  5.11156
Epoch 4, cost is  5.06759
Training took 0.108665 minutes
Weight histogram
[1335  528  147 1388  523 1670  191  110  145   38] [ -1.45440539e-02  -1.30848215e-02  -1.16255890e-02  -1.01663566e-02
  -8.70712410e-03  -7.24789164e-03  -5.78865918e-03  -4.32942672e-03
  -2.87019426e-03  -1.41096180e-03   4.82706637e-05]
[834 458 425 486 568 596 631 655 689 733] [ -1.45440539e-02  -1.30848215e-02  -1.16255890e-02  -1.01663566e-02
  -8.70712410e-03  -7.24789164e-03  -5.78865918e-03  -4.32942672e-03
  -2.87019426e-03  -1.41096180e-03   4.82706637e-05]
-0.838098
0.854585
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.41264
Epoch 1, cost is  4.32219
Epoch 2, cost is  4.27236
Epoch 3, cost is  4.20987
Epoch 4, cost is  4.16008
Training took 0.070879 minutes
Weight histogram
[1165  703  683 1150  636 1068  347  468 1579  301] [-0.03564107 -0.03205967 -0.02847827 -0.02489686 -0.02131546 -0.01773406
 -0.01415266 -0.01057126 -0.00698985 -0.00340845  0.00017295]
[1883  830  835  338  433  464  510  625 1041 1141] [-0.03564107 -0.03205967 -0.02847827 -0.02489686 -0.02131546 -0.01773406
 -0.01415266 -0.01057126 -0.00698985 -0.00340845  0.00017295]
-0.681538
0.638561
fine tuning ...
Epoch 0
Fine tuning took 0.051105 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052892 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051229 minutes
{0: [0.013546798029556651, 0.014778325123152709, 0.012315270935960592, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96921182266009853, 0.97044334975369462, 0.97413793103448276, 0.9642857142857143], 5: [0.017241379310344827, 0.014778325123152709, 0.013546798029556651, 0.0086206896551724137], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380089 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.29835
Epoch 1, cost is  5.21465
Epoch 2, cost is  5.15907
Epoch 3, cost is  5.11156
Epoch 4, cost is  5.06759
Training took 0.109322 minutes
Weight histogram
[1335  528  147 1388  523 1670  191  110  145   38] [ -1.45440539e-02  -1.30848215e-02  -1.16255890e-02  -1.01663566e-02
  -8.70712410e-03  -7.24789164e-03  -5.78865918e-03  -4.32942672e-03
  -2.87019426e-03  -1.41096180e-03   4.82706637e-05]
[834 458 425 486 568 596 631 655 689 733] [ -1.45440539e-02  -1.30848215e-02  -1.16255890e-02  -1.01663566e-02
  -8.70712410e-03  -7.24789164e-03  -5.78865918e-03  -4.32942672e-03
  -2.87019426e-03  -1.41096180e-03   4.82706637e-05]
-0.838098
0.854585
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.02544
Epoch 1, cost is  3.92422
Epoch 2, cost is  3.82387
Epoch 3, cost is  3.75175
Epoch 4, cost is  3.68187
Training took 0.085007 minutes
Weight histogram
[1879  944 1006  336  542  451 2217  405  233   87] [-0.01867443 -0.01677939 -0.01488435 -0.01298931 -0.01109426 -0.00919922
 -0.00730418 -0.00540914 -0.0035141  -0.00161906  0.00027599]
[1411  641  572  589  659  574  645  919 1100  990] [-0.01867443 -0.01677939 -0.01488435 -0.01298931 -0.01109426 -0.00919922
 -0.00730418 -0.00540914 -0.0035141  -0.00161906  0.00027599]
-0.485711
0.432521
fine tuning ...
Epoch 0
Fine tuning took 0.054111 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055218 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053227 minutes
{0: [0.011083743842364532, 0.017241379310344827, 0.012315270935960592, 0.012315270935960592], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97536945812807885, 0.96798029556650245, 0.97413793103448276, 0.9642857142857143], 5: [0.013546798029556651, 0.014778325123152709, 0.013546798029556651, 0.023399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380318 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.29835
Epoch 1, cost is  5.21465
Epoch 2, cost is  5.15907
Epoch 3, cost is  5.11156
Epoch 4, cost is  5.06759
Training took 0.108767 minutes
Weight histogram
[1335  528  147 1388  523 1670  191  110  145   38] [ -1.45440539e-02  -1.30848215e-02  -1.16255890e-02  -1.01663566e-02
  -8.70712410e-03  -7.24789164e-03  -5.78865918e-03  -4.32942672e-03
  -2.87019426e-03  -1.41096180e-03   4.82706637e-05]
[834 458 425 486 568 596 631 655 689 733] [ -1.45440539e-02  -1.30848215e-02  -1.16255890e-02  -1.01663566e-02
  -8.70712410e-03  -7.24789164e-03  -5.78865918e-03  -4.32942672e-03
  -2.87019426e-03  -1.41096180e-03   4.82706637e-05]
-0.838098
0.854585
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.67646
Epoch 1, cost is  3.57691
Epoch 2, cost is  3.49685
Epoch 3, cost is  3.42074
Epoch 4, cost is  3.36223
Training took 0.107551 minutes
Weight histogram
[2625 1367  236 1777  621  509  599  235   90   41] [ -1.03202136e-02  -9.28316786e-03  -8.24612210e-03  -7.20907633e-03
  -6.17203057e-03  -5.13498480e-03  -4.09793904e-03  -3.06089328e-03
  -2.02384751e-03  -9.86801748e-04   5.02440162e-05]
[1121  546  491  474  539  743  940 1021 1198 1027] [ -1.03202136e-02  -9.28316786e-03  -8.24612210e-03  -7.20907633e-03
  -6.17203057e-03  -5.13498480e-03  -4.09793904e-03  -3.06089328e-03
  -2.02384751e-03  -9.86801748e-04   5.02440162e-05]
-0.485711
0.412906
fine tuning ...
Epoch 0
Fine tuning took 0.055868 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056034 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057477 minutes
{0: [0.020935960591133004, 0.013546798029556651, 0.011083743842364532, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.96059113300492616, 0.97660098522167482, 0.96551724137931039], 5: [0.014778325123152709, 0.025862068965517241, 0.012315270935960592, 0.012315270935960592], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379495 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.65881
Epoch 1, cost is  4.55944
Epoch 2, cost is  4.48916
Epoch 3, cost is  4.43014
Epoch 4, cost is  4.3789
Training took 0.151907 minutes
Weight histogram
[1588  328   87 1610  559  822  980   54   30   17] [ -7.76823983e-03  -6.98751689e-03  -6.20679396e-03  -5.42607102e-03
  -4.64534809e-03  -3.86462515e-03  -3.08390222e-03  -2.30317929e-03
  -1.52245635e-03  -7.41733417e-04   3.89895176e-05]
[780 402 463 528 580 576 613 678 702 753] [ -7.76823983e-03  -6.98751689e-03  -6.20679396e-03  -5.42607102e-03
  -4.64534809e-03  -3.86462515e-03  -3.08390222e-03  -2.30317929e-03
  -1.52245635e-03  -7.41733417e-04   3.89895176e-05]
-0.646611
0.665106
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.93555
Epoch 1, cost is  4.86953
Epoch 2, cost is  4.82664
Epoch 3, cost is  4.77534
Epoch 4, cost is  4.73566
Training took 0.084355 minutes
Weight histogram
[1316  616  782  755  438  328  521 1123 2053  168] [ -2.40406319e-02  -2.16326698e-02  -1.92247077e-02  -1.68167455e-02
  -1.44087834e-02  -1.20008212e-02  -9.59285907e-03  -7.18489692e-03
  -4.77693478e-03  -2.36897263e-03   3.89895176e-05]
[1709 1030  489  322  383  607  716  768 1051 1025] [ -2.40406319e-02  -2.16326698e-02  -1.92247077e-02  -1.68167455e-02
  -1.44087834e-02  -1.20008212e-02  -9.59285907e-03  -7.18489692e-03
  -4.77693478e-03  -2.36897263e-03   3.89895176e-05]
-0.682381
0.667366
fine tuning ...
Epoch 0
Fine tuning took 0.060985 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060703 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060787 minutes
{0: [0.0049261083743842365, 0.022167487684729065, 0.020935960591133004, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.99014778325123154, 0.96305418719211822, 0.96059113300492616, 0.94950738916256161], 5: [0.0049261083743842365, 0.014778325123152709, 0.018472906403940888, 0.02832512315270936], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380490 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.65881
Epoch 1, cost is  4.55944
Epoch 2, cost is  4.48916
Epoch 3, cost is  4.43014
Epoch 4, cost is  4.3789
Training took 0.152398 minutes
Weight histogram
[1588  328   87 1610  559  822  980   54   30   17] [ -7.76823983e-03  -6.98751689e-03  -6.20679396e-03  -5.42607102e-03
  -4.64534809e-03  -3.86462515e-03  -3.08390222e-03  -2.30317929e-03
  -1.52245635e-03  -7.41733417e-04   3.89895176e-05]
[780 402 463 528 580 576 613 678 702 753] [ -7.76823983e-03  -6.98751689e-03  -6.20679396e-03  -5.42607102e-03
  -4.64534809e-03  -3.86462515e-03  -3.08390222e-03  -2.30317929e-03
  -1.52245635e-03  -7.41733417e-04   3.89895176e-05]
-0.646611
0.665106
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.3766
Epoch 1, cost is  4.27222
Epoch 2, cost is  4.18257
Epoch 3, cost is  4.10035
Epoch 4, cost is  4.02265
Training took 0.109924 minutes
Weight histogram
[1856  750  938  455  542  434 1120 1805  141   59] [ -1.29055744e-02  -1.16111180e-02  -1.03166616e-02  -9.02220523e-03
  -7.72774884e-03  -6.43329244e-03  -5.13883605e-03  -3.84437966e-03
  -2.54992327e-03  -1.25546687e-03   3.89895176e-05]
[1339  614  715  816  531  829  850  869  774  763] [ -1.29055744e-02  -1.16111180e-02  -1.03166616e-02  -9.02220523e-03
  -7.72774884e-03  -6.43329244e-03  -5.13883605e-03  -3.84437966e-03
  -2.54992327e-03  -1.25546687e-03   3.89895176e-05]
-0.499232
0.472202
fine tuning ...
Epoch 0
Fine tuning took 0.063059 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063059 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063716 minutes
{0: [0.034482758620689655, 0.033251231527093597, 0.025862068965517241, 0.043103448275862072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93965517241379315, 0.91748768472906406, 0.93349753694581283, 0.9211822660098522], 5: [0.025862068965517241, 0.049261083743842367, 0.04064039408866995, 0.035714285714285712], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380507 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.65881
Epoch 1, cost is  4.55944
Epoch 2, cost is  4.48916
Epoch 3, cost is  4.43014
Epoch 4, cost is  4.3789
Training took 0.152062 minutes
Weight histogram
[1588  328   87 1610  559  822  980   54   30   17] [ -7.76823983e-03  -6.98751689e-03  -6.20679396e-03  -5.42607102e-03
  -4.64534809e-03  -3.86462515e-03  -3.08390222e-03  -2.30317929e-03
  -1.52245635e-03  -7.41733417e-04   3.89895176e-05]
[780 402 463 528 580 576 613 678 702 753] [ -7.76823983e-03  -6.98751689e-03  -6.20679396e-03  -5.42607102e-03
  -4.64534809e-03  -3.86462515e-03  -3.08390222e-03  -2.30317929e-03
  -1.52245635e-03  -7.41733417e-04   3.89895176e-05]
-0.646611
0.665106
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.88713
Epoch 1, cost is  3.75392
Epoch 2, cost is  3.65201
Epoch 3, cost is  3.57146
Epoch 4, cost is  3.49467
Training took 0.151131 minutes
Weight histogram
[1982 1265  747  332 1055 1819  721  104   46   29] [ -7.05414452e-03  -6.34378224e-03  -5.63341996e-03  -4.92305767e-03
  -4.21269539e-03  -3.50233311e-03  -2.79197083e-03  -2.08160855e-03
  -1.37124627e-03  -6.60883990e-04   4.94782907e-05]
[1135  488  565  748 1108 1097  760  755  707  737] [ -7.05414452e-03  -6.34378224e-03  -5.63341996e-03  -4.92305767e-03
  -4.21269539e-03  -3.50233311e-03  -2.79197083e-03  -2.08160855e-03
  -1.37124627e-03  -6.60883990e-04   4.94782907e-05]
-0.406318
0.405887
fine tuning ...
Epoch 0
Fine tuning took 0.067626 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068594 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068130 minutes
{0: [0.039408866995073892, 0.038177339901477834, 0.043103448275862072, 0.050492610837438424], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93472906403940892, 0.91379310344827591, 0.92487684729064035, 0.89039408866995073], 5: [0.025862068965517241, 0.048029556650246302, 0.032019704433497539, 0.059113300492610835], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380757 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.1264
Epoch 1, cost is  4.01629
Epoch 2, cost is  3.93545
Epoch 3, cost is  3.86906
Epoch 4, cost is  3.81202
Training took 0.207849 minutes
Weight histogram
[1674  288   42 1400  599  689 1325   33   16    9] [ -4.69387043e-03  -4.22035453e-03  -3.74683862e-03  -3.27332272e-03
  -2.79980682e-03  -2.32629091e-03  -1.85277501e-03  -1.37925910e-03
  -9.05743198e-04  -4.32227293e-04   4.12886111e-05]
[699 428 453 480 542 557 602 730 777 807] [ -4.69387043e-03  -4.22035453e-03  -3.74683862e-03  -3.27332272e-03
  -2.79980682e-03  -2.32629091e-03  -1.85277501e-03  -1.37925910e-03
  -9.05743198e-04  -4.32227293e-04   4.12886111e-05]
-0.459593
0.475685
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.20794
Epoch 1, cost is  5.142
Epoch 2, cost is  5.09008
Epoch 3, cost is  5.04346
Epoch 4, cost is  5.00253
Training took 0.106986 minutes
Weight histogram
[1382  574  749  810  455  344  510  758 2410  108] [ -1.59843899e-02  -1.43818221e-02  -1.27792542e-02  -1.11766864e-02
  -9.57411851e-03  -7.97155066e-03  -6.36898281e-03  -4.76641495e-03
  -3.16384710e-03  -1.56127924e-03   4.12886111e-05]
[1884 1081  358  427  565  647  705  800  806  827] [ -1.59843899e-02  -1.43818221e-02  -1.27792542e-02  -1.11766864e-02
  -9.57411851e-03  -7.97155066e-03  -6.36898281e-03  -4.76641495e-03
  -3.16384710e-03  -1.56127924e-03   4.12886111e-05]
-0.671745
0.619753
fine tuning ...
Epoch 0
Fine tuning took 0.068772 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068732 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069151 minutes
{0: [0.029556650246305417, 0.032019704433497539, 0.030788177339901478, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94950738916256161, 0.93719211822660098, 0.93596059113300489, 0.9211822660098522], 5: [0.020935960591133004, 0.030788177339901478, 0.033251231527093597, 0.051724137931034482], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379962 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.1264
Epoch 1, cost is  4.01629
Epoch 2, cost is  3.93545
Epoch 3, cost is  3.86906
Epoch 4, cost is  3.81202
Training took 0.209545 minutes
Weight histogram
[1674  288   42 1400  599  689 1325   33   16    9] [ -4.69387043e-03  -4.22035453e-03  -3.74683862e-03  -3.27332272e-03
  -2.79980682e-03  -2.32629091e-03  -1.85277501e-03  -1.37925910e-03
  -9.05743198e-04  -4.32227293e-04   4.12886111e-05]
[699 428 453 480 542 557 602 730 777 807] [ -4.69387043e-03  -4.22035453e-03  -3.74683862e-03  -3.27332272e-03
  -2.79980682e-03  -2.32629091e-03  -1.85277501e-03  -1.37925910e-03
  -9.05743198e-04  -4.32227293e-04   4.12886111e-05]
-0.459593
0.475685
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.60966
Epoch 1, cost is  4.52801
Epoch 2, cost is  4.46435
Epoch 3, cost is  4.41
Epoch 4, cost is  4.35592
Training took 0.153151 minutes
Weight histogram
[1460  529 1743  286  524  431  612 1674  800   41] [ -8.84178001e-03  -7.95347315e-03  -7.06516629e-03  -6.17685943e-03
  -5.28855256e-03  -4.40024570e-03  -3.51193884e-03  -2.62363198e-03
  -1.73532511e-03  -8.47018251e-04   4.12886111e-05]
[1398  919 1005  564  729  661  672  676  707  769] [ -8.84178001e-03  -7.95347315e-03  -7.06516629e-03  -6.17685943e-03
  -5.28855256e-03  -4.40024570e-03  -3.51193884e-03  -2.62363198e-03
  -1.73532511e-03  -8.47018251e-04   4.12886111e-05]
-0.64149
0.497189
fine tuning ...
Epoch 0
Fine tuning took 0.074893 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074866 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074287 minutes
{0: [0.045566502463054187, 0.043103448275862072, 0.048029556650246302, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90886699507389157, 0.91995073891625612, 0.90640394088669951, 0.91871921182266014], 5: [0.045566502463054187, 0.036945812807881777, 0.045566502463054187, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381357 minutes
Weight histogram
[   5    7   22  229  504  880 1602 2083  693   50] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
[ 123  182  270  400  512  619  340  580 1362 1687] [ -4.69115912e-04  -3.77863925e-04  -2.86611938e-04  -1.95359951e-04
  -1.04107964e-04  -1.28559768e-05   7.83960102e-05   1.69647997e-04
   2.60899984e-04   3.52151971e-04   4.43403958e-04]
-0.791512
0.815157
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.1264
Epoch 1, cost is  4.01629
Epoch 2, cost is  3.93545
Epoch 3, cost is  3.86906
Epoch 4, cost is  3.81202
Training took 0.208043 minutes
Weight histogram
[1674  288   42 1400  599  689 1325   33   16    9] [ -4.69387043e-03  -4.22035453e-03  -3.74683862e-03  -3.27332272e-03
  -2.79980682e-03  -2.32629091e-03  -1.85277501e-03  -1.37925910e-03
  -9.05743198e-04  -4.32227293e-04   4.12886111e-05]
[699 428 453 480 542 557 602 730 777 807] [ -4.69387043e-03  -4.22035453e-03  -3.74683862e-03  -3.27332272e-03
  -2.79980682e-03  -2.32629091e-03  -1.85277501e-03  -1.37925910e-03
  -9.05743198e-04  -4.32227293e-04   4.12886111e-05]
-0.459593
0.475685
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.19016
Epoch 1, cost is  4.09082
Epoch 2, cost is  4.01729
Epoch 3, cost is  3.95092
Epoch 4, cost is  3.89656
Training took 0.208625 minutes
Weight histogram
[1771  284 1795  284  784  655 2399   75   36   17] [ -5.07785380e-03  -4.56593956e-03  -4.05402532e-03  -3.54211108e-03
  -3.03019683e-03  -2.51828259e-03  -2.00636835e-03  -1.49445411e-03
  -9.82539871e-04  -4.70625630e-04   4.12886111e-05]
[1137  682  861  983  963  613  653  739  715  754] [ -5.07785380e-03  -4.56593956e-03  -4.05402532e-03  -3.54211108e-03
  -3.03019683e-03  -2.51828259e-03  -2.00636835e-03  -1.49445411e-03
  -9.82539871e-04  -4.70625630e-04   4.12886111e-05]
-0.367362
0.364668
fine tuning ...
Epoch 0
Fine tuning took 0.079425 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079023 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079356 minutes
{0: [0.064039408866995079, 0.048029556650246302, 0.049261083743842367, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88916256157635465, 0.91133004926108374, 0.91379310344827591, 0.9285714285714286], 5: [0.046798029556650245, 0.04064039408866995, 0.036945812807881777, 0.032019704433497539], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380217 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.02258
Epoch 1, cost is  4.95475
Epoch 2, cost is  4.91488
Epoch 3, cost is  4.88018
Epoch 4, cost is  4.84921
Training took 0.109446 minutes
Weight histogram
[1210  550 1028 1005  329 1645 1819  236  198   80] [ -1.99446883e-02  -1.79453924e-02  -1.59460965e-02  -1.39468006e-02
  -1.19475047e-02  -9.94820882e-03  -7.94891292e-03  -5.94961703e-03
  -3.95032113e-03  -1.95102523e-03   4.82706637e-05]
[ 973  536  572  710  760  810  844  917  947 1031] [ -1.99446883e-02  -1.79453924e-02  -1.59460965e-02  -1.39468006e-02
  -1.19475047e-02  -9.94820882e-03  -7.94891292e-03  -5.94961703e-03
  -3.95032113e-03  -1.95102523e-03   4.82706637e-05]
-1.16323
1.2306
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.2683
Epoch 1, cost is  4.20174
Epoch 2, cost is  4.15393
Epoch 3, cost is  4.11471
Epoch 4, cost is  4.07059
Training took 0.070733 minutes
Weight histogram
[2492 1059  587 1033  892 1152  504  194 1896  316] [-0.03817296 -0.03433837 -0.03050378 -0.02666919 -0.0228346  -0.019
 -0.01516541 -0.01133082 -0.00749623 -0.00366164  0.00017295]
[2044  966  714  466  548  605  813 1309 1279 1381] [-0.03817296 -0.03433837 -0.03050378 -0.02666919 -0.0228346  -0.019
 -0.01516541 -0.01133082 -0.00749623 -0.00366164  0.00017295]
-0.769385
0.762403
fine tuning ...
Epoch 0
Fine tuning took 0.052718 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052236 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053250 minutes
{0: [0.0086206896551724137, 0.014778325123152709, 0.01600985221674877, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.98522167487684731, 0.96921182266009853, 0.97413793103448276, 0.96305418719211822], 5: [0.0061576354679802959, 0.01600985221674877, 0.009852216748768473, 0.022167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379676 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.02258
Epoch 1, cost is  4.95475
Epoch 2, cost is  4.91488
Epoch 3, cost is  4.88018
Epoch 4, cost is  4.84921
Training took 0.109258 minutes
Weight histogram
[1210  550 1028 1005  329 1645 1819  236  198   80] [ -1.99446883e-02  -1.79453924e-02  -1.59460965e-02  -1.39468006e-02
  -1.19475047e-02  -9.94820882e-03  -7.94891292e-03  -5.94961703e-03
  -3.95032113e-03  -1.95102523e-03   4.82706637e-05]
[ 973  536  572  710  760  810  844  917  947 1031] [ -1.99446883e-02  -1.79453924e-02  -1.59460965e-02  -1.39468006e-02
  -1.19475047e-02  -9.94820882e-03  -7.94891292e-03  -5.94961703e-03
  -3.95032113e-03  -1.95102523e-03   4.82706637e-05]
-1.16323
1.2306
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.80062
Epoch 1, cost is  3.73515
Epoch 2, cost is  3.69535
Epoch 3, cost is  3.64715
Epoch 4, cost is  3.61005
Training took 0.086135 minutes
Weight histogram
[3028 1197 1288  524  539  527 2250  418  256   98] [-0.01987686 -0.01786157 -0.01584629 -0.013831   -0.01181572 -0.00980044
 -0.00778515 -0.00576987 -0.00375458 -0.0017393   0.00027599]
[1527  702  655  718  718  715 1028 1253 1287 1522] [-0.01987686 -0.01786157 -0.01584629 -0.013831   -0.01181572 -0.00980044
 -0.00778515 -0.00576987 -0.00375458 -0.0017393   0.00027599]
-0.542458
0.517224
fine tuning ...
Epoch 0
Fine tuning took 0.052669 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053535 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053962 minutes
{0: [0.018472906403940888, 0.011083743842364532, 0.011083743842364532, 0.0073891625615763543], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96305418719211822, 0.97290640394088668, 0.98029556650246308, 0.98275862068965514], 5: [0.018472906403940888, 0.01600985221674877, 0.0086206896551724137, 0.009852216748768473], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379097 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.02258
Epoch 1, cost is  4.95475
Epoch 2, cost is  4.91488
Epoch 3, cost is  4.88018
Epoch 4, cost is  4.84921
Training took 0.106806 minutes
Weight histogram
[1210  550 1028 1005  329 1645 1819  236  198   80] [ -1.99446883e-02  -1.79453924e-02  -1.59460965e-02  -1.39468006e-02
  -1.19475047e-02  -9.94820882e-03  -7.94891292e-03  -5.94961703e-03
  -3.95032113e-03  -1.95102523e-03   4.82706637e-05]
[ 973  536  572  710  760  810  844  917  947 1031] [ -1.99446883e-02  -1.79453924e-02  -1.59460965e-02  -1.39468006e-02
  -1.19475047e-02  -9.94820882e-03  -7.94891292e-03  -5.94961703e-03
  -3.95032113e-03  -1.95102523e-03   4.82706637e-05]
-1.16323
1.2306
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.47982
Epoch 1, cost is  3.3886
Epoch 2, cost is  3.31346
Epoch 3, cost is  3.23851
Epoch 4, cost is  3.17535
Training took 0.107894 minutes
Weight histogram
[3574 2303  189 1097 1369  571  533  329  115   45] [ -1.09849889e-02  -9.88146559e-03  -8.77794230e-03  -7.67441901e-03
  -6.57089572e-03  -5.46737243e-03  -4.36384914e-03  -3.26032585e-03
  -2.15680256e-03  -1.05327927e-03   5.02440162e-05]
[1243  622  585  613  871 1157 1374 1266 1256 1138] [ -1.09849889e-02  -9.88146559e-03  -8.77794230e-03  -7.67441901e-03
  -6.57089572e-03  -5.46737243e-03  -4.36384914e-03  -3.26032585e-03
  -2.15680256e-03  -1.05327927e-03   5.02440162e-05]
-0.485711
0.438695
fine tuning ...
Epoch 0
Fine tuning took 0.057130 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056292 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057967 minutes
{0: [0.024630541871921183, 0.017241379310344827, 0.017241379310344827, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96674876847290636, 0.9642857142857143, 0.97413793103448276, 0.93719211822660098], 5: [0.0086206896551724137, 0.018472906403940888, 0.0086206896551724137, 0.035714285714285712], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379717 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.2824
Epoch 1, cost is  4.20664
Epoch 2, cost is  4.15849
Epoch 3, cost is  4.1147
Epoch 4, cost is  4.07799
Training took 0.151603 minutes
Weight histogram
[1622  330 1240  747  272 1750 1045 1013   56   25] [ -1.03901634e-02  -9.34724811e-03  -8.30433282e-03  -7.26141752e-03
  -6.21850223e-03  -5.17558694e-03  -4.13267165e-03  -3.08975636e-03
  -2.04684107e-03  -1.00392577e-03   3.89895176e-05]
[ 873  520  608  717  718  784  851  919 1024 1086] [ -1.03901634e-02  -9.34724811e-03  -8.30433282e-03  -7.26141752e-03
  -6.21850223e-03  -5.17558694e-03  -4.13267165e-03  -3.08975636e-03
  -2.04684107e-03  -1.00392577e-03   3.89895176e-05]
-0.733431
0.833501
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.75738
Epoch 1, cost is  4.70148
Epoch 2, cost is  4.65489
Epoch 3, cost is  4.61465
Epoch 4, cost is  4.57596
Training took 0.087486 minutes
Weight histogram
[1780 1385  806  979  678  417  553 1220 2083  224] [ -2.73436420e-02  -2.46053788e-02  -2.18671157e-02  -1.91288525e-02
  -1.63905894e-02  -1.36523262e-02  -1.09140631e-02  -8.17579993e-03
  -5.43753678e-03  -2.69927363e-03   3.89895176e-05]
[1915 1151  363  436  720  871 1084 1237 1197 1151] [ -2.73436420e-02  -2.46053788e-02  -2.18671157e-02  -1.91288525e-02
  -1.63905894e-02  -1.36523262e-02  -1.09140631e-02  -8.17579993e-03
  -5.43753678e-03  -2.69927363e-03   3.89895176e-05]
-0.757497
0.799939
fine tuning ...
Epoch 0
Fine tuning took 0.059165 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059517 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059308 minutes
{0: [0.018472906403940888, 0.025862068965517241, 0.020935960591133004, 0.04064039408866995], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96551724137931039, 0.94581280788177335, 0.95812807881773399, 0.92610837438423643], 5: [0.01600985221674877, 0.02832512315270936, 0.020935960591133004, 0.033251231527093597], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378722 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.2824
Epoch 1, cost is  4.20664
Epoch 2, cost is  4.15849
Epoch 3, cost is  4.1147
Epoch 4, cost is  4.07799
Training took 0.152971 minutes
Weight histogram
[1622  330 1240  747  272 1750 1045 1013   56   25] [ -1.03901634e-02  -9.34724811e-03  -8.30433282e-03  -7.26141752e-03
  -6.21850223e-03  -5.17558694e-03  -4.13267165e-03  -3.08975636e-03
  -2.04684107e-03  -1.00392577e-03   3.89895176e-05]
[ 873  520  608  717  718  784  851  919 1024 1086] [ -1.03901634e-02  -9.34724811e-03  -8.30433282e-03  -7.26141752e-03
  -6.21850223e-03  -5.17558694e-03  -4.13267165e-03  -3.08975636e-03
  -2.04684107e-03  -1.00392577e-03   3.89895176e-05]
-0.733431
0.833501
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.04936
Epoch 1, cost is  3.97283
Epoch 2, cost is  3.9092
Epoch 3, cost is  3.85233
Epoch 4, cost is  3.80266
Training took 0.110787 minutes
Weight histogram
[2283 1688 1019  842  370  609  713 2309  224   68] [ -1.41351083e-02  -1.27176985e-02  -1.13002888e-02  -9.88287898e-03
  -8.46546919e-03  -7.04805941e-03  -5.63064962e-03  -4.21323984e-03
  -2.79583005e-03  -1.37842027e-03   3.89895176e-05]
[1487  798  989  727 1027 1057 1026  945 1034 1035] [ -1.41351083e-02  -1.27176985e-02  -1.13002888e-02  -9.88287898e-03
  -8.46546919e-03  -7.04805941e-03  -5.63064962e-03  -4.21323984e-03
  -2.79583005e-03  -1.37842027e-03   3.89895176e-05]
-0.631907
0.579201
fine tuning ...
Epoch 0
Fine tuning took 0.063598 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062878 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063994 minutes
{0: [0.043103448275862072, 0.043103448275862072, 0.046798029556650245, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9285714285714286, 0.89901477832512311, 0.92733990147783252, 0.92980295566502458], 5: [0.02832512315270936, 0.057881773399014777, 0.025862068965517241, 0.030788177339901478], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378598 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.2824
Epoch 1, cost is  4.20664
Epoch 2, cost is  4.15849
Epoch 3, cost is  4.1147
Epoch 4, cost is  4.07799
Training took 0.151855 minutes
Weight histogram
[1622  330 1240  747  272 1750 1045 1013   56   25] [ -1.03901634e-02  -9.34724811e-03  -8.30433282e-03  -7.26141752e-03
  -6.21850223e-03  -5.17558694e-03  -4.13267165e-03  -3.08975636e-03
  -2.04684107e-03  -1.00392577e-03   3.89895176e-05]
[ 873  520  608  717  718  784  851  919 1024 1086] [ -1.03901634e-02  -9.34724811e-03  -8.30433282e-03  -7.26141752e-03
  -6.21850223e-03  -5.17558694e-03  -4.13267165e-03  -3.08975636e-03
  -2.04684107e-03  -1.00392577e-03   3.89895176e-05]
-0.733431
0.833501
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.5257
Epoch 1, cost is  3.44015
Epoch 2, cost is  3.37161
Epoch 3, cost is  3.31115
Epoch 4, cost is  3.25803
Training took 0.154141 minutes
Weight histogram
[1886 2118 1452  588  650 1262 1851  222   63   33] [ -7.95371737e-03  -7.15339781e-03  -6.35307824e-03  -5.55275867e-03
  -4.75243911e-03  -3.95211954e-03  -3.15179997e-03  -2.35148041e-03
  -1.55116084e-03  -7.50841276e-04   4.94782907e-05]
[1260  632  840 1321 1336  911  892  917  995 1021] [ -7.95371737e-03  -7.15339781e-03  -6.35307824e-03  -5.55275867e-03
  -4.75243911e-03  -3.95211954e-03  -3.15179997e-03  -2.35148041e-03
  -1.55116084e-03  -7.50841276e-04   4.94782907e-05]
-0.47545
0.464779
fine tuning ...
Epoch 0
Fine tuning took 0.069004 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069180 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067675 minutes
{0: [0.043103448275862072, 0.046798029556650245, 0.027093596059113302, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.92364532019704437, 0.90270935960591137, 0.93103448275862066, 0.92364532019704437], 5: [0.033251231527093597, 0.050492610837438424, 0.041871921182266007, 0.02832512315270936], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378418 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.75814
Epoch 1, cost is  3.67708
Epoch 2, cost is  3.62402
Epoch 3, cost is  3.57069
Epoch 4, cost is  3.53407
Training took 0.207489 minutes
Weight histogram
[1816  188 1804  197  467 1551 1146  892   26   13] [ -5.98888285e-03  -5.38586571e-03  -4.78284856e-03  -4.17983141e-03
  -3.57681427e-03  -2.97379712e-03  -2.37077998e-03  -1.76776283e-03
  -1.16474568e-03  -5.61728535e-04   4.12886111e-05]
[ 790  548  573  646  690  757  960  986 1060 1090] [ -5.98888285e-03  -5.38586571e-03  -4.78284856e-03  -4.17983141e-03
  -3.57681427e-03  -2.97379712e-03  -2.37077998e-03  -1.76776283e-03
  -1.16474568e-03  -5.61728535e-04   4.12886111e-05]
-0.569911
0.582512
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.04969
Epoch 1, cost is  4.99354
Epoch 2, cost is  4.952
Epoch 3, cost is  4.92131
Epoch 4, cost is  4.89059
Training took 0.108049 minutes
Weight histogram
[1774 1591  634  978  786  342  530  573 1916 1001] [ -1.81040969e-02  -1.62895583e-02  -1.44750198e-02  -1.26604812e-02
  -1.08459427e-02  -9.03140413e-03  -7.21686558e-03  -5.40232703e-03
  -3.58778849e-03  -1.77324994e-03   4.12886111e-05]
[2221  913  491  672  817  904 1017 1027 1011 1052] [ -1.81040969e-02  -1.62895583e-02  -1.44750198e-02  -1.26604812e-02
  -1.08459427e-02  -9.03140413e-03  -7.21686558e-03  -5.40232703e-03
  -3.58778849e-03  -1.77324994e-03   4.12886111e-05]
-0.809728
0.759098
fine tuning ...
Epoch 0
Fine tuning took 0.069059 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069571 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069807 minutes
{0: [0.045566502463054187, 0.02832512315270936, 0.023399014778325122, 0.034482758620689655], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93103448275862066, 0.92733990147783252, 0.93842364532019706, 0.93103448275862066], 5: [0.023399014778325122, 0.044334975369458129, 0.038177339901477834, 0.034482758620689655], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378799 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.75814
Epoch 1, cost is  3.67708
Epoch 2, cost is  3.62402
Epoch 3, cost is  3.57069
Epoch 4, cost is  3.53407
Training took 0.207843 minutes
Weight histogram
[1816  188 1804  197  467 1551 1146  892   26   13] [ -5.98888285e-03  -5.38586571e-03  -4.78284856e-03  -4.17983141e-03
  -3.57681427e-03  -2.97379712e-03  -2.37077998e-03  -1.76776283e-03
  -1.16474568e-03  -5.61728535e-04   4.12886111e-05]
[ 790  548  573  646  690  757  960  986 1060 1090] [ -5.98888285e-03  -5.38586571e-03  -4.78284856e-03  -4.17983141e-03
  -3.57681427e-03  -2.97379712e-03  -2.37077998e-03  -1.76776283e-03
  -1.16474568e-03  -5.61728535e-04   4.12886111e-05]
-0.569911
0.582512
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.3088
Epoch 1, cost is  4.24443
Epoch 2, cost is  4.19338
Epoch 3, cost is  4.14813
Epoch 4, cost is  4.10776
Training took 0.153326 minutes
Weight histogram
[1663 1366  927 1675  427  664  483  959 1907   54] [ -1.04168048e-02  -9.37099547e-03  -8.32518613e-03  -7.27937679e-03
  -6.23356744e-03  -5.18775810e-03  -4.14194876e-03  -3.09613942e-03
  -2.05033007e-03  -1.00452073e-03   4.12886111e-05]
[1611 1198  909  876  824  841  852  943 1019 1052] [ -1.04168048e-02  -9.37099547e-03  -8.32518613e-03  -7.27937679e-03
  -6.23356744e-03  -5.18775810e-03  -4.14194876e-03  -3.09613942e-03
  -2.05033007e-03  -1.00452073e-03   4.12886111e-05]
-0.697582
0.582274
fine tuning ...
Epoch 0
Fine tuning took 0.074175 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074150 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073821 minutes
{0: [0.04064039408866995, 0.054187192118226604, 0.034482758620689655, 0.051724137931034482], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91995073891625612, 0.90640394088669951, 0.93719211822660098, 0.91133004926108374], 5: [0.039408866995073892, 0.039408866995073892, 0.02832512315270936, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378771 minutes
Weight histogram
[   6   15  145  567 1119 2339 2027 1238  599   45] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 137  192  319  470  670  478  481  791 2295 2267] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.838267
0.871143
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.75814
Epoch 1, cost is  3.67708
Epoch 2, cost is  3.62402
Epoch 3, cost is  3.57069
Epoch 4, cost is  3.53407
Training took 0.207781 minutes
Weight histogram
[1816  188 1804  197  467 1551 1146  892   26   13] [ -5.98888285e-03  -5.38586571e-03  -4.78284856e-03  -4.17983141e-03
  -3.57681427e-03  -2.97379712e-03  -2.37077998e-03  -1.76776283e-03
  -1.16474568e-03  -5.61728535e-04   4.12886111e-05]
[ 790  548  573  646  690  757  960  986 1060 1090] [ -5.98888285e-03  -5.38586571e-03  -4.78284856e-03  -4.17983141e-03
  -3.57681427e-03  -2.97379712e-03  -2.37077998e-03  -1.76776283e-03
  -1.16474568e-03  -5.61728535e-04   4.12886111e-05]
-0.569911
0.582512
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.83932
Epoch 1, cost is  3.77748
Epoch 2, cost is  3.7275
Epoch 3, cost is  3.68953
Epoch 4, cost is  3.65137
Training took 0.207858 minutes
Weight histogram
[1693 1808  543 1873  540  689 2122  787   50   20] [ -5.86459460e-03  -5.27400628e-03  -4.68341796e-03  -4.09282963e-03
  -3.50224131e-03  -2.91165299e-03  -2.32106467e-03  -1.73047635e-03
  -1.13988803e-03  -5.49299710e-04   4.12886111e-05]
[1295  929 1170 1230  769  876  895  931  978 1052] [ -5.86459460e-03  -5.27400628e-03  -4.68341796e-03  -4.09282963e-03
  -3.50224131e-03  -2.91165299e-03  -2.32106467e-03  -1.73047635e-03
  -1.13988803e-03  -5.49299710e-04   4.12886111e-05]
-0.443291
0.46206
fine tuning ...
Epoch 0
Fine tuning took 0.078529 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078269 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079536 minutes
{0: [0.051724137931034482, 0.065270935960591137, 0.080049261083743842, 0.05295566502463054], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89901477832512311, 0.88423645320197042, 0.86822660098522164, 0.91379310344827591], 5: [0.049261083743842367, 0.050492610837438424, 0.051724137931034482, 0.033251231527093597], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379254 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.73352
Epoch 1, cost is  4.68521
Epoch 2, cost is  4.65543
Epoch 3, cost is  4.63187
Epoch 4, cost is  4.61035
Training took 0.109355 minutes
Weight histogram
[1650  729 1178  438 1735  680 1541 1799  254  121] [ -2.44616866e-02  -2.20106909e-02  -1.95596952e-02  -1.71086994e-02
  -1.46577037e-02  -1.22067080e-02  -9.75571225e-03  -7.30471652e-03
  -4.85372079e-03  -2.40272506e-03   4.82706637e-05]
[1056  608  722  842  918  956 1049 1089 1319 1566] [ -2.44616866e-02  -2.20106909e-02  -1.95596952e-02  -1.71086994e-02
  -1.46577037e-02  -1.22067080e-02  -9.75571225e-03  -7.30471652e-03
  -4.85372079e-03  -2.40272506e-03   4.82706637e-05]
-1.44442
1.53407
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.11103
Epoch 1, cost is  4.05713
Epoch 2, cost is  4.02016
Epoch 3, cost is  3.99459
Epoch 4, cost is  3.96653
Training took 0.072310 minutes
Weight histogram
[1965 2621 1151  658 1372  846 1006  276 1886  369] [-0.04275858 -0.03846543 -0.03417227 -0.02987912 -0.02558597 -0.02129281
 -0.01699966 -0.01270651 -0.00841336 -0.0041202   0.00017295]
[2128 1048  648  543  605  726 1200 1417 1437 2398] [-0.04275858 -0.03846543 -0.03417227 -0.02987912 -0.02558597 -0.02129281
 -0.01699966 -0.01270651 -0.00841336 -0.0041202   0.00017295]
-0.821323
0.901683
fine tuning ...
Epoch 0
Fine tuning took 0.050833 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052370 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052326 minutes
{0: [0.012315270935960592, 0.01600985221674877, 0.020935960591133004, 0.009852216748768473], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97167487684729059, 0.97536945812807885, 0.96674876847290636, 0.97536945812807885], 5: [0.01600985221674877, 0.0086206896551724137, 0.012315270935960592, 0.014778325123152709], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379331 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.73352
Epoch 1, cost is  4.68521
Epoch 2, cost is  4.65543
Epoch 3, cost is  4.63187
Epoch 4, cost is  4.61035
Training took 0.106655 minutes
Weight histogram
[1650  729 1178  438 1735  680 1541 1799  254  121] [ -2.44616866e-02  -2.20106909e-02  -1.95596952e-02  -1.71086994e-02
  -1.46577037e-02  -1.22067080e-02  -9.75571225e-03  -7.30471652e-03
  -4.85372079e-03  -2.40272506e-03   4.82706637e-05]
[1056  608  722  842  918  956 1049 1089 1319 1566] [ -2.44616866e-02  -2.20106909e-02  -1.95596952e-02  -1.71086994e-02
  -1.46577037e-02  -1.22067080e-02  -9.75571225e-03  -7.30471652e-03
  -4.85372079e-03  -2.40272506e-03   4.82706637e-05]
-1.44442
1.53407
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.76164
Epoch 1, cost is  3.68659
Epoch 2, cost is  3.63883
Epoch 3, cost is  3.57796
Epoch 4, cost is  3.52468
Training took 0.085814 minutes
Weight histogram
[2011 3313 1266 1234  405  642 1444 1427  287  121] [-0.02217423 -0.0199292  -0.01768418 -0.01543916 -0.01319414 -0.01094912
 -0.0087041  -0.00645908 -0.00421406 -0.00196903  0.00027599]
[1606  753  738  827  725  989 1363 1331 1668 2150] [-0.02217423 -0.0199292  -0.01768418 -0.01543916 -0.01319414 -0.01094912
 -0.0087041  -0.00645908 -0.00421406 -0.00196903  0.00027599]
-0.60226
0.550103
fine tuning ...
Epoch 0
Fine tuning took 0.052644 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054643 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053693 minutes
{0: [0.011083743842364532, 0.019704433497536946, 0.027093596059113302, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.97044334975369462, 0.94950738916256161, 0.95197044334975367], 5: [0.024630541871921183, 0.009852216748768473, 0.023399014778325122, 0.024630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379809 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.73352
Epoch 1, cost is  4.68521
Epoch 2, cost is  4.65543
Epoch 3, cost is  4.63187
Epoch 4, cost is  4.61035
Training took 0.106914 minutes
Weight histogram
[1650  729 1178  438 1735  680 1541 1799  254  121] [ -2.44616866e-02  -2.20106909e-02  -1.95596952e-02  -1.71086994e-02
  -1.46577037e-02  -1.22067080e-02  -9.75571225e-03  -7.30471652e-03
  -4.85372079e-03  -2.40272506e-03   4.82706637e-05]
[1056  608  722  842  918  956 1049 1089 1319 1566] [ -2.44616866e-02  -2.20106909e-02  -1.95596952e-02  -1.71086994e-02
  -1.46577037e-02  -1.22067080e-02  -9.75571225e-03  -7.30471652e-03
  -4.85372079e-03  -2.40272506e-03   4.82706637e-05]
-1.44442
1.53407
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.26864
Epoch 1, cost is  3.18851
Epoch 2, cost is  3.12301
Epoch 3, cost is  3.0617
Epoch 4, cost is  3.00403
Training took 0.108822 minutes
Weight histogram
[2590 3656 1786  479 1817  567  490  568  147   50] [ -1.18729249e-02  -1.06806080e-02  -9.48829109e-03  -8.29597420e-03
  -7.10365731e-03  -5.91134042e-03  -4.71902354e-03  -3.52670665e-03
  -2.33438976e-03  -1.14207287e-03   5.02440162e-05]
[1345  705  674  839 1255 1547 1488 1436 1438 1423] [ -1.18729249e-02  -1.06806080e-02  -9.48829109e-03  -8.29597420e-03
  -7.10365731e-03  -5.91134042e-03  -4.71902354e-03  -3.52670665e-03
  -2.33438976e-03  -1.14207287e-03   5.02440162e-05]
-0.485711
0.467836
fine tuning ...
Epoch 0
Fine tuning took 0.057393 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057111 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057410 minutes
{0: [0.035714285714285712, 0.024630541871921183, 0.023399014778325122, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94581280788177335, 0.95443349753694584, 0.94458128078817738, 0.93349753694581283], 5: [0.018472906403940888, 0.020935960591133004, 0.032019704433497539, 0.027093596059113302], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379794 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.02879
Epoch 1, cost is  3.9654
Epoch 2, cost is  3.93045
Epoch 3, cost is  3.89519
Epoch 4, cost is  3.86542
Training took 0.152353 minutes
Weight histogram
[1926 1263  732 1153  904 1144 1270 1622   80   31] [ -1.22965258e-02  -1.10629742e-02  -9.82942271e-03  -8.59587118e-03
  -7.36231965e-03  -6.12876812e-03  -4.89521659e-03  -3.66166507e-03
  -2.42811354e-03  -1.19456201e-03   3.89895176e-05]
[ 946  626  760  824  862  982 1052 1194 1354 1525] [ -1.22965258e-02  -1.10629742e-02  -9.82942271e-03  -8.59587118e-03
  -7.36231965e-03  -6.12876812e-03  -4.89521659e-03  -3.66166507e-03
  -2.42811354e-03  -1.19456201e-03   3.89895176e-05]
-0.856196
1.00353
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.5043
Epoch 1, cost is  4.45337
Epoch 2, cost is  4.41323
Epoch 3, cost is  4.38006
Epoch 4, cost is  4.34742
Training took 0.086911 minutes
Weight histogram
[2016 1893 1587  705 1134  607  442  930 1938  898] [ -3.04143876e-02  -2.73690499e-02  -2.43237122e-02  -2.12783745e-02
  -1.82330367e-02  -1.51876990e-02  -1.21423613e-02  -9.09702361e-03
  -6.05168590e-03  -3.00634819e-03   3.89895176e-05]
[2075 1085  432  630  949 1135 1414 1358 1437 1635] [ -3.04143876e-02  -2.73690499e-02  -2.43237122e-02  -2.12783745e-02
  -1.82330367e-02  -1.51876990e-02  -1.21423613e-02  -9.09702361e-03
  -6.05168590e-03  -3.00634819e-03   3.89895176e-05]
-0.829082
0.96788
fine tuning ...
Epoch 0
Fine tuning took 0.059010 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060562 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060929 minutes
{0: [0.020935960591133004, 0.032019704433497539, 0.039408866995073892, 0.036945812807881777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.95935960591133007, 0.91748768472906406, 0.9211822660098522, 0.92241379310344829], 5: [0.019704433497536946, 0.050492610837438424, 0.039408866995073892, 0.04064039408866995], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379512 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.02879
Epoch 1, cost is  3.9654
Epoch 2, cost is  3.93045
Epoch 3, cost is  3.89519
Epoch 4, cost is  3.86542
Training took 0.152979 minutes
Weight histogram
[1926 1263  732 1153  904 1144 1270 1622   80   31] [ -1.22965258e-02  -1.10629742e-02  -9.82942271e-03  -8.59587118e-03
  -7.36231965e-03  -6.12876812e-03  -4.89521659e-03  -3.66166507e-03
  -2.42811354e-03  -1.19456201e-03   3.89895176e-05]
[ 946  626  760  824  862  982 1052 1194 1354 1525] [ -1.22965258e-02  -1.10629742e-02  -9.82942271e-03  -8.59587118e-03
  -7.36231965e-03  -6.12876812e-03  -4.89521659e-03  -3.66166507e-03
  -2.42811354e-03  -1.19456201e-03   3.89895176e-05]
-0.856196
1.00353
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.76976
Epoch 1, cost is  3.70802
Epoch 2, cost is  3.66332
Epoch 3, cost is  3.61986
Epoch 4, cost is  3.58325
Training took 0.109859 minutes
Weight histogram
[3495 2249  861  995  476  656  482 2121  740   75] [ -1.49544021e-02  -1.34550629e-02  -1.19557238e-02  -1.04563846e-02
  -8.95704546e-03  -7.45770630e-03  -5.95836713e-03  -4.45902797e-03
  -2.95968881e-03  -1.46034965e-03   3.89895176e-05]
[1608  966 1066 1012 1217 1207 1104 1196 1329 1445] [ -1.49544021e-02  -1.34550629e-02  -1.19557238e-02  -1.04563846e-02
  -8.95704546e-03  -7.45770630e-03  -5.95836713e-03  -4.45902797e-03
  -2.95968881e-03  -1.46034965e-03   3.89895176e-05]
-0.724264
0.705801
fine tuning ...
Epoch 0
Fine tuning took 0.064129 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063891 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063969 minutes
{0: [0.04064039408866995, 0.054187192118226604, 0.057881773399014777, 0.056650246305418719], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90517241379310343, 0.89408866995073888, 0.89655172413793105, 0.91133004926108374], 5: [0.054187192118226604, 0.051724137931034482, 0.045566502463054187, 0.032019704433497539], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379865 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.02879
Epoch 1, cost is  3.9654
Epoch 2, cost is  3.93045
Epoch 3, cost is  3.89519
Epoch 4, cost is  3.86542
Training took 0.152099 minutes
Weight histogram
[1926 1263  732 1153  904 1144 1270 1622   80   31] [ -1.22965258e-02  -1.10629742e-02  -9.82942271e-03  -8.59587118e-03
  -7.36231965e-03  -6.12876812e-03  -4.89521659e-03  -3.66166507e-03
  -2.42811354e-03  -1.19456201e-03   3.89895176e-05]
[ 946  626  760  824  862  982 1052 1194 1354 1525] [ -1.22965258e-02  -1.10629742e-02  -9.82942271e-03  -8.59587118e-03
  -7.36231965e-03  -6.12876812e-03  -4.89521659e-03  -3.66166507e-03
  -2.42811354e-03  -1.19456201e-03   3.89895176e-05]
-0.856196
1.00353
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.28255
Epoch 1, cost is  3.21724
Epoch 2, cost is  3.15919
Epoch 3, cost is  3.11806
Epoch 4, cost is  3.07107
Training took 0.153674 minutes
Weight histogram
[2030 2011 2136 1640  442 1187 1946  631   89   38] [ -8.80182255e-03  -7.91669247e-03  -7.03156238e-03  -6.14643230e-03
  -5.26130221e-03  -4.37617213e-03  -3.49104205e-03  -2.60591196e-03
  -1.72078188e-03  -8.35651793e-04   4.94782907e-05]
[1363  784 1165 1702 1163 1059 1098 1171 1291 1354] [ -8.80182255e-03  -7.91669247e-03  -7.03156238e-03  -6.14643230e-03
  -5.26130221e-03  -4.37617213e-03  -3.49104205e-03  -2.60591196e-03
  -1.72078188e-03  -8.35651793e-04   4.94782907e-05]
-0.521959
0.523157
fine tuning ...
Epoch 0
Fine tuning took 0.069348 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068809 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067613 minutes
{0: [0.054187192118226604, 0.054187192118226604, 0.067733990147783252, 0.075123152709359611], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90024630541871919, 0.89778325123152714, 0.89039408866995073, 0.88793103448275867], 5: [0.045566502463054187, 0.048029556650246302, 0.041871921182266007, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379826 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.51155
Epoch 1, cost is  3.44398
Epoch 2, cost is  3.3986
Epoch 3, cost is  3.36044
Epoch 4, cost is  3.3249
Training took 0.207598 minutes
Weight histogram
[1860  231 1849 1086  992  535 1518 1990   47   17] [ -7.46764289e-03  -6.71674974e-03  -5.96585659e-03  -5.21496344e-03
  -4.46407029e-03  -3.71317714e-03  -2.96228399e-03  -2.21139084e-03
  -1.46049769e-03  -7.09604539e-04   4.12886111e-05]
[ 876  642  696  785  853 1085 1144 1242 1357 1445] [ -7.46764289e-03  -6.71674974e-03  -5.96585659e-03  -5.21496344e-03
  -4.46407029e-03  -3.71317714e-03  -2.96228399e-03  -2.21139084e-03
  -1.46049769e-03  -7.09604539e-04   4.12886111e-05]
-0.64774
0.682724
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.8464
Epoch 1, cost is  4.79439
Epoch 2, cost is  4.76589
Epoch 3, cost is  4.73888
Epoch 4, cost is  4.71614
Training took 0.109968 minutes
Weight histogram
[2142 1918 1693  523 1133  615  439  697 1116 1874] [ -1.99664328e-02  -1.79656607e-02  -1.59648885e-02  -1.39641164e-02
  -1.19633443e-02  -9.96257211e-03  -7.96179996e-03  -5.96102782e-03
  -3.96025568e-03  -1.95948353e-03   4.12886111e-05]
[2465  795  616  881  987 1143 1170 1155 1329 1609] [ -1.99664328e-02  -1.79656607e-02  -1.59648885e-02  -1.39641164e-02
  -1.19633443e-02  -9.96257211e-03  -7.96179996e-03  -5.96102782e-03
  -3.96025568e-03  -1.95948353e-03   4.12886111e-05]
-0.917277
0.901464
fine tuning ...
Epoch 0
Fine tuning took 0.067763 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069068 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068461 minutes
{0: [0.036945812807881777, 0.030788177339901478, 0.034482758620689655, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.92980295566502458, 0.92980295566502458, 0.92610837438423643, 0.92980295566502458], 5: [0.033251231527093597, 0.039408866995073892, 0.039408866995073892, 0.030788177339901478], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378662 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.51155
Epoch 1, cost is  3.44398
Epoch 2, cost is  3.3986
Epoch 3, cost is  3.36044
Epoch 4, cost is  3.3249
Training took 0.208193 minutes
Weight histogram
[1860  231 1849 1086  992  535 1518 1990   47   17] [ -7.46764289e-03  -6.71674974e-03  -5.96585659e-03  -5.21496344e-03
  -4.46407029e-03  -3.71317714e-03  -2.96228399e-03  -2.21139084e-03
  -1.46049769e-03  -7.09604539e-04   4.12886111e-05]
[ 876  642  696  785  853 1085 1144 1242 1357 1445] [ -7.46764289e-03  -6.71674974e-03  -5.96585659e-03  -5.21496344e-03
  -4.46407029e-03  -3.71317714e-03  -2.96228399e-03  -2.21139084e-03
  -1.46049769e-03  -7.09604539e-04   4.12886111e-05]
-0.64774
0.682724
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.18809
Epoch 1, cost is  4.13583
Epoch 2, cost is  4.10062
Epoch 3, cost is  4.06806
Epoch 4, cost is  4.03945
Training took 0.152604 minutes
Weight histogram
[2501 1455 1611 1171 1253  572  556  912 2057   62] [ -1.13165425e-02  -1.01807594e-02  -9.04497629e-03  -7.90919318e-03
  -6.77341006e-03  -5.63762695e-03  -4.50184384e-03  -3.36606073e-03
  -2.23027761e-03  -1.09449450e-03   4.12886111e-05]
[1786 1445  894 1001  967  986 1100 1182 1335 1454] [ -1.13165425e-02  -1.01807594e-02  -9.04497629e-03  -7.90919318e-03
  -6.77341006e-03  -5.63762695e-03  -4.50184384e-03  -3.36606073e-03
  -2.23027761e-03  -1.09449450e-03   4.12886111e-05]
-0.717547
0.690897
fine tuning ...
Epoch 0
Fine tuning took 0.073088 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073384 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073916 minutes
{0: [0.057881773399014777, 0.051724137931034482, 0.064039408866995079, 0.070197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90394088669950734, 0.88177339901477836, 0.88177339901477836, 0.89655172413793105], 5: [0.038177339901477834, 0.066502463054187194, 0.054187192118226604, 0.033251231527093597], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379338 minutes
Weight histogram
[   6   15  145  567 1130 2449 2562 2246  944   61] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
[ 147  217  340  540  731  419  643 1721 2426 2941] [ -4.69115912e-04  -3.56532383e-04  -2.43948854e-04  -1.31365325e-04
  -1.87817961e-05   9.38017329e-05   2.06385262e-04   3.18968791e-04
   4.31552320e-04   5.44135849e-04   6.56719378e-04]
-0.991578
1.01702
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.51155
Epoch 1, cost is  3.44398
Epoch 2, cost is  3.3986
Epoch 3, cost is  3.36044
Epoch 4, cost is  3.3249
Training took 0.208480 minutes
Weight histogram
[1860  231 1849 1086  992  535 1518 1990   47   17] [ -7.46764289e-03  -6.71674974e-03  -5.96585659e-03  -5.21496344e-03
  -4.46407029e-03  -3.71317714e-03  -2.96228399e-03  -2.21139084e-03
  -1.46049769e-03  -7.09604539e-04   4.12886111e-05]
[ 876  642  696  785  853 1085 1144 1242 1357 1445] [ -7.46764289e-03  -6.71674974e-03  -5.96585659e-03  -5.21496344e-03
  -4.46407029e-03  -3.71317714e-03  -2.96228399e-03  -2.21139084e-03
  -1.46049769e-03  -7.09604539e-04   4.12886111e-05]
-0.64774
0.682724
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.69164
Epoch 1, cost is  3.63654
Epoch 2, cost is  3.59901
Epoch 3, cost is  3.56598
Epoch 4, cost is  3.53611
Training took 0.207051 minutes
Weight histogram
[1975 1793 2041  891 1357  816 1012 2179   62   24] [ -6.55749626e-03  -5.89761778e-03  -5.23773929e-03  -4.57786080e-03
  -3.91798231e-03  -3.25810383e-03  -2.59822534e-03  -1.93834685e-03
  -1.27846836e-03  -6.18589876e-04   4.12886111e-05]
[1434 1182 1479 1061  983 1067 1106 1161 1296 1381] [ -6.55749626e-03  -5.89761778e-03  -5.23773929e-03  -4.57786080e-03
  -3.91798231e-03  -3.25810383e-03  -2.59822534e-03  -1.93834685e-03
  -1.27846836e-03  -6.18589876e-04   4.12886111e-05]
-0.528178
0.537347
fine tuning ...
Epoch 0
Fine tuning took 0.079399 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079897 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078501 minutes
{0: [0.062807881773399021, 0.05295566502463054, 0.068965517241379309, 0.067733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8854679802955665, 0.89901477832512311, 0.87068965517241381, 0.87931034482758619], 5: [0.051724137931034482, 0.048029556650246302, 0.060344827586206899, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379479 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.60468
Epoch 1, cost is  4.56684
Epoch 2, cost is  4.54073
Epoch 3, cost is  4.52136
Epoch 4, cost is  4.50284
Training took 0.107009 minutes
Weight histogram
[1808 1873  995 1002 1348 1003 1656 1998  314  153] [ -2.75007691e-02  -2.47458651e-02  -2.19909612e-02  -1.92360572e-02
  -1.64811532e-02  -1.37262492e-02  -1.09713453e-02  -8.21644127e-03
  -5.46153729e-03  -2.70663332e-03   4.82706637e-05]
[1139  688  875  983 1064 1153 1223 1475 1762 1788] [ -2.75007691e-02  -2.47458651e-02  -2.19909612e-02  -1.92360572e-02
  -1.64811532e-02  -1.37262492e-02  -1.09713453e-02  -8.21644127e-03
  -5.46153729e-03  -2.70663332e-03   4.82706637e-05]
-1.64529
1.74886
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.16606
Epoch 1, cost is  4.12372
Epoch 2, cost is  4.09282
Epoch 3, cost is  4.06861
Epoch 4, cost is  4.05411
Training took 0.070552 minutes
Weight histogram
[3990 2621 1151  658 1372  846 1006  276 1886  369] [-0.04275858 -0.03846543 -0.03417227 -0.02987912 -0.02558597 -0.02129281
 -0.01699966 -0.01270651 -0.00841336 -0.0041202   0.00017295]
[2224 1164  573  624  697 1005 1537 1543 2399 2409] [-0.04275858 -0.03846543 -0.03417227 -0.02987912 -0.02558597 -0.02129281
 -0.01699966 -0.01270651 -0.00841336 -0.0041202   0.00017295]
-0.883855
1.07228
fine tuning ...
Epoch 0
Fine tuning took 0.052483 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051954 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053358 minutes
{0: [0.012315270935960592, 0.01600985221674877, 0.012315270935960592, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97536945812807885, 0.97290640394088668, 0.98029556650246308, 0.95566502463054193], 5: [0.012315270935960592, 0.011083743842364532, 0.0073891625615763543, 0.020935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380299 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.60468
Epoch 1, cost is  4.56684
Epoch 2, cost is  4.54073
Epoch 3, cost is  4.52136
Epoch 4, cost is  4.50284
Training took 0.106825 minutes
Weight histogram
[1808 1873  995 1002 1348 1003 1656 1998  314  153] [ -2.75007691e-02  -2.47458651e-02  -2.19909612e-02  -1.92360572e-02
  -1.64811532e-02  -1.37262492e-02  -1.09713453e-02  -8.21644127e-03
  -5.46153729e-03  -2.70663332e-03   4.82706637e-05]
[1139  688  875  983 1064 1153 1223 1475 1762 1788] [ -2.75007691e-02  -2.47458651e-02  -2.19909612e-02  -1.92360572e-02
  -1.64811532e-02  -1.37262492e-02  -1.09713453e-02  -8.21644127e-03
  -5.46153729e-03  -2.70663332e-03   4.82706637e-05]
-1.64529
1.74886
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.62463
Epoch 1, cost is  3.54423
Epoch 2, cost is  3.4987
Epoch 3, cost is  3.44456
Epoch 4, cost is  3.399
Training took 0.085973 minutes
Weight histogram
[4036 3303 1276 1234  405  642 1442 1429  287  121] [-0.02217873 -0.01993326 -0.01768779 -0.01544232 -0.01319685 -0.01095137
 -0.0087059  -0.00646043 -0.00421496 -0.00196949  0.00027599]
[1705  839  856  907  922 1488 1442 1900 2377 1739] [-0.02217873 -0.01993326 -0.01768779 -0.01544232 -0.01319685 -0.01095137
 -0.0087059  -0.00646043 -0.00421496 -0.00196949  0.00027599]
-0.669564
0.621422
fine tuning ...
Epoch 0
Fine tuning took 0.054054 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053224 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054044 minutes
{0: [0.017241379310344827, 0.025862068965517241, 0.018472906403940888, 0.024630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.95320197044334976, 0.96674876847290636, 0.93596059113300489], 5: [0.018472906403940888, 0.020935960591133004, 0.014778325123152709, 0.039408866995073892], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378761 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.60468
Epoch 1, cost is  4.56684
Epoch 2, cost is  4.54073
Epoch 3, cost is  4.52136
Epoch 4, cost is  4.50284
Training took 0.109483 minutes
Weight histogram
[1808 1873  995 1002 1348 1003 1656 1998  314  153] [ -2.75007691e-02  -2.47458651e-02  -2.19909612e-02  -1.92360572e-02
  -1.64811532e-02  -1.37262492e-02  -1.09713453e-02  -8.21644127e-03
  -5.46153729e-03  -2.70663332e-03   4.82706637e-05]
[1139  688  875  983 1064 1153 1223 1475 1762 1788] [ -2.75007691e-02  -2.47458651e-02  -2.19909612e-02  -1.92360572e-02
  -1.64811532e-02  -1.37262492e-02  -1.09713453e-02  -8.21644127e-03
  -5.46153729e-03  -2.70663332e-03   4.82706637e-05]
-1.64529
1.74886
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.10321
Epoch 1, cost is  3.04137
Epoch 2, cost is  2.9911
Epoch 3, cost is  2.9499
Epoch 4, cost is  2.9076
Training took 0.108943 minutes
Weight histogram
[3981 3560 2463  186 1914  731  538  590  157   55] [ -1.23763476e-02  -1.11336884e-02  -9.89102924e-03  -8.64837008e-03
  -7.40571093e-03  -6.16305177e-03  -4.92039261e-03  -3.67773346e-03
  -2.43507430e-03  -1.19241514e-03   5.02440162e-05]
[1441  788  781 1177 1572 1793 1635 1630 1626 1732] [ -1.23763476e-02  -1.11336884e-02  -9.89102924e-03  -8.64837008e-03
  -7.40571093e-03  -6.16305177e-03  -4.92039261e-03  -3.67773346e-03
  -2.43507430e-03  -1.19241514e-03   5.02440162e-05]
-0.514758
0.502037
fine tuning ...
Epoch 0
Fine tuning took 0.055844 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056872 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055762 minutes
{0: [0.009852216748768473, 0.027093596059113302, 0.020935960591133004, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96798029556650245, 0.93842364532019706, 0.95812807881773399, 0.9211822660098522], 5: [0.022167487684729065, 0.034482758620689655, 0.020935960591133004, 0.055418719211822662], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379967 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.86931
Epoch 1, cost is  3.81576
Epoch 2, cost is  3.77898
Epoch 3, cost is  3.75419
Epoch 4, cost is  3.7288
Training took 0.152231 minutes
Weight histogram
[1971 1980 1536  516 1719  350 1941 1989  111   37] [ -1.38416300e-02  -1.24535680e-02  -1.10655061e-02  -9.67744412e-03
  -8.28938217e-03  -6.90132022e-03  -5.51325827e-03  -4.12519633e-03
  -2.73713438e-03  -1.34907243e-03   3.89895176e-05]
[1019  740  891  943 1058 1166 1318 1526 1736 1753] [ -1.38416300e-02  -1.24535680e-02  -1.10655061e-02  -9.67744412e-03
  -8.28938217e-03  -6.90132022e-03  -5.51325827e-03  -4.12519633e-03
  -2.73713438e-03  -1.34907243e-03   3.89895176e-05]
-0.99681
1.15654
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.43311
Epoch 1, cost is  4.38675
Epoch 2, cost is  4.3563
Epoch 3, cost is  4.33505
Epoch 4, cost is  4.30762
Training took 0.084281 minutes
Weight histogram
[4041 1893 1587  705 1134  607  442  930 1938  898] [ -3.04143876e-02  -2.73690499e-02  -2.43237122e-02  -2.12783745e-02
  -1.82330367e-02  -1.51876990e-02  -1.21423613e-02  -9.09702361e-03
  -6.05168590e-03  -3.00634819e-03   3.89895176e-05]
[2259 1004  537  899 1155 1564 1553 1625 1827 1752] [ -3.04143876e-02  -2.73690499e-02  -2.43237122e-02  -2.12783745e-02
  -1.82330367e-02  -1.51876990e-02  -1.21423613e-02  -9.09702361e-03
  -6.05168590e-03  -3.00634819e-03   3.89895176e-05]
-0.929431
1.11666
fine tuning ...
Epoch 0
Fine tuning took 0.059252 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060789 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059664 minutes
{0: [0.035714285714285712, 0.04064039408866995, 0.048029556650246302, 0.056650246305418719], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94211822660098521, 0.91871921182266014, 0.91502463054187189, 0.88916256157635465], 5: [0.022167487684729065, 0.04064039408866995, 0.036945812807881777, 0.054187192118226604], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382496 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.86931
Epoch 1, cost is  3.81576
Epoch 2, cost is  3.77898
Epoch 3, cost is  3.75419
Epoch 4, cost is  3.7288
Training took 0.153152 minutes
Weight histogram
[1971 1980 1536  516 1719  350 1941 1989  111   37] [ -1.38416300e-02  -1.24535680e-02  -1.10655061e-02  -9.67744412e-03
  -8.28938217e-03  -6.90132022e-03  -5.51325827e-03  -4.12519633e-03
  -2.73713438e-03  -1.34907243e-03   3.89895176e-05]
[1019  740  891  943 1058 1166 1318 1526 1736 1753] [ -1.38416300e-02  -1.24535680e-02  -1.10655061e-02  -9.67744412e-03
  -8.28938217e-03  -6.90132022e-03  -5.51325827e-03  -4.12519633e-03
  -2.73713438e-03  -1.34907243e-03   3.89895176e-05]
-0.99681
1.15654
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.69422
Epoch 1, cost is  3.64167
Epoch 2, cost is  3.60431
Epoch 3, cost is  3.56908
Epoch 4, cost is  3.53781
Training took 0.109583 minutes
Weight histogram
[4530 2636 1051 1259  598  593  526 1830 1072   80] [ -1.55048007e-02  -1.39504217e-02  -1.23960427e-02  -1.08416636e-02
  -9.28728462e-03  -7.73290560e-03  -6.17852658e-03  -4.62414755e-03
  -3.06976853e-03  -1.51538951e-03   3.89895176e-05]
[1725 1162 1072 1335 1411 1249 1349 1501 1675 1696] [ -1.55048007e-02  -1.39504217e-02  -1.23960427e-02  -1.08416636e-02
  -9.28728462e-03  -7.73290560e-03  -6.17852658e-03  -4.62414755e-03
  -3.06976853e-03  -1.51538951e-03   3.89895176e-05]
-0.808015
0.792723
fine tuning ...
Epoch 0
Fine tuning took 0.062933 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.064398 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062165 minutes
{0: [0.050492610837438424, 0.043103448275862072, 0.041871921182266007, 0.055418719211822662], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9076354679802956, 0.8928571428571429, 0.92364532019704437, 0.9076354679802956], 5: [0.041871921182266007, 0.064039408866995079, 0.034482758620689655, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378629 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.86931
Epoch 1, cost is  3.81576
Epoch 2, cost is  3.77898
Epoch 3, cost is  3.75419
Epoch 4, cost is  3.7288
Training took 0.152592 minutes
Weight histogram
[1971 1980 1536  516 1719  350 1941 1989  111   37] [ -1.38416300e-02  -1.24535680e-02  -1.10655061e-02  -9.67744412e-03
  -8.28938217e-03  -6.90132022e-03  -5.51325827e-03  -4.12519633e-03
  -2.73713438e-03  -1.34907243e-03   3.89895176e-05]
[1019  740  891  943 1058 1166 1318 1526 1736 1753] [ -1.38416300e-02  -1.24535680e-02  -1.10655061e-02  -9.67744412e-03
  -8.28938217e-03  -6.90132022e-03  -5.51325827e-03  -4.12519633e-03
  -2.73713438e-03  -1.34907243e-03   3.89895176e-05]
-0.99681
1.15654
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.17274
Epoch 1, cost is  3.11652
Epoch 2, cost is  3.07663
Epoch 3, cost is  3.04702
Epoch 4, cost is  3.00832
Training took 0.151012 minutes
Weight histogram
[3769 2154 2141 1552  615 1093 2035  681   95   40] [ -9.03845578e-03  -8.12966238e-03  -7.22086897e-03  -6.31207556e-03
  -5.40328215e-03  -4.49448875e-03  -3.58569534e-03  -2.67690193e-03
  -1.76810852e-03  -8.59315117e-04   4.94782907e-05]
[1464  962 1651 1625 1216 1231 1333 1474 1576 1643] [ -9.03845578e-03  -8.12966238e-03  -7.22086897e-03  -6.31207556e-03
  -5.40328215e-03  -4.49448875e-03  -3.58569534e-03  -2.67690193e-03
  -1.76810852e-03  -8.59315117e-04   4.94782907e-05]
-0.571629
0.596842
fine tuning ...
Epoch 0
Fine tuning took 0.069334 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068267 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069017 minutes
{0: [0.056650246305418719, 0.075123152709359611, 0.056650246305418719, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90024630541871919, 0.8571428571428571, 0.88916256157635465, 0.8928571428571429], 5: [0.043103448275862072, 0.067733990147783252, 0.054187192118226604, 0.045566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378786 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.29151
Epoch 1, cost is  3.23083
Epoch 2, cost is  3.19433
Epoch 3, cost is  3.15653
Epoch 4, cost is  3.12798
Training took 0.207167 minutes
Weight histogram
[1980 1858  469 1707 1742  316 1970 1734  353   21] [ -8.47735163e-03  -7.62548760e-03  -6.77362358e-03  -5.92175956e-03
  -5.06989553e-03  -4.21803151e-03  -3.36616748e-03  -2.51430346e-03
  -1.66243944e-03  -8.10575413e-04   4.12886111e-05]
[ 957  735  828  918 1125 1277 1387 1529 1675 1719] [ -8.47735163e-03  -7.62548760e-03  -6.77362358e-03  -5.92175956e-03
  -5.06989553e-03  -4.21803151e-03  -3.36616748e-03  -2.51430346e-03
  -1.66243944e-03  -8.10575413e-04   4.12886111e-05]
-0.742353
0.774549
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.73928
Epoch 1, cost is  4.70791
Epoch 2, cost is  4.68354
Epoch 3, cost is  4.66154
Epoch 4, cost is  4.64423
Training took 0.110443 minutes
Weight histogram
[3051 1990 2074  910 1109  772  478  677 1098 2016] [ -2.14002561e-02  -1.92561016e-02  -1.71119471e-02  -1.49677927e-02
  -1.28236382e-02  -1.06794837e-02  -8.53532927e-03  -6.39117480e-03
  -4.24702033e-03  -2.10286586e-03   4.12886111e-05]
[2723  688  786 1056 1230 1298 1319 1492 1815 1768] [ -2.14002561e-02  -1.92561016e-02  -1.71119471e-02  -1.49677927e-02
  -1.28236382e-02  -1.06794837e-02  -8.53532927e-03  -6.39117480e-03
  -4.24702033e-03  -2.10286586e-03   4.12886111e-05]
-1.09487
1.10595
fine tuning ...
Epoch 0
Fine tuning took 0.068145 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069383 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068209 minutes
{0: [0.02832512315270936, 0.04064039408866995, 0.041871921182266007, 0.045566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94334975369458129, 0.91625615763546797, 0.92733990147783252, 0.91871921182266014], 5: [0.02832512315270936, 0.043103448275862072, 0.030788177339901478, 0.035714285714285712], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380157 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.29151
Epoch 1, cost is  3.23083
Epoch 2, cost is  3.19433
Epoch 3, cost is  3.15653
Epoch 4, cost is  3.12798
Training took 0.207459 minutes
Weight histogram
[1980 1858  469 1707 1742  316 1970 1734  353   21] [ -8.47735163e-03  -7.62548760e-03  -6.77362358e-03  -5.92175956e-03
  -5.06989553e-03  -4.21803151e-03  -3.36616748e-03  -2.51430346e-03
  -1.66243944e-03  -8.10575413e-04   4.12886111e-05]
[ 957  735  828  918 1125 1277 1387 1529 1675 1719] [ -8.47735163e-03  -7.62548760e-03  -6.77362358e-03  -5.92175956e-03
  -5.06989553e-03  -4.21803151e-03  -3.36616748e-03  -2.51430346e-03
  -1.66243944e-03  -8.10575413e-04   4.12886111e-05]
-0.742353
0.774549
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.01372
Epoch 1, cost is  3.96622
Epoch 2, cost is  3.93462
Epoch 3, cost is  3.90641
Epoch 4, cost is  3.88148
Training took 0.152160 minutes
Weight histogram
[3887 1836 1499  867 1859  496  608  997 2061   65] [ -1.17141213e-02  -1.05385803e-02  -9.36303934e-03  -8.18749835e-03
  -7.01195735e-03  -5.83641636e-03  -4.66087536e-03  -3.48533437e-03
  -2.30979338e-03  -1.13425238e-03   4.12886111e-05]
[1974 1498 1100 1093 1120 1221 1346 1536 1657 1630] [ -1.17141213e-02  -1.05385803e-02  -9.36303934e-03  -8.18749835e-03
  -7.01195735e-03  -5.83641636e-03  -4.66087536e-03  -3.48533437e-03
  -2.30979338e-03  -1.13425238e-03   4.12886111e-05]
-0.864729
0.759107
fine tuning ...
Epoch 0
Fine tuning took 0.074714 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074677 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074000 minutes
{0: [0.064039408866995079, 0.073891625615763554, 0.071428571428571425, 0.062807881773399021], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8854679802955665, 0.86699507389162567, 0.8719211822660099, 0.89655172413793105], 5: [0.050492610837438424, 0.059113300492610835, 0.056650246305418719, 0.04064039408866995], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.383929 minutes
Weight histogram
[   6   23  339  905 2162 3140 2574 1615 1186  200] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 3816] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-0.991578
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.29151
Epoch 1, cost is  3.23083
Epoch 2, cost is  3.19433
Epoch 3, cost is  3.15653
Epoch 4, cost is  3.12798
Training took 0.213760 minutes
Weight histogram
[1980 1858  469 1707 1742  316 1970 1734  353   21] [ -8.47735163e-03  -7.62548760e-03  -6.77362358e-03  -5.92175956e-03
  -5.06989553e-03  -4.21803151e-03  -3.36616748e-03  -2.51430346e-03
  -1.66243944e-03  -8.10575413e-04   4.12886111e-05]
[ 957  735  828  918 1125 1277 1387 1529 1675 1719] [ -8.47735163e-03  -7.62548760e-03  -6.77362358e-03  -5.92175956e-03
  -5.06989553e-03  -4.21803151e-03  -3.36616748e-03  -2.51430346e-03
  -1.66243944e-03  -8.10575413e-04   4.12886111e-05]
-0.742353
0.774549
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.52034
Epoch 1, cost is  3.46626
Epoch 2, cost is  3.43365
Epoch 3, cost is  3.40419
Epoch 4, cost is  3.37606
Training took 0.208295 minutes
Weight histogram
[2406 2200 1708 1733 1879  698  821 2623   79   28] [ -7.12344144e-03  -6.40696843e-03  -5.69049543e-03  -4.97402242e-03
  -4.25754942e-03  -3.54107641e-03  -2.82460341e-03  -2.10813040e-03
  -1.39165740e-03  -6.75184394e-04   4.12886111e-05]
[1578 1420 1642 1045 1207 1243 1322 1486 1586 1646] [ -7.12344144e-03  -6.40696843e-03  -5.69049543e-03  -4.97402242e-03
  -4.25754942e-03  -3.54107641e-03  -2.82460341e-03  -2.10813040e-03
  -1.39165740e-03  -6.75184394e-04   4.12886111e-05]
-0.598186
0.623506
fine tuning ...
Epoch 0
Fine tuning took 0.078237 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079229 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079884 minutes
{0: [0.054187192118226604, 0.081280788177339899, 0.1145320197044335, 0.067733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88793103448275867, 0.85837438423645318, 0.83497536945812811, 0.86699507389162567], 5: [0.057881773399014777, 0.060344827586206899, 0.050492610837438424, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378934 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.42937
Epoch 1, cost is  4.38682
Epoch 2, cost is  4.3662
Epoch 3, cost is  4.34781
Epoch 4, cost is  4.33493
Training took 0.106457 minutes
Weight histogram
[3188 1959  892 1461  549 1880 1567 2194  301  184] [ -2.92680375e-02  -2.63364067e-02  -2.34047759e-02  -2.04731451e-02
  -1.75415143e-02  -1.46098834e-02  -1.16782526e-02  -8.74662179e-03
  -5.81499097e-03  -2.88336016e-03   4.82706637e-05]
[1208  783 1011 1124 1205 1319 1444 1960 1967 2154] [ -2.92680375e-02  -2.63364067e-02  -2.34047759e-02  -2.04731451e-02
  -1.75415143e-02  -1.46098834e-02  -1.16782526e-02  -8.74662179e-03
  -5.81499097e-03  -2.88336016e-03   4.82706637e-05]
-1.80214
1.92995
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.09075
Epoch 1, cost is  4.05468
Epoch 2, cost is  4.01461
Epoch 3, cost is  3.99623
Epoch 4, cost is  3.96752
Training took 0.071988 minutes
Weight histogram
[5532 1843 2071  711 1391  867 1194  317 1891  383] [-0.04484495 -0.04034316 -0.03584137 -0.03133958 -0.02683779 -0.022336
 -0.01783421 -0.01333242 -0.00883063 -0.00432884  0.00017295]
[2304 1239  533  692  802 1340 1646 1980 2746 2918] [-0.04484495 -0.04034316 -0.03584137 -0.03133958 -0.02683779 -0.022336
 -0.01783421 -0.01333242 -0.00883063 -0.00432884  0.00017295]
-0.949665
1.20227
fine tuning ...
Epoch 0
Fine tuning took 0.050984 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051524 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052520 minutes
{0: [0.014778325123152709, 0.014778325123152709, 0.01600985221674877, 0.0086206896551724137], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96674876847290636, 0.95935960591133007, 0.96921182266009853, 0.96798029556650245], 5: [0.018472906403940888, 0.025862068965517241, 0.014778325123152709, 0.023399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380709 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.42937
Epoch 1, cost is  4.38682
Epoch 2, cost is  4.3662
Epoch 3, cost is  4.34781
Epoch 4, cost is  4.33493
Training took 0.108785 minutes
Weight histogram
[3188 1959  892 1461  549 1880 1567 2194  301  184] [ -2.92680375e-02  -2.63364067e-02  -2.34047759e-02  -2.04731451e-02
  -1.75415143e-02  -1.46098834e-02  -1.16782526e-02  -8.74662179e-03
  -5.81499097e-03  -2.88336016e-03   4.82706637e-05]
[1208  783 1011 1124 1205 1319 1444 1960 1967 2154] [ -2.92680375e-02  -2.63364067e-02  -2.34047759e-02  -2.04731451e-02
  -1.75415143e-02  -1.46098834e-02  -1.16782526e-02  -8.74662179e-03
  -5.81499097e-03  -2.88336016e-03   4.82706637e-05]
-1.80214
1.92995
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.42664
Epoch 1, cost is  3.35371
Epoch 2, cost is  3.3163
Epoch 3, cost is  3.28304
Epoch 4, cost is  3.24657
Training took 0.085016 minutes
Weight histogram
[6061 3303 1276 1234  405  642 1442 1429  287  121] [-0.02217873 -0.01993326 -0.01768779 -0.01544232 -0.01319685 -0.01095137
 -0.0087059  -0.00646043 -0.00421496 -0.00196949  0.00027599]
[1808  924  976  956 1307 1692 1987 2574 1969 2007] [-0.02217873 -0.01993326 -0.01768779 -0.01544232 -0.01319685 -0.01095137
 -0.0087059  -0.00646043 -0.00421496 -0.00196949  0.00027599]
-0.732646
0.692356
fine tuning ...
Epoch 0
Fine tuning took 0.052577 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052781 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053752 minutes
{0: [0.033251231527093597, 0.032019704433497539, 0.019704433497536946, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93842364532019706, 0.93965517241379315, 0.96674876847290636, 0.94581280788177335], 5: [0.02832512315270936, 0.02832512315270936, 0.013546798029556651, 0.02832512315270936], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380734 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.42937
Epoch 1, cost is  4.38682
Epoch 2, cost is  4.3662
Epoch 3, cost is  4.34781
Epoch 4, cost is  4.33493
Training took 0.106845 minutes
Weight histogram
[3188 1959  892 1461  549 1880 1567 2194  301  184] [ -2.92680375e-02  -2.63364067e-02  -2.34047759e-02  -2.04731451e-02
  -1.75415143e-02  -1.46098834e-02  -1.16782526e-02  -8.74662179e-03
  -5.81499097e-03  -2.88336016e-03   4.82706637e-05]
[1208  783 1011 1124 1205 1319 1444 1960 1967 2154] [ -2.92680375e-02  -2.63364067e-02  -2.34047759e-02  -2.04731451e-02
  -1.75415143e-02  -1.46098834e-02  -1.16782526e-02  -8.74662179e-03
  -5.81499097e-03  -2.88336016e-03   4.82706637e-05]
-1.80214
1.92995
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.97811
Epoch 1, cost is  2.92206
Epoch 2, cost is  2.88162
Epoch 3, cost is  2.86588
Epoch 4, cost is  2.82427
Training took 0.107235 minutes
Weight histogram
[5843 2701 3413  212 1675 1000  539  595  167   55] [ -1.25530949e-02  -1.12927610e-02  -1.00324271e-02  -8.77209322e-03
  -7.51175933e-03  -6.25142544e-03  -4.99109154e-03  -3.73075765e-03
  -2.47042376e-03  -1.21008987e-03   5.02440162e-05]
[1518  844  905 1456 1923 1815 1644 1864 1857 2374] [ -1.25530949e-02  -1.12927610e-02  -1.00324271e-02  -8.77209322e-03
  -7.51175933e-03  -6.25142544e-03  -4.99109154e-03  -3.73075765e-03
  -2.47042376e-03  -1.21008987e-03   5.02440162e-05]
-0.557094
0.529372
fine tuning ...
Epoch 0
Fine tuning took 0.055710 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057308 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057960 minutes
{0: [0.027093596059113302, 0.020935960591133004, 0.023399014778325122, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93842364532019706, 0.94704433497536944, 0.95073891625615758, 0.96059113300492616], 5: [0.034482758620689655, 0.032019704433497539, 0.025862068965517241, 0.01600985221674877], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379962 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.70181
Epoch 1, cost is  3.6568
Epoch 2, cost is  3.62724
Epoch 3, cost is  3.60474
Epoch 4, cost is  3.5825
Training took 0.151282 minutes
Weight histogram
[1990 2007 2019 1706  365 1930 1697 1384 1030   47] [ -1.55679509e-02  -1.40072569e-02  -1.24465628e-02  -1.08858688e-02
  -9.32517474e-03  -7.76448069e-03  -6.20378665e-03  -4.64309261e-03
  -3.08239857e-03  -1.52170452e-03   3.89895176e-05]
[1089  839 1015 1068 1231 1388 1574 1899 1944 2128] [ -1.55679509e-02  -1.40072569e-02  -1.24465628e-02  -1.08858688e-02
  -9.32517474e-03  -7.76448069e-03  -6.20378665e-03  -4.64309261e-03
  -3.08239857e-03  -1.52170452e-03   3.89895176e-05]
-1.15323
1.27872
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.31689
Epoch 1, cost is  4.26553
Epoch 2, cost is  4.23947
Epoch 3, cost is  4.20992
Epoch 4, cost is  4.19116
Training took 0.088092 minutes
Weight histogram
[5802 1730 1565  965 1128  716  464  739 1819 1272] [ -3.17322090e-02  -2.85550892e-02  -2.53779693e-02  -2.22008495e-02
  -1.90237296e-02  -1.58466098e-02  -1.26694899e-02  -9.49237005e-03
  -6.31525019e-03  -3.13813034e-03   3.89895176e-05]
[2422  937  637 1147 1477 1751 1635 2062 1918 2214] [ -3.17322090e-02  -2.85550892e-02  -2.53779693e-02  -2.22008495e-02
  -1.90237296e-02  -1.58466098e-02  -1.26694899e-02  -9.49237005e-03
  -6.31525019e-03  -3.13813034e-03   3.89895176e-05]
-1.06827
1.22965
fine tuning ...
Epoch 0
Fine tuning took 0.060358 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059613 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060913 minutes
{0: [0.046798029556650245, 0.04064039408866995, 0.051724137931034482, 0.059113300492610835], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91379310344827591, 0.92980295566502458, 0.90886699507389157, 0.91133004926108374], 5: [0.039408866995073892, 0.029556650246305417, 0.039408866995073892, 0.029556650246305417], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379404 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.70181
Epoch 1, cost is  3.6568
Epoch 2, cost is  3.62724
Epoch 3, cost is  3.60474
Epoch 4, cost is  3.5825
Training took 0.152277 minutes
Weight histogram
[1990 2007 2019 1706  365 1930 1697 1384 1030   47] [ -1.55679509e-02  -1.40072569e-02  -1.24465628e-02  -1.08858688e-02
  -9.32517474e-03  -7.76448069e-03  -6.20378665e-03  -4.64309261e-03
  -3.08239857e-03  -1.52170452e-03   3.89895176e-05]
[1089  839 1015 1068 1231 1388 1574 1899 1944 2128] [ -1.55679509e-02  -1.40072569e-02  -1.24465628e-02  -1.08858688e-02
  -9.32517474e-03  -7.76448069e-03  -6.20378665e-03  -4.64309261e-03
  -3.08239857e-03  -1.52170452e-03   3.89895176e-05]
-1.15323
1.27872
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.50944
Epoch 1, cost is  3.45517
Epoch 2, cost is  3.41913
Epoch 3, cost is  3.39155
Epoch 4, cost is  3.3622
Training took 0.111106 minutes
Weight histogram
[3392 4361 2183 1045 1009  465  637 1480 1536   92] [ -1.66726243e-02  -1.50014629e-02  -1.33303015e-02  -1.16591401e-02
  -9.98797876e-03  -8.31681738e-03  -6.64565600e-03  -4.97449462e-03
  -3.30333324e-03  -1.63217186e-03   3.89895176e-05]
[1841 1357 1211 1521 1487 1458 1599 1839 1907 1980] [ -1.66726243e-02  -1.50014629e-02  -1.33303015e-02  -1.16591401e-02
  -9.98797876e-03  -8.31681738e-03  -6.64565600e-03  -4.97449462e-03
  -3.30333324e-03  -1.63217186e-03   3.89895176e-05]
-0.869494
0.864217
fine tuning ...
Epoch 0
Fine tuning took 0.062307 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063151 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063547 minutes
{0: [0.076354679802955669, 0.073891625615763554, 0.065270935960591137, 0.05295566502463054], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89162561576354682, 0.89408866995073888, 0.89039408866995073, 0.90886699507389157], 5: [0.032019704433497539, 0.032019704433497539, 0.044334975369458129, 0.038177339901477834], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378745 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.70181
Epoch 1, cost is  3.6568
Epoch 2, cost is  3.62724
Epoch 3, cost is  3.60474
Epoch 4, cost is  3.5825
Training took 0.153293 minutes
Weight histogram
[1990 2007 2019 1706  365 1930 1697 1384 1030   47] [ -1.55679509e-02  -1.40072569e-02  -1.24465628e-02  -1.08858688e-02
  -9.32517474e-03  -7.76448069e-03  -6.20378665e-03  -4.64309261e-03
  -3.08239857e-03  -1.52170452e-03   3.89895176e-05]
[1089  839 1015 1068 1231 1388 1574 1899 1944 2128] [ -1.55679509e-02  -1.40072569e-02  -1.24465628e-02  -1.08858688e-02
  -9.32517474e-03  -7.76448069e-03  -6.20378665e-03  -4.64309261e-03
  -3.08239857e-03  -1.52170452e-03   3.89895176e-05]
-1.15323
1.27872
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.98368
Epoch 1, cost is  2.93712
Epoch 2, cost is  2.90243
Epoch 3, cost is  2.8769
Epoch 4, cost is  2.8459
Training took 0.153057 minutes
Weight histogram
[2835 3607 2165 2171 1326  786 1869 1286  111   44] [ -9.79189482e-03  -8.80775751e-03  -7.82362019e-03  -6.83948288e-03
  -5.85534557e-03  -4.87120826e-03  -3.88707095e-03  -2.90293364e-03
  -1.91879633e-03  -9.34659020e-04   4.94782907e-05]
[1542 1137 1955 1551 1318 1420 1560 1685 1797 2235] [ -9.79189482e-03  -8.80775751e-03  -7.82362019e-03  -6.83948288e-03
  -5.85534557e-03  -4.87120826e-03  -3.88707095e-03  -2.90293364e-03
  -1.91879633e-03  -9.34659020e-04   4.94782907e-05]
-0.633756
0.667178
fine tuning ...
Epoch 0
Fine tuning took 0.068905 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068069 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069315 minutes
{0: [0.080049261083743842, 0.083743842364532015, 0.084975369458128072, 0.071428571428571425], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8571428571428571, 0.86576354679802958, 0.86822660098522164, 0.88423645320197042], 5: [0.062807881773399021, 0.050492610837438424, 0.046798029556650245, 0.044334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379664 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.10772
Epoch 1, cost is  3.05428
Epoch 2, cost is  3.01999
Epoch 3, cost is  2.99145
Epoch 4, cost is  2.96124
Training took 0.207423 minutes
Weight histogram
[3486 1644  895 1863  898 1292 1799 1329  945   24] [ -9.04344022e-03  -8.13496734e-03  -7.22649446e-03  -6.31802157e-03
  -5.40954869e-03  -4.50107581e-03  -3.59260292e-03  -2.68413004e-03
  -1.77565716e-03  -8.67184272e-04   4.12886111e-05]
[1026  821  937 1053 1352 1481 1602 1797 1911 2195] [ -9.04344022e-03  -8.13496734e-03  -7.22649446e-03  -6.31802157e-03
  -5.40954869e-03  -4.50107581e-03  -3.59260292e-03  -2.68413004e-03
  -1.77565716e-03  -8.67184272e-04   4.12886111e-05]
-0.791476
0.860459
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.68427
Epoch 1, cost is  4.64847
Epoch 2, cost is  4.62587
Epoch 3, cost is  4.60915
Epoch 4, cost is  4.59228
Training took 0.108208 minutes
Weight histogram
[4980 2032 2055  973 1048  819  491  687 1080 2035] [ -2.14930903e-02  -1.93396524e-02  -1.71862145e-02  -1.50327766e-02
  -1.28793387e-02  -1.07259009e-02  -8.57246296e-03  -6.41902507e-03
  -4.26558717e-03  -2.11214928e-03   4.12886111e-05]
[2889  655  962 1213 1424 1447 1478 2010 1951 2171] [ -2.14930903e-02  -1.93396524e-02  -1.71862145e-02  -1.50327766e-02
  -1.28793387e-02  -1.07259009e-02  -8.57246296e-03  -6.41902507e-03
  -4.26558717e-03  -2.11214928e-03   4.12886111e-05]
-1.29321
1.26969
fine tuning ...
Epoch 0
Fine tuning took 0.068612 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069538 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069402 minutes
{0: [0.033251231527093597, 0.033251231527093597, 0.036945812807881777, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93103448275862066, 0.91871921182266014, 0.92487684729064035, 0.92364532019704437], 5: [0.035714285714285712, 0.048029556650246302, 0.038177339901477834, 0.032019704433497539], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380018 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.10772
Epoch 1, cost is  3.05428
Epoch 2, cost is  3.01999
Epoch 3, cost is  2.99145
Epoch 4, cost is  2.96124
Training took 0.206772 minutes
Weight histogram
[3486 1644  895 1863  898 1292 1799 1329  945   24] [ -9.04344022e-03  -8.13496734e-03  -7.22649446e-03  -6.31802157e-03
  -5.40954869e-03  -4.50107581e-03  -3.59260292e-03  -2.68413004e-03
  -1.77565716e-03  -8.67184272e-04   4.12886111e-05]
[1026  821  937 1053 1352 1481 1602 1797 1911 2195] [ -9.04344022e-03  -8.13496734e-03  -7.22649446e-03  -6.31802157e-03
  -5.40954869e-03  -4.50107581e-03  -3.59260292e-03  -2.68413004e-03
  -1.77565716e-03  -8.67184272e-04   4.12886111e-05]
-0.791476
0.860459
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.93431
Epoch 1, cost is  3.88945
Epoch 2, cost is  3.86387
Epoch 3, cost is  3.83645
Epoch 4, cost is  3.82077
Training took 0.153960 minutes
Weight histogram
[5474 1911 1537 1133 1857  482  658 1015 2064   69] [ -1.20294169e-02  -1.08223463e-02  -9.61527577e-03  -8.40820522e-03
  -7.20113467e-03  -5.99406412e-03  -4.78699358e-03  -3.57992303e-03
  -2.37285248e-03  -1.16578194e-03   4.12886111e-05]
[2156 1523 1276 1231 1284 1462 1642 1846 1828 1952] [ -1.20294169e-02  -1.08223463e-02  -9.61527577e-03  -8.40820522e-03
  -7.20113467e-03  -5.99406412e-03  -4.78699358e-03  -3.57992303e-03
  -2.37285248e-03  -1.16578194e-03   4.12886111e-05]
-0.999114
0.850003
fine tuning ...
Epoch 0
Fine tuning took 0.074228 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074946 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074390 minutes
{0: [0.068965517241379309, 0.083743842364532015, 0.082512315270935957, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87438423645320196, 0.83743842364532017, 0.87068965517241381, 0.82635467980295563], 5: [0.056650246305418719, 0.078817733990147784, 0.046798029556650245, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379378 minutes
Weight histogram
[   6   23  339  905 2162 3151 2945 2966 1473  205] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
[ 152  235  342  626  723  429  699 2050 3078 5841] [ -4.69115912e-04  -3.40483198e-04  -2.11850484e-04  -8.32177699e-05
   4.54149442e-05   1.74047658e-04   3.02680372e-04   4.31313086e-04
   5.59945800e-04   6.88578514e-04   8.17211228e-04]
-1.03938
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.10772
Epoch 1, cost is  3.05428
Epoch 2, cost is  3.01999
Epoch 3, cost is  2.99145
Epoch 4, cost is  2.96124
Training took 0.209031 minutes
Weight histogram
[3486 1644  895 1863  898 1292 1799 1329  945   24] [ -9.04344022e-03  -8.13496734e-03  -7.22649446e-03  -6.31802157e-03
  -5.40954869e-03  -4.50107581e-03  -3.59260292e-03  -2.68413004e-03
  -1.77565716e-03  -8.67184272e-04   4.12886111e-05]
[1026  821  937 1053 1352 1481 1602 1797 1911 2195] [ -9.04344022e-03  -8.13496734e-03  -7.22649446e-03  -6.31802157e-03
  -5.40954869e-03  -4.50107581e-03  -3.59260292e-03  -2.68413004e-03
  -1.77565716e-03  -8.67184272e-04   4.12886111e-05]
-0.791476
0.860459
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.40236
Epoch 1, cost is  3.35946
Epoch 2, cost is  3.32893
Epoch 3, cost is  3.30543
Epoch 4, cost is  3.27929
Training took 0.206489 minutes
Weight histogram
[3195 2595 2007 2145 1450  988  958 2737   95   30] [ -7.52904592e-03  -6.77201247e-03  -6.01497901e-03  -5.25794556e-03
  -4.50091211e-03  -3.74387865e-03  -2.98684520e-03  -2.22981175e-03
  -1.47277830e-03  -7.15744842e-04   4.12886111e-05]
[1698 1621 1621 1214 1339 1410 1571 1715 1792 2219] [ -7.52904592e-03  -6.77201247e-03  -6.01497901e-03  -5.25794556e-03
  -4.50091211e-03  -3.74387865e-03  -2.98684520e-03  -2.22981175e-03
  -1.47277830e-03  -7.15744842e-04   4.12886111e-05]
-0.657919
0.690044
fine tuning ...
Epoch 0
Fine tuning took 0.078979 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078835 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078724 minutes
{0: [0.082512315270935957, 0.082512315270935957, 0.078817733990147784, 0.093596059113300489], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8423645320197044, 0.83128078817733986, 0.83374384236453203, 0.81650246305418717], 5: [0.075123152709359611, 0.086206896551724144, 0.087438423645320201, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379593 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.36984
Epoch 1, cost is  4.33409
Epoch 2, cost is  4.31239
Epoch 3, cost is  4.29769
Epoch 4, cost is  4.2839
Training took 0.108232 minutes
Weight histogram
[3752 1892 2007 1149 1072 1778 1203 2318  836  193] [ -3.18617634e-02  -2.86707600e-02  -2.54797566e-02  -2.22887532e-02
  -1.90977498e-02  -1.59067464e-02  -1.27157430e-02  -9.52473957e-03
  -6.33373616e-03  -3.14273275e-03   4.82706637e-05]
[1272  880 1126 1254 1368 1482 1913 2121 2288 2496] [ -3.18617634e-02  -2.86707600e-02  -2.54797566e-02  -2.22887532e-02
  -1.90977498e-02  -1.59067464e-02  -1.27157430e-02  -9.52473957e-03
  -6.33373616e-03  -3.14273275e-03   4.82706637e-05]
-2.03383
2.14097
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.15948
Epoch 1, cost is  4.10579
Epoch 2, cost is  4.07416
Epoch 3, cost is  4.05572
Epoch 4, cost is  4.03523
Training took 0.070673 minutes
Weight histogram
[6734 1936 2509  887 1171 1061 1295  344 1878  410] [-0.04633591 -0.04168502 -0.03703413 -0.03238325 -0.02773236 -0.02308148
 -0.01843059 -0.01377971 -0.00912882 -0.00447794  0.00017295]
[2392 1221  615  754  940 1719 1765 2796 2973 3050] [-0.04633591 -0.04168502 -0.03703413 -0.03238325 -0.02773236 -0.02308148
 -0.01843059 -0.01377971 -0.00912882 -0.00447794  0.00017295]
-1.04763
1.23348
fine tuning ...
Epoch 0
Fine tuning took 0.053390 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053155 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051782 minutes
{0: [0.009852216748768473, 0.0061576354679802959, 0.0086206896551724137, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97536945812807885, 0.96798029556650245, 0.96921182266009853, 0.95935960591133007], 5: [0.014778325123152709, 0.025862068965517241, 0.022167487684729065, 0.025862068965517241], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381388 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.36984
Epoch 1, cost is  4.33409
Epoch 2, cost is  4.31239
Epoch 3, cost is  4.29769
Epoch 4, cost is  4.2839
Training took 0.107895 minutes
Weight histogram
[3752 1892 2007 1149 1072 1778 1203 2318  836  193] [ -3.18617634e-02  -2.86707600e-02  -2.54797566e-02  -2.22887532e-02
  -1.90977498e-02  -1.59067464e-02  -1.27157430e-02  -9.52473957e-03
  -6.33373616e-03  -3.14273275e-03   4.82706637e-05]
[1272  880 1126 1254 1368 1482 1913 2121 2288 2496] [ -3.18617634e-02  -2.86707600e-02  -2.54797566e-02  -2.22887532e-02
  -1.90977498e-02  -1.59067464e-02  -1.27157430e-02  -9.52473957e-03
  -6.33373616e-03  -3.14273275e-03   4.82706637e-05]
-2.03383
2.14097
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.38558
Epoch 1, cost is  3.33079
Epoch 2, cost is  3.29862
Epoch 3, cost is  3.26111
Epoch 4, cost is  3.24408
Training took 0.086348 minutes
Weight histogram
[7920 2555 1790 1403  551  661 1215 1692  307  131] [-0.02286851 -0.02055406 -0.01823961 -0.01592516 -0.01361071 -0.01129626
 -0.00898181 -0.00666736 -0.00435291 -0.00203846  0.00027599]
[1899 1000 1126 1040 1758 1874 2703 2299 2138 2388] [-0.02286851 -0.02055406 -0.01823961 -0.01592516 -0.01361071 -0.01129626
 -0.00898181 -0.00666736 -0.00435291 -0.00203846  0.00027599]
-0.810997
0.80525
fine tuning ...
Epoch 0
Fine tuning took 0.053820 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053343 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053366 minutes
{0: [0.034482758620689655, 0.034482758620689655, 0.027093596059113302, 0.02832512315270936], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.92733990147783252, 0.93719211822660098, 0.95197044334975367, 0.94088669950738912], 5: [0.038177339901477834, 0.02832512315270936, 0.020935960591133004, 0.030788177339901478], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378900 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.36984
Epoch 1, cost is  4.33409
Epoch 2, cost is  4.31239
Epoch 3, cost is  4.29769
Epoch 4, cost is  4.2839
Training took 0.109869 minutes
Weight histogram
[3752 1892 2007 1149 1072 1778 1203 2318  836  193] [ -3.18617634e-02  -2.86707600e-02  -2.54797566e-02  -2.22887532e-02
  -1.90977498e-02  -1.59067464e-02  -1.27157430e-02  -9.52473957e-03
  -6.33373616e-03  -3.14273275e-03   4.82706637e-05]
[1272  880 1126 1254 1368 1482 1913 2121 2288 2496] [ -3.18617634e-02  -2.86707600e-02  -2.54797566e-02  -2.22887532e-02
  -1.90977498e-02  -1.59067464e-02  -1.27157430e-02  -9.52473957e-03
  -6.33373616e-03  -3.14273275e-03   4.82706637e-05]
-2.03383
2.14097
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.88307
Epoch 1, cost is  2.8534
Epoch 2, cost is  2.80981
Epoch 3, cost is  2.79225
Epoch 4, cost is  2.77274
Training took 0.108577 minutes
Weight histogram
[6240 3114 3874  930 1361 1226  582  668  173   57] [ -1.29415859e-02  -1.16424029e-02  -1.03432199e-02  -9.04403689e-03
  -7.74485391e-03  -6.44567092e-03  -5.14648793e-03  -3.84730494e-03
  -2.54812196e-03  -1.24893897e-03   5.02440162e-05]
[1599  913 1080 1759 2075 1930 1954 1973 2395 2547] [ -1.29415859e-02  -1.16424029e-02  -1.03432199e-02  -9.04403689e-03
  -7.74485391e-03  -6.44567092e-03  -5.14648793e-03  -3.84730494e-03
  -2.54812196e-03  -1.24893897e-03   5.02440162e-05]
-0.588574
0.565128
fine tuning ...
Epoch 0
Fine tuning took 0.055727 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056909 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057060 minutes
{0: [0.036945812807881777, 0.035714285714285712, 0.024630541871921183, 0.035714285714285712], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94088669950738912, 0.91995073891625612, 0.95197044334975367, 0.93226600985221675], 5: [0.022167487684729065, 0.044334975369458129, 0.023399014778325122, 0.032019704433497539], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378914 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.58846
Epoch 1, cost is  3.54942
Epoch 2, cost is  3.52199
Epoch 3, cost is  3.50097
Epoch 4, cost is  3.48358
Training took 0.152948 minutes
Weight histogram
[3714 1937 2075 1541  799 1888 1290 1488 1416   52] [ -1.63724329e-02  -1.47312907e-02  -1.30901484e-02  -1.14490062e-02
  -9.80786395e-03  -8.16672171e-03  -6.52557946e-03  -4.88443722e-03
  -3.24329497e-03  -1.60215273e-03   3.89895176e-05]
[1156  951 1125 1232 1404 1620 1961 2111 2287 2353] [ -1.63724329e-02  -1.47312907e-02  -1.30901484e-02  -1.14490062e-02
  -9.80786395e-03  -8.16672171e-03  -6.52557946e-03  -4.88443722e-03
  -3.24329497e-03  -1.60215273e-03   3.89895176e-05]
-1.2747
1.36658
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.30441
Epoch 1, cost is  4.26245
Epoch 2, cost is  4.24175
Epoch 3, cost is  4.22294
Epoch 4, cost is  4.20334
Training took 0.084138 minutes
Weight histogram
[7827 1730 1565  965 1128  716  464  739 1819 1272] [ -3.17322090e-02  -2.85550892e-02  -2.53779693e-02  -2.22008495e-02
  -1.90237296e-02  -1.58466098e-02  -1.26694899e-02  -9.49237005e-03
  -6.31525019e-03  -3.13813034e-03   3.89895176e-05]
[2592  867  818 1333 1803 1847 2073 2117 2349 2426] [ -3.17322090e-02  -2.85550892e-02  -2.53779693e-02  -2.22008495e-02
  -1.90237296e-02  -1.58466098e-02  -1.26694899e-02  -9.49237005e-03
  -6.31525019e-03  -3.13813034e-03   3.89895176e-05]
-1.1971
1.3643
fine tuning ...
Epoch 0
Fine tuning took 0.059785 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060561 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060161 minutes
{0: [0.055418719211822662, 0.045566502463054187, 0.02832512315270936, 0.036945812807881777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91748768472906406, 0.91748768472906406, 0.95073891625615758, 0.92733990147783252], 5: [0.027093596059113302, 0.036945812807881777, 0.020935960591133004, 0.035714285714285712], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378644 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.58846
Epoch 1, cost is  3.54942
Epoch 2, cost is  3.52199
Epoch 3, cost is  3.50097
Epoch 4, cost is  3.48358
Training took 0.151108 minutes
Weight histogram
[3714 1937 2075 1541  799 1888 1290 1488 1416   52] [ -1.63724329e-02  -1.47312907e-02  -1.30901484e-02  -1.14490062e-02
  -9.80786395e-03  -8.16672171e-03  -6.52557946e-03  -4.88443722e-03
  -3.24329497e-03  -1.60215273e-03   3.89895176e-05]
[1156  951 1125 1232 1404 1620 1961 2111 2287 2353] [ -1.63724329e-02  -1.47312907e-02  -1.30901484e-02  -1.14490062e-02
  -9.80786395e-03  -8.16672171e-03  -6.52557946e-03  -4.88443722e-03
  -3.24329497e-03  -1.60215273e-03   3.89895176e-05]
-1.2747
1.36658
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.45318
Epoch 1, cost is  3.40036
Epoch 2, cost is  3.3777
Epoch 3, cost is  3.34708
Epoch 4, cost is  3.32707
Training took 0.113479 minutes
Weight histogram
[4955 4618 2289 1033 1098  447  670 1459 1562   94] [ -1.68210045e-02  -1.51350051e-02  -1.34490057e-02  -1.17630063e-02
  -1.00770069e-02  -8.39100747e-03  -6.70500807e-03  -5.01900867e-03
  -3.33300928e-03  -1.64700988e-03   3.89895176e-05]
[1954 1532 1362 1722 1541 1661 1967 2092 2162 2232] [ -1.68210045e-02  -1.51350051e-02  -1.34490057e-02  -1.17630063e-02
  -1.00770069e-02  -8.39100747e-03  -6.70500807e-03  -5.01900867e-03
  -3.33300928e-03  -1.64700988e-03   3.89895176e-05]
-0.947602
0.996191
fine tuning ...
Epoch 0
Fine tuning took 0.063024 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062821 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062133 minutes
{0: [0.059113300492610835, 0.10098522167487685, 0.081280788177339899, 0.078817733990147784], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90394088669950734, 0.84852216748768472, 0.87068965517241381, 0.8780788177339901], 5: [0.036945812807881777, 0.050492610837438424, 0.048029556650246302, 0.043103448275862072], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378762 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.58846
Epoch 1, cost is  3.54942
Epoch 2, cost is  3.52199
Epoch 3, cost is  3.50097
Epoch 4, cost is  3.48358
Training took 0.152665 minutes
Weight histogram
[3714 1937 2075 1541  799 1888 1290 1488 1416   52] [ -1.63724329e-02  -1.47312907e-02  -1.30901484e-02  -1.14490062e-02
  -9.80786395e-03  -8.16672171e-03  -6.52557946e-03  -4.88443722e-03
  -3.24329497e-03  -1.60215273e-03   3.89895176e-05]
[1156  951 1125 1232 1404 1620 1961 2111 2287 2353] [ -1.63724329e-02  -1.47312907e-02  -1.30901484e-02  -1.14490062e-02
  -9.80786395e-03  -8.16672171e-03  -6.52557946e-03  -4.88443722e-03
  -3.24329497e-03  -1.60215273e-03   3.89895176e-05]
-1.2747
1.36658
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.96225
Epoch 1, cost is  2.91955
Epoch 2, cost is  2.88552
Epoch 3, cost is  2.85917
Epoch 4, cost is  2.84002
Training took 0.152934 minutes
Weight histogram
[4467 3580 2160 2495 1405  712 1842 1407  112   45] [ -9.92780551e-03  -8.93007713e-03  -7.93234875e-03  -6.93462037e-03
  -5.93689199e-03  -4.93916361e-03  -3.94143523e-03  -2.94370685e-03
  -1.94597847e-03  -9.48250090e-04   4.94782907e-05]
[1638 1344 2241 1508 1487 1621 1840 1959 2389 2198] [ -9.92780551e-03  -8.93007713e-03  -7.93234875e-03  -6.93462037e-03
  -5.93689199e-03  -4.93916361e-03  -3.94143523e-03  -2.94370685e-03
  -1.94597847e-03  -9.48250090e-04   4.94782907e-05]
-0.693096
0.742111
fine tuning ...
Epoch 0
Fine tuning took 0.068522 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069066 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068351 minutes
{0: [0.11822660098522167, 0.088669950738916259, 0.051724137931034482, 0.070197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83620689655172409, 0.85591133004926112, 0.8854679802955665, 0.8780788177339901], 5: [0.045566502463054187, 0.055418719211822662, 0.062807881773399021, 0.051724137931034482], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378641 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.98542
Epoch 1, cost is  2.93974
Epoch 2, cost is  2.90936
Epoch 3, cost is  2.88321
Epoch 4, cost is  2.85815
Training took 0.207979 minutes
Weight histogram
[3325 2593 1916  803 1453 1951  821 1555 1757   26] [ -9.81326215e-03  -8.82780707e-03  -7.84235200e-03  -6.85689692e-03
  -5.87144185e-03  -4.88598677e-03  -3.90053169e-03  -2.91507662e-03
  -1.92962154e-03  -9.44166465e-04   4.12886111e-05]
[1110  910 1079 1285 1554 1702 1943 2077 2383 2157] [ -9.81326215e-03  -8.82780707e-03  -7.84235200e-03  -6.85689692e-03
  -5.87144185e-03  -4.88598677e-03  -3.90053169e-03  -2.91507662e-03
  -1.92962154e-03  -9.44166465e-04   4.12886111e-05]
-0.872845
0.934468
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.64969
Epoch 1, cost is  4.61666
Epoch 2, cost is  4.59886
Epoch 3, cost is  4.58318
Epoch 4, cost is  4.57175
Training took 0.106517 minutes
Weight histogram
[6541 2199 2242 1063 1030  844  484  697 1022 2103] [ -2.17740238e-02  -1.95924925e-02  -1.74109613e-02  -1.52294301e-02
  -1.30478988e-02  -1.08663676e-02  -8.68483634e-03  -6.50330510e-03
  -4.32177387e-03  -2.14024263e-03   4.12886111e-05]
[2945  738 1154 1418 1574 1585 1987 2154 2325 2345] [ -2.17740238e-02  -1.95924925e-02  -1.74109613e-02  -1.52294301e-02
  -1.30478988e-02  -1.08663676e-02  -8.68483634e-03  -6.50330510e-03
  -4.32177387e-03  -2.14024263e-03   4.12886111e-05]
-1.44421
1.4066
fine tuning ...
Epoch 0
Fine tuning took 0.069339 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069449 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068799 minutes
{0: [0.032019704433497539, 0.041871921182266007, 0.067733990147783252, 0.064039408866995079], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93103448275862066, 0.91133004926108374, 0.89162561576354682, 0.89655172413793105], 5: [0.036945812807881777, 0.046798029556650245, 0.04064039408866995, 0.039408866995073892], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380697 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.98542
Epoch 1, cost is  2.93974
Epoch 2, cost is  2.90936
Epoch 3, cost is  2.88321
Epoch 4, cost is  2.85815
Training took 0.206330 minutes
Weight histogram
[3325 2593 1916  803 1453 1951  821 1555 1757   26] [ -9.81326215e-03  -8.82780707e-03  -7.84235200e-03  -6.85689692e-03
  -5.87144185e-03  -4.88598677e-03  -3.90053169e-03  -2.91507662e-03
  -1.92962154e-03  -9.44166465e-04   4.12886111e-05]
[1110  910 1079 1285 1554 1702 1943 2077 2383 2157] [ -9.81326215e-03  -8.82780707e-03  -7.84235200e-03  -6.85689692e-03
  -5.87144185e-03  -4.88598677e-03  -3.90053169e-03  -2.91507662e-03
  -1.92962154e-03  -9.44166465e-04   4.12886111e-05]
-0.872845
0.934468
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.82965
Epoch 1, cost is  3.78901
Epoch 2, cost is  3.76598
Epoch 3, cost is  3.74426
Epoch 4, cost is  3.7298
Training took 0.151077 minutes
Weight histogram
[4843 3773 1493 1792 1725  554  814  904 2251   76] [ -1.27011761e-02  -1.14269296e-02  -1.01526832e-02  -8.87843669e-03
  -7.60419022e-03  -6.32994375e-03  -5.05569728e-03  -3.78145080e-03
  -2.50720433e-03  -1.23295786e-03   4.12886111e-05]
[2334 1580 1403 1361 1504 1675 2005 2001 2137 2225] [ -1.27011761e-02  -1.14269296e-02  -1.01526832e-02  -8.87843669e-03
  -7.60419022e-03  -6.32994375e-03  -5.05569728e-03  -3.78145080e-03
  -2.50720433e-03  -1.23295786e-03   4.12886111e-05]
-1.13321
0.99247
fine tuning ...
Epoch 0
Fine tuning took 0.073269 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073922 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074430 minutes
{0: [0.087438423645320201, 0.077586206896551727, 0.070197044334975367, 0.082512315270935957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8423645320197044, 0.85960591133004927, 0.85960591133004927, 0.86206896551724133], 5: [0.070197044334975367, 0.062807881773399021, 0.070197044334975367, 0.055418719211822662], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379795 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3180 3229 2284  360] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 164  275  451  641  681  547 1287 2890 5875 3389] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.02014
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.98542
Epoch 1, cost is  2.93974
Epoch 2, cost is  2.90936
Epoch 3, cost is  2.88321
Epoch 4, cost is  2.85815
Training took 0.214138 minutes
Weight histogram
[3325 2593 1916  803 1453 1951  821 1555 1757   26] [ -9.81326215e-03  -8.82780707e-03  -7.84235200e-03  -6.85689692e-03
  -5.87144185e-03  -4.88598677e-03  -3.90053169e-03  -2.91507662e-03
  -1.92962154e-03  -9.44166465e-04   4.12886111e-05]
[1110  910 1079 1285 1554 1702 1943 2077 2383 2157] [ -9.81326215e-03  -8.82780707e-03  -7.84235200e-03  -6.85689692e-03
  -5.87144185e-03  -4.88598677e-03  -3.90053169e-03  -2.91507662e-03
  -1.92962154e-03  -9.44166465e-04   4.12886111e-05]
-0.872845
0.934468
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.36567
Epoch 1, cost is  3.32335
Epoch 2, cost is  3.29648
Epoch 3, cost is  3.27635
Epoch 4, cost is  3.25811
Training took 0.207975 minutes
Weight histogram
[4241 2800 2301 2265 1302 1260 1079 2843  101   33] [ -7.82802422e-03  -7.04109293e-03  -6.25416165e-03  -5.46723037e-03
  -4.68029909e-03  -3.89336780e-03  -3.10643652e-03  -2.31950524e-03
  -1.53257395e-03  -7.45642672e-04   4.12886111e-05]
[1855 1902 1561 1435 1515 1650 1872 1981 2425 2029] [ -7.82802422e-03  -7.04109293e-03  -6.25416165e-03  -5.46723037e-03
  -4.68029909e-03  -3.89336780e-03  -3.10643652e-03  -2.31950524e-03
  -1.53257395e-03  -7.45642672e-04   4.12886111e-05]
-0.748095
0.752191
fine tuning ...
Epoch 0
Fine tuning took 0.077746 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079491 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078912 minutes
{0: [0.12315270935960591, 0.11083743842364532, 0.084975369458128072, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80172413793103448, 0.81157635467980294, 0.81650246305418717, 0.80295566502463056], 5: [0.075123152709359611, 0.077586206896551727, 0.098522167487684734, 0.076354679802955669], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379242 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.26444
Epoch 1, cost is  4.2307
Epoch 2, cost is  4.21344
Epoch 3, cost is  4.19965
Epoch 4, cost is  4.18795
Training took 0.111135 minutes
Weight histogram
[3590 3244 2358  922 1600 1596  882 2533 1292  208] [ -3.34999375e-02  -3.01451167e-02  -2.67902959e-02  -2.34354751e-02
  -2.00806542e-02  -1.67258334e-02  -1.33710126e-02  -1.00161918e-02
  -6.66137097e-03  -3.30655015e-03   4.82706637e-05]
[1335  974 1243 1365 1508 1679 2287 2362 2600 2872] [ -3.34999375e-02  -3.01451167e-02  -2.67902959e-02  -2.34354751e-02
  -2.00806542e-02  -1.67258334e-02  -1.33710126e-02  -1.00161918e-02
  -6.66137097e-03  -3.30655015e-03   4.82706637e-05]
-2.21303
2.30094
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.01662
Epoch 1, cost is  3.96464
Epoch 2, cost is  3.92495
Epoch 3, cost is  3.90691
Epoch 4, cost is  3.88621
Training took 0.070681 minutes
Weight histogram
[8759 1936 2509  887 1171 1061 1295  344 1878  410] [-0.04633591 -0.04168502 -0.03703413 -0.03238325 -0.02773236 -0.02308148
 -0.01843059 -0.01377971 -0.00912882 -0.00447794  0.00017295]
[2473 1200  685  826 1237 1862 2086 3163 3344 3374] [-0.04633591 -0.04168502 -0.03703413 -0.03238325 -0.02773236 -0.02308148
 -0.01843059 -0.01377971 -0.00912882 -0.00447794  0.00017295]
-1.16094
1.37743
fine tuning ...
Epoch 0
Fine tuning took 0.050814 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052241 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051202 minutes
{0: [0.01600985221674877, 0.017241379310344827, 0.02832512315270936, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9568965517241379, 0.96059113300492616, 0.95197044334975367, 0.96551724137931039], 5: [0.027093596059113302, 0.022167487684729065, 0.019704433497536946, 0.017241379310344827], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379103 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.26444
Epoch 1, cost is  4.2307
Epoch 2, cost is  4.21344
Epoch 3, cost is  4.19965
Epoch 4, cost is  4.18795
Training took 0.108429 minutes
Weight histogram
[3590 3244 2358  922 1600 1596  882 2533 1292  208] [ -3.34999375e-02  -3.01451167e-02  -2.67902959e-02  -2.34354751e-02
  -2.00806542e-02  -1.67258334e-02  -1.33710126e-02  -1.00161918e-02
  -6.66137097e-03  -3.30655015e-03   4.82706637e-05]
[1335  974 1243 1365 1508 1679 2287 2362 2600 2872] [ -3.34999375e-02  -3.01451167e-02  -2.67902959e-02  -2.34354751e-02
  -2.00806542e-02  -1.67258334e-02  -1.33710126e-02  -1.00161918e-02
  -6.66137097e-03  -3.30655015e-03   4.82706637e-05]
-2.21303
2.30094
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.32399
Epoch 1, cost is  3.27928
Epoch 2, cost is  3.25668
Epoch 3, cost is  3.24076
Epoch 4, cost is  3.20879
Training took 0.085039 minutes
Weight histogram
[9945 2555 1790 1403  551  661 1215 1692  307  131] [-0.02286851 -0.02055406 -0.01823961 -0.01592516 -0.01361071 -0.01129626
 -0.00898181 -0.00666736 -0.00435291 -0.00203846  0.00027599]
[1968 1066 1180 1223 1912 2212 2941 2266 2392 3090] [-0.02286851 -0.02055406 -0.01823961 -0.01592516 -0.01361071 -0.01129626
 -0.00898181 -0.00666736 -0.00435291 -0.00203846  0.00027599]
-0.936339
0.888983
fine tuning ...
Epoch 0
Fine tuning took 0.052621 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052917 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053753 minutes
{0: [0.029556650246305417, 0.038177339901477834, 0.019704433497536946, 0.034482758620689655], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93842364532019706, 0.92610837438423643, 0.94950738916256161, 0.94334975369458129], 5: [0.032019704433497539, 0.035714285714285712, 0.030788177339901478, 0.022167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378604 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.26444
Epoch 1, cost is  4.2307
Epoch 2, cost is  4.21344
Epoch 3, cost is  4.19965
Epoch 4, cost is  4.18795
Training took 0.110294 minutes
Weight histogram
[3590 3244 2358  922 1600 1596  882 2533 1292  208] [ -3.34999375e-02  -3.01451167e-02  -2.67902959e-02  -2.34354751e-02
  -2.00806542e-02  -1.67258334e-02  -1.33710126e-02  -1.00161918e-02
  -6.66137097e-03  -3.30655015e-03   4.82706637e-05]
[1335  974 1243 1365 1508 1679 2287 2362 2600 2872] [ -3.34999375e-02  -3.01451167e-02  -2.67902959e-02  -2.34354751e-02
  -2.00806542e-02  -1.67258334e-02  -1.33710126e-02  -1.00161918e-02
  -6.66137097e-03  -3.30655015e-03   4.82706637e-05]
-2.21303
2.30094
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.9328
Epoch 1, cost is  2.88651
Epoch 2, cost is  2.85485
Epoch 3, cost is  2.83048
Epoch 4, cost is  2.80966
Training took 0.111002 minutes
Weight histogram
[6638 4171 3955 1403 1037 1516  578  713  180   59] [ -1.33161880e-02  -1.19795448e-02  -1.06429016e-02  -9.30625840e-03
  -7.96961520e-03  -6.63297200e-03  -5.29632879e-03  -3.95968559e-03
  -2.62304239e-03  -1.28639919e-03   5.02440162e-05]
[1662  961 1267 1938 2226 1962 2130 2248 2739 3117] [ -1.33161880e-02  -1.19795448e-02  -1.06429016e-02  -9.30625840e-03
  -7.96961520e-03  -6.63297200e-03  -5.29632879e-03  -3.95968559e-03
  -2.62304239e-03  -1.28639919e-03   5.02440162e-05]
-0.633735
0.619724
fine tuning ...
Epoch 0
Fine tuning took 0.055606 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056827 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055955 minutes
{0: [0.035714285714285712, 0.04064039408866995, 0.044334975369458129, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93842364532019706, 0.9285714285714286, 0.92733990147783252, 0.92733990147783252], 5: [0.025862068965517241, 0.030788177339901478, 0.02832512315270936, 0.033251231527093597], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379095 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.47179
Epoch 1, cost is  3.43645
Epoch 2, cost is  3.41017
Epoch 3, cost is  3.39118
Epoch 4, cost is  3.37505
Training took 0.153018 minutes
Weight histogram
[5205 2041 2229 1380 1194 1872  888 1890 1469   57] [ -1.68181043e-02  -1.51323949e-02  -1.34466855e-02  -1.17609762e-02
  -1.00752668e-02  -8.38955740e-03  -6.70384801e-03  -5.01813863e-03
  -3.33242925e-03  -1.64671987e-03   3.89895176e-05]
[1218 1051 1215 1371 1578 1854 2238 2353 2511 2836] [ -1.68181043e-02  -1.51323949e-02  -1.34466855e-02  -1.17609762e-02
  -1.00752668e-02  -8.38955740e-03  -6.70384801e-03  -5.01813863e-03
  -3.33242925e-03  -1.64671987e-03   3.89895176e-05]
-1.42251
1.47288
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.27852
Epoch 1, cost is  4.23121
Epoch 2, cost is  4.21117
Epoch 3, cost is  4.19322
Epoch 4, cost is  4.18069
Training took 0.084362 minutes
Weight histogram
[9852 1730 1565  965 1128  716  464  739 1819 1272] [ -3.17322090e-02  -2.85550892e-02  -2.53779693e-02  -2.22008495e-02
  -1.90237296e-02  -1.58466098e-02  -1.26694899e-02  -9.49237005e-03
  -6.31525019e-03  -3.13813034e-03   3.89895176e-05]
[2743  810  994 1489 2083 1923 2395 2355 2605 2853] [ -3.17322090e-02  -2.85550892e-02  -2.53779693e-02  -2.22008495e-02
  -1.90237296e-02  -1.58466098e-02  -1.26694899e-02  -9.49237005e-03
  -6.31525019e-03  -3.13813034e-03   3.89895176e-05]
-1.27865
1.40825
fine tuning ...
Epoch 0
Fine tuning took 0.058949 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059883 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060862 minutes
{0: [0.033251231527093597, 0.039408866995073892, 0.043103448275862072, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.92980295566502458, 0.91502463054187189, 0.92364532019704437, 0.91009852216748766], 5: [0.036945812807881777, 0.045566502463054187, 0.033251231527093597, 0.041871921182266007], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380234 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.47179
Epoch 1, cost is  3.43645
Epoch 2, cost is  3.41017
Epoch 3, cost is  3.39118
Epoch 4, cost is  3.37505
Training took 0.151150 minutes
Weight histogram
[5205 2041 2229 1380 1194 1872  888 1890 1469   57] [ -1.68181043e-02  -1.51323949e-02  -1.34466855e-02  -1.17609762e-02
  -1.00752668e-02  -8.38955740e-03  -6.70384801e-03  -5.01813863e-03
  -3.33242925e-03  -1.64671987e-03   3.89895176e-05]
[1218 1051 1215 1371 1578 1854 2238 2353 2511 2836] [ -1.68181043e-02  -1.51323949e-02  -1.34466855e-02  -1.17609762e-02
  -1.00752668e-02  -8.38955740e-03  -6.70384801e-03  -5.01813863e-03
  -3.33242925e-03  -1.64671987e-03   3.89895176e-05]
-1.42251
1.47288
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.32914
Epoch 1, cost is  3.28379
Epoch 2, cost is  3.25703
Epoch 3, cost is  3.22922
Epoch 4, cost is  3.21193
Training took 0.111033 minutes
Weight histogram
[6060 4814 2810 1067 1163  506  704 1428 1600   98] [ -1.72369592e-02  -1.55093643e-02  -1.37817694e-02  -1.20541746e-02
  -1.03265797e-02  -8.59898484e-03  -6.87138997e-03  -5.14379509e-03
  -3.41620022e-03  -1.68860535e-03   3.89895176e-05]
[2051 1579 1596 1812 1699 1902 2200 2274 2378 2759] [ -1.72369592e-02  -1.55093643e-02  -1.37817694e-02  -1.20541746e-02
  -1.03265797e-02  -8.59898484e-03  -6.87138997e-03  -5.14379509e-03
  -3.41620022e-03  -1.68860535e-03   3.89895176e-05]
-1.00804
1.08687
fine tuning ...
Epoch 0
Fine tuning took 0.063258 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063685 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063778 minutes
{0: [0.083743842364532015, 0.064039408866995079, 0.068965517241379309, 0.078817733990147784], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86576354679802958, 0.86576354679802958, 0.86699507389162567, 0.86330049261083741], 5: [0.050492610837438424, 0.070197044334975367, 0.064039408866995079, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379311 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.47179
Epoch 1, cost is  3.43645
Epoch 2, cost is  3.41017
Epoch 3, cost is  3.39118
Epoch 4, cost is  3.37505
Training took 0.152914 minutes
Weight histogram
[5205 2041 2229 1380 1194 1872  888 1890 1469   57] [ -1.68181043e-02  -1.51323949e-02  -1.34466855e-02  -1.17609762e-02
  -1.00752668e-02  -8.38955740e-03  -6.70384801e-03  -5.01813863e-03
  -3.33242925e-03  -1.64671987e-03   3.89895176e-05]
[1218 1051 1215 1371 1578 1854 2238 2353 2511 2836] [ -1.68181043e-02  -1.51323949e-02  -1.34466855e-02  -1.17609762e-02
  -1.00752668e-02  -8.38955740e-03  -6.70384801e-03  -5.01813863e-03
  -3.33242925e-03  -1.64671987e-03   3.89895176e-05]
-1.42251
1.47288
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.83778
Epoch 1, cost is  2.79402
Epoch 2, cost is  2.77262
Epoch 3, cost is  2.75323
Epoch 4, cost is  2.74119
Training took 0.151094 minutes
Weight histogram
[5835 3981 2213 2265 1801  632 1740 1620  116   47] [ -1.02075478e-02  -9.18184520e-03  -8.15614259e-03  -7.13043998e-03
  -6.10473737e-03  -5.07903476e-03  -4.05333215e-03  -3.02762954e-03
  -2.00192693e-03  -9.76224319e-04   4.94782907e-05]
[1715 1555 2326 1576 1656 1850 2041 2346 2462 2723] [ -1.02075478e-02  -9.18184520e-03  -8.15614259e-03  -7.13043998e-03
  -6.10473737e-03  -5.07903476e-03  -4.05333215e-03  -3.02762954e-03
  -2.00192693e-03  -9.76224319e-04   4.94782907e-05]
-0.764731
0.807051
fine tuning ...
Epoch 0
Fine tuning took 0.067617 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068462 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068840 minutes
{0: [0.068965517241379309, 0.083743842364532015, 0.082512315270935957, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8571428571428571, 0.83251231527093594, 0.84852216748768472, 0.85467980295566504], 5: [0.073891625615763554, 0.083743842364532015, 0.068965517241379309, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379211 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.86789
Epoch 1, cost is  2.82647
Epoch 2, cost is  2.80404
Epoch 3, cost is  2.77406
Epoch 4, cost is  2.75508
Training took 0.206719 minutes
Weight histogram
[4004 3643 1952  508 1988 1950  348 1992 1812   28] [ -1.01341149e-02  -9.11657453e-03  -8.09903418e-03  -7.08149384e-03
  -6.06395349e-03  -5.04641314e-03  -4.02887279e-03  -3.01133244e-03
  -1.99379209e-03  -9.76251738e-04   4.12886111e-05]
[1186 1017 1189 1504 1755 1952 2209 2456 2411 2546] [ -1.01341149e-02  -9.11657453e-03  -8.09903418e-03  -7.08149384e-03
  -6.06395349e-03  -5.04641314e-03  -4.02887279e-03  -3.01133244e-03
  -1.99379209e-03  -9.76251738e-04   4.12886111e-05]
-0.94751
1.02001
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.60076
Epoch 1, cost is  4.56902
Epoch 2, cost is  4.54824
Epoch 3, cost is  4.53885
Epoch 4, cost is  4.52953
Training took 0.108538 minutes
Weight histogram
[7523 2570 2157 1672  871 1090  445  751 1065 2106] [ -2.27466021e-02  -2.04678130e-02  -1.81890239e-02  -1.59102349e-02
  -1.36314458e-02  -1.13526567e-02  -9.07386766e-03  -6.79507859e-03
  -4.51628953e-03  -2.23750046e-03   4.12886111e-05]
[2994  830 1297 1587 1705 1733 2361 2375 2525 2843] [ -2.27466021e-02  -2.04678130e-02  -1.81890239e-02  -1.59102349e-02
  -1.36314458e-02  -1.13526567e-02  -9.07386766e-03  -6.79507859e-03
  -4.51628953e-03  -2.23750046e-03   4.12886111e-05]
-1.62608
1.57443
fine tuning ...
Epoch 0
Fine tuning took 0.069563 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068018 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069898 minutes
{0: [0.044334975369458129, 0.054187192118226604, 0.054187192118226604, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91009852216748766, 0.91009852216748766, 0.90147783251231528, 0.91502463054187189], 5: [0.045566502463054187, 0.035714285714285712, 0.044334975369458129, 0.04064039408866995], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380893 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.86789
Epoch 1, cost is  2.82647
Epoch 2, cost is  2.80404
Epoch 3, cost is  2.77406
Epoch 4, cost is  2.75508
Training took 0.206056 minutes
Weight histogram
[4004 3643 1952  508 1988 1950  348 1992 1812   28] [ -1.01341149e-02  -9.11657453e-03  -8.09903418e-03  -7.08149384e-03
  -6.06395349e-03  -5.04641314e-03  -4.02887279e-03  -3.01133244e-03
  -1.99379209e-03  -9.76251738e-04   4.12886111e-05]
[1186 1017 1189 1504 1755 1952 2209 2456 2411 2546] [ -1.01341149e-02  -9.11657453e-03  -8.09903418e-03  -7.08149384e-03
  -6.06395349e-03  -5.04641314e-03  -4.02887279e-03  -3.01133244e-03
  -1.99379209e-03  -9.76251738e-04   4.12886111e-05]
-0.94751
1.02001
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.7855
Epoch 1, cost is  3.74519
Epoch 2, cost is  3.71987
Epoch 3, cost is  3.70331
Epoch 4, cost is  3.68835
Training took 0.150913 minutes
Weight histogram
[3275 6307 2085 1794 1592 1106  743  804 2460   84] [ -1.34962201e-02  -1.21424692e-02  -1.07887184e-02  -9.43496749e-03
  -8.08121662e-03  -6.72746575e-03  -5.37371488e-03  -4.01996401e-03
  -2.66621313e-03  -1.31246226e-03   4.12886111e-05]
[2481 1650 1481 1481 1695 1937 2168 2220 2348 2789] [ -1.34962201e-02  -1.21424692e-02  -1.07887184e-02  -9.43496749e-03
  -8.08121662e-03  -6.72746575e-03  -5.37371488e-03  -4.01996401e-03
  -2.66621313e-03  -1.31246226e-03   4.12886111e-05]
-1.23502
1.08091
fine tuning ...
Epoch 0
Fine tuning took 0.073904 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073334 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073323 minutes
{0: [0.077586206896551727, 0.089901477832512317, 0.083743842364532015, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.85098522167487689, 0.85591133004926112, 0.82389162561576357, 0.83004926108374388], 5: [0.071428571428571425, 0.054187192118226604, 0.092364532019704432, 0.068965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379797 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3424 4415 2875  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 165  279  456  641  687  579 1548 2765 6347 4758] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.03315
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.86789
Epoch 1, cost is  2.82647
Epoch 2, cost is  2.80404
Epoch 3, cost is  2.77406
Epoch 4, cost is  2.75508
Training took 0.205866 minutes
Weight histogram
[4004 3643 1952  508 1988 1950  348 1992 1812   28] [ -1.01341149e-02  -9.11657453e-03  -8.09903418e-03  -7.08149384e-03
  -6.06395349e-03  -5.04641314e-03  -4.02887279e-03  -3.01133244e-03
  -1.99379209e-03  -9.76251738e-04   4.12886111e-05]
[1186 1017 1189 1504 1755 1952 2209 2456 2411 2546] [ -1.01341149e-02  -9.11657453e-03  -8.09903418e-03  -7.08149384e-03
  -6.06395349e-03  -5.04641314e-03  -4.02887279e-03  -3.01133244e-03
  -1.99379209e-03  -9.76251738e-04   4.12886111e-05]
-0.94751
1.02001
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.25992
Epoch 1, cost is  3.21895
Epoch 2, cost is  3.20091
Epoch 3, cost is  3.18132
Epoch 4, cost is  3.16526
Training took 0.207716 minutes
Weight histogram
[5153 3075 2548 2055 1395 1944 1018 2913  115   34] [ -8.10693949e-03  -7.29211668e-03  -6.47729387e-03  -5.66247106e-03
  -4.84764825e-03  -4.03282544e-03  -3.21800263e-03  -2.40317982e-03
  -1.58835701e-03  -7.73534199e-04   4.12886111e-05]
[1991 2141 1527 1597 1694 1919 2109 2457 2320 2495] [ -8.10693949e-03  -7.29211668e-03  -6.47729387e-03  -5.66247106e-03
  -4.84764825e-03  -4.03282544e-03  -3.21800263e-03  -2.40317982e-03
  -1.58835701e-03  -7.73534199e-04   4.12886111e-05]
-0.821056
0.824864
fine tuning ...
Epoch 0
Fine tuning took 0.078784 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079024 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078351 minutes
{0: [0.086206896551724144, 0.10714285714285714, 0.10837438423645321, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8214285714285714, 0.82389162561576357, 0.78940886699507384, 0.80418719211822665], 5: [0.092364532019704432, 0.068965517241379309, 0.10221674876847291, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378953 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.26701
Epoch 1, cost is  4.23386
Epoch 2, cost is  4.21451
Epoch 3, cost is  4.20377
Epoch 4, cost is  4.19015
Training took 0.108608 minutes
Weight histogram
[3585 3372 2782 2208 1373 1083 1741 2025 1847  234] [ -3.61285508e-02  -3.25108686e-02  -2.88931865e-02  -2.52755043e-02
  -2.16578222e-02  -1.80401401e-02  -1.44224579e-02  -1.08047758e-02
  -7.18709362e-03  -3.56941148e-03   4.82706637e-05]
[1397 1071 1357 1493 1657 2081 2429 2647 2975 3143] [ -3.61285508e-02  -3.25108686e-02  -2.88931865e-02  -2.52755043e-02
  -2.16578222e-02  -1.80401401e-02  -1.44224579e-02  -1.08047758e-02
  -7.18709362e-03  -3.56941148e-03   4.82706637e-05]
-2.31136
2.31165
training layer 2, rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.0355
Epoch 1, cost is  3.99516
Epoch 2, cost is  3.97931
Epoch 3, cost is  3.95479
Epoch 4, cost is  3.93553
Training took 0.072130 minutes
Weight histogram
[10784  1936  2509   887  1171  1061  1295   344  1878   410] [-0.04633591 -0.04168502 -0.03703413 -0.03238325 -0.02773236 -0.02308148
 -0.01843059 -0.01377971 -0.00912882 -0.00447794  0.00017295]
[2564 1187  764  921 1576 1996 2957 3367 3537 3406] [-0.04633591 -0.04168502 -0.03703413 -0.03238325 -0.02773236 -0.02308148
 -0.01843059 -0.01377971 -0.00912882 -0.00447794  0.00017295]
-1.26827
1.39187
fine tuning ...
Epoch 0
Fine tuning took 0.053509 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051232 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053207 minutes
{0: [0.014778325123152709, 0.024630541871921183, 0.017241379310344827, 0.012315270935960592], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96798029556650245, 0.94950738916256161, 0.9642857142857143, 0.97536945812807885], 5: [0.017241379310344827, 0.025862068965517241, 0.018472906403940888, 0.012315270935960592], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378724 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.26701
Epoch 1, cost is  4.23386
Epoch 2, cost is  4.21451
Epoch 3, cost is  4.20377
Epoch 4, cost is  4.19015
Training took 0.110244 minutes
Weight histogram
[3585 3372 2782 2208 1373 1083 1741 2025 1847  234] [ -3.61285508e-02  -3.25108686e-02  -2.88931865e-02  -2.52755043e-02
  -2.16578222e-02  -1.80401401e-02  -1.44224579e-02  -1.08047758e-02
  -7.18709362e-03  -3.56941148e-03   4.82706637e-05]
[1397 1071 1357 1493 1657 2081 2429 2647 2975 3143] [ -3.61285508e-02  -3.25108686e-02  -2.88931865e-02  -2.52755043e-02
  -2.16578222e-02  -1.80401401e-02  -1.44224579e-02  -1.08047758e-02
  -7.18709362e-03  -3.56941148e-03   4.82706637e-05]
-2.31136
2.31165
training layer 2, rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.27996
Epoch 1, cost is  3.23041
Epoch 2, cost is  3.20804
Epoch 3, cost is  3.18719
Epoch 4, cost is  3.17489
Training took 0.086210 minutes
Weight histogram
[11865  2492  1846  1496   534   668  1222  1700   314   138] [-0.02305641 -0.02072317 -0.01838993 -0.01605669 -0.01372345 -0.01139021
 -0.00905697 -0.00672373 -0.00439049 -0.00205725  0.00027599]
[2031 1135 1221 1470 2045 2819 2749 2403 2847 3555] [-0.02305641 -0.02072317 -0.01838993 -0.01605669 -0.01372345 -0.01139021
 -0.00905697 -0.00672373 -0.00439049 -0.00205725  0.00027599]
-1.06283
0.942768
fine tuning ...
Epoch 0
Fine tuning took 0.052511 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053581 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053699 minutes
{0: [0.030788177339901478, 0.043103448275862072, 0.022167487684729065, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.95443349753694584, 0.9285714285714286, 0.95812807881773399, 0.9568965517241379], 5: [0.014778325123152709, 0.02832512315270936, 0.019704433497536946, 0.020935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381025 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.26701
Epoch 1, cost is  4.23386
Epoch 2, cost is  4.21451
Epoch 3, cost is  4.20377
Epoch 4, cost is  4.19015
Training took 0.106587 minutes
Weight histogram
[3585 3372 2782 2208 1373 1083 1741 2025 1847  234] [ -3.61285508e-02  -3.25108686e-02  -2.88931865e-02  -2.52755043e-02
  -2.16578222e-02  -1.80401401e-02  -1.44224579e-02  -1.08047758e-02
  -7.18709362e-03  -3.56941148e-03   4.82706637e-05]
[1397 1071 1357 1493 1657 2081 2429 2647 2975 3143] [ -3.61285508e-02  -3.25108686e-02  -2.88931865e-02  -2.52755043e-02
  -2.16578222e-02  -1.80401401e-02  -1.44224579e-02  -1.08047758e-02
  -7.18709362e-03  -3.56941148e-03   4.82706637e-05]
-2.31136
2.31165
training layer 2, rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.77917
Epoch 1, cost is  2.74877
Epoch 2, cost is  2.71478
Epoch 3, cost is  2.70187
Epoch 4, cost is  2.67202
Training took 0.107384 minutes
Weight histogram
[6881 5326 3989 1977  908 1622  608  715  188   61] [ -1.36777125e-02  -1.23049169e-02  -1.09321212e-02  -9.55932557e-03
  -8.18652991e-03  -6.81373426e-03  -5.44093860e-03  -4.06814295e-03
  -2.69534729e-03  -1.32255164e-03   5.02440162e-05]
[1720 1026 1458 2264 2221 2173 2205 2682 3009 3517] [ -1.36777125e-02  -1.23049169e-02  -1.09321212e-02  -9.55932557e-03
  -8.18652991e-03  -6.81373426e-03  -5.44093860e-03  -4.06814295e-03
  -2.69534729e-03  -1.32255164e-03   5.02440162e-05]
-0.678044
0.677069
fine tuning ...
Epoch 0
Fine tuning took 0.055751 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057189 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056612 minutes
{0: [0.023399014778325122, 0.02832512315270936, 0.030788177339901478, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.95566502463054193, 0.94334975369458129, 0.94211822660098521, 0.95812807881773399], 5: [0.020935960591133004, 0.02832512315270936, 0.027093596059113302, 0.019704433497536946], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378891 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.38944
Epoch 1, cost is  3.35292
Epoch 2, cost is  3.32856
Epoch 3, cost is  3.31218
Epoch 4, cost is  3.29741
Training took 0.152532 minutes
Weight histogram
[4914 3088 2026 2085 1846 1560  665 2370 1631   65] [ -1.79089475e-02  -1.61141538e-02  -1.43193601e-02  -1.25245664e-02
  -1.07297727e-02  -8.93497901e-03  -7.14018531e-03  -5.34539160e-03
  -3.55059789e-03  -1.75580419e-03   3.89895176e-05]
[1279 1155 1307 1516 1771 2154 2417 2634 2851 3166] [ -1.79089475e-02  -1.61141538e-02  -1.43193601e-02  -1.25245664e-02
  -1.07297727e-02  -8.93497901e-03  -7.14018531e-03  -5.34539160e-03
  -3.55059789e-03  -1.75580419e-03   3.89895176e-05]
-1.56364
1.5518
training layer 2, rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.26408
Epoch 1, cost is  4.23218
Epoch 2, cost is  4.21689
Epoch 3, cost is  4.20445
Epoch 4, cost is  4.19513
Training took 0.085516 minutes
Weight histogram
[11877  1730  1565   965  1128   716   464   739  1819  1272] [ -3.17322090e-02  -2.85550892e-02  -2.53779693e-02  -2.22008495e-02
  -1.90237296e-02  -1.58466098e-02  -1.26694899e-02  -9.49237005e-03
  -6.31525019e-03  -3.13813034e-03   3.89895176e-05]
[2903  745 1204 1807 2167 2289 2445 2732 2931 3052] [ -3.17322090e-02  -2.85550892e-02  -2.53779693e-02  -2.22008495e-02
  -1.90237296e-02  -1.58466098e-02  -1.26694899e-02  -9.49237005e-03
  -6.31525019e-03  -3.13813034e-03   3.89895176e-05]
-1.39426
1.51831
fine tuning ...
Epoch 0
Fine tuning took 0.060024 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059304 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059832 minutes
{0: [0.059113300492610835, 0.048029556650246302, 0.059113300492610835, 0.038177339901477834], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88916256157635465, 0.89162561576354682, 0.90517241379310343, 0.91379310344827591], 5: [0.051724137931034482, 0.060344827586206899, 0.035714285714285712, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378940 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.38944
Epoch 1, cost is  3.35292
Epoch 2, cost is  3.32856
Epoch 3, cost is  3.31218
Epoch 4, cost is  3.29741
Training took 0.152365 minutes
Weight histogram
[4914 3088 2026 2085 1846 1560  665 2370 1631   65] [ -1.79089475e-02  -1.61141538e-02  -1.43193601e-02  -1.25245664e-02
  -1.07297727e-02  -8.93497901e-03  -7.14018531e-03  -5.34539160e-03
  -3.55059789e-03  -1.75580419e-03   3.89895176e-05]
[1279 1155 1307 1516 1771 2154 2417 2634 2851 3166] [ -1.79089475e-02  -1.61141538e-02  -1.43193601e-02  -1.25245664e-02
  -1.07297727e-02  -8.93497901e-03  -7.14018531e-03  -5.34539160e-03
  -3.55059789e-03  -1.75580419e-03   3.89895176e-05]
-1.56364
1.5518
training layer 2, rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.25058
Epoch 1, cost is  3.216
Epoch 2, cost is  3.19279
Epoch 3, cost is  3.17299
Epoch 4, cost is  3.15615
Training took 0.111116 minutes
Weight histogram
[7263 4801 3003 1381 1281  603  783 1335 1720  105] [ -1.79142319e-02  -1.61189097e-02  -1.43235876e-02  -1.25282655e-02
  -1.07329433e-02  -8.93762117e-03  -7.14229904e-03  -5.34697690e-03
  -3.55165476e-03  -1.75633262e-03   3.89895176e-05]
[2156 1635 1827 1876 1864 2178 2398 2502 2764 3075] [ -1.79142319e-02  -1.61189097e-02  -1.43235876e-02  -1.25282655e-02
  -1.07329433e-02  -8.93762117e-03  -7.14229904e-03  -5.34697690e-03
  -3.55165476e-03  -1.75633262e-03   3.89895176e-05]
-1.09292
1.18626
fine tuning ...
Epoch 0
Fine tuning took 0.062646 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062854 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062905 minutes
{0: [0.092364532019704432, 0.094827586206896547, 0.075123152709359611, 0.071428571428571425], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84605911330049266, 0.82635467980295563, 0.83866995073891626, 0.87561576354679804], 5: [0.061576354679802957, 0.078817733990147784, 0.086206896551724144, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378732 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.38944
Epoch 1, cost is  3.35292
Epoch 2, cost is  3.32856
Epoch 3, cost is  3.31218
Epoch 4, cost is  3.29741
Training took 0.150683 minutes
Weight histogram
[4914 3088 2026 2085 1846 1560  665 2370 1631   65] [ -1.79089475e-02  -1.61141538e-02  -1.43193601e-02  -1.25245664e-02
  -1.07297727e-02  -8.93497901e-03  -7.14018531e-03  -5.34539160e-03
  -3.55059789e-03  -1.75580419e-03   3.89895176e-05]
[1279 1155 1307 1516 1771 2154 2417 2634 2851 3166] [ -1.79089475e-02  -1.61141538e-02  -1.43193601e-02  -1.25245664e-02
  -1.07297727e-02  -8.93497901e-03  -7.14018531e-03  -5.34539160e-03
  -3.55059789e-03  -1.75580419e-03   3.89895176e-05]
-1.56364
1.5518
training layer 2, rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.71603
Epoch 1, cost is  2.67703
Epoch 2, cost is  2.6544
Epoch 3, cost is  2.6377
Epoch 4, cost is  2.62151
Training took 0.154142 minutes
Weight histogram
[5227 4876 3149 2835 1693  654 1508 2147  135   51] [ -1.08056236e-02  -9.72011342e-03  -8.63460323e-03  -7.54909304e-03
  -6.46358285e-03  -5.37807266e-03  -4.29256247e-03  -3.20705228e-03
  -2.12154209e-03  -1.03603190e-03   4.94782907e-05]
[1789 1772 2320 1692 1814 2050 2242 2721 2660 3215] [ -1.08056236e-02  -9.72011342e-03  -8.63460323e-03  -7.54909304e-03
  -6.46358285e-03  -5.37807266e-03  -4.29256247e-03  -3.20705228e-03
  -2.12154209e-03  -1.03603190e-03   4.94782907e-05]
-0.814072
0.843638
fine tuning ...
Epoch 0
Fine tuning took 0.068417 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068077 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068401 minutes
{0: [0.083743842364532015, 0.10344827586206896, 0.11083743842364532, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.85344827586206895, 0.83497536945812811, 0.82758620689655171, 0.83743842364532017], 5: [0.062807881773399021, 0.061576354679802957, 0.061576354679802957, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378815 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.79224
Epoch 1, cost is  2.73848
Epoch 2, cost is  2.70983
Epoch 3, cost is  2.68668
Epoch 4, cost is  2.66696
Training took 0.207446 minutes
Weight histogram
[2071 4231 3661 2018 1758 1380 1055 2021 2020   35] [ -1.12308990e-02  -1.01036803e-02  -8.97646149e-03  -7.84924273e-03
  -6.72202397e-03  -5.59480520e-03  -4.46758644e-03  -3.34036768e-03
  -2.21314892e-03  -1.08593015e-03   4.12886111e-05]
[1254 1112 1306 1715 1935 2222 2423 2746 2624 2913] [ -1.12308990e-02  -1.01036803e-02  -8.97646149e-03  -7.84924273e-03
  -6.72202397e-03  -5.59480520e-03  -4.46758644e-03  -3.34036768e-03
  -2.21314892e-03  -1.08593015e-03   4.12886111e-05]
-1.01313
1.0753
training layer 2, rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.56555
Epoch 1, cost is  4.53162
Epoch 2, cost is  4.51818
Epoch 3, cost is  4.50616
Epoch 4, cost is  4.4971
Training took 0.110412 minutes
Weight histogram
[4683 6741 2057 2102  832 1303  566  716 1156 2119] [ -2.39467453e-02  -2.15479419e-02  -1.91491385e-02  -1.67503351e-02
  -1.43515317e-02  -1.19527284e-02  -9.55392496e-03  -7.15512157e-03
  -4.75631818e-03  -2.35751478e-03   4.12886111e-05]
[3042  943 1460 1763 1821 2174 2479 2707 2872 3014] [ -2.39467453e-02  -2.15479419e-02  -1.91491385e-02  -1.67503351e-02
  -1.43515317e-02  -1.19527284e-02  -9.55392496e-03  -7.15512157e-03
  -4.75631818e-03  -2.35751478e-03   4.12886111e-05]
-1.79993
1.7128
fine tuning ...
Epoch 0
Fine tuning took 0.069266 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068142 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068178 minutes
{0: [0.071428571428571425, 0.065270935960591137, 0.049261083743842367, 0.055418719211822662], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88793103448275867, 0.88423645320197042, 0.87931034482758619, 0.90024630541871919], 5: [0.04064039408866995, 0.050492610837438424, 0.071428571428571425, 0.044334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378849 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.79224
Epoch 1, cost is  2.73848
Epoch 2, cost is  2.70983
Epoch 3, cost is  2.68668
Epoch 4, cost is  2.66696
Training took 0.206792 minutes
Weight histogram
[2071 4231 3661 2018 1758 1380 1055 2021 2020   35] [ -1.12308990e-02  -1.01036803e-02  -8.97646149e-03  -7.84924273e-03
  -6.72202397e-03  -5.59480520e-03  -4.46758644e-03  -3.34036768e-03
  -2.21314892e-03  -1.08593015e-03   4.12886111e-05]
[1254 1112 1306 1715 1935 2222 2423 2746 2624 2913] [ -1.12308990e-02  -1.01036803e-02  -8.97646149e-03  -7.84924273e-03
  -6.72202397e-03  -5.59480520e-03  -4.46758644e-03  -3.34036768e-03
  -2.21314892e-03  -1.08593015e-03   4.12886111e-05]
-1.01313
1.0753
training layer 2, rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.68725
Epoch 1, cost is  3.65307
Epoch 2, cost is  3.63308
Epoch 3, cost is  3.61873
Epoch 4, cost is  3.60556
Training took 0.152597 minutes
Weight histogram
[4118 6127 3014 1803 1150 1920  696  785 2568   94] [ -1.40166720e-02  -1.26108760e-02  -1.12050799e-02  -9.79928384e-03
  -8.39348778e-03  -6.98769171e-03  -5.58189565e-03  -4.17609958e-03
  -2.77030352e-03  -1.36450745e-03   4.12886111e-05]
[2627 1728 1558 1616 1864 2220 2301 2466 2735 3160] [ -1.40166720e-02  -1.26108760e-02  -1.12050799e-02  -9.79928384e-03
  -8.39348778e-03  -6.98769171e-03  -5.58189565e-03  -4.17609958e-03
  -2.77030352e-03  -1.36450745e-03   4.12886111e-05]
-1.304
1.16009
fine tuning ...
Epoch 0
Fine tuning took 0.073855 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074311 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074488 minutes
{0: [0.10221674876847291, 0.11822660098522167, 0.12438423645320197, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8288177339901478, 0.81157635467980294, 0.77586206896551724, 0.81650246305418717], 5: [0.068965517241379309, 0.070197044334975367, 0.099753694581280791, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381593 minutes
Weight histogram
[   6   25  410 1043 2593 3070 3740 5687 3312  364] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
[ 166  281  459  648  684  587 1651 2705 6632 6437] [ -4.69115912e-04  -3.35560011e-04  -2.02004111e-04  -6.84482104e-05
   6.51076902e-05   1.98663591e-04   3.32219491e-04   4.65775392e-04
   5.99331292e-04   7.32887193e-04   8.66443093e-04]
-1.45479
1.09328
training layer 1, rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.79224
Epoch 1, cost is  2.73848
Epoch 2, cost is  2.70983
Epoch 3, cost is  2.68668
Epoch 4, cost is  2.66696
Training took 0.206147 minutes
Weight histogram
[2071 4231 3661 2018 1758 1380 1055 2021 2020   35] [ -1.12308990e-02  -1.01036803e-02  -8.97646149e-03  -7.84924273e-03
  -6.72202397e-03  -5.59480520e-03  -4.46758644e-03  -3.34036768e-03
  -2.21314892e-03  -1.08593015e-03   4.12886111e-05]
[1254 1112 1306 1715 1935 2222 2423 2746 2624 2913] [ -1.12308990e-02  -1.01036803e-02  -8.97646149e-03  -7.84924273e-03
  -6.72202397e-03  -5.59480520e-03  -4.46758644e-03  -3.34036768e-03
  -2.21314892e-03  -1.08593015e-03   4.12886111e-05]
-1.01313
1.0753
training layer 2, rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.16186
Epoch 1, cost is  3.11847
Epoch 2, cost is  3.09935
Epoch 3, cost is  3.08351
Epoch 4, cost is  3.06513
Training took 0.207766 minutes
Weight histogram
[4557 4470 2853 2197 2068 1948  951 2561  631   39] [ -8.57590325e-03  -7.71418407e-03  -6.85246488e-03  -5.99074569e-03
  -5.12902651e-03  -4.26730732e-03  -3.40558813e-03  -2.54386895e-03
  -1.68214976e-03  -8.20430575e-04   4.12886111e-05]
[2124 2332 1535 1745 1880 2161 2324 2740 2542 2892] [ -8.57590325e-03  -7.71418407e-03  -6.85246488e-03  -5.99074569e-03
  -5.12902651e-03  -4.26730732e-03  -3.40558813e-03  -2.54386895e-03
  -1.68214976e-03  -8.20430575e-04   4.12886111e-05]
-0.913484
0.869322
fine tuning ...
Epoch 0
Fine tuning took 0.078083 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079393 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079153 minutes
{0: [0.1354679802955665, 0.11330049261083744, 0.11083743842364532, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.78940886699507384, 0.75862068965517238, 0.78694581280788178], 5: [0.080049261083743842, 0.097290640394088676, 0.13054187192118227, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}
