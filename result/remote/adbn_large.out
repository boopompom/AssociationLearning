Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentADBN6
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237704 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.26309
Epoch 1, cost is  2.18488
Epoch 2, cost is  2.00589
Epoch 3, cost is  1.91505
Epoch 4, cost is  1.85911
Training took 0.152447 minutes
Weight histogram
[855 467 289 143  87  65  33  28  25  33] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
[ 57  49  65  93 124 176 224 317 406 514] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
-2.52514
2.67599
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239355 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.30432
Epoch 1, cost is  2.12289
Epoch 2, cost is  1.95189
Epoch 3, cost is  1.86735
Epoch 4, cost is  1.80936
Training took 0.151634 minutes
Weight histogram
[1687  978  548  288  168  117   81   59   58   66] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
[125  96 128 183 251 340 468 656 860 943] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
-2.52514
2.97284
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  3.50695
Epoch 1, cost is  1.98688
Epoch 2, cost is  1.71178
Epoch 3, cost is  1.57435
Epoch 4, cost is  1.49239
Training took 0.089811 minutes
Weight histogram
[584 399 309 231 150  96  73  50  65  68] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[126  87  92 118 138 176 224 277 346 441] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.5425
3.84016
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081400 minutes
Epoch 0
Fine tuning took 0.081950 minutes
Epoch 0
Fine tuning took 0.081480 minutes
{'zero': {0: [0.077586206896551727, 0.16625615763546797, 0.19334975369458129, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84359605911330049, 0.6219211822660099, 0.63916256157635465, 0.65394088669950734], 5: [0.078817733990147784, 0.21182266009852216, 0.16748768472906403, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.077586206896551727, 0.049261083743842367, 0.04064039408866995, 0.038177339901477834], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84359605911330049, 0.86822660098522164, 0.90147783251231528, 0.89039408866995073], 5: [0.078817733990147784, 0.082512315270935957, 0.057881773399014777, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.077586206896551727, 0.057881773399014777, 0.04064039408866995, 0.049261083743842367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84359605911330049, 0.84482758620689657, 0.86083743842364535, 0.87068965517241381], 5: [0.078817733990147784, 0.097290640394088676, 0.098522167487684734, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.077586206896551727, 0.04064039408866995, 0.022167487684729065, 0.024630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84359605911330049, 0.90886699507389157, 0.9211822660098522, 0.91871921182266014], 5: [0.078817733990147784, 0.050492610837438424, 0.056650246305418719, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237974 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.26309
Epoch 1, cost is  2.18488
Epoch 2, cost is  2.00589
Epoch 3, cost is  1.91505
Epoch 4, cost is  1.85911
Training took 0.151818 minutes
Weight histogram
[855 467 289 143  87  65  33  28  25  33] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
[ 57  49  65  93 124 176 224 317 406 514] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
-2.52514
2.67599
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.236364 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.30432
Epoch 1, cost is  2.12289
Epoch 2, cost is  1.95189
Epoch 3, cost is  1.86735
Epoch 4, cost is  1.80936
Training took 0.151521 minutes
Weight histogram
[1687  978  548  288  168  117   81   59   58   66] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
[125  96 128 183 251 340 468 656 860 943] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
-2.52514
2.97284
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.2861
Epoch 1, cost is  1.65688
Epoch 2, cost is  1.33452
Epoch 3, cost is  1.17516
Epoch 4, cost is  1.07965
Training took 0.107133 minutes
Weight histogram
[586 437 300 204 154 104  63  48  76  53] [-0.1542796  -0.1392472  -0.12421479 -0.10918239 -0.09414998 -0.07911757
 -0.06408517 -0.04905276 -0.03402036 -0.01898795 -0.00395554]
[130  84  88 112 144 183 224 280 349 431] [-0.1542796  -0.1392472  -0.12421479 -0.10918239 -0.09414998 -0.07911757
 -0.06408517 -0.04905276 -0.03402036 -0.01898795 -0.00395554]
-2.67468
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082453 minutes
Epoch 0
Fine tuning took 0.082867 minutes
Epoch 0
Fine tuning took 0.083664 minutes
{'zero': {0: [0.087438423645320201, 0.21305418719211822, 0.23029556650246305, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82019704433497542, 0.52093596059113301, 0.57758620689655171, 0.58497536945812811], 5: [0.092364532019704432, 0.26600985221674878, 0.19211822660098521, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.087438423645320201, 0.062807881773399021, 0.029556650246305417, 0.043103448275862072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82019704433497542, 0.82266009852216748, 0.86206896551724133, 0.86206896551724133], 5: [0.092364532019704432, 0.1145320197044335, 0.10837438423645321, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.087438423645320201, 0.077586206896551727, 0.046798029556650245, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82019704433497542, 0.77832512315270941, 0.84359605911330049, 0.81403940886699511], 5: [0.092364532019704432, 0.14408866995073891, 0.10960591133004927, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.087438423645320201, 0.071428571428571425, 0.014778325123152709, 0.032019704433497539], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82019704433497542, 0.80295566502463056, 0.90394088669950734, 0.88300492610837433], 5: [0.092364532019704432, 0.12561576354679804, 0.081280788177339899, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239938 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.26309
Epoch 1, cost is  2.18488
Epoch 2, cost is  2.00589
Epoch 3, cost is  1.91505
Epoch 4, cost is  1.85911
Training took 0.152515 minutes
Weight histogram
[855 467 289 143  87  65  33  28  25  33] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
[ 57  49  65  93 124 176 224 317 406 514] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
-2.52514
2.67599
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239559 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.30432
Epoch 1, cost is  2.12289
Epoch 2, cost is  1.95189
Epoch 3, cost is  1.86735
Epoch 4, cost is  1.80936
Training took 0.152836 minutes
Weight histogram
[1687  978  548  288  168  117   81   59   58   66] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
[125  96 128 183 251 340 468 656 860 943] [-0.08969781 -0.08107989 -0.07246198 -0.06384407 -0.05522615 -0.04660824
 -0.03799033 -0.02937241 -0.0207545  -0.01213658 -0.00351867]
-2.52514
2.97284
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.05889
Epoch 1, cost is  1.37896
Epoch 2, cost is  1.02373
Epoch 3, cost is  0.870714
Epoch 4, cost is  0.780974
Training took 0.157022 minutes
Weight histogram
[660 416 276 183 142 104  72  55  77  40] [-0.10313658 -0.09320314 -0.08326969 -0.07333624 -0.0634028  -0.05346935
 -0.0435359  -0.03360245 -0.02366901 -0.01373556 -0.00380211]
[137  72  84 110 138 174 217 283 358 452] [-0.10313658 -0.09320314 -0.08326969 -0.07333624 -0.0634028  -0.05346935
 -0.0435359  -0.03360245 -0.02366901 -0.01373556 -0.00380211]
-1.99174
2.92356
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089116 minutes
Epoch 0
Fine tuning took 0.089649 minutes
Epoch 0
Fine tuning took 0.088224 minutes
{'zero': {0: [0.14655172413793102, 0.24014778325123154, 0.31773399014778325, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.53325123152709364, 0.52216748768472909, 0.59482758620689657], 5: [0.11945812807881774, 0.22660098522167488, 0.16009852216748768, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14655172413793102, 0.26231527093596058, 0.27093596059113301, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.54679802955665024, 0.57512315270935965, 0.63054187192118227], 5: [0.11945812807881774, 0.19088669950738915, 0.1539408866995074, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14655172413793102, 0.24507389162561577, 0.25, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.56403940886699511, 0.59729064039408863, 0.65640394088669951], 5: [0.11945812807881774, 0.19088669950738915, 0.15270935960591134, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14655172413793102, 0.25738916256157635, 0.2857142857142857, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.53201970443349755, 0.55295566502463056, 0.65024630541871919], 5: [0.11945812807881774, 0.2105911330049261, 0.16133004926108374, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237789 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.78145
Epoch 1, cost is  4.01168
Epoch 2, cost is  3.19263
Epoch 3, cost is  2.80275
Epoch 4, cost is  2.57149
Training took 0.153561 minutes
Weight histogram
[479 305 272 181 177 142 115 135 206  13] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
[283 125 132 148 152 182 203 239 258 303] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
-0.773161
1.41476
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237409 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.91158
Epoch 1, cost is  4.19597
Epoch 2, cost is  3.27482
Epoch 3, cost is  2.8183
Epoch 4, cost is  2.54844
Training took 0.153203 minutes
Weight histogram
[841 625 519 384 365 299 256 278 456  27] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
[601 265 278 303 317 378 415 481 545 467] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
-0.773161
1.41476
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  6.41776
Epoch 1, cost is  5.57271
Epoch 2, cost is  4.65637
Epoch 3, cost is  3.97376
Epoch 4, cost is  3.51312
Training took 0.092271 minutes
Weight histogram
[220 172 189 151 162 134 168 202 469 158] [-0.06757683 -0.06085427 -0.05413171 -0.04740915 -0.04068659 -0.03396403
 -0.02724148 -0.02051892 -0.01379636 -0.0070738  -0.00035124]
[496 220 166 136 137 159 163 171 184 193] [-0.06757683 -0.06085427 -0.05413171 -0.04740915 -0.04068659 -0.03396403
 -0.02724148 -0.02051892 -0.01379636 -0.0070738  -0.00035124]
-0.788926
1.16061
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082915 minutes
Epoch 0
Fine tuning took 0.081237 minutes
Epoch 0
Fine tuning took 0.082657 minutes
{'zero': {0: [0.087438423645320201, 0.033251231527093597, 0.067733990147783252, 0.089901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83128078817733986, 0.92487684729064035, 0.86699507389162567, 0.80788177339901479], 5: [0.081280788177339899, 0.041871921182266007, 0.065270935960591137, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.087438423645320201, 0.030788177339901478, 0.038177339901477834, 0.082512315270935957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83128078817733986, 0.91995073891625612, 0.91009852216748766, 0.84359605911330049], 5: [0.081280788177339899, 0.049261083743842367, 0.051724137931034482, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.087438423645320201, 0.039408866995073892, 0.044334975369458129, 0.082512315270935957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83128078817733986, 0.91625615763546797, 0.9076354679802956, 0.8288177339901478], 5: [0.081280788177339899, 0.044334975369458129, 0.048029556650246302, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.087438423645320201, 0.035714285714285712, 0.050492610837438424, 0.080049261083743842], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83128078817733986, 0.9285714285714286, 0.90024630541871919, 0.82758620689655171], 5: [0.081280788177339899, 0.035714285714285712, 0.049261083743842367, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239879 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.78145
Epoch 1, cost is  4.01168
Epoch 2, cost is  3.19263
Epoch 3, cost is  2.80275
Epoch 4, cost is  2.57149
Training took 0.151801 minutes
Weight histogram
[479 305 272 181 177 142 115 135 206  13] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
[283 125 132 148 152 182 203 239 258 303] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
-0.773161
1.41476
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237183 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.91158
Epoch 1, cost is  4.19597
Epoch 2, cost is  3.27482
Epoch 3, cost is  2.8183
Epoch 4, cost is  2.54844
Training took 0.151799 minutes
Weight histogram
[841 625 519 384 365 299 256 278 456  27] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
[601 265 278 303 317 378 415 481 545 467] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
-0.773161
1.41476
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.33546
Epoch 1, cost is  5.47601
Epoch 2, cost is  4.44402
Epoch 3, cost is  3.62381
Epoch 4, cost is  3.11061
Training took 0.106711 minutes
Weight histogram
[238 186 181 166 152 138 261 207 474  22] [-0.04952583 -0.04460967 -0.03969351 -0.03477735 -0.02986119 -0.02494503
 -0.02002887 -0.01511271 -0.01019655 -0.00528039 -0.00036423]
[498 252 151 126 136 148 157 166 185 206] [-0.04952583 -0.04460967 -0.03969351 -0.03477735 -0.02986119 -0.02494503
 -0.02002887 -0.01511271 -0.01019655 -0.00528039 -0.00036423]
-0.620686
1.02993
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082778 minutes
Epoch 0
Fine tuning took 0.084418 minutes
Epoch 0
Fine tuning took 0.082912 minutes
{'zero': {0: [0.081280788177339899, 0.059113300492610835, 0.088669950738916259, 0.084975369458128072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8288177339901478, 0.83251231527093594, 0.81034482758620685, 0.78078817733990147], 5: [0.089901477832512317, 0.10837438423645321, 0.10098522167487685, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.081280788177339899, 0.048029556650246302, 0.071428571428571425, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8288177339901478, 0.8928571428571429, 0.86083743842364535, 0.81896551724137934], 5: [0.089901477832512317, 0.059113300492610835, 0.067733990147783252, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.081280788177339899, 0.049261083743842367, 0.065270935960591137, 0.056650246305418719], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8288177339901478, 0.89532019704433496, 0.84975369458128081, 0.81280788177339902], 5: [0.089901477832512317, 0.055418719211822662, 0.084975369458128072, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.081280788177339899, 0.029556650246305417, 0.078817733990147784, 0.055418719211822662], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8288177339901478, 0.88916256157635465, 0.85221674876847286, 0.84359605911330049], 5: [0.089901477832512317, 0.081280788177339899, 0.068965517241379309, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.238683 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.78145
Epoch 1, cost is  4.01168
Epoch 2, cost is  3.19263
Epoch 3, cost is  2.80275
Epoch 4, cost is  2.57149
Training took 0.154336 minutes
Weight histogram
[479 305 272 181 177 142 115 135 206  13] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
[283 125 132 148 152 182 203 239 258 303] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
-0.773161
1.41476
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.238082 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.91158
Epoch 1, cost is  4.19597
Epoch 2, cost is  3.27482
Epoch 3, cost is  2.8183
Epoch 4, cost is  2.54844
Training took 0.153557 minutes
Weight histogram
[841 625 519 384 365 299 256 278 456  27] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
[601 265 278 303 317 378 415 481 545 467] [-0.04169498 -0.0375504  -0.03340583 -0.02926125 -0.02511667 -0.02097209
 -0.01682752 -0.01268294 -0.00853836 -0.00439378 -0.00024921]
-0.773161
1.41476
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.11062
Epoch 1, cost is  5.26645
Epoch 2, cost is  4.1582
Epoch 3, cost is  3.19919
Epoch 4, cost is  2.64512
Training took 0.158477 minutes
Weight histogram
[289 210 180 165 169 321 295 353  34   9] [-0.03077359 -0.02773064 -0.0246877  -0.02164475 -0.01860181 -0.01555886
 -0.01251592 -0.00947297 -0.00643003 -0.00338708 -0.00034414]
[581 237 122 122 131 137 151 162 186 196] [-0.03077359 -0.02773064 -0.0246877  -0.02164475 -0.01860181 -0.01555886
 -0.01251592 -0.00947297 -0.00643003 -0.00338708 -0.00034414]
-0.491401
0.93933
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088213 minutes
Epoch 0
Fine tuning took 0.088263 minutes
Epoch 0
Fine tuning took 0.088237 minutes
{'zero': {0: [0.11699507389162561, 0.12192118226600986, 0.15147783251231528, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.67980295566502458, 0.65394088669950734, 0.63300492610837433], 5: [0.12315270935960591, 0.19827586206896552, 0.19458128078817735, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11699507389162561, 0.099753694581280791, 0.11206896551724138, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.74507389162561577, 0.71305418719211822, 0.66379310344827591], 5: [0.12315270935960591, 0.15517241379310345, 0.1748768472906404, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11699507389162561, 0.091133004926108374, 0.12807881773399016, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.71305418719211822, 0.70073891625615758, 0.63423645320197042], 5: [0.12315270935960591, 0.19581280788177341, 0.17118226600985223, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11699507389162561, 0.093596059113300489, 0.13054187192118227, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.73152709359605916, 0.69088669950738912, 0.67241379310344829], 5: [0.12315270935960591, 0.1748768472906404, 0.17857142857142858, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.238268 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.64858
Epoch 1, cost is  6.46994
Epoch 2, cost is  6.33369
Epoch 3, cost is  6.17384
Epoch 4, cost is  5.96543
Training took 0.154693 minutes
Weight histogram
[614 910 216 102  62  42  29  21  17  12] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
[746 287 216 164 143 118 102  93  80  76] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
-0.198158
0.281704
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.238318 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.65553
Epoch 1, cost is  6.48935
Epoch 2, cost is  6.37186
Epoch 3, cost is  6.2397
Epoch 4, cost is  6.07242
Training took 0.154598 minutes
Weight histogram
[ 973 2065  436  208  126   84   58   42   33   25] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
[1560  618  451  349  288  239  208  181   80   76] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
-0.198158
0.281704
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  6.78108
Epoch 1, cost is  6.61833
Epoch 2, cost is  6.47965
Epoch 3, cost is  6.35108
Epoch 4, cost is  6.2242
Training took 0.093173 minutes
Weight histogram
[1365  279  132   81   53   38   28   20   16   13] [ -8.89928360e-03  -8.01333111e-03  -7.12737862e-03  -6.24142613e-03
  -5.35547364e-03  -4.46952115e-03  -3.58356866e-03  -2.69761617e-03
  -1.81166368e-03  -9.25711195e-04  -3.97587064e-05]
[637 267 197 175 150 132 134 115 108 110] [ -8.89928360e-03  -8.01333111e-03  -7.12737862e-03  -6.24142613e-03
  -5.35547364e-03  -4.46952115e-03  -3.58356866e-03  -2.69761617e-03
  -1.81166368e-03  -9.25711195e-04  -3.97587064e-05]
-0.0575593
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083859 minutes
Epoch 0
Fine tuning took 0.085678 minutes
Epoch 0
Fine tuning took 0.083897 minutes
{'zero': {0: [0.11083743842364532, 0.27832512315270935, 0.14901477832512317, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.44827586206896552, 0.64162561576354682, 0.47906403940886699], 5: [0.14778325123152711, 0.27339901477832512, 0.20935960591133004, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11083743842364532, 0.24630541871921183, 0.14901477832512317, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.48768472906403942, 0.62931034482758619, 0.5], 5: [0.14778325123152711, 0.26600985221674878, 0.22167487684729065, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11083743842364532, 0.28817733990147781, 0.1748768472906404, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.47044334975369456, 0.61083743842364535, 0.4963054187192118], 5: [0.14778325123152711, 0.2413793103448276, 0.21428571428571427, 0.27093596059113301], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11083743842364532, 0.2376847290640394, 0.18226600985221675, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.51354679802955661, 0.65640394088669951, 0.49261083743842365], 5: [0.14778325123152711, 0.24876847290640394, 0.16133004926108374, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.238417 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.64858
Epoch 1, cost is  6.46994
Epoch 2, cost is  6.33369
Epoch 3, cost is  6.17384
Epoch 4, cost is  5.96543
Training took 0.159830 minutes
Weight histogram
[614 910 216 102  62  42  29  21  17  12] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
[746 287 216 164 143 118 102  93  80  76] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
-0.198158
0.281704
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.262486 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.65553
Epoch 1, cost is  6.48935
Epoch 2, cost is  6.37186
Epoch 3, cost is  6.2397
Epoch 4, cost is  6.07242
Training took 0.169921 minutes
Weight histogram
[ 973 2065  436  208  126   84   58   42   33   25] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
[1560  618  451  349  288  239  208  181   80   76] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
-0.198158
0.281704
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.69941
Epoch 1, cost is  6.49615
Epoch 2, cost is  6.34803
Epoch 3, cost is  6.20879
Epoch 4, cost is  6.07326
Training took 0.115092 minutes
Weight histogram
[1227  366  156   92   60   42   30   22   16   14] [ -9.23346821e-03  -8.31640231e-03  -7.39933640e-03  -6.48227050e-03
  -5.56520459e-03  -4.64813869e-03  -3.73107278e-03  -2.81400687e-03
  -1.89694097e-03  -9.79875064e-04  -6.28091584e-05]
[609 263 202 174 154 135 137 123 113 115] [ -9.23346821e-03  -8.31640231e-03  -7.39933640e-03  -6.48227050e-03
  -5.56520459e-03  -4.64813869e-03  -3.73107278e-03  -2.81400687e-03
  -1.89694097e-03  -9.79875064e-04  -6.28091584e-05]
-0.0561428
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.085590 minutes
Epoch 0
Fine tuning took 0.083899 minutes
Epoch 0
Fine tuning took 0.083868 minutes
{'zero': {0: [0.11330049261083744, 0.26600985221674878, 0.18596059113300492, 0.29433497536945813], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.47413793103448276, 0.59852216748768472, 0.44458128078817732], 5: [0.14901477832512317, 0.25985221674876846, 0.21551724137931033, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11330049261083744, 0.21305418719211822, 0.17733990147783252, 0.2857142857142857], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.48275862068965519, 0.58990147783251234, 0.45812807881773399], 5: [0.14901477832512317, 0.30418719211822659, 0.23275862068965517, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11330049261083744, 0.25985221674876846, 0.17733990147783252, 0.28448275862068967], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.48152709359605911, 0.59852216748768472, 0.43842364532019706], 5: [0.14901477832512317, 0.25862068965517243, 0.22413793103448276, 0.27709359605911332], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11330049261083744, 0.25985221674876846, 0.17610837438423646, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.49507389162561577, 0.60837438423645318, 0.48275862068965519], 5: [0.14901477832512317, 0.24507389162561577, 0.21551724137931033, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.237687 minutes
Weight histogram
[ 12  34  82 161 270 361 340 342 332  91] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
[ 36 247 317 202 243 145 245 143 269 178] [ -1.34912640e-04  -7.91539671e-05  -2.33952946e-05   3.23633780e-05
   8.81220505e-05   1.43880723e-04   1.99639396e-04   2.55398068e-04
   3.11156741e-04   3.66915413e-04   4.22674086e-04]
-0.304704
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.64858
Epoch 1, cost is  6.46994
Epoch 2, cost is  6.33369
Epoch 3, cost is  6.17384
Epoch 4, cost is  5.96543
Training took 0.152264 minutes
Weight histogram
[614 910 216 102  62  42  29  21  17  12] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
[746 287 216 164 143 118 102  93  80  76] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
-0.198158
0.281704
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.239458 minutes
Weight histogram
[ 19 106 293 537 834 774 805 579  97   6] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
[119 447 535 497 463 483 454 466 313 273] [ -1.34912640e-04  -7.09012587e-05  -6.88987784e-06   5.71215031e-05
   1.21132884e-04   1.85144265e-04   2.49155646e-04   3.13167027e-04
   3.77178408e-04   4.41189788e-04   5.05201169e-04]
-0.315419
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.65553
Epoch 1, cost is  6.48935
Epoch 2, cost is  6.37186
Epoch 3, cost is  6.2397
Epoch 4, cost is  6.07242
Training took 0.153012 minutes
Weight histogram
[ 973 2065  436  208  126   84   58   42   33   25] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
[1560  618  451  349  288  239  208  181   80   76] [ -7.86768738e-03  -7.07314487e-03  -6.27860235e-03  -5.48405984e-03
  -4.68951732e-03  -3.89497481e-03  -3.10043229e-03  -2.30588977e-03
  -1.51134726e-03  -7.16804744e-04   7.77377718e-05]
-0.198158
0.281704
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.4573
Epoch 1, cost is  6.14984
Epoch 2, cost is  5.97016
Epoch 3, cost is  5.80493
Epoch 4, cost is  5.6572
Training took 0.165808 minutes
Weight histogram
[856 578 237 126  79  52  37  26  19  15] [ -1.00774989e-02  -9.07470742e-03  -8.07191599e-03  -7.06912456e-03
  -6.06633312e-03  -5.06354169e-03  -4.06075026e-03  -3.05795883e-03
  -2.05516740e-03  -1.05237596e-03  -4.95845306e-05]
[567 245 199 169 166 140 140 139 127 133] [ -1.00774989e-02  -9.07470742e-03  -8.07191599e-03  -7.06912456e-03
  -6.06633312e-03  -5.06354169e-03  -4.06075026e-03  -3.05795883e-03
  -2.05516740e-03  -1.05237596e-03  -4.95845306e-05]
-0.0554406
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.090827 minutes
Epoch 0
Fine tuning took 0.101613 minutes
Epoch 0
Fine tuning took 0.092559 minutes
{'zero': {0: [0.11576354679802955, 0.2857142857142857, 0.16748768472906403, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.45566502463054187, 0.6428571428571429, 0.44704433497536944], 5: [0.14039408866995073, 0.25862068965517243, 0.18965517241379309, 0.27709359605911332], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11576354679802955, 0.24014778325123154, 0.16502463054187191, 0.26970443349753692], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.48645320197044334, 0.62315270935960587, 0.41379310344827586], 5: [0.14039408866995073, 0.27339901477832512, 0.21182266009852216, 0.31650246305418717], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11576354679802955, 0.27709359605911332, 0.17733990147783252, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.46921182266009853, 0.61576354679802958, 0.42857142857142855], 5: [0.14039408866995073, 0.2536945812807882, 0.20689655172413793, 0.31034482758620691], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11576354679802955, 0.2894088669950739, 0.17980295566502463, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.49876847290640391, 0.6428571428571429, 0.44704433497536944], 5: [0.14039408866995073, 0.21182266009852216, 0.17733990147783252, 0.28448275862068967], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227381 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.6978
Epoch 1, cost is  1.60084
Epoch 2, cost is  1.55065
Epoch 3, cost is  1.51319
Epoch 4, cost is  1.48557
Training took 0.160881 minutes
Weight histogram
[1621  403 1013  488  214  127   70   45   29   40] [-0.11138962 -0.10060253 -0.08981543 -0.07902834 -0.06824124 -0.05745415
 -0.04666705 -0.03587996 -0.02509286 -0.01430577 -0.00351867]
[  68   68  103  147  225  323  458  605 1008 1045] [-0.11138962 -0.10060253 -0.08981543 -0.07902834 -0.06824124 -0.05745415
 -0.04666705 -0.03587996 -0.02509286 -0.01430577 -0.00351867]
-3.25899
3.61779
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.256970 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.64222
Epoch 1, cost is  1.54392
Epoch 2, cost is  1.49616
Epoch 3, cost is  1.46051
Epoch 4, cost is  1.43059
Training took 0.168328 minutes
Weight histogram
[1879  650 1764  834  369  214  129   89   66   81] [-0.10844683 -0.09795401 -0.0874612  -0.07696838 -0.06647557 -0.05598275
 -0.04548993 -0.03499712 -0.0245043  -0.01401149 -0.00351867]
[ 144  133  196  290  433  633  945 1198 1028 1075] [-0.10844683 -0.09795401 -0.0874612  -0.07696838 -0.06647557 -0.05598275
 -0.04548993 -0.03499712 -0.0245043  -0.01401149 -0.00351867]
-3.04563
3.88061
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.46811
Epoch 1, cost is  1.97032
Epoch 2, cost is  1.68467
Epoch 3, cost is  1.54493
Epoch 4, cost is  1.45979
Training took 0.098667 minutes
Weight histogram
[1136  808  614  482  300  200  144  100  130  136] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[255 175 187 236 281 353 450 549 689 875] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.5425
3.84016
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084435 minutes
Epoch 0
Fine tuning took 0.084855 minutes
Epoch 0
Fine tuning took 0.084140 minutes
{'zero': {0: [0.055418719211822662, 0.22536945812807882, 0.24261083743842365, 0.30418719211822659], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84975369458128081, 0.59482758620689657, 0.58743842364532017, 0.50862068965517238], 5: [0.094827586206896547, 0.17980295566502463, 0.16995073891625614, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.055418719211822662, 0.039408866995073892, 0.065270935960591137, 0.051724137931034482], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84975369458128081, 0.87684729064039413, 0.85837438423645318, 0.86822660098522164], 5: [0.094827586206896547, 0.083743842364532015, 0.076354679802955669, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.055418719211822662, 0.062807881773399021, 0.055418719211822662, 0.068965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84975369458128081, 0.83990147783251234, 0.85591133004926112, 0.84113300492610843], 5: [0.094827586206896547, 0.097290640394088676, 0.088669950738916259, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.055418719211822662, 0.027093596059113302, 0.041871921182266007, 0.038177339901477834], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84975369458128081, 0.91379310344827591, 0.89532019704433496, 0.90886699507389157], 5: [0.094827586206896547, 0.059113300492610835, 0.062807881773399021, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227744 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.6978
Epoch 1, cost is  1.60084
Epoch 2, cost is  1.55065
Epoch 3, cost is  1.51319
Epoch 4, cost is  1.48557
Training took 0.152526 minutes
Weight histogram
[1621  403 1013  488  214  127   70   45   29   40] [-0.11138962 -0.10060253 -0.08981543 -0.07902834 -0.06824124 -0.05745415
 -0.04666705 -0.03587996 -0.02509286 -0.01430577 -0.00351867]
[  68   68  103  147  225  323  458  605 1008 1045] [-0.11138962 -0.10060253 -0.08981543 -0.07902834 -0.06824124 -0.05745415
 -0.04666705 -0.03587996 -0.02509286 -0.01430577 -0.00351867]
-3.25899
3.61779
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227258 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.64222
Epoch 1, cost is  1.54392
Epoch 2, cost is  1.49616
Epoch 3, cost is  1.46051
Epoch 4, cost is  1.43059
Training took 0.152800 minutes
Weight histogram
[1879  650 1764  834  369  214  129   89   66   81] [-0.10844683 -0.09795401 -0.0874612  -0.07696838 -0.06647557 -0.05598275
 -0.04548993 -0.03499712 -0.0245043  -0.01401149 -0.00351867]
[ 144  133  196  290  433  633  945 1198 1028 1075] [-0.10844683 -0.09795401 -0.0874612  -0.07696838 -0.06647557 -0.05598275
 -0.04548993 -0.03499712 -0.0245043  -0.01401149 -0.00351867]
-3.04563
3.88061
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.26675
Epoch 1, cost is  1.65181
Epoch 2, cost is  1.31057
Epoch 3, cost is  1.14272
Epoch 4, cost is  1.04586
Training took 0.107935 minutes
Weight histogram
[1114  892  614  431  310  208  127   96  151  107] [-0.1542796  -0.1392472  -0.12421479 -0.10918239 -0.09414998 -0.07911757
 -0.06408517 -0.04905276 -0.03402036 -0.01898795 -0.00395554]
[263 170 182 230 290 367 453 560 694 841] [-0.1542796  -0.1392472  -0.12421479 -0.10918239 -0.09414998 -0.07911757
 -0.06408517 -0.04905276 -0.03402036 -0.01898795 -0.00395554]
-2.85237
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082764 minutes
Epoch 0
Fine tuning took 0.083618 minutes
Epoch 0
Fine tuning took 0.084379 minutes
{'zero': {0: [0.062807881773399021, 0.33990147783251229, 0.33990147783251229, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83866995073891626, 0.47536945812807879, 0.4605911330049261, 0.53201970443349755], 5: [0.098522167487684734, 0.18472906403940886, 0.19950738916256158, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.062807881773399021, 0.15024630541871922, 0.11083743842364532, 0.055418719211822662], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83866995073891626, 0.74014778325123154, 0.79187192118226601, 0.81527093596059108], 5: [0.098522167487684734, 0.10960591133004927, 0.097290640394088676, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.062807881773399021, 0.12315270935960591, 0.096059113300492605, 0.067733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83866995073891626, 0.73152709359605916, 0.75862068965517238, 0.79433497536945807], 5: [0.098522167487684734, 0.14532019704433496, 0.14532019704433496, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.062807881773399021, 0.11699507389162561, 0.081280788177339899, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83866995073891626, 0.76600985221674878, 0.81403940886699511, 0.84113300492610843], 5: [0.098522167487684734, 0.11699507389162561, 0.10467980295566502, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227743 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.6978
Epoch 1, cost is  1.60084
Epoch 2, cost is  1.55065
Epoch 3, cost is  1.51319
Epoch 4, cost is  1.48557
Training took 0.151264 minutes
Weight histogram
[1621  403 1013  488  214  127   70   45   29   40] [-0.11138962 -0.10060253 -0.08981543 -0.07902834 -0.06824124 -0.05745415
 -0.04666705 -0.03587996 -0.02509286 -0.01430577 -0.00351867]
[  68   68  103  147  225  323  458  605 1008 1045] [-0.11138962 -0.10060253 -0.08981543 -0.07902834 -0.06824124 -0.05745415
 -0.04666705 -0.03587996 -0.02509286 -0.01430577 -0.00351867]
-3.25899
3.61779
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227562 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.64222
Epoch 1, cost is  1.54392
Epoch 2, cost is  1.49616
Epoch 3, cost is  1.46051
Epoch 4, cost is  1.43059
Training took 0.151978 minutes
Weight histogram
[1879  650 1764  834  369  214  129   89   66   81] [-0.10844683 -0.09795401 -0.0874612  -0.07696838 -0.06647557 -0.05598275
 -0.04548993 -0.03499712 -0.0245043  -0.01401149 -0.00351867]
[ 144  133  196  290  433  633  945 1198 1028 1075] [-0.10844683 -0.09795401 -0.0874612  -0.07696838 -0.06647557 -0.05598275
 -0.04548993 -0.03499712 -0.0245043  -0.01401149 -0.00351867]
-3.04563
3.88061
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.05433
Epoch 1, cost is  1.3954
Epoch 2, cost is  1.01961
Epoch 3, cost is  0.851322
Epoch 4, cost is  0.753165
Training took 0.158107 minutes
Weight histogram
[1127  847  619  422  303  220  160  115  156   81] [-0.1063529  -0.09609733 -0.08584176 -0.07558618 -0.06533061 -0.05507504
 -0.04481947 -0.03456389 -0.02430832 -0.01405275 -0.00379718]
[277 149 173 228 283 354 442 559 711 874] [-0.1063529  -0.09609733 -0.08584176 -0.07558618 -0.06533061 -0.05507504
 -0.04481947 -0.03456389 -0.02430832 -0.01405275 -0.00379718]
-1.99174
2.92356
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089281 minutes
Epoch 0
Fine tuning took 0.089162 minutes
Epoch 0
Fine tuning took 0.088413 minutes
{'zero': {0: [0.10098522167487685, 0.36945812807881773, 0.35837438423645318, 0.3251231527093596], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.44950738916256155, 0.39285714285714285, 0.44704433497536944], 5: [0.16748768472906403, 0.18103448275862069, 0.24876847290640394, 0.22783251231527094], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10098522167487685, 0.3817733990147783, 0.31034482758620691, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.43965517241379309, 0.44950738916256155, 0.48029556650246308], 5: [0.16748768472906403, 0.17857142857142858, 0.24014778325123154, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10098522167487685, 0.31896551724137934, 0.25985221674876846, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.49384236453201968, 0.52339901477832518, 0.56403940886699511], 5: [0.16748768472906403, 0.18719211822660098, 0.21674876847290642, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10098522167487685, 0.35221674876847292, 0.36699507389162561, 0.31034482758620691], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.45320197044334976, 0.41748768472906406, 0.46305418719211822], 5: [0.16748768472906403, 0.19458128078817735, 0.21551724137931033, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226137 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.26007
Epoch 1, cost is  2.11491
Epoch 2, cost is  2.01423
Epoch 3, cost is  1.9363
Epoch 4, cost is  1.87581
Training took 0.152862 minutes
Weight histogram
[1455  530  519  382  291  241  179  149  276   28] [-0.05351774 -0.04819089 -0.04286404 -0.03753718 -0.03221033 -0.02688348
 -0.02155662 -0.01622977 -0.01090291 -0.00557606 -0.00024921]
[339 195 214 257 306 375 481 544 613 726] [-0.05351774 -0.04819089 -0.04286404 -0.03753718 -0.03221033 -0.02688348
 -0.02155662 -0.01622977 -0.01090291 -0.00557606 -0.00024921]
-1.0307
1.84597
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225776 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.2107
Epoch 1, cost is  2.05736
Epoch 2, cost is  1.95347
Epoch 3, cost is  1.87373
Epoch 4, cost is  1.81278
Training took 0.154447 minutes
Weight histogram
[1271  648  806  840  630  478  403  337  603   59] [-0.05463042 -0.0491923  -0.04375418 -0.03831606 -0.03287794 -0.02743981
 -0.02200169 -0.01656357 -0.01112545 -0.00568733 -0.00024921]
[728 407 452 534 639 777 660 535 607 736] [-0.05463042 -0.0491923  -0.04375418 -0.03831606 -0.03287794 -0.02743981
 -0.02200169 -0.01656357 -0.01112545 -0.00568733 -0.00024921]
-0.913904
1.41476
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.45322
Epoch 1, cost is  5.67211
Epoch 2, cost is  4.7517
Epoch 3, cost is  4.04905
Epoch 4, cost is  3.61866
Training took 0.092287 minutes
Weight histogram
[424 345 350 303 323 270 343 385 828 479] [-0.06757683 -0.06085381 -0.05413079 -0.04740778 -0.04068476 -0.03396175
 -0.02723873 -0.02051572 -0.0137927  -0.00706969 -0.00034667]
[1025  433  327  277  288  322  328  353  378  319] [-0.06757683 -0.06085381 -0.05413079 -0.04740778 -0.04068476 -0.03396175
 -0.02723873 -0.02051572 -0.0137927  -0.00706969 -0.00034667]
-0.788926
1.30357
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082921 minutes
Epoch 0
Fine tuning took 0.082659 minutes
Epoch 0
Fine tuning took 0.081415 minutes
{'zero': {0: [0.024630541871921183, 0.0073891625615763543, 0.018472906403940888, 0.029556650246305417], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96059113300492616, 0.96059113300492616, 0.95073891625615758, 0.91256157635467983], 5: [0.014778325123152709, 0.032019704433497539, 0.030788177339901478, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.024630541871921183, 0.0073891625615763543, 0.0073891625615763543, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96059113300492616, 0.96059113300492616, 0.96182266009852213, 0.95935960591133007], 5: [0.014778325123152709, 0.032019704433497539, 0.030788177339901478, 0.023399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.024630541871921183, 0.0073891625615763543, 0.01600985221674877, 0.02832512315270936], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96059113300492616, 0.97044334975369462, 0.9642857142857143, 0.93719211822660098], 5: [0.014778325123152709, 0.022167487684729065, 0.019704433497536946, 0.034482758620689655], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.024630541871921183, 0.0036945812807881772, 0.012315270935960592, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96059113300492616, 0.98275862068965514, 0.9642857142857143, 0.93596059113300489], 5: [0.014778325123152709, 0.013546798029556651, 0.023399014778325122, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226210 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.26007
Epoch 1, cost is  2.11491
Epoch 2, cost is  2.01423
Epoch 3, cost is  1.9363
Epoch 4, cost is  1.87581
Training took 0.151071 minutes
Weight histogram
[1455  530  519  382  291  241  179  149  276   28] [-0.05351774 -0.04819089 -0.04286404 -0.03753718 -0.03221033 -0.02688348
 -0.02155662 -0.01622977 -0.01090291 -0.00557606 -0.00024921]
[339 195 214 257 306 375 481 544 613 726] [-0.05351774 -0.04819089 -0.04286404 -0.03753718 -0.03221033 -0.02688348
 -0.02155662 -0.01622977 -0.01090291 -0.00557606 -0.00024921]
-1.0307
1.84597
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226228 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.2107
Epoch 1, cost is  2.05736
Epoch 2, cost is  1.95347
Epoch 3, cost is  1.87373
Epoch 4, cost is  1.81278
Training took 0.152571 minutes
Weight histogram
[1271  648  806  840  630  478  403  337  603   59] [-0.05463042 -0.0491923  -0.04375418 -0.03831606 -0.03287794 -0.02743981
 -0.02200169 -0.01656357 -0.01112545 -0.00568733 -0.00024921]
[728 407 452 534 639 777 660 535 607 736] [-0.05463042 -0.0491923  -0.04375418 -0.03831606 -0.03287794 -0.02743981
 -0.02200169 -0.01656357 -0.01112545 -0.00568733 -0.00024921]
-0.913904
1.41476
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.37746
Epoch 1, cost is  5.59062
Epoch 2, cost is  4.56024
Epoch 3, cost is  3.73285
Epoch 4, cost is  3.2283
Training took 0.108179 minutes
Weight histogram
[ 382  394  372  322  340  267  460  463 1004   46] [-0.04952583 -0.04460907 -0.03969231 -0.03477555 -0.02985879 -0.02494202
 -0.02002526 -0.0151085  -0.01019174 -0.00527498 -0.00035821]
[1039  501  286  260  283  302  323  343  390  323] [-0.04952583 -0.04460907 -0.03969231 -0.03477555 -0.02985879 -0.02494202
 -0.02002526 -0.0151085  -0.01019174 -0.00527498 -0.00035821]
-0.620686
1.09736
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084269 minutes
Epoch 0
Fine tuning took 0.082781 minutes
Epoch 0
Fine tuning took 0.083577 minutes
{'zero': {0: [0.033251231527093597, 0.009852216748768473, 0.073891625615763554, 0.071428571428571425], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93719211822660098, 0.92980295566502458, 0.83004926108374388, 0.82019704433497542], 5: [0.029556650246305417, 0.060344827586206899, 0.096059113300492605, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.033251231527093597, 0.014778325123152709, 0.034482758620689655, 0.060344827586206899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93719211822660098, 0.93349753694581283, 0.90024630541871919, 0.88300492610837433], 5: [0.029556650246305417, 0.051724137931034482, 0.065270935960591137, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.033251231527093597, 0.011083743842364532, 0.051724137931034482, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93719211822660098, 0.94458128078817738, 0.8928571428571429, 0.87438423645320196], 5: [0.029556650246305417, 0.044334975369458129, 0.055418719211822662, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.033251231527093597, 0.0012315270935960591, 0.02832512315270936, 0.033251231527093597], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93719211822660098, 0.95197044334975367, 0.90270935960591137, 0.87438423645320196], 5: [0.029556650246305417, 0.046798029556650245, 0.068965517241379309, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226574 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.26007
Epoch 1, cost is  2.11491
Epoch 2, cost is  2.01423
Epoch 3, cost is  1.9363
Epoch 4, cost is  1.87581
Training took 0.153365 minutes
Weight histogram
[1455  530  519  382  291  241  179  149  276   28] [-0.05351774 -0.04819089 -0.04286404 -0.03753718 -0.03221033 -0.02688348
 -0.02155662 -0.01622977 -0.01090291 -0.00557606 -0.00024921]
[339 195 214 257 306 375 481 544 613 726] [-0.05351774 -0.04819089 -0.04286404 -0.03753718 -0.03221033 -0.02688348
 -0.02155662 -0.01622977 -0.01090291 -0.00557606 -0.00024921]
-1.0307
1.84597
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225320 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.2107
Epoch 1, cost is  2.05736
Epoch 2, cost is  1.95347
Epoch 3, cost is  1.87373
Epoch 4, cost is  1.81278
Training took 0.152470 minutes
Weight histogram
[1271  648  806  840  630  478  403  337  603   59] [-0.05463042 -0.0491923  -0.04375418 -0.03831606 -0.03287794 -0.02743981
 -0.02200169 -0.01656357 -0.01112545 -0.00568733 -0.00024921]
[728 407 452 534 639 777 660 535 607 736] [-0.05463042 -0.0491923  -0.04375418 -0.03831606 -0.03287794 -0.02743981
 -0.02200169 -0.01656357 -0.01112545 -0.00568733 -0.00024921]
-0.913904
1.41476
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.1712
Epoch 1, cost is  5.39928
Epoch 2, cost is  4.3221
Epoch 3, cost is  3.37407
Epoch 4, cost is  2.79185
Training took 0.157965 minutes
Weight histogram
[457 451 391 334 334 536 715 738  76  18] [-0.03077359 -0.02772926 -0.02468493 -0.02164061 -0.01859628 -0.01555195
 -0.01250763 -0.0094633  -0.00641897 -0.00337465 -0.00033032]
[1198  455  245  256  265  287  307  342  372  323] [-0.03077359 -0.02772926 -0.02468493 -0.02164061 -0.01859628 -0.01555195
 -0.01250763 -0.0094633  -0.00641897 -0.00337465 -0.00033032]
-0.558863
1.01448
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089973 minutes
Epoch 0
Fine tuning took 0.089399 minutes
Epoch 0
Fine tuning took 0.088629 minutes
{'zero': {0: [0.075123152709359611, 0.14408866995073891, 0.16625615763546797, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84975369458128081, 0.67610837438423643, 0.61699507389162567, 0.56034482758620685], 5: [0.075123152709359611, 0.17980295566502463, 0.21674876847290642, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.075123152709359611, 0.070197044334975367, 0.12192118226600986, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84975369458128081, 0.79802955665024633, 0.72660098522167482, 0.67364532019704437], 5: [0.075123152709359611, 0.13177339901477833, 0.15147783251231528, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.075123152709359611, 0.10467980295566502, 0.13669950738916256, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84975369458128081, 0.73399014778325122, 0.66995073891625612, 0.58743842364532017], 5: [0.075123152709359611, 0.16133004926108374, 0.19334975369458129, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.075123152709359611, 0.081280788177339899, 0.1268472906403941, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84975369458128081, 0.77955665024630538, 0.71921182266009853, 0.62315270935960587], 5: [0.075123152709359611, 0.13916256157635468, 0.1539408866995074, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226727 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.63311
Epoch 1, cost is  5.3316
Epoch 2, cost is  5.06426
Epoch 3, cost is  4.83212
Epoch 4, cost is  4.63711
Training took 0.152687 minutes
Weight histogram
[ 415  368  385  406  460 1524  310  103   50   29] [ -1.57681461e-02  -1.41835578e-02  -1.25989694e-02  -1.10143810e-02
  -9.42979258e-03  -7.84520419e-03  -6.26061579e-03  -4.67602740e-03
  -3.09143901e-03  -1.50685062e-03   7.77377718e-05]
[1471  460  309  261  247  246  252  267  262  275] [ -1.57681461e-02  -1.41835578e-02  -1.25989694e-02  -1.10143810e-02
  -9.42979258e-03  -7.84520419e-03  -6.26061579e-03  -4.67602740e-03
  -3.09143901e-03  -1.50685062e-03   7.77377718e-05]
-0.33989
0.469818
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226695 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.78629
Epoch 1, cost is  5.53267
Epoch 2, cost is  5.2947
Epoch 3, cost is  5.07016
Epoch 4, cost is  4.86929
Training took 0.153967 minutes
Weight histogram
[ 426  393  412  441 2147 1682  306  142   78   48] [ -1.38152130e-02  -1.24259179e-02  -1.10366228e-02  -9.64732776e-03
  -8.25803269e-03  -6.86873761e-03  -5.47944253e-03  -4.09014746e-03
  -2.70085238e-03  -1.31155730e-03   7.77377718e-05]
[2948  915  488  271  250  246  241  242  239  235] [ -1.38152130e-02  -1.24259179e-02  -1.10366228e-02  -9.64732776e-03
  -8.25803269e-03  -6.86873761e-03  -5.47944253e-03  -4.09014746e-03
  -2.70085238e-03  -1.31155730e-03   7.77377718e-05]
-0.340613
0.47077
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80285
Epoch 1, cost is  6.6625
Epoch 2, cost is  6.54208
Epoch 3, cost is  6.42706
Epoch 4, cost is  6.31179
Training took 0.091369 minutes
Weight histogram
[1365 1758  372  188  120   84   60   43   33   27] [ -8.89928360e-03  -8.01293877e-03  -7.12659394e-03  -6.24024911e-03
  -5.35390428e-03  -4.46755946e-03  -3.58121463e-03  -2.69486980e-03
  -1.80852497e-03  -9.22180145e-04  -3.58353172e-05]
[1395  588  436  374  329  290  273  162   99  104] [ -8.89928360e-03  -8.01293877e-03  -7.12659394e-03  -6.24024911e-03
  -5.35390428e-03  -4.46755946e-03  -3.58121463e-03  -2.69486980e-03
  -1.80852497e-03  -9.22180145e-04  -3.58353172e-05]
-0.0580142
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081770 minutes
Epoch 0
Fine tuning took 0.081789 minutes
Epoch 0
Fine tuning took 0.083057 minutes
{'zero': {0: [0.17241379310344829, 0.17857142857142858, 0.30788177339901479, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53078817733990147, 0.66995073891625612, 0.54433497536945807, 0.61699507389162567], 5: [0.29679802955665024, 0.15147783251231528, 0.14778325123152711, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17241379310344829, 0.16502463054187191, 0.29926108374384236, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53078817733990147, 0.69950738916256161, 0.51970443349753692, 0.63669950738916259], 5: [0.29679802955665024, 0.1354679802955665, 0.18103448275862069, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17241379310344829, 0.16502463054187191, 0.31157635467980294, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53078817733990147, 0.68719211822660098, 0.53201970443349755, 0.65024630541871919], 5: [0.29679802955665024, 0.14778325123152711, 0.15640394088669951, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17241379310344829, 0.16379310344827586, 0.30418719211822659, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53078817733990147, 0.66748768472906406, 0.53694581280788178, 0.63054187192118227], 5: [0.29679802955665024, 0.16871921182266009, 0.15886699507389163, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226167 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.63311
Epoch 1, cost is  5.3316
Epoch 2, cost is  5.06426
Epoch 3, cost is  4.83212
Epoch 4, cost is  4.63711
Training took 0.153021 minutes
Weight histogram
[ 415  368  385  406  460 1524  310  103   50   29] [ -1.57681461e-02  -1.41835578e-02  -1.25989694e-02  -1.10143810e-02
  -9.42979258e-03  -7.84520419e-03  -6.26061579e-03  -4.67602740e-03
  -3.09143901e-03  -1.50685062e-03   7.77377718e-05]
[1471  460  309  261  247  246  252  267  262  275] [ -1.57681461e-02  -1.41835578e-02  -1.25989694e-02  -1.10143810e-02
  -9.42979258e-03  -7.84520419e-03  -6.26061579e-03  -4.67602740e-03
  -3.09143901e-03  -1.50685062e-03   7.77377718e-05]
-0.33989
0.469818
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226497 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.78629
Epoch 1, cost is  5.53267
Epoch 2, cost is  5.2947
Epoch 3, cost is  5.07016
Epoch 4, cost is  4.86929
Training took 0.151927 minutes
Weight histogram
[ 426  393  412  441 2147 1682  306  142   78   48] [ -1.38152130e-02  -1.24259179e-02  -1.10366228e-02  -9.64732776e-03
  -8.25803269e-03  -6.86873761e-03  -5.47944253e-03  -4.09014746e-03
  -2.70085238e-03  -1.31155730e-03   7.77377718e-05]
[2948  915  488  271  250  246  241  242  239  235] [ -1.38152130e-02  -1.24259179e-02  -1.10366228e-02  -9.64732776e-03
  -8.25803269e-03  -6.86873761e-03  -5.47944253e-03  -4.09014746e-03
  -2.70085238e-03  -1.31155730e-03   7.77377718e-05]
-0.340613
0.47077
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.73393
Epoch 1, cost is  6.56188
Epoch 2, cost is  6.4332
Epoch 3, cost is  6.30773
Epoch 4, cost is  6.18183
Training took 0.108481 minutes
Weight histogram
[1227 1725  475  220  136   92   65   47   34   29] [ -9.23346821e-03  -8.31598616e-03  -7.39850410e-03  -6.48102205e-03
  -5.56353999e-03  -4.64605794e-03  -3.72857588e-03  -2.81109383e-03
  -1.89361177e-03  -9.76129715e-04  -5.86476599e-05]
[1325  574  441  375  336  301  280  201  107  110] [ -9.23346821e-03  -8.31598616e-03  -7.39850410e-03  -6.48102205e-03
  -5.56353999e-03  -4.64605794e-03  -3.72857588e-03  -2.81109383e-03
  -1.89361177e-03  -9.76129715e-04  -5.86476599e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.085288 minutes
Epoch 0
Fine tuning took 0.094540 minutes
Epoch 0
Fine tuning took 0.086019 minutes
{'zero': {0: [0.18472906403940886, 0.1748768472906404, 0.31896551724137934, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.63423645320197042, 0.49261083743842365, 0.61576354679802958], 5: [0.27586206896551724, 0.19088669950738915, 0.18842364532019704, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18472906403940886, 0.17610837438423646, 0.33128078817733991, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.64162561576354682, 0.48275862068965519, 0.63300492610837433], 5: [0.27586206896551724, 0.18226600985221675, 0.18596059113300492, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18472906403940886, 0.20073891625615764, 0.33374384236453203, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.6428571428571429, 0.49753694581280788, 0.6354679802955665], 5: [0.27586206896551724, 0.15640394088669951, 0.16871921182266009, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18472906403940886, 0.1625615763546798, 0.33497536945812806, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.65640394088669951, 0.5, 0.63423645320197042], 5: [0.27586206896551724, 0.18103448275862069, 0.16502463054187191, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.242171 minutes
Weight histogram
[ 21  99 225 523 486 640 712 842 437  65] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
[185 524 384 375 349 389 398 461 472 513] [ -1.34912640e-04  -5.42305381e-05   2.64515635e-05   1.07133665e-04
   1.87815767e-04   2.68497868e-04   3.49179970e-04   4.29862071e-04
   5.10544173e-04   5.91226274e-04   6.71908376e-04]
-0.409468
0.507344
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.63311
Epoch 1, cost is  5.3316
Epoch 2, cost is  5.06426
Epoch 3, cost is  4.83212
Epoch 4, cost is  4.63711
Training took 0.154184 minutes
Weight histogram
[ 415  368  385  406  460 1524  310  103   50   29] [ -1.57681461e-02  -1.41835578e-02  -1.25989694e-02  -1.10143810e-02
  -9.42979258e-03  -7.84520419e-03  -6.26061579e-03  -4.67602740e-03
  -3.09143901e-03  -1.50685062e-03   7.77377718e-05]
[1471  460  309  261  247  246  252  267  262  275] [ -1.57681461e-02  -1.41835578e-02  -1.25989694e-02  -1.10143810e-02
  -9.42979258e-03  -7.84520419e-03  -6.26061579e-03  -4.67602740e-03
  -3.09143901e-03  -1.50685062e-03   7.77377718e-05]
-0.33989
0.469818
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226064 minutes
Weight histogram
[  31  204  512  957  959 1069  926  776  513  128] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
[362 806 791 741 749 492 489 445 535 665] [ -1.34912640e-04  -5.65532508e-05   2.18061381e-05   1.00165527e-04
   1.78524916e-04   2.56884305e-04   3.35243694e-04   4.13603082e-04
   4.91962471e-04   5.70321860e-04   6.48681249e-04]
-0.405142
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.78629
Epoch 1, cost is  5.53267
Epoch 2, cost is  5.2947
Epoch 3, cost is  5.07016
Epoch 4, cost is  4.86929
Training took 0.159429 minutes
Weight histogram
[ 426  393  412  441 2147 1682  306  142   78   48] [ -1.38152130e-02  -1.24259179e-02  -1.10366228e-02  -9.64732776e-03
  -8.25803269e-03  -6.86873761e-03  -5.47944253e-03  -4.09014746e-03
  -2.70085238e-03  -1.31155730e-03   7.77377718e-05]
[2948  915  488  271  250  246  241  242  239  235] [ -1.38152130e-02  -1.24259179e-02  -1.10366228e-02  -9.64732776e-03
  -8.25803269e-03  -6.86873761e-03  -5.47944253e-03  -4.09014746e-03
  -2.70085238e-03  -1.31155730e-03   7.77377718e-05]
-0.340613
0.47077
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.52989
Epoch 1, cost is  6.27614
Epoch 2, cost is  6.12122
Epoch 3, cost is  5.97444
Epoch 4, cost is  5.83975
Training took 0.175153 minutes
Weight histogram
[ 929 1484  810  324  181  115   80   56   40   31] [ -1.00774989e-02  -9.07420639e-03  -8.07091393e-03  -7.06762147e-03
  -6.06432901e-03  -5.06103654e-03  -4.05774408e-03  -3.05445162e-03
  -2.05115916e-03  -1.04786670e-03  -4.45742335e-05]
[1205  545  419  380  346  332  300  268  125  130] [ -1.00774989e-02  -9.07420639e-03  -8.07091393e-03  -7.06762147e-03
  -6.06432901e-03  -5.06103654e-03  -4.05774408e-03  -3.05445162e-03
  -2.05115916e-03  -1.04786670e-03  -4.45742335e-05]
-0.0590699
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.092337 minutes
Epoch 0
Fine tuning took 0.092447 minutes
Epoch 0
Fine tuning took 0.089052 minutes
{'zero': {0: [0.18472906403940886, 0.17118226600985223, 0.35098522167487683, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.64901477832512311, 0.50123152709359609, 0.58866995073891626], 5: [0.26231527093596058, 0.17980295566502463, 0.14778325123152711, 0.25123152709359609], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18472906403940886, 0.16748768472906403, 0.31896551724137934, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.67364532019704437, 0.50123152709359609, 0.61330049261083741], 5: [0.26231527093596058, 0.15886699507389163, 0.17980295566502463, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18472906403940886, 0.18719211822660098, 0.32142857142857145, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.67733990147783252, 0.51847290640394084, 0.62068965517241381], 5: [0.26231527093596058, 0.1354679802955665, 0.16009852216748768, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18472906403940886, 0.16502463054187191, 0.33004926108374383, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55295566502463056, 0.66995073891625612, 0.51970443349753692, 0.60837438423645318], 5: [0.26231527093596058, 0.16502463054187191, 0.15024630541871922, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.249498 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.44249
Epoch 1, cost is  1.36646
Epoch 2, cost is  1.32661
Epoch 3, cost is  1.2976
Epoch 4, cost is  1.27555
Training took 0.168048 minutes
Weight histogram
[2008 1569  504 1103  444  224   87   54   38   44] [-0.12596126 -0.113717   -0.10147274 -0.08922848 -0.07698422 -0.06473996
 -0.05249571 -0.04025145 -0.02800719 -0.01576293 -0.00351867]
[  75   82  128  206  300  466  662 1086 1388 1682] [-0.12596126 -0.113717   -0.10147274 -0.08922848 -0.07698422 -0.06473996
 -0.05249571 -0.04025145 -0.02800719 -0.01576293 -0.00351867]
-3.43051
4.35293
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.256200 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.39367
Epoch 1, cost is  1.32286
Epoch 2, cost is  1.28406
Epoch 3, cost is  1.25649
Epoch 4, cost is  1.23166
Training took 0.194060 minutes
Weight histogram
[1992 1838  826 1896  752  350  169  106   83   88] [-0.12249706 -0.11059922 -0.09870138 -0.08680354 -0.0749057  -0.06300786
 -0.05111003 -0.03921219 -0.02731435 -0.01541651 -0.00351867]
[ 157  159  244  385  595  932 1313 1164 1406 1745] [-0.12249706 -0.11059922 -0.09870138 -0.08680354 -0.0749057  -0.06300786
 -0.05111003 -0.03921219 -0.02731435 -0.01541651 -0.00351867]
-3.5798
4.46819
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.46508
Epoch 1, cost is  1.96922
Epoch 2, cost is  1.68722
Epoch 3, cost is  1.54818
Epoch 4, cost is  1.46098
Training took 0.099087 minutes
Weight histogram
[1680 1235  923  728  445  296  217  151  198  202] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[ 386  266  281  358  423  534  676  828 1043 1280] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.68512
3.84016
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082537 minutes
Epoch 0
Fine tuning took 0.085643 minutes
Epoch 0
Fine tuning took 0.082214 minutes
{'zero': {0: [0.093596059113300489, 0.27339901477832512, 0.32266009852216748, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79556650246305416, 0.50492610837438423, 0.43103448275862066, 0.48399014778325122], 5: [0.11083743842364532, 0.22167487684729065, 0.24630541871921183, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.093596059113300489, 0.049261083743842367, 0.059113300492610835, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79556650246305416, 0.85098522167487689, 0.84359605911330049, 0.85221674876847286], 5: [0.11083743842364532, 0.099753694581280791, 0.097290640394088676, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.093596059113300489, 0.046798029556650245, 0.056650246305418719, 0.045566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79556650246305416, 0.82266009852216748, 0.81280788177339902, 0.81896551724137934], 5: [0.11083743842364532, 0.13054187192118227, 0.13054187192118227, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.093596059113300489, 0.036945812807881777, 0.043103448275862072, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79556650246305416, 0.90024630541871919, 0.87931034482758619, 0.90147783251231528], 5: [0.11083743842364532, 0.062807881773399021, 0.077586206896551727, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.250346 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.44249
Epoch 1, cost is  1.36646
Epoch 2, cost is  1.32661
Epoch 3, cost is  1.2976
Epoch 4, cost is  1.27555
Training took 0.169003 minutes
Weight histogram
[2008 1569  504 1103  444  224   87   54   38   44] [-0.12596126 -0.113717   -0.10147274 -0.08922848 -0.07698422 -0.06473996
 -0.05249571 -0.04025145 -0.02800719 -0.01576293 -0.00351867]
[  75   82  128  206  300  466  662 1086 1388 1682] [-0.12596126 -0.113717   -0.10147274 -0.08922848 -0.07698422 -0.06473996
 -0.05249571 -0.04025145 -0.02800719 -0.01576293 -0.00351867]
-3.43051
4.35293
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.256936 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.39367
Epoch 1, cost is  1.32286
Epoch 2, cost is  1.28406
Epoch 3, cost is  1.25649
Epoch 4, cost is  1.23166
Training took 0.151603 minutes
Weight histogram
[1992 1838  826 1896  752  350  169  106   83   88] [-0.12249706 -0.11059922 -0.09870138 -0.08680354 -0.0749057  -0.06300786
 -0.05111003 -0.03921219 -0.02731435 -0.01541651 -0.00351867]
[ 157  159  244  385  595  932 1313 1164 1406 1745] [-0.12249706 -0.11059922 -0.09870138 -0.08680354 -0.0749057  -0.06300786
 -0.05111003 -0.03921219 -0.02731435 -0.01541651 -0.00351867]
-3.5798
4.46819
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.2661
Epoch 1, cost is  1.66117
Epoch 2, cost is  1.31569
Epoch 3, cost is  1.14601
Epoch 4, cost is  1.04263
Training took 0.106829 minutes
Weight histogram
[1672 1326  921  645  469  320  194  141  227  160] [-0.1542796  -0.1392472  -0.12421479 -0.10918239 -0.09414998 -0.07911757
 -0.06408517 -0.04905276 -0.03402036 -0.01898795 -0.00395554]
[ 397  259  276  349  436  555  678  839 1035 1251] [-0.1542796  -0.1392472  -0.12421479 -0.10918239 -0.09414998 -0.07911757
 -0.06408517 -0.04905276 -0.03402036 -0.01898795 -0.00395554]
-2.85237
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083262 minutes
Epoch 0
Fine tuning took 0.084567 minutes
Epoch 0
Fine tuning took 0.083072 minutes
{'zero': {0: [0.12315270935960591, 0.30788177339901479, 0.3891625615763547, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75123152709359609, 0.42857142857142855, 0.37068965517241381, 0.41133004926108374], 5: [0.12561576354679804, 0.26354679802955666, 0.24014778325123154, 0.25123152709359609], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12315270935960591, 0.14901477832512317, 0.12561576354679804, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75123152709359609, 0.67364532019704437, 0.69088669950738912, 0.74876847290640391], 5: [0.12561576354679804, 0.17733990147783252, 0.18349753694581281, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12315270935960591, 0.1354679802955665, 0.10467980295566502, 0.096059113300492605], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75123152709359609, 0.68965517241379315, 0.70073891625615758, 0.74014778325123154], 5: [0.12561576354679804, 0.1748768472906404, 0.19458128078817735, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12315270935960591, 0.15024630541871922, 0.10591133004926108, 0.062807881773399021], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75123152709359609, 0.62684729064039413, 0.67610837438423643, 0.74876847290640391], 5: [0.12561576354679804, 0.2229064039408867, 0.21798029556650247, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226259 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.44249
Epoch 1, cost is  1.36646
Epoch 2, cost is  1.32661
Epoch 3, cost is  1.2976
Epoch 4, cost is  1.27555
Training took 0.155073 minutes
Weight histogram
[2008 1569  504 1103  444  224   87   54   38   44] [-0.12596126 -0.113717   -0.10147274 -0.08922848 -0.07698422 -0.06473996
 -0.05249571 -0.04025145 -0.02800719 -0.01576293 -0.00351867]
[  75   82  128  206  300  466  662 1086 1388 1682] [-0.12596126 -0.113717   -0.10147274 -0.08922848 -0.07698422 -0.06473996
 -0.05249571 -0.04025145 -0.02800719 -0.01576293 -0.00351867]
-3.43051
4.35293
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225776 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.39367
Epoch 1, cost is  1.32286
Epoch 2, cost is  1.28406
Epoch 3, cost is  1.25649
Epoch 4, cost is  1.23166
Training took 0.153762 minutes
Weight histogram
[1992 1838  826 1896  752  350  169  106   83   88] [-0.12249706 -0.11059922 -0.09870138 -0.08680354 -0.0749057  -0.06300786
 -0.05111003 -0.03921219 -0.02731435 -0.01541651 -0.00351867]
[ 157  159  244  385  595  932 1313 1164 1406 1745] [-0.12249706 -0.11059922 -0.09870138 -0.08680354 -0.0749057  -0.06300786
 -0.05111003 -0.03921219 -0.02731435 -0.01541651 -0.00351867]
-3.5798
4.46819
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.05397
Epoch 1, cost is  1.39961
Epoch 2, cost is  1.0186
Epoch 3, cost is  0.842266
Epoch 4, cost is  0.744591
Training took 0.159338 minutes
Weight histogram
[1725 1280  898  619  455  326  244  171  236  121] [-0.1063529  -0.09609733 -0.08584176 -0.07558618 -0.06533061 -0.05507504
 -0.04481947 -0.03456389 -0.02430832 -0.01405275 -0.00379718]
[ 420  228  266  347  432  536  676  837 1072 1261] [-0.1063529  -0.09609733 -0.08584176 -0.07558618 -0.06533061 -0.05507504
 -0.04481947 -0.03456389 -0.02430832 -0.01405275 -0.00379718]
-1.99174
2.92356
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089558 minutes
Epoch 0
Fine tuning took 0.089268 minutes
Epoch 0
Fine tuning took 0.089334 minutes
{'zero': {0: [0.17733990147783252, 0.30418719211822659, 0.44704433497536944, 0.31527093596059114], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.49137931034482757, 0.33620689655172414, 0.39532019704433496], 5: [0.16502463054187191, 0.20443349753694581, 0.21674876847290642, 0.2894088669950739], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17733990147783252, 0.29433497536945813, 0.43596059113300495, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.48152709359605911, 0.36206896551724138, 0.38669950738916259], 5: [0.16502463054187191, 0.22413793103448276, 0.2019704433497537, 0.28694581280788178], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17733990147783252, 0.28325123152709358, 0.41748768472906406, 0.27709359605911332], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.48275862068965519, 0.40147783251231528, 0.44827586206896552], 5: [0.16502463054187191, 0.23399014778325122, 0.18103448275862069, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17733990147783252, 0.28078817733990147, 0.47290640394088668, 0.33251231527093594], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.49014778325123154, 0.33128078817733991, 0.36699507389162561], 5: [0.16502463054187191, 0.22906403940886699, 0.19581280788177341, 0.30049261083743845], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226614 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.73653
Epoch 1, cost is  1.67089
Epoch 2, cost is  1.62194
Epoch 3, cost is  1.58598
Epoch 4, cost is  1.55322
Training took 0.152604 minutes
Weight histogram
[1963 1447  610  608  403  275  249  180  294   46] [-0.06068742 -0.0546436  -0.04859978 -0.04255596 -0.03651214 -0.03046832
 -0.02442449 -0.01838067 -0.01233685 -0.00629303 -0.00024921]
[ 379  243  280  354  452  598  683  823 1063 1200] [-0.06068742 -0.0546436  -0.04859978 -0.04255596 -0.03651214 -0.03046832
 -0.02442449 -0.01838067 -0.01233685 -0.00629303 -0.00024921]
-1.16531
2.06688
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225418 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.68233
Epoch 1, cost is  1.61689
Epoch 2, cost is  1.57134
Epoch 3, cost is  1.53479
Epoch 4, cost is  1.50172
Training took 0.152448 minutes
Weight histogram
[1913 1180  837  920  914  645  535  393  655  108] [-0.06271979 -0.05647273 -0.05022568 -0.04397862 -0.03773156 -0.0314845
 -0.02523744 -0.01899038 -0.01274332 -0.00649627 -0.00024921]
[ 807  509  580  727  921  769  657  802 1092 1236] [-0.06271979 -0.05647273 -0.05022568 -0.04397862 -0.03773156 -0.0314845
 -0.02523744 -0.01899038 -0.01274332 -0.00649627 -0.00024921]
-1.14229
1.47107
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.4668
Epoch 1, cost is  5.696
Epoch 2, cost is  4.77603
Epoch 3, cost is  4.07334
Epoch 4, cost is  3.65068
Training took 0.090847 minutes
Weight histogram
[ 596  533  518  455  490  402  526  581 1162  812] [-0.06757683 -0.06085315 -0.05412947 -0.04740579 -0.04068211 -0.03395844
 -0.02723476 -0.02051108 -0.0137874  -0.00706373 -0.00034005]
[1533  661  492  425  443  484  501  536  586  414] [-0.06757683 -0.06085315 -0.05412947 -0.04740579 -0.04068211 -0.03395844
 -0.02723476 -0.02051108 -0.0137874  -0.00706373 -0.00034005]
-0.788926
1.34971
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083079 minutes
Epoch 0
Fine tuning took 0.082063 minutes
Epoch 0
Fine tuning took 0.082415 minutes
{'zero': {0: [0.033251231527093597, 0.013546798029556651, 0.011083743842364532, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9211822660098522, 0.96305418719211822, 0.93349753694581283, 0.91256157635467983], 5: [0.045566502463054187, 0.023399014778325122, 0.055418719211822662, 0.067733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.033251231527093597, 0.009852216748768473, 0.012315270935960592, 0.01600985221674877], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9211822660098522, 0.95935960591133007, 0.93719211822660098, 0.94581280788177335], 5: [0.045566502463054187, 0.030788177339901478, 0.050492610837438424, 0.038177339901477834], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.033251231527093597, 0.0061576354679802959, 0.009852216748768473, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9211822660098522, 0.96551724137931039, 0.95197044334975367, 0.94827586206896552], 5: [0.045566502463054187, 0.02832512315270936, 0.038177339901477834, 0.04064039408866995], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.033251231527093597, 0.0049261083743842365, 0.012315270935960592, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9211822660098522, 0.96921182266009853, 0.94950738916256161, 0.93349753694581283], 5: [0.045566502463054187, 0.025862068965517241, 0.038177339901477834, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227203 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.73653
Epoch 1, cost is  1.67089
Epoch 2, cost is  1.62194
Epoch 3, cost is  1.58598
Epoch 4, cost is  1.55322
Training took 0.152054 minutes
Weight histogram
[1963 1447  610  608  403  275  249  180  294   46] [-0.06068742 -0.0546436  -0.04859978 -0.04255596 -0.03651214 -0.03046832
 -0.02442449 -0.01838067 -0.01233685 -0.00629303 -0.00024921]
[ 379  243  280  354  452  598  683  823 1063 1200] [-0.06068742 -0.0546436  -0.04859978 -0.04255596 -0.03651214 -0.03046832
 -0.02442449 -0.01838067 -0.01233685 -0.00629303 -0.00024921]
-1.16531
2.06688
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227700 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.68233
Epoch 1, cost is  1.61689
Epoch 2, cost is  1.57134
Epoch 3, cost is  1.53479
Epoch 4, cost is  1.50172
Training took 0.151957 minutes
Weight histogram
[1913 1180  837  920  914  645  535  393  655  108] [-0.06271979 -0.05647273 -0.05022568 -0.04397862 -0.03773156 -0.0314845
 -0.02523744 -0.01899038 -0.01274332 -0.00649627 -0.00024921]
[ 807  509  580  727  921  769  657  802 1092 1236] [-0.06271979 -0.05647273 -0.05022568 -0.04397862 -0.03773156 -0.0314845
 -0.02523744 -0.01899038 -0.01274332 -0.00649627 -0.00024921]
-1.14229
1.47107
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.392
Epoch 1, cost is  5.62121
Epoch 2, cost is  4.59895
Epoch 3, cost is  3.77045
Epoch 4, cost is  3.27718
Training took 0.108445 minutes
Weight histogram
[ 499  614  561  481  525  414  659  734 1517   71] [-0.04952583 -0.04460857 -0.03969131 -0.03477405 -0.02985679 -0.02493952
 -0.02002226 -0.015105   -0.01018774 -0.00527047 -0.00035321]
[1552  761  431  399  428  455  485  524  595  445] [-0.04952583 -0.04460857 -0.03969131 -0.03477405 -0.02985679 -0.02493952
 -0.02002226 -0.015105   -0.01018774 -0.00527047 -0.00035321]
-0.634967
1.12083
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083241 minutes
Epoch 0
Fine tuning took 0.082602 minutes
Epoch 0
Fine tuning took 0.082975 minutes
{'zero': {0: [0.030788177339901478, 0.045566502463054187, 0.051724137931034482, 0.067733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89655172413793105, 0.89408866995073888, 0.88054187192118227, 0.77955665024630538], 5: [0.072660098522167482, 0.060344827586206899, 0.067733990147783252, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.030788177339901478, 0.024630541871921183, 0.023399014778325122, 0.043103448275862072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89655172413793105, 0.93103448275862066, 0.91995073891625612, 0.88669950738916259], 5: [0.072660098522167482, 0.044334975369458129, 0.056650246305418719, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.030788177339901478, 0.020935960591133004, 0.034482758620689655, 0.049261083743842367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89655172413793105, 0.93103448275862066, 0.90517241379310343, 0.84605911330049266], 5: [0.072660098522167482, 0.048029556650246302, 0.060344827586206899, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.030788177339901478, 0.02832512315270936, 0.017241379310344827, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89655172413793105, 0.94458128078817738, 0.92610837438423643, 0.88669950738916259], 5: [0.072660098522167482, 0.027093596059113302, 0.056650246305418719, 0.087438423645320201], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226274 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.73653
Epoch 1, cost is  1.67089
Epoch 2, cost is  1.62194
Epoch 3, cost is  1.58598
Epoch 4, cost is  1.55322
Training took 0.153053 minutes
Weight histogram
[1963 1447  610  608  403  275  249  180  294   46] [-0.06068742 -0.0546436  -0.04859978 -0.04255596 -0.03651214 -0.03046832
 -0.02442449 -0.01838067 -0.01233685 -0.00629303 -0.00024921]
[ 379  243  280  354  452  598  683  823 1063 1200] [-0.06068742 -0.0546436  -0.04859978 -0.04255596 -0.03651214 -0.03046832
 -0.02442449 -0.01838067 -0.01233685 -0.00629303 -0.00024921]
-1.16531
2.06688
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225918 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.68233
Epoch 1, cost is  1.61689
Epoch 2, cost is  1.57134
Epoch 3, cost is  1.53479
Epoch 4, cost is  1.50172
Training took 0.153062 minutes
Weight histogram
[1913 1180  837  920  914  645  535  393  655  108] [-0.06271979 -0.05647273 -0.05022568 -0.04397862 -0.03773156 -0.0314845
 -0.02523744 -0.01899038 -0.01274332 -0.00649627 -0.00024921]
[ 807  509  580  727  921  769  657  802 1092 1236] [-0.06271979 -0.05647273 -0.05022568 -0.04397862 -0.03773156 -0.0314845
 -0.02523744 -0.01899038 -0.01274332 -0.00649627 -0.00024921]
-1.14229
1.47107
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.1923
Epoch 1, cost is  5.44602
Epoch 2, cost is  4.39087
Epoch 3, cost is  3.45281
Epoch 4, cost is  2.8788
Training took 0.158600 minutes
Weight histogram
[ 592  705  585  525  504  770 1131 1114  122   27] [-0.03077359 -0.02772926 -0.02468493 -0.02164061 -0.01859628 -0.01555195
 -0.01250763 -0.0094633  -0.00641897 -0.00337465 -0.00033032]
[1778  693  374  389  395  430  465  508  557  486] [-0.03077359 -0.02772926 -0.02468493 -0.02164061 -0.01859628 -0.01555195
 -0.01250763 -0.0094633  -0.00641897 -0.00337465 -0.00033032]
-0.558863
1.02393
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088108 minutes
Epoch 0
Fine tuning took 0.089033 minutes
Epoch 0
Fine tuning took 0.088124 minutes
{'zero': {0: [0.059113300492610835, 0.13916256157635468, 0.20812807881773399, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84482758620689657, 0.64039408866995073, 0.58743842364532017, 0.54556650246305416], 5: [0.096059113300492605, 0.22044334975369459, 0.20443349753694581, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.059113300492610835, 0.084975369458128072, 0.12192118226600986, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84482758620689657, 0.76354679802955661, 0.71798029556650245, 0.67241379310344829], 5: [0.096059113300492605, 0.15147783251231528, 0.16009852216748768, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.059113300492610835, 0.094827586206896547, 0.16748768472906403, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84482758620689657, 0.73522167487684731, 0.66502463054187189, 0.61822660098522164], 5: [0.096059113300492605, 0.16995073891625614, 0.16748768472906403, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.059113300492610835, 0.078817733990147784, 0.1354679802955665, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84482758620689657, 0.77832512315270941, 0.69211822660098521, 0.67610837438423643], 5: [0.096059113300492605, 0.14285714285714285, 0.17241379310344829, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226275 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.39202
Epoch 1, cost is  4.21276
Epoch 2, cost is  4.0536
Epoch 3, cost is  3.9071
Epoch 4, cost is  3.77237
Training took 0.153298 minutes
Weight histogram
[ 747  689  521  580  540  593 1330  904  123   48] [ -2.31152736e-02  -2.07959724e-02  -1.84766713e-02  -1.61573702e-02
  -1.38380690e-02  -1.15187679e-02  -9.19946676e-03  -6.88016563e-03
  -4.56086450e-03  -2.24156336e-03   7.77377718e-05]
[1809  531  419  418  441  458  488  500  503  508] [ -2.31152736e-02  -2.07959724e-02  -1.84766713e-02  -1.61573702e-02
  -1.38380690e-02  -1.15187679e-02  -9.19946676e-03  -6.88016563e-03
  -4.56086450e-03  -2.24156336e-03   7.77377718e-05]
-0.399566
0.624704
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226185 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.62672
Epoch 1, cost is  4.43666
Epoch 2, cost is  4.26829
Epoch 3, cost is  4.11292
Epoch 4, cost is  3.97306
Training took 0.153809 minutes
Weight histogram
[ 687  638  553  587  579  661 3479  649  185   82] [ -2.07097735e-02  -1.86310224e-02  -1.65522713e-02  -1.44735201e-02
  -1.23947690e-02  -1.03160179e-02  -8.23726675e-03  -6.15851562e-03
  -4.07976449e-03  -2.00101336e-03   7.77377718e-05]
[3719  797  459  434  431  440  445  451  458  466] [ -2.07097735e-02  -1.86310224e-02  -1.65522713e-02  -1.44735201e-02
  -1.23947690e-02  -1.03160179e-02  -8.23726675e-03  -6.15851562e-03
  -4.07976449e-03  -2.00101336e-03   7.77377718e-05]
-0.426244
0.592507
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.815
Epoch 1, cost is  6.69004
Epoch 2, cost is  6.58228
Epoch 3, cost is  6.4793
Epoch 4, cost is  6.37927
Training took 0.089409 minutes
Weight histogram
[1365 3058  744  323  196  134   95   67   51   42] [ -8.89928360e-03  -8.01283747e-03  -7.12639134e-03  -6.23994521e-03
  -5.35349908e-03  -4.46705296e-03  -3.58060683e-03  -2.69416070e-03
  -1.80771457e-03  -9.21268446e-04  -3.48223184e-05]
[2141  906  675  572  506  448  413  211   99  104] [ -8.89928360e-03  -8.01283747e-03  -7.12639134e-03  -6.23994521e-03
  -5.35349908e-03  -4.46705296e-03  -3.58060683e-03  -2.69416070e-03
  -1.80771457e-03  -9.21268446e-04  -3.48223184e-05]
-0.0580142
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082592 minutes
Epoch 0
Fine tuning took 0.082292 minutes
Epoch 0
Fine tuning took 0.081856 minutes
{'zero': {0: [0.46798029556650245, 0.025862068965517241, 0.47290640394088668, 0.43842364532019706], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27093596059113301, 0.73645320197044339, 0.30295566502463056, 0.28817733990147781], 5: [0.26108374384236455, 0.2376847290640394, 0.22413793103448276, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.46798029556650245, 0.014778325123152709, 0.53201970443349755, 0.46305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27093596059113301, 0.76970443349753692, 0.26600985221674878, 0.27709359605911332], 5: [0.26108374384236455, 0.21551724137931033, 0.2019704433497537, 0.25985221674876846], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.46798029556650245, 0.030788177339901478, 0.53448275862068961, 0.45197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27093596059113301, 0.76108374384236455, 0.25738916256157635, 0.25615763546798032], 5: [0.26108374384236455, 0.20812807881773399, 0.20812807881773399, 0.29187192118226601], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.46798029556650245, 0.025862068965517241, 0.54187192118226601, 0.46674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27093596059113301, 0.76108374384236455, 0.26231527093596058, 0.27586206896551724], 5: [0.26108374384236455, 0.21305418719211822, 0.19581280788177341, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226599 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.39202
Epoch 1, cost is  4.21276
Epoch 2, cost is  4.0536
Epoch 3, cost is  3.9071
Epoch 4, cost is  3.77237
Training took 0.152898 minutes
Weight histogram
[ 747  689  521  580  540  593 1330  904  123   48] [ -2.31152736e-02  -2.07959724e-02  -1.84766713e-02  -1.61573702e-02
  -1.38380690e-02  -1.15187679e-02  -9.19946676e-03  -6.88016563e-03
  -4.56086450e-03  -2.24156336e-03   7.77377718e-05]
[1809  531  419  418  441  458  488  500  503  508] [ -2.31152736e-02  -2.07959724e-02  -1.84766713e-02  -1.61573702e-02
  -1.38380690e-02  -1.15187679e-02  -9.19946676e-03  -6.88016563e-03
  -4.56086450e-03  -2.24156336e-03   7.77377718e-05]
-0.399566
0.624704
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226045 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.62672
Epoch 1, cost is  4.43666
Epoch 2, cost is  4.26829
Epoch 3, cost is  4.11292
Epoch 4, cost is  3.97306
Training took 0.151667 minutes
Weight histogram
[ 687  638  553  587  579  661 3479  649  185   82] [ -2.07097735e-02  -1.86310224e-02  -1.65522713e-02  -1.44735201e-02
  -1.23947690e-02  -1.03160179e-02  -8.23726675e-03  -6.15851562e-03
  -4.07976449e-03  -2.00101336e-03   7.77377718e-05]
[3719  797  459  434  431  440  445  451  458  466] [ -2.07097735e-02  -1.86310224e-02  -1.65522713e-02  -1.44735201e-02
  -1.23947690e-02  -1.03160179e-02  -8.23726675e-03  -6.15851562e-03
  -4.07976449e-03  -2.00101336e-03   7.77377718e-05]
-0.426244
0.592507
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.75427
Epoch 1, cost is  6.60264
Epoch 2, cost is  6.48914
Epoch 3, cost is  6.38
Epoch 4, cost is  6.27314
Training took 0.108913 minutes
Weight histogram
[1227 2740 1077  386  224  149  101   74   53   44] [ -9.23346821e-03  -8.31585475e-03  -7.39824128e-03  -6.48062782e-03
  -5.56301435e-03  -4.64540089e-03  -3.72778742e-03  -2.81017396e-03
  -1.89256049e-03  -9.74947029e-04  -5.73335637e-05]
[2042  890  678  582  515  465  426  260  107  110] [ -9.23346821e-03  -8.31585475e-03  -7.39824128e-03  -6.48062782e-03
  -5.56301435e-03  -4.64540089e-03  -3.72778742e-03  -2.81017396e-03
  -1.89256049e-03  -9.74947029e-04  -5.73335637e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082853 minutes
Epoch 0
Fine tuning took 0.084357 minutes
Epoch 0
Fine tuning took 0.086821 minutes
{'zero': {0: [0.45073891625615764, 0.023399014778325122, 0.50369458128078815, 0.48029556650246308], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27955665024630544, 0.7142857142857143, 0.26600985221674878, 0.23029556650246305], 5: [0.26970443349753692, 0.26231527093596058, 0.23029556650246305, 0.2894088669950739], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.45073891625615764, 0.029556650246305417, 0.52216748768472909, 0.4642857142857143], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27955665024630544, 0.70443349753694584, 0.27955665024630544, 0.26724137931034481], 5: [0.26970443349753692, 0.26600985221674878, 0.19827586206896552, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.45073891625615764, 0.030788177339901478, 0.51108374384236455, 0.46798029556650245], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27955665024630544, 0.70073891625615758, 0.28817733990147781, 0.24753694581280788], 5: [0.26970443349753692, 0.26847290640394089, 0.20073891625615764, 0.28448275862068967], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.45073891625615764, 0.018472906403940888, 0.4963054187192118, 0.48891625615763545], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27955665024630544, 0.68842364532019706, 0.29064039408866993, 0.21551724137931033], 5: [0.26970443349753692, 0.29310344827586204, 0.21305418719211822, 0.29556650246305421], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.258217 minutes
Weight histogram
[  31  133  390  578  666  809 1030 1346 1051   41] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
[330 643 439 467 479 554 634 698 889 942] [ -1.34912640e-04  -4.26750077e-05   4.95626242e-05   1.41800256e-04
   2.34037888e-04   3.26275520e-04   4.18513152e-04   5.10750784e-04
   6.02988416e-04   6.95226048e-04   7.87463679e-04]
-0.479412
0.609537
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.39202
Epoch 1, cost is  4.21276
Epoch 2, cost is  4.0536
Epoch 3, cost is  3.9071
Epoch 4, cost is  3.77237
Training took 0.168914 minutes
Weight histogram
[ 747  689  521  580  540  593 1330  904  123   48] [ -2.31152736e-02  -2.07959724e-02  -1.84766713e-02  -1.61573702e-02
  -1.38380690e-02  -1.15187679e-02  -9.19946676e-03  -6.88016563e-03
  -4.56086450e-03  -2.24156336e-03   7.77377718e-05]
[1809  531  419  418  441  458  488  500  503  508] [ -2.31152736e-02  -2.07959724e-02  -1.84766713e-02  -1.61573702e-02
  -1.38380690e-02  -1.15187679e-02  -9.19946676e-03  -6.88016563e-03
  -4.56086450e-03  -2.24156336e-03   7.77377718e-05]
-0.399566
0.624704
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227048 minutes
Weight histogram
[  46  296  736 1155 1117 1175  940 1339 1151  145] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
[ 570 1050  951  919  587  580  688  795  892 1068] [ -1.34912640e-04  -4.57756192e-05   4.33614012e-05   1.32498422e-04
   2.21635442e-04   3.10772462e-04   3.99909483e-04   4.89046503e-04
   5.78183524e-04   6.67320544e-04   7.56457564e-04]
-0.491154
0.442752
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.62672
Epoch 1, cost is  4.43666
Epoch 2, cost is  4.26829
Epoch 3, cost is  4.11292
Epoch 4, cost is  3.97306
Training took 0.154063 minutes
Weight histogram
[ 687  638  553  587  579  661 3479  649  185   82] [ -2.07097735e-02  -1.86310224e-02  -1.65522713e-02  -1.44735201e-02
  -1.23947690e-02  -1.03160179e-02  -8.23726675e-03  -6.15851562e-03
  -4.07976449e-03  -2.00101336e-03   7.77377718e-05]
[3719  797  459  434  431  440  445  451  458  466] [ -2.07097735e-02  -1.86310224e-02  -1.65522713e-02  -1.44735201e-02
  -1.23947690e-02  -1.03160179e-02  -8.23726675e-03  -6.15851562e-03
  -4.07976449e-03  -2.00101336e-03   7.77377718e-05]
-0.426244
0.592507
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.57219
Epoch 1, cost is  6.3521
Epoch 2, cost is  6.21997
Epoch 3, cost is  6.09773
Epoch 4, cost is  5.98617
Training took 0.163867 minutes
Weight histogram
[ 929 1930 1806  596  304  186  126   88   62   48] [ -1.00774989e-02  -9.07412279e-03  -8.07074673e-03  -7.06737067e-03
  -6.06399461e-03  -5.06061855e-03  -4.05724249e-03  -3.05386643e-03
  -2.05049037e-03  -1.04711431e-03  -4.37382441e-05]
[1848  844  647  597  531  521  462  370  125  130] [ -1.00774989e-02  -9.07412279e-03  -8.07074673e-03  -7.06737067e-03
  -6.06399461e-03  -5.06061855e-03  -4.05724249e-03  -3.05386643e-03
  -2.05049037e-03  -1.04711431e-03  -4.37382441e-05]
-0.0601954
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.091882 minutes
Epoch 0
Fine tuning took 0.101263 minutes
Epoch 0
Fine tuning took 0.089119 minutes
{'zero': {0: [0.42733990147783252, 0.014778325123152709, 0.48522167487684731, 0.49876847290640391], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27216748768472904, 0.71305418719211822, 0.26477832512315269, 0.23522167487684728], 5: [0.30049261083743845, 0.27216748768472904, 0.25, 0.26600985221674878], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.42733990147783252, 0.025862068965517241, 0.52339901477832518, 0.50985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27216748768472904, 0.72906403940886699, 0.23891625615763548, 0.24384236453201971], 5: [0.30049261083743845, 0.24507389162561577, 0.2376847290640394, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.42733990147783252, 0.033251231527093597, 0.4963054187192118, 0.48029556650246308], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27216748768472904, 0.72906403940886699, 0.27216748768472904, 0.24753694581280788], 5: [0.30049261083743845, 0.2376847290640394, 0.23152709359605911, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.42733990147783252, 0.020935960591133004, 0.50492610837438423, 0.48029556650246308], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.27216748768472904, 0.69458128078817738, 0.2413793103448276, 0.2376847290640394], 5: [0.30049261083743845, 0.28448275862068967, 0.2536945812807882, 0.28201970443349755], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227087 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.26865
Epoch 1, cost is  1.20392
Epoch 2, cost is  1.1716
Epoch 3, cost is  1.14569
Epoch 4, cost is  1.1231
Training took 0.166329 minutes
Weight histogram
[2217 1831 1901  681  848  310  149   72   44   47] [-0.13868089 -0.12516467 -0.11164845 -0.09813222 -0.084616   -0.07109978
 -0.05758356 -0.04406734 -0.03055111 -0.01703489 -0.00351867]
[  79   94  154  251  387  584 1004 1182 1917 2448] [-0.13868089 -0.12516467 -0.11164845 -0.09813222 -0.084616   -0.07109978
 -0.05758356 -0.04406734 -0.03055111 -0.01703489 -0.00351867]
-3.99307
4.8317
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.256305 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.22547
Epoch 1, cost is  1.16206
Epoch 2, cost is  1.13004
Epoch 3, cost is  1.10476
Epoch 4, cost is  1.08373
Training took 0.169166 minutes
Weight histogram
[2377 1770 1880 1657 1364  513  246  133   86   99] [-0.13368584 -0.12066912 -0.10765241 -0.09463569 -0.08161897 -0.06860226
 -0.05558554 -0.04256882 -0.0295521  -0.01653539 -0.00351867]
[ 166  181  295  471  761 1194 1409 1241 1953 2454] [-0.13368584 -0.12066912 -0.10765241 -0.09463569 -0.08161897 -0.06860226
 -0.05558554 -0.04256882 -0.0295521  -0.01653539 -0.00351867]
-4.04413
4.97178
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.45959
Epoch 1, cost is  1.97162
Epoch 2, cost is  1.67695
Epoch 3, cost is  1.53469
Epoch 4, cost is  1.44648
Training took 0.096011 minutes
Weight histogram
[2124 1731 1222 1003  597  401  286  203  266  267] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[ 518  359  382  480  567  716  900 1113 1392 1673] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.68512
3.84016
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083684 minutes
Epoch 0
Fine tuning took 0.084329 minutes
Epoch 0
Fine tuning took 0.084042 minutes
{'zero': {0: [0.088669950738916259, 0.29187192118226601, 0.40147783251231528, 0.40763546798029554], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.46182266009852219, 0.34729064039408869, 0.40640394088669951], 5: [0.16625615763546797, 0.24630541871921183, 0.25123152709359609, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.088669950738916259, 0.057881773399014777, 0.041871921182266007, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.81650246305418717, 0.82389162561576357, 0.84975369458128081], 5: [0.16625615763546797, 0.12561576354679804, 0.13423645320197045, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.088669950738916259, 0.077586206896551727, 0.065270935960591137, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.77586206896551724, 0.79679802955665024, 0.83990147783251234], 5: [0.16625615763546797, 0.14655172413793102, 0.13793103448275862, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.088669950738916259, 0.024630541871921183, 0.032019704433497539, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.89532019704433496, 0.82266009852216748, 0.87315270935960587], 5: [0.16625615763546797, 0.080049261083743842, 0.14532019704433496, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.229048 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.26865
Epoch 1, cost is  1.20392
Epoch 2, cost is  1.1716
Epoch 3, cost is  1.14569
Epoch 4, cost is  1.1231
Training took 0.151671 minutes
Weight histogram
[2217 1831 1901  681  848  310  149   72   44   47] [-0.13868089 -0.12516467 -0.11164845 -0.09813222 -0.084616   -0.07109978
 -0.05758356 -0.04406734 -0.03055111 -0.01703489 -0.00351867]
[  79   94  154  251  387  584 1004 1182 1917 2448] [-0.13868089 -0.12516467 -0.11164845 -0.09813222 -0.084616   -0.07109978
 -0.05758356 -0.04406734 -0.03055111 -0.01703489 -0.00351867]
-3.99307
4.8317
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225254 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.22547
Epoch 1, cost is  1.16206
Epoch 2, cost is  1.13004
Epoch 3, cost is  1.10476
Epoch 4, cost is  1.08373
Training took 0.152880 minutes
Weight histogram
[2377 1770 1880 1657 1364  513  246  133   86   99] [-0.13368584 -0.12066912 -0.10765241 -0.09463569 -0.08161897 -0.06860226
 -0.05558554 -0.04256882 -0.0295521  -0.01653539 -0.00351867]
[ 166  181  295  471  761 1194 1409 1241 1953 2454] [-0.13368584 -0.12066912 -0.10765241 -0.09463569 -0.08161897 -0.06860226
 -0.05558554 -0.04256882 -0.0295521  -0.01653539 -0.00351867]
-4.04413
4.97178
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.26058
Epoch 1, cost is  1.65728
Epoch 2, cost is  1.31084
Epoch 3, cost is  1.13373
Epoch 4, cost is  1.03208
Training took 0.112098 minutes
Weight histogram
[2254 1727 1242  857  622  434  258  190  303  213] [-0.15442972 -0.1393823  -0.12433488 -0.10928747 -0.09424005 -0.07919263
 -0.06414521 -0.0490978  -0.03405038 -0.01900296 -0.00395554]
[ 534  349  374  471  585  742  910 1119 1380 1636] [-0.15442972 -0.1393823  -0.12433488 -0.10928747 -0.09424005 -0.07919263
 -0.06414521 -0.0490978  -0.03405038 -0.01900296 -0.00395554]
-2.98724
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.086503 minutes
Epoch 0
Fine tuning took 0.085248 minutes
Epoch 0
Fine tuning took 0.083954 minutes
{'zero': {0: [0.097290640394088676, 0.40024630541871919, 0.40640394088669951, 0.44581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.37931034482758619, 0.29802955665024633, 0.33497536945812806], 5: [0.11822660098522167, 0.22044334975369459, 0.29556650246305421, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.097290640394088676, 0.16379310344827586, 0.087438423645320201, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.70812807881773399, 0.77709359605911332, 0.78694581280788178], 5: [0.11822660098522167, 0.12807881773399016, 0.1354679802955665, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.097290640394088676, 0.16379310344827586, 0.12561576354679804, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.68226600985221675, 0.70443349753694584, 0.74384236453201968], 5: [0.11822660098522167, 0.1539408866995074, 0.16995073891625614, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.097290640394088676, 0.1145320197044335, 0.05295566502463054, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.75985221674876846, 0.82635467980295563, 0.8214285714285714], 5: [0.11822660098522167, 0.12561576354679804, 0.1206896551724138, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226201 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.26865
Epoch 1, cost is  1.20392
Epoch 2, cost is  1.1716
Epoch 3, cost is  1.14569
Epoch 4, cost is  1.1231
Training took 0.151706 minutes
Weight histogram
[2217 1831 1901  681  848  310  149   72   44   47] [-0.13868089 -0.12516467 -0.11164845 -0.09813222 -0.084616   -0.07109978
 -0.05758356 -0.04406734 -0.03055111 -0.01703489 -0.00351867]
[  79   94  154  251  387  584 1004 1182 1917 2448] [-0.13868089 -0.12516467 -0.11164845 -0.09813222 -0.084616   -0.07109978
 -0.05758356 -0.04406734 -0.03055111 -0.01703489 -0.00351867]
-3.99307
4.8317
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226600 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.22547
Epoch 1, cost is  1.16206
Epoch 2, cost is  1.13004
Epoch 3, cost is  1.10476
Epoch 4, cost is  1.08373
Training took 0.153431 minutes
Weight histogram
[2377 1770 1880 1657 1364  513  246  133   86   99] [-0.13368584 -0.12066912 -0.10765241 -0.09463569 -0.08161897 -0.06860226
 -0.05558554 -0.04256882 -0.0295521  -0.01653539 -0.00351867]
[ 166  181  295  471  761 1194 1409 1241 1953 2454] [-0.13368584 -0.12066912 -0.10765241 -0.09463569 -0.08161897 -0.06860226
 -0.05558554 -0.04256882 -0.0295521  -0.01653539 -0.00351867]
-4.04413
4.97178
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.05222
Epoch 1, cost is  1.40523
Epoch 2, cost is  1.01672
Epoch 3, cost is  0.838913
Epoch 4, cost is  0.733619
Training took 0.157711 minutes
Weight histogram
[2318 1692 1212  805  610  434  326  229  313  161] [-0.10636774 -0.09611068 -0.08585362 -0.07559657 -0.06533951 -0.05508246
 -0.0448254  -0.03456834 -0.02431129 -0.01405423 -0.00379718]
[ 564  308  359  466  577  715  899 1113 1417 1682] [-0.10636774 -0.09611068 -0.08585362 -0.07559657 -0.06533951 -0.05508246
 -0.0448254  -0.03456834 -0.02431129 -0.01405423 -0.00379718]
-1.99174
2.92356
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088750 minutes
Epoch 0
Fine tuning took 0.089302 minutes
Epoch 0
Fine tuning took 0.088419 minutes
{'zero': {0: [0.19334975369458129, 0.41871921182266009, 0.34482758620689657, 0.51847290640394084], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.3854679802955665, 0.36699507389162561, 0.32019704433497537], 5: [0.1145320197044335, 0.19581280788177341, 0.28817733990147781, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19334975369458129, 0.41748768472906406, 0.35837438423645318, 0.52586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.35591133004926107, 0.37807881773399016, 0.30049261083743845], 5: [0.1145320197044335, 0.22660098522167488, 0.26354679802955666, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19334975369458129, 0.37315270935960593, 0.33990147783251229, 0.49137931034482757], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.42857142857142855, 0.39901477832512317, 0.33743842364532017], 5: [0.1145320197044335, 0.19827586206896552, 0.26108374384236455, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19334975369458129, 0.41379310344827586, 0.34852216748768472, 0.56280788177339902], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.35714285714285715, 0.34113300492610837, 0.27832512315270935], 5: [0.1145320197044335, 0.22906403940886699, 0.31034482758620691, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226222 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.46142
Epoch 1, cost is  1.41935
Epoch 2, cost is  1.38877
Epoch 3, cost is  1.36504
Epoch 4, cost is  1.34325
Training took 0.152755 minutes
Weight histogram
[2479 1791 1440  615  551  355  292  206  270  101] [-0.06636207 -0.05975078 -0.0531395  -0.04652821 -0.03991692 -0.03330564
 -0.02669435 -0.02008307 -0.01347178 -0.00686049 -0.00024921]
[ 410  283  341  449  590  754  895 1195 1449 1734] [-0.06636207 -0.05975078 -0.0531395  -0.04652821 -0.03991692 -0.03330564
 -0.02669435 -0.02008307 -0.01347178 -0.00686049 -0.00024921]
-1.33984
2.2144
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225290 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.42085
Epoch 1, cost is  1.37976
Epoch 2, cost is  1.34951
Epoch 3, cost is  1.32508
Epoch 4, cost is  1.30588
Training took 0.154126 minutes
Weight histogram
[2327 1708 1474  615 1310  792  609  462  563  265] [-0.06852407 -0.06169658 -0.0548691  -0.04804161 -0.04121413 -0.03438664
 -0.02755915 -0.02073167 -0.01390418 -0.00707669 -0.00024921]
[ 868  586  699  903 1016  723  869 1197 1465 1799] [-0.06852407 -0.06169658 -0.0548691  -0.04804161 -0.04121413 -0.03438664
 -0.02755915 -0.02073167 -0.01390418 -0.00707669 -0.00024921]
-1.35083
1.58202
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.47693
Epoch 1, cost is  5.72264
Epoch 2, cost is  4.79204
Epoch 3, cost is  4.08463
Epoch 4, cost is  3.67824
Training took 0.089325 minutes
Weight histogram
[ 769  722  682  599  633  554  692  807 1494 1148] [-0.06757683 -0.06085255 -0.05412827 -0.047404   -0.04067972 -0.03395544
 -0.02723117 -0.02050689 -0.01378261 -0.00705834 -0.00033406]
[2032  894  658  569  589  638  671  718  791  540] [-0.06757683 -0.06085255 -0.05412827 -0.047404   -0.04067972 -0.03395544
 -0.02723117 -0.02050689 -0.01378261 -0.00705834 -0.00033406]
-0.802425
1.34971
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081197 minutes
Epoch 0
Fine tuning took 0.081104 minutes
Epoch 0
Fine tuning took 0.081538 minutes
{'zero': {0: [0.13423645320197045, 0.011083743842364532, 0.0086206896551724137, 0.024630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82266009852216748, 0.93965517241379315, 0.89778325123152714, 0.91871921182266014], 5: [0.043103448275862072, 0.049261083743842367, 0.093596059113300489, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13423645320197045, 0.0073891625615763543, 0.0061576354679802959, 0.020935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82266009852216748, 0.9568965517241379, 0.91995073891625612, 0.93472906403940892], 5: [0.043103448275862072, 0.035714285714285712, 0.073891625615763554, 0.044334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13423645320197045, 0.0049261083743842365, 0.0073891625615763543, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82266009852216748, 0.95320197044334976, 0.93226600985221675, 0.93965517241379315], 5: [0.043103448275862072, 0.041871921182266007, 0.060344827586206899, 0.045566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13423645320197045, 0.0061576354679802959, 0.0061576354679802959, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82266009852216748, 0.96551724137931039, 0.92241379310344829, 0.94581280788177335], 5: [0.043103448275862072, 0.02832512315270936, 0.071428571428571425, 0.039408866995073892], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225873 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.46142
Epoch 1, cost is  1.41935
Epoch 2, cost is  1.38877
Epoch 3, cost is  1.36504
Epoch 4, cost is  1.34325
Training took 0.152485 minutes
Weight histogram
[2479 1791 1440  615  551  355  292  206  270  101] [-0.06636207 -0.05975078 -0.0531395  -0.04652821 -0.03991692 -0.03330564
 -0.02669435 -0.02008307 -0.01347178 -0.00686049 -0.00024921]
[ 410  283  341  449  590  754  895 1195 1449 1734] [-0.06636207 -0.05975078 -0.0531395  -0.04652821 -0.03991692 -0.03330564
 -0.02669435 -0.02008307 -0.01347178 -0.00686049 -0.00024921]
-1.33984
2.2144
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225892 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.42085
Epoch 1, cost is  1.37976
Epoch 2, cost is  1.34951
Epoch 3, cost is  1.32508
Epoch 4, cost is  1.30588
Training took 0.151375 minutes
Weight histogram
[2327 1708 1474  615 1310  792  609  462  563  265] [-0.06852407 -0.06169658 -0.0548691  -0.04804161 -0.04121413 -0.03438664
 -0.02755915 -0.02073167 -0.01390418 -0.00707669 -0.00024921]
[ 868  586  699  903 1016  723  869 1197 1465 1799] [-0.06852407 -0.06169658 -0.0548691  -0.04804161 -0.04121413 -0.03438664
 -0.02755915 -0.02073167 -0.01390418 -0.00707669 -0.00024921]
-1.35083
1.58202
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.40438
Epoch 1, cost is  5.64984
Epoch 2, cost is  4.62482
Epoch 3, cost is  3.7994
Epoch 4, cost is  3.32224
Training took 0.106479 minutes
Weight histogram
[ 601  845  753  640  692  560  859 1025 2028   97] [-0.04952583 -0.04460802 -0.0396902  -0.03477238 -0.02985456 -0.02493675
 -0.02001893 -0.01510111 -0.0101833  -0.00526548 -0.00034766]
[2042 1032  576  536  567  602  641  700  794  610] [-0.04952583 -0.04460802 -0.0396902  -0.03477238 -0.02985456 -0.02493675
 -0.02001893 -0.01510111 -0.0101833  -0.00526548 -0.00034766]
-0.634967
1.13669
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084106 minutes
Epoch 0
Fine tuning took 0.082929 minutes
Epoch 0
Fine tuning took 0.082511 minutes
{'zero': {0: [0.12807881773399016, 0.035714285714285712, 0.056650246305418719, 0.060344827586206899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80911330049261088, 0.88177339901477836, 0.83128078817733986, 0.80418719211822665], 5: [0.062807881773399021, 0.082512315270935957, 0.11206896551724138, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12807881773399016, 0.017241379310344827, 0.030788177339901478, 0.032019704433497539], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80911330049261088, 0.95197044334975367, 0.9076354679802956, 0.8854679802955665], 5: [0.062807881773399021, 0.030788177339901478, 0.061576354679802957, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12807881773399016, 0.009852216748768473, 0.022167487684729065, 0.035714285714285712], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80911330049261088, 0.9211822660098522, 0.8854679802955665, 0.86945812807881773], 5: [0.062807881773399021, 0.068965517241379309, 0.092364532019704432, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12807881773399016, 0.009852216748768473, 0.029556650246305417, 0.038177339901477834], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80911330049261088, 0.94088669950738912, 0.87438423645320196, 0.90517241379310343], 5: [0.062807881773399021, 0.049261083743842367, 0.096059113300492605, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227247 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.46142
Epoch 1, cost is  1.41935
Epoch 2, cost is  1.38877
Epoch 3, cost is  1.36504
Epoch 4, cost is  1.34325
Training took 0.150656 minutes
Weight histogram
[2479 1791 1440  615  551  355  292  206  270  101] [-0.06636207 -0.05975078 -0.0531395  -0.04652821 -0.03991692 -0.03330564
 -0.02669435 -0.02008307 -0.01347178 -0.00686049 -0.00024921]
[ 410  283  341  449  590  754  895 1195 1449 1734] [-0.06636207 -0.05975078 -0.0531395  -0.04652821 -0.03991692 -0.03330564
 -0.02669435 -0.02008307 -0.01347178 -0.00686049 -0.00024921]
-1.33984
2.2144
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227448 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.42085
Epoch 1, cost is  1.37976
Epoch 2, cost is  1.34951
Epoch 3, cost is  1.32508
Epoch 4, cost is  1.30588
Training took 0.150722 minutes
Weight histogram
[2327 1708 1474  615 1310  792  609  462  563  265] [-0.06852407 -0.06169658 -0.0548691  -0.04804161 -0.04121413 -0.03438664
 -0.02755915 -0.02073167 -0.01390418 -0.00707669 -0.00024921]
[ 868  586  699  903 1016  723  869 1197 1465 1799] [-0.06852407 -0.06169658 -0.0548691  -0.04804161 -0.04121413 -0.03438664
 -0.02755915 -0.02073167 -0.01390418 -0.00707669 -0.00024921]
-1.35083
1.58202
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.21019
Epoch 1, cost is  5.48246
Epoch 2, cost is  4.43301
Epoch 3, cost is  3.50955
Epoch 4, cost is  2.94394
Training took 0.157710 minutes
Weight histogram
[ 692  983  777  708  693  991 1557 1490  174   35] [-0.03077359 -0.02772865 -0.02468371 -0.02163877 -0.01859383 -0.01554889
 -0.01250395 -0.00945901 -0.00641407 -0.00336913 -0.00032419]
[2350  933  505  517  524  572  620  680  738  661] [-0.03077359 -0.02772865 -0.02468371 -0.02163877 -0.01859383 -0.01554889
 -0.01250395 -0.00945901 -0.00641407 -0.00336913 -0.00032419]
-0.558863
1.02393
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088886 minutes
Epoch 0
Fine tuning took 0.089367 minutes
Epoch 0
Fine tuning took 0.088257 minutes
{'zero': {0: [0.14162561576354679, 0.1354679802955665, 0.19334975369458129, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77463054187192115, 0.71305418719211822, 0.60098522167487689, 0.57635467980295563], 5: [0.083743842364532015, 0.15147783251231528, 0.20566502463054187, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14162561576354679, 0.054187192118226604, 0.091133004926108374, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77463054187192115, 0.85221674876847286, 0.72783251231527091, 0.71798029556650245], 5: [0.083743842364532015, 0.093596059113300489, 0.18103448275862069, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14162561576354679, 0.068965517241379309, 0.12315270935960591, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77463054187192115, 0.83374384236453203, 0.68226600985221675, 0.60960591133004927], 5: [0.083743842364532015, 0.097290640394088676, 0.19458128078817735, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14162561576354679, 0.062807881773399021, 0.099753694581280791, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77463054187192115, 0.86945812807881773, 0.72536945812807885, 0.69704433497536944], 5: [0.083743842364532015, 0.067733990147783252, 0.1748768472906404, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227605 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.57549
Epoch 1, cost is  3.45358
Epoch 2, cost is  3.34546
Epoch 3, cost is  3.24323
Epoch 4, cost is  3.14968
Training took 0.153867 minutes
Weight histogram
[1048  882  900  808  695  710  778 1919  290   70] [ -2.95287631e-02  -2.65681131e-02  -2.36074630e-02  -2.06468129e-02
  -1.76861628e-02  -1.47255127e-02  -1.17648626e-02  -8.80421250e-03
  -5.84356241e-03  -2.88291232e-03   7.77377718e-05]
[2024  617  566  605  650  675  678  739  753  793] [ -2.95287631e-02  -2.65681131e-02  -2.36074630e-02  -2.06468129e-02
  -1.76861628e-02  -1.47255127e-02  -1.17648626e-02  -8.80421250e-03
  -5.84356241e-03  -2.88291232e-03   7.77377718e-05]
-0.460728
0.749662
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226644 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.78174
Epoch 1, cost is  3.65112
Epoch 2, cost is  3.53533
Epoch 3, cost is  3.41481
Epoch 4, cost is  3.31675
Training took 0.152000 minutes
Weight histogram
[ 987  855  827  823  694  782  861 3781  395  120] [ -2.68282462e-02  -2.41376478e-02  -2.14470494e-02  -1.87564510e-02
  -1.60658526e-02  -1.33752542e-02  -1.06846558e-02  -7.99405742e-03
  -5.30345902e-03  -2.61286062e-03   7.77377718e-05]
[4198  684  613  598  629  629  647  698  698  731] [ -2.68282462e-02  -2.41376478e-02  -2.14470494e-02  -1.87564510e-02
  -1.60658526e-02  -1.33752542e-02  -1.06846558e-02  -7.99405742e-03
  -5.30345902e-03  -2.61286062e-03   7.77377718e-05]
-0.472176
0.72473
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.82312
Epoch 1, cost is  6.70678
Epoch 2, cost is  6.60643
Epoch 3, cost is  6.51075
Epoch 4, cost is  6.41814
Training took 0.091060 minutes
Weight histogram
[1365 4011 1422  482  282  187  131   93   70   57] [ -8.89928360e-03  -8.01271107e-03  -7.12613855e-03  -6.23956602e-03
  -5.35299350e-03  -4.46642097e-03  -3.57984845e-03  -2.69327592e-03
  -1.80670340e-03  -9.20130871e-04  -3.35583463e-05]
[2933 1237  936  778  698  607  497  211   99  104] [ -8.89928360e-03  -8.01271107e-03  -7.12613855e-03  -6.23956602e-03
  -5.35299350e-03  -4.46642097e-03  -3.57984845e-03  -2.69327592e-03
  -1.80670340e-03  -9.20130871e-04  -3.35583463e-05]
-0.0589014
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082188 minutes
Epoch 0
Fine tuning took 0.082630 minutes
Epoch 0
Fine tuning took 0.081948 minutes
{'zero': {0: [0.16502463054187191, 0.17118226600985223, 0.19827586206896552, 0.35837438423645318], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77339901477832518, 0.6711822660098522, 0.67610837438423643, 0.50862068965517238], 5: [0.061576354679802957, 0.15763546798029557, 0.12561576354679804, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16502463054187191, 0.17364532019704434, 0.20073891625615764, 0.37684729064039407], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77339901477832518, 0.6711822660098522, 0.66133004926108374, 0.49753694581280788], 5: [0.061576354679802957, 0.15517241379310345, 0.13793103448275862, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16502463054187191, 0.18596059113300492, 0.18719211822660098, 0.37438423645320196], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77339901477832518, 0.65394088669950734, 0.68596059113300489, 0.5073891625615764], 5: [0.061576354679802957, 0.16009852216748768, 0.1268472906403941, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16502463054187191, 0.16379310344827586, 0.19827586206896552, 0.34359605911330049], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77339901477832518, 0.6785714285714286, 0.66748768472906406, 0.52832512315270941], 5: [0.061576354679802957, 0.15763546798029557, 0.13423645320197045, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.228733 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.57549
Epoch 1, cost is  3.45358
Epoch 2, cost is  3.34546
Epoch 3, cost is  3.24323
Epoch 4, cost is  3.14968
Training took 0.152491 minutes
Weight histogram
[1048  882  900  808  695  710  778 1919  290   70] [ -2.95287631e-02  -2.65681131e-02  -2.36074630e-02  -2.06468129e-02
  -1.76861628e-02  -1.47255127e-02  -1.17648626e-02  -8.80421250e-03
  -5.84356241e-03  -2.88291232e-03   7.77377718e-05]
[2024  617  566  605  650  675  678  739  753  793] [ -2.95287631e-02  -2.65681131e-02  -2.36074630e-02  -2.06468129e-02
  -1.76861628e-02  -1.47255127e-02  -1.17648626e-02  -8.80421250e-03
  -5.84356241e-03  -2.88291232e-03   7.77377718e-05]
-0.460728
0.749662
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226745 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.78174
Epoch 1, cost is  3.65112
Epoch 2, cost is  3.53533
Epoch 3, cost is  3.41481
Epoch 4, cost is  3.31675
Training took 0.153745 minutes
Weight histogram
[ 987  855  827  823  694  782  861 3781  395  120] [ -2.68282462e-02  -2.41376478e-02  -2.14470494e-02  -1.87564510e-02
  -1.60658526e-02  -1.33752542e-02  -1.06846558e-02  -7.99405742e-03
  -5.30345902e-03  -2.61286062e-03   7.77377718e-05]
[4198  684  613  598  629  629  647  698  698  731] [ -2.68282462e-02  -2.41376478e-02  -2.14470494e-02  -1.87564510e-02
  -1.60658526e-02  -1.33752542e-02  -1.06846558e-02  -7.99405742e-03
  -5.30345902e-03  -2.61286062e-03   7.77377718e-05]
-0.472176
0.72473
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.7665
Epoch 1, cost is  6.6269
Epoch 2, cost is  6.52131
Epoch 3, cost is  6.42183
Epoch 4, cost is  6.32271
Training took 0.108733 minutes
Weight histogram
[1227 2991 2391  584  323  208  141  102   73   60] [ -9.23346821e-03  -8.31580675e-03  -7.39814529e-03  -6.48048383e-03
  -5.56282238e-03  -4.64516092e-03  -3.72749946e-03  -2.80983800e-03
  -1.89217654e-03  -9.74515079e-04  -5.68536198e-05]
[2804 1225  937  790  715  633  519  260  107  110] [ -9.23346821e-03  -8.31580675e-03  -7.39814529e-03  -6.48048383e-03
  -5.56282238e-03  -4.64516092e-03  -3.72749946e-03  -2.80983800e-03
  -1.89217654e-03  -9.74515079e-04  -5.68536198e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082554 minutes
Epoch 0
Fine tuning took 0.083495 minutes
Epoch 0
Fine tuning took 0.084492 minutes
{'zero': {0: [0.18226600985221675, 0.19088669950738915, 0.22660098522167488, 0.34113300492610837], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.66871921182266014, 0.62684729064039413, 0.56034482758620685], 5: [0.054187192118226604, 0.14039408866995073, 0.14655172413793102, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18226600985221675, 0.17364532019704434, 0.24876847290640394, 0.33128078817733991], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.69211822660098521, 0.6219211822660099, 0.59729064039408863], 5: [0.054187192118226604, 0.13423645320197045, 0.12931034482758622, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18226600985221675, 0.20320197044334976, 0.22167487684729065, 0.31403940886699505], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.63916256157635465, 0.62931034482758619, 0.58620689655172409], 5: [0.054187192118226604, 0.15763546798029557, 0.14901477832512317, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18226600985221675, 0.19088669950738915, 0.20812807881773399, 0.34113300492610837], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.67364532019704437, 0.6354679802955665, 0.55911330049261088], 5: [0.054187192118226604, 0.1354679802955665, 0.15640394088669951, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227927 minutes
Weight histogram
[  34  161  449  606  756  856 1324 1849 1769  296] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
[ 538  571  524  655  590  739  951 1000 1225 1307] [ -1.34912640e-04  -3.76487005e-05   5.96152386e-05   1.56879178e-04
   2.54143117e-04   3.51407056e-04   4.48670995e-04   5.45934934e-04
   6.43198873e-04   7.40462812e-04   8.37726751e-04]
-0.513507
0.701201
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.57549
Epoch 1, cost is  3.45358
Epoch 2, cost is  3.34546
Epoch 3, cost is  3.24323
Epoch 4, cost is  3.14968
Training took 0.153183 minutes
Weight histogram
[1048  882  900  808  695  710  778 1919  290   70] [ -2.95287631e-02  -2.65681131e-02  -2.36074630e-02  -2.06468129e-02
  -1.76861628e-02  -1.47255127e-02  -1.17648626e-02  -8.80421250e-03
  -5.84356241e-03  -2.88291232e-03   7.77377718e-05]
[2024  617  566  605  650  675  678  739  753  793] [ -2.95287631e-02  -2.65681131e-02  -2.36074630e-02  -2.06468129e-02
  -1.76861628e-02  -1.47255127e-02  -1.17648626e-02  -8.80421250e-03
  -5.84356241e-03  -2.88291232e-03   7.77377718e-05]
-0.460728
0.749662
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227154 minutes
Weight histogram
[  53  342  838 1249 1251 1080 1113 1902 1874  423] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
[ 762 1162 1078  844  691  795  892 1086 1277 1538] [ -1.34912640e-04  -4.08284977e-05   5.32556442e-05   1.47339786e-04
   2.41423928e-04   3.35508070e-04   4.29592212e-04   5.23676354e-04
   6.17760496e-04   7.11844637e-04   8.05928779e-04]
-0.575562
0.459658
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.78174
Epoch 1, cost is  3.65112
Epoch 2, cost is  3.53533
Epoch 3, cost is  3.41481
Epoch 4, cost is  3.31675
Training took 0.152606 minutes
Weight histogram
[ 987  855  827  823  694  782  861 3781  395  120] [ -2.68282462e-02  -2.41376478e-02  -2.14470494e-02  -1.87564510e-02
  -1.60658526e-02  -1.33752542e-02  -1.06846558e-02  -7.99405742e-03
  -5.30345902e-03  -2.61286062e-03   7.77377718e-05]
[4198  684  613  598  629  629  647  698  698  731] [ -2.68282462e-02  -2.41376478e-02  -2.14470494e-02  -1.87564510e-02
  -1.60658526e-02  -1.33752542e-02  -1.06846558e-02  -7.99405742e-03
  -5.30345902e-03  -2.61286062e-03   7.77377718e-05]
-0.472176
0.72473
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.5984
Epoch 1, cost is  6.39769
Epoch 2, cost is  6.27771
Epoch 3, cost is  6.16614
Epoch 4, cost is  6.06453
Training took 0.157761 minutes
Weight histogram
[ 929 1954 3105  956  444  264  176  122   84   66] [ -1.00774989e-02  -9.07406167e-03  -8.07062448e-03  -7.06718730e-03
  -6.06375011e-03  -5.06031292e-03  -4.05687574e-03  -3.05343855e-03
  -2.05000137e-03  -1.04656418e-03  -4.31269946e-05]
[2536 1162  903  825  738  715  596  370  125  130] [ -1.00774989e-02  -9.07406167e-03  -8.07062448e-03  -7.06718730e-03
  -6.06375011e-03  -5.06031292e-03  -4.05687574e-03  -3.05343855e-03
  -2.05000137e-03  -1.04656418e-03  -4.31269946e-05]
-0.0601954
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089460 minutes
Epoch 0
Fine tuning took 0.091993 minutes
Epoch 0
Fine tuning took 0.092468 minutes
{'zero': {0: [0.19950738916256158, 0.20566502463054187, 0.18349753694581281, 0.39408866995073893], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.66009852216748766, 0.6576354679802956, 0.46305418719211822], 5: [0.057881773399014777, 0.13423645320197045, 0.15886699507389163, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19950738916256158, 0.18103448275862069, 0.18103448275862069, 0.34482758620689657], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.70197044334975367, 0.66256157635467983, 0.52709359605911332], 5: [0.057881773399014777, 0.11699507389162561, 0.15640394088669951, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19950738916256158, 0.2019704433497537, 0.20443349753694581, 0.32758620689655171], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.66379310344827591, 0.6219211822660099, 0.52832512315270941], 5: [0.057881773399014777, 0.13423645320197045, 0.17364532019704434, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19950738916256158, 0.18965517241379309, 0.21305418719211822, 0.36945812807881773], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.67980295566502458, 0.61576354679802958, 0.49507389162561577], 5: [0.057881773399014777, 0.13054187192118227, 0.17118226600985223, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.248557 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.13331
Epoch 1, cost is  1.07595
Epoch 2, cost is  1.04346
Epoch 3, cost is  1.02156
Epoch 4, cost is  1.0027
Training took 0.151121 minutes
Weight histogram
[3202 2532 1229 1132 1159  503  182   88   48   50] [-0.14932036 -0.13474019 -0.12016003 -0.10557986 -0.09099969 -0.07641952
 -0.06183935 -0.04725918 -0.03267901 -0.01809884 -0.00351867]
[  85  105  174  300  463  733 1207 1711 2376 2971] [-0.14932036 -0.13474019 -0.12016003 -0.10557986 -0.09099969 -0.07641952
 -0.06183935 -0.04725918 -0.03267901 -0.01809884 -0.00351867]
-4.11005
5.12858
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225339 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.10521
Epoch 1, cost is  1.04742
Epoch 2, cost is  1.01634
Epoch 3, cost is  0.993486
Epoch 4, cost is  0.975023
Training took 0.152353 minutes
Weight histogram
[3317 2553 1382 1239 2161  798  329  167   99  105] [-0.14417157 -0.13010628 -0.11604099 -0.1019757  -0.08791041 -0.07384512
 -0.05977983 -0.04571454 -0.03164925 -0.01758396 -0.00351867]
[ 175  203  332  558  917 1442 1366 1708 2398 3051] [-0.14417157 -0.13010628 -0.11604099 -0.1019757  -0.08791041 -0.07384512
 -0.05977983 -0.04571454 -0.03164925 -0.01758396 -0.00351867]
-4.34258
5.56814
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.43811
Epoch 1, cost is  1.94057
Epoch 2, cost is  1.6537
Epoch 3, cost is  1.51039
Epoch 4, cost is  1.41895
Training took 0.091052 minutes
Weight histogram
[2581 2214 1543 1266  743  497  358  256  333  334] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[ 653  449  480  600  707  895 1125 1393 1734 2089] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.68512
3.94212
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084400 minutes
Epoch 0
Fine tuning took 0.085227 minutes
Epoch 0
Fine tuning took 0.083476 minutes
{'zero': {0: [0.13054187192118227, 0.24876847290640394, 0.29310344827586204, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.49261083743842365, 0.48522167487684731, 0.47167487684729065], 5: [0.13300492610837439, 0.25862068965517243, 0.22167487684729065, 0.25985221674876846], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13054187192118227, 0.051724137931034482, 0.054187192118226604, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.86083743842364535, 0.88054187192118227, 0.90394088669950734], 5: [0.13300492610837439, 0.087438423645320201, 0.065270935960591137, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13054187192118227, 0.067733990147783252, 0.062807881773399021, 0.046798029556650245], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.82758620689655171, 0.84359605911330049, 0.87315270935960587], 5: [0.13300492610837439, 0.10467980295566502, 0.093596059113300489, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13054187192118227, 0.02832512315270936, 0.024630541871921183, 0.012315270935960592], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73645320197044339, 0.90886699507389157, 0.94211822660098521, 0.93965517241379315], 5: [0.13300492610837439, 0.062807881773399021, 0.033251231527093597, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.254976 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.13331
Epoch 1, cost is  1.07595
Epoch 2, cost is  1.04346
Epoch 3, cost is  1.02156
Epoch 4, cost is  1.0027
Training took 0.154060 minutes
Weight histogram
[3202 2532 1229 1132 1159  503  182   88   48   50] [-0.14932036 -0.13474019 -0.12016003 -0.10557986 -0.09099969 -0.07641952
 -0.06183935 -0.04725918 -0.03267901 -0.01809884 -0.00351867]
[  85  105  174  300  463  733 1207 1711 2376 2971] [-0.14932036 -0.13474019 -0.12016003 -0.10557986 -0.09099969 -0.07641952
 -0.06183935 -0.04725918 -0.03267901 -0.01809884 -0.00351867]
-4.11005
5.12858
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225265 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.10521
Epoch 1, cost is  1.04742
Epoch 2, cost is  1.01634
Epoch 3, cost is  0.993486
Epoch 4, cost is  0.975023
Training took 0.152130 minutes
Weight histogram
[3317 2553 1382 1239 2161  798  329  167   99  105] [-0.14417157 -0.13010628 -0.11604099 -0.1019757  -0.08791041 -0.07384512
 -0.05977983 -0.04571454 -0.03164925 -0.01758396 -0.00351867]
[ 175  203  332  558  917 1442 1366 1708 2398 3051] [-0.14417157 -0.13010628 -0.11604099 -0.1019757  -0.08791041 -0.07384512
 -0.05977983 -0.04571454 -0.03164925 -0.01758396 -0.00351867]
-4.34258
5.56814
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.24768
Epoch 1, cost is  1.64802
Epoch 2, cost is  1.30053
Epoch 3, cost is  1.12279
Epoch 4, cost is  1.01457
Training took 0.115922 minutes
Weight histogram
[2809 2157 1549 1072  782  545  328  236  381  266] [-0.15470643 -0.13963134 -0.12455626 -0.10948117 -0.09440608 -0.07933099
 -0.0642559  -0.04918081 -0.03410572 -0.01903063 -0.00395554]
[ 672  438  471  592  733  929 1136 1398 1717 2039] [-0.15470643 -0.13963134 -0.12455626 -0.10948117 -0.09440608 -0.07933099
 -0.0642559  -0.04918081 -0.03410572 -0.01903063 -0.00395554]
-3.11941
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.094362 minutes
Epoch 0
Fine tuning took 0.084902 minutes
Epoch 0
Fine tuning took 0.087196 minutes
{'zero': {0: [0.1145320197044335, 0.3608374384236453, 0.3288177339901478, 0.31527093596059114], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79433497536945807, 0.39039408866995073, 0.36945812807881773, 0.41995073891625617], 5: [0.091133004926108374, 0.24876847290640394, 0.30172413793103448, 0.26477832512315269], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1145320197044335, 0.18596059113300492, 0.13054187192118227, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79433497536945807, 0.66995073891625612, 0.72536945812807885, 0.75615763546798032], 5: [0.091133004926108374, 0.14408866995073891, 0.14408866995073891, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1145320197044335, 0.17857142857142858, 0.13793103448275862, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79433497536945807, 0.68965517241379315, 0.72660098522167482, 0.76354679802955661], 5: [0.091133004926108374, 0.13177339901477833, 0.1354679802955665, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1145320197044335, 0.15886699507389163, 0.094827586206896547, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79433497536945807, 0.74384236453201968, 0.7857142857142857, 0.79187192118226601], 5: [0.091133004926108374, 0.097290640394088676, 0.11945812807881774, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.251865 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.13331
Epoch 1, cost is  1.07595
Epoch 2, cost is  1.04346
Epoch 3, cost is  1.02156
Epoch 4, cost is  1.0027
Training took 0.160100 minutes
Weight histogram
[3202 2532 1229 1132 1159  503  182   88   48   50] [-0.14932036 -0.13474019 -0.12016003 -0.10557986 -0.09099969 -0.07641952
 -0.06183935 -0.04725918 -0.03267901 -0.01809884 -0.00351867]
[  85  105  174  300  463  733 1207 1711 2376 2971] [-0.14932036 -0.13474019 -0.12016003 -0.10557986 -0.09099969 -0.07641952
 -0.06183935 -0.04725918 -0.03267901 -0.01809884 -0.00351867]
-4.11005
5.12858
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225911 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.10521
Epoch 1, cost is  1.04742
Epoch 2, cost is  1.01634
Epoch 3, cost is  0.993486
Epoch 4, cost is  0.975023
Training took 0.151401 minutes
Weight histogram
[3317 2553 1382 1239 2161  798  329  167   99  105] [-0.14417157 -0.13010628 -0.11604099 -0.1019757  -0.08791041 -0.07384512
 -0.05977983 -0.04571454 -0.03164925 -0.01758396 -0.00351867]
[ 175  203  332  558  917 1442 1366 1708 2398 3051] [-0.14417157 -0.13010628 -0.11604099 -0.1019757  -0.08791041 -0.07384512
 -0.05977983 -0.04571454 -0.03164925 -0.01758396 -0.00351867]
-4.34258
5.56814
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.04514
Epoch 1, cost is  1.39777
Epoch 2, cost is  1.0
Epoch 3, cost is  0.820642
Epoch 4, cost is  0.711542
Training took 0.156767 minutes
Weight histogram
[2869 2117 1498 1024  766  551  419  285  395  201] [-0.10689014 -0.09658085 -0.08627155 -0.07596225 -0.06565296 -0.05534366
 -0.04503436 -0.03472507 -0.02441577 -0.01410647 -0.00379718]
[ 710  392  456  586  727  898 1129 1402 1770 2055] [-0.10689014 -0.09658085 -0.08627155 -0.07596225 -0.06565296 -0.05534366
 -0.04503436 -0.03472507 -0.02441577 -0.01410647 -0.00379718]
-2.01397
2.92356
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089768 minutes
Epoch 0
Fine tuning took 0.089509 minutes
Epoch 0
Fine tuning took 0.087740 minutes
{'zero': {0: [0.18349753694581281, 0.38793103448275862, 0.33620689655172414, 0.30541871921182268], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.40517241379310343, 0.36206896551724138, 0.42980295566502463], 5: [0.098522167487684734, 0.20689655172413793, 0.30172413793103448, 0.26477832512315269], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18349753694581281, 0.39778325123152708, 0.33743842364532017, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.39532019704433496, 0.39532019704433496, 0.43349753694581283], 5: [0.098522167487684734, 0.20689655172413793, 0.26724137931034481, 0.27832512315270935], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18349753694581281, 0.34975369458128081, 0.29064039408866993, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.41995073891625617, 0.40763546798029554, 0.47660098522167488], 5: [0.098522167487684734, 0.23029556650246305, 0.30172413793103448, 0.27093596059113301], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18349753694581281, 0.36576354679802958, 0.3460591133004926, 0.33004926108374383], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.39162561576354682, 0.35714285714285715, 0.40517241379310343], 5: [0.098522167487684734, 0.24261083743842365, 0.29679802955665024, 0.26477832512315269], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226564 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.27984
Epoch 1, cost is  1.24845
Epoch 2, cost is  1.22617
Epoch 3, cost is  1.20683
Epoch 4, cost is  1.19011
Training took 0.152192 minutes
Weight histogram
[3369 2331 1463  899  683  449  301  238  241  151] [-0.07114601 -0.06405633 -0.05696665 -0.04987697 -0.04278729 -0.03569761
 -0.02860793 -0.02151825 -0.01442857 -0.00733889 -0.00024921]
[ 432  316  402  533  738  894 1197 1441 1889 2283] [-0.07114601 -0.06405633 -0.05696665 -0.04987697 -0.04278729 -0.03569761
 -0.02860793 -0.02151825 -0.01442857 -0.00733889 -0.00024921]
-1.66788
2.30098
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225080 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.25434
Epoch 1, cost is  1.22112
Epoch 2, cost is  1.19788
Epoch 3, cost is  1.17982
Epoch 4, cost is  1.1643
Training took 0.153561 minutes
Weight histogram
[3380 2176 1292 1125 1118  969  673  532  502  383] [-0.07334977 -0.06603972 -0.05872966 -0.0514196  -0.04410955 -0.03679949
 -0.02948943 -0.02217938 -0.01486932 -0.00755926 -0.00024921]
[ 914  648  815 1067  947  848 1158 1431 1918 2404] [-0.07334977 -0.06603972 -0.05872966 -0.0514196  -0.04410955 -0.03679949
 -0.02948943 -0.02217938 -0.01486932 -0.00755926 -0.00024921]
-1.54675
1.7981
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.48299
Epoch 1, cost is  5.7346
Epoch 2, cost is  4.78878
Epoch 3, cost is  4.08123
Epoch 4, cost is  3.69013
Training took 0.091992 minutes
Weight histogram
[ 927  913  860  742  773  697  865 1039 1789 1520] [-0.06757683 -0.06085222 -0.05412761 -0.047403   -0.04067839 -0.03395379
 -0.02722918 -0.02050457 -0.01377996 -0.00705535 -0.00033074]
[2520 1146  820  713  737  795  841  909 1011  633] [-0.06757683 -0.06085222 -0.05412761 -0.047403   -0.04067839 -0.03395379
 -0.02722918 -0.02050457 -0.01377996 -0.00705535 -0.00033074]
-0.829642
1.34971
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080696 minutes
Epoch 0
Fine tuning took 0.082237 minutes
Epoch 0
Fine tuning took 0.083095 minutes
{'zero': {0: [0.043103448275862072, 0.01600985221674877, 0.020935960591133004, 0.04064039408866995], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83990147783251234, 0.95443349753694584, 0.92980295566502458, 0.88793103448275867], 5: [0.11699507389162561, 0.029556650246305417, 0.049261083743842367, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.043103448275862072, 0.0086206896551724137, 0.011083743842364532, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83990147783251234, 0.97413793103448276, 0.95320197044334976, 0.92241379310344829], 5: [0.11699507389162561, 0.017241379310344827, 0.035714285714285712, 0.055418719211822662], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.043103448275862072, 0.014778325123152709, 0.0073891625615763543, 0.02832512315270936], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83990147783251234, 0.94581280788177335, 0.95073891625615758, 0.90640394088669951], 5: [0.11699507389162561, 0.039408866995073892, 0.041871921182266007, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.043103448275862072, 0.0049261083743842365, 0.0061576354679802959, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83990147783251234, 0.96798029556650245, 0.95443349753694584, 0.91995073891625612], 5: [0.11699507389162561, 0.027093596059113302, 0.039408866995073892, 0.060344827586206899], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226522 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.27984
Epoch 1, cost is  1.24845
Epoch 2, cost is  1.22617
Epoch 3, cost is  1.20683
Epoch 4, cost is  1.19011
Training took 0.151733 minutes
Weight histogram
[3369 2331 1463  899  683  449  301  238  241  151] [-0.07114601 -0.06405633 -0.05696665 -0.04987697 -0.04278729 -0.03569761
 -0.02860793 -0.02151825 -0.01442857 -0.00733889 -0.00024921]
[ 432  316  402  533  738  894 1197 1441 1889 2283] [-0.07114601 -0.06405633 -0.05696665 -0.04987697 -0.04278729 -0.03569761
 -0.02860793 -0.02151825 -0.01442857 -0.00733889 -0.00024921]
-1.66788
2.30098
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224984 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.25434
Epoch 1, cost is  1.22112
Epoch 2, cost is  1.19788
Epoch 3, cost is  1.17982
Epoch 4, cost is  1.1643
Training took 0.150323 minutes
Weight histogram
[3380 2176 1292 1125 1118  969  673  532  502  383] [-0.07334977 -0.06603972 -0.05872966 -0.0514196  -0.04410955 -0.03679949
 -0.02948943 -0.02217938 -0.01486932 -0.00755926 -0.00024921]
[ 914  648  815 1067  947  848 1158 1431 1918 2404] [-0.07334977 -0.06603972 -0.05872966 -0.0514196  -0.04410955 -0.03679949
 -0.02948943 -0.02217938 -0.01486932 -0.00755926 -0.00024921]
-1.54675
1.7981
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.41033
Epoch 1, cost is  5.66399
Epoch 2, cost is  4.62623
Epoch 3, cost is  3.80589
Epoch 4, cost is  3.35004
Training took 0.107460 minutes
Weight histogram
[ 727 1054  945  801  863  709 1042 1324 2536  124] [-0.04952583 -0.0446076  -0.03968936 -0.03477112 -0.02985288 -0.02493464
 -0.0200164  -0.01509816 -0.01017992 -0.00526168 -0.00034344]
[2522 1312  722  666  710  746  802  883  998  764] [-0.04952583 -0.0446076  -0.03968936 -0.03477112 -0.02985288 -0.02493464
 -0.0200164  -0.01509816 -0.01017992 -0.00526168 -0.00034344]
-0.634967
1.14927
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083700 minutes
Epoch 0
Fine tuning took 0.082511 minutes
Epoch 0
Fine tuning took 0.082782 minutes
{'zero': {0: [0.072660098522167482, 0.073891625615763554, 0.061576354679802957, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81034482758620685, 0.83620689655172409, 0.82266009852216748, 0.78325123152709364], 5: [0.11699507389162561, 0.089901477832512317, 0.11576354679802955, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.072660098522167482, 0.024630541871921183, 0.034482758620689655, 0.036945812807881777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81034482758620685, 0.91379310344827591, 0.9076354679802956, 0.88916256157635465], 5: [0.11699507389162561, 0.061576354679802957, 0.057881773399014777, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.072660098522167482, 0.034482758620689655, 0.034482758620689655, 0.064039408866995079], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81034482758620685, 0.90886699507389157, 0.88669950738916259, 0.85591133004926112], 5: [0.11699507389162561, 0.056650246305418719, 0.078817733990147784, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.072660098522167482, 0.023399014778325122, 0.032019704433497539, 0.045566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81034482758620685, 0.91871921182266014, 0.89901477832512311, 0.89039408866995073], 5: [0.11699507389162561, 0.057881773399014777, 0.068965517241379309, 0.064039408866995079], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226226 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.27984
Epoch 1, cost is  1.24845
Epoch 2, cost is  1.22617
Epoch 3, cost is  1.20683
Epoch 4, cost is  1.19011
Training took 0.151534 minutes
Weight histogram
[3369 2331 1463  899  683  449  301  238  241  151] [-0.07114601 -0.06405633 -0.05696665 -0.04987697 -0.04278729 -0.03569761
 -0.02860793 -0.02151825 -0.01442857 -0.00733889 -0.00024921]
[ 432  316  402  533  738  894 1197 1441 1889 2283] [-0.07114601 -0.06405633 -0.05696665 -0.04987697 -0.04278729 -0.03569761
 -0.02860793 -0.02151825 -0.01442857 -0.00733889 -0.00024921]
-1.66788
2.30098
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225807 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.25434
Epoch 1, cost is  1.22112
Epoch 2, cost is  1.19788
Epoch 3, cost is  1.17982
Epoch 4, cost is  1.1643
Training took 0.150636 minutes
Weight histogram
[3380 2176 1292 1125 1118  969  673  532  502  383] [-0.07334977 -0.06603972 -0.05872966 -0.0514196  -0.04410955 -0.03679949
 -0.02948943 -0.02217938 -0.01486932 -0.00755926 -0.00024921]
[ 914  648  815 1067  947  848 1158 1431 1918 2404] [-0.07334977 -0.06603972 -0.05872966 -0.0514196  -0.04410955 -0.03679949
 -0.02948943 -0.02217938 -0.01486932 -0.00755926 -0.00024921]
-1.54675
1.7981
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.21967
Epoch 1, cost is  5.50266
Epoch 2, cost is  4.44909
Epoch 3, cost is  3.53352
Epoch 4, cost is  2.97214
Training took 0.158567 minutes
Weight histogram
[ 790 1258  967  893  877 1193 2009 1863  230   45] [-0.03077359 -0.02772865 -0.02468371 -0.02163877 -0.01859383 -0.01554889
 -0.01250395 -0.00945901 -0.00641407 -0.00336913 -0.00032419]
[2913 1185  635  646  658  719  779  856  924  810] [-0.03077359 -0.02772865 -0.02468371 -0.02163877 -0.01859383 -0.01554889
 -0.01250395 -0.00945901 -0.00641407 -0.00336913 -0.00032419]
-0.558863
1.02393
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089628 minutes
Epoch 0
Fine tuning took 0.088959 minutes
Epoch 0
Fine tuning took 0.089821 minutes
{'zero': {0: [0.056650246305418719, 0.16748768472906403, 0.19211822660098521, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79926108374384242, 0.68719211822660098, 0.62561576354679804, 0.59359605911330049], 5: [0.14408866995073891, 0.14532019704433496, 0.18226600985221675, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.056650246305418719, 0.076354679802955669, 0.078817733990147784, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79926108374384242, 0.83374384236453203, 0.82758620689655171, 0.70197044334975367], 5: [0.14408866995073891, 0.089901477832512317, 0.093596059113300489, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.056650246305418719, 0.1206896551724138, 0.11699507389162561, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79926108374384242, 0.75862068965517238, 0.76354679802955661, 0.63916256157635465], 5: [0.14408866995073891, 0.1206896551724138, 0.11945812807881774, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.056650246305418719, 0.075123152709359611, 0.081280788177339899, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79926108374384242, 0.84359605911330049, 0.81896551724137934, 0.7142857142857143], 5: [0.14408866995073891, 0.081280788177339899, 0.099753694581280791, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226735 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.01454
Epoch 1, cost is  2.93058
Epoch 2, cost is  2.85057
Epoch 3, cost is  2.77905
Epoch 4, cost is  2.7168
Training took 0.151739 minutes
Weight histogram
[1455 1190 1119 1019  959  836  825 1613 1015   94] [ -3.48012634e-02  -3.13133632e-02  -2.78254631e-02  -2.43375630e-02
  -2.08496629e-02  -1.73617628e-02  -1.38738627e-02  -1.03859626e-02
  -6.89806246e-03  -3.41016234e-03   7.77377718e-05]
[2188  713  724  789  830  852  927  961 1040 1101] [ -3.48012634e-02  -3.13133632e-02  -2.78254631e-02  -2.43375630e-02
  -2.08496629e-02  -1.73617628e-02  -1.38738627e-02  -1.03859626e-02
  -6.89806246e-03  -3.41016234e-03   7.77377718e-05]
-0.505972
0.926845
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226013 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.16411
Epoch 1, cost is  3.07203
Epoch 2, cost is  2.98181
Epoch 3, cost is  2.90505
Epoch 4, cost is  2.8314
Training took 0.151639 minutes
Weight histogram
[1305 1121 1114  978  990  852  952 3714  960  164] [ -3.22615504e-02  -2.90276216e-02  -2.57936928e-02  -2.25597640e-02
  -1.93258351e-02  -1.60919063e-02  -1.28579775e-02  -9.62404869e-03
  -6.39011987e-03  -3.15619105e-03   7.77377718e-05]
[4395  797  755  781  792  841  873  906  988 1022] [ -3.22615504e-02  -2.90276216e-02  -2.57936928e-02  -2.25597640e-02
  -1.93258351e-02  -1.60919063e-02  -1.28579775e-02  -9.62404869e-03
  -6.39011987e-03  -3.15619105e-03   7.77377718e-05]
-0.502862
0.821673
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.82922
Epoch 1, cost is  6.71967
Epoch 2, cost is  6.62548
Epoch 3, cost is  6.53579
Epoch 4, cost is  6.44907
Training took 0.090836 minutes
Weight histogram
[1365 4011 3013  667  375  243  169  120   89   73] [ -8.89928360e-03  -8.01271107e-03  -7.12613855e-03  -6.23956602e-03
  -5.35299350e-03  -4.46642097e-03  -3.57984845e-03  -2.69327592e-03
  -1.80670340e-03  -9.20130871e-04  -3.35583463e-05]
[3776 1595 1213 1006  898  726  497  211   99  104] [ -8.89928360e-03  -8.01271107e-03  -7.12613855e-03  -6.23956602e-03
  -5.35299350e-03  -4.46642097e-03  -3.57984845e-03  -2.69327592e-03
  -1.80670340e-03  -9.20130871e-04  -3.35583463e-05]
-0.0589014
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082434 minutes
Epoch 0
Fine tuning took 0.082340 minutes
Epoch 0
Fine tuning took 0.081287 minutes
{'zero': {0: [0.091133004926108374, 0.13916256157635468, 0.19704433497536947, 0.32019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87684729064039413, 0.59852216748768472, 0.57389162561576357, 0.31280788177339902], 5: [0.032019704433497539, 0.26231527093596058, 0.22906403940886699, 0.36699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.091133004926108374, 0.13423645320197045, 0.19950738916256158, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87684729064039413, 0.6145320197044335, 0.59852216748768472, 0.30295566502463056], 5: [0.032019704433497539, 0.25123152709359609, 0.2019704433497537, 0.35960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.091133004926108374, 0.1145320197044335, 0.18965517241379309, 0.33620689655172414], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87684729064039413, 0.62561576354679804, 0.59975369458128081, 0.32142857142857145], 5: [0.032019704433497539, 0.25985221674876846, 0.2105911330049261, 0.34236453201970446], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.091133004926108374, 0.14162561576354679, 0.18472906403940886, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87684729064039413, 0.60344827586206895, 0.61576354679802958, 0.29679802955665024], 5: [0.032019704433497539, 0.25492610837438423, 0.19950738916256158, 0.36576354679802958], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226385 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.01454
Epoch 1, cost is  2.93058
Epoch 2, cost is  2.85057
Epoch 3, cost is  2.77905
Epoch 4, cost is  2.7168
Training took 0.151759 minutes
Weight histogram
[1455 1190 1119 1019  959  836  825 1613 1015   94] [ -3.48012634e-02  -3.13133632e-02  -2.78254631e-02  -2.43375630e-02
  -2.08496629e-02  -1.73617628e-02  -1.38738627e-02  -1.03859626e-02
  -6.89806246e-03  -3.41016234e-03   7.77377718e-05]
[2188  713  724  789  830  852  927  961 1040 1101] [ -3.48012634e-02  -3.13133632e-02  -2.78254631e-02  -2.43375630e-02
  -2.08496629e-02  -1.73617628e-02  -1.38738627e-02  -1.03859626e-02
  -6.89806246e-03  -3.41016234e-03   7.77377718e-05]
-0.505972
0.926845
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225363 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.16411
Epoch 1, cost is  3.07203
Epoch 2, cost is  2.98181
Epoch 3, cost is  2.90505
Epoch 4, cost is  2.8314
Training took 0.155085 minutes
Weight histogram
[1305 1121 1114  978  990  852  952 3714  960  164] [ -3.22615504e-02  -2.90276216e-02  -2.57936928e-02  -2.25597640e-02
  -1.93258351e-02  -1.60919063e-02  -1.28579775e-02  -9.62404869e-03
  -6.39011987e-03  -3.15619105e-03   7.77377718e-05]
[4395  797  755  781  792  841  873  906  988 1022] [ -3.22615504e-02  -2.90276216e-02  -2.57936928e-02  -2.25597640e-02
  -1.93258351e-02  -1.60919063e-02  -1.28579775e-02  -9.62404869e-03
  -6.39011987e-03  -3.15619105e-03   7.77377718e-05]
-0.502862
0.821673
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.7763
Epoch 1, cost is  6.64539
Epoch 2, cost is  6.54718
Epoch 3, cost is  6.45446
Epoch 4, cost is  6.36218
Training took 0.108083 minutes
Weight histogram
[1227 2991 3899  823  430  271  183  131   94   76] [ -9.23346821e-03  -8.31579792e-03  -7.39812762e-03  -6.48045732e-03
  -5.56278703e-03  -4.64511673e-03  -3.72744643e-03  -2.80977614e-03
  -1.89210584e-03  -9.74435546e-04  -5.67652496e-05]
[3618 1575 1221 1020  928  767  519  260  107  110] [ -9.23346821e-03  -8.31579792e-03  -7.39812762e-03  -6.48045732e-03
  -5.56278703e-03  -4.64511673e-03  -3.72744643e-03  -2.80977614e-03
  -1.89210584e-03  -9.74435546e-04  -5.67652496e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083264 minutes
Epoch 0
Fine tuning took 0.083558 minutes
Epoch 0
Fine tuning took 0.083050 minutes
{'zero': {0: [0.10221674876847291, 0.10344827586206896, 0.23399014778325122, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86699507389162567, 0.61576354679802958, 0.52463054187192115, 0.3251231527093596], 5: [0.030788177339901478, 0.28078817733990147, 0.2413793103448276, 0.3854679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10221674876847291, 0.12192118226600986, 0.24507389162561577, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86699507389162567, 0.60344827586206895, 0.54064039408866993, 0.33004926108374383], 5: [0.030788177339901478, 0.27463054187192121, 0.21428571428571427, 0.4039408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10221674876847291, 0.12438423645320197, 0.2413793103448276, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86699507389162567, 0.58866995073891626, 0.52955665024630538, 0.31157635467980294], 5: [0.030788177339901478, 0.28694581280788178, 0.22906403940886699, 0.41256157635467983], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10221674876847291, 0.10344827586206896, 0.22413793103448276, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86699507389162567, 0.60591133004926112, 0.56034482758620685, 0.32266009852216748], 5: [0.030788177339901478, 0.29064039408866993, 0.21551724137931033, 0.40147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226230 minutes
Weight histogram
[  37  188  522  615  835  920 1450 2383 2502  673] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
[ 613  627  640  696  809  864 1170 1373 1570 1763] [ -1.34912640e-04  -3.37790785e-05   6.73544826e-05   1.68488044e-04
   2.69621605e-04   3.70755166e-04   4.71888727e-04   5.73022288e-04
   6.74155849e-04   7.75289410e-04   8.76422971e-04]
-0.545633
0.772971
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.01454
Epoch 1, cost is  2.93058
Epoch 2, cost is  2.85057
Epoch 3, cost is  2.77905
Epoch 4, cost is  2.7168
Training took 0.152808 minutes
Weight histogram
[1455 1190 1119 1019  959  836  825 1613 1015   94] [ -3.48012634e-02  -3.13133632e-02  -2.78254631e-02  -2.43375630e-02
  -2.08496629e-02  -1.73617628e-02  -1.38738627e-02  -1.03859626e-02
  -6.89806246e-03  -3.41016234e-03   7.77377718e-05]
[2188  713  724  789  830  852  927  961 1040 1101] [ -3.48012634e-02  -3.13133632e-02  -2.78254631e-02  -2.43375630e-02
  -2.08496629e-02  -1.73617628e-02  -1.38738627e-02  -1.03859626e-02
  -6.89806246e-03  -3.41016234e-03   7.77377718e-05]
-0.505972
0.926845
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225734 minutes
Weight histogram
[  66  444 1074 1290 1348 1105 1648 2907 2111  157] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
[ 945 1191 1246  794  808  945 1114 1406 1662 2039] [ -1.34912640e-04  -3.28304275e-05   6.92517846e-05   1.71333997e-04
   2.73416209e-04   3.75498421e-04   4.77580633e-04   5.79662845e-04
   6.81745057e-04   7.83827269e-04   8.85909481e-04]
-0.65868
0.460735
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.16411
Epoch 1, cost is  3.07203
Epoch 2, cost is  2.98181
Epoch 3, cost is  2.90505
Epoch 4, cost is  2.8314
Training took 0.154459 minutes
Weight histogram
[1305 1121 1114  978  990  852  952 3714  960  164] [ -3.22615504e-02  -2.90276216e-02  -2.57936928e-02  -2.25597640e-02
  -1.93258351e-02  -1.60919063e-02  -1.28579775e-02  -9.62404869e-03
  -6.39011987e-03  -3.15619105e-03   7.77377718e-05]
[4395  797  755  781  792  841  873  906  988 1022] [ -3.22615504e-02  -2.90276216e-02  -2.57936928e-02  -2.25597640e-02
  -1.93258351e-02  -1.60919063e-02  -1.28579775e-02  -9.62404869e-03
  -6.39011987e-03  -3.15619105e-03   7.77377718e-05]
-0.502862
0.821673
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.61851
Epoch 1, cost is  6.43285
Epoch 2, cost is  6.32194
Epoch 3, cost is  6.21933
Epoch 4, cost is  6.12626
Training took 0.157785 minutes
Weight histogram
[ 929 1954 4284 1431  603  347  228  157  108   84] [ -1.00774989e-02  -9.07403377e-03  -8.07056868e-03  -7.06710359e-03
  -6.06363850e-03  -5.06017341e-03  -4.05670833e-03  -3.05324324e-03
  -2.04977815e-03  -1.04631306e-03  -4.28479761e-05]
[3268 1509 1185 1065  972  905  596  370  125  130] [ -1.00774989e-02  -9.07403377e-03  -8.07056868e-03  -7.06710359e-03
  -6.06363850e-03  -5.06017341e-03  -4.05670833e-03  -3.05324324e-03
  -2.04977815e-03  -1.04631306e-03  -4.28479761e-05]
-0.0601954
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088711 minutes
Epoch 0
Fine tuning took 0.088729 minutes
Epoch 0
Fine tuning took 0.088787 minutes
{'zero': {0: [0.080049261083743842, 0.097290640394088676, 0.2376847290640394, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89655172413793105, 0.61699507389162567, 0.4963054187192118, 0.35960591133004927], 5: [0.023399014778325122, 0.2857142857142857, 0.26600985221674878, 0.31403940886699505], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.080049261083743842, 0.1145320197044335, 0.20812807881773399, 0.3251231527093596], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89655172413793105, 0.62931034482758619, 0.5431034482758621, 0.37807881773399016], 5: [0.023399014778325122, 0.25615763546798032, 0.24876847290640394, 0.29679802955665024], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.080049261083743842, 0.098522167487684734, 0.22044334975369459, 0.33004926108374383], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89655172413793105, 0.64162561576354682, 0.52709359605911332, 0.37931034482758619], 5: [0.023399014778325122, 0.25985221674876846, 0.25246305418719212, 0.29064039408866993], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.080049261083743842, 0.086206896551724144, 0.22413793103448276, 0.33251231527093594], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89655172413793105, 0.64039408866995073, 0.54556650246305416, 0.34359605911330049], 5: [0.023399014778325122, 0.27339901477832512, 0.23029556650246305, 0.32389162561576357], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226383 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.02662
Epoch 1, cost is  0.973005
Epoch 2, cost is  0.945204
Epoch 3, cost is  0.926775
Epoch 4, cost is  0.907786
Training took 0.151524 minutes
Weight histogram
[3933 2123 2037 1959  927  717  249   95   56   54] [-0.15894042 -0.14339824 -0.12785607 -0.11231389 -0.09677172 -0.08122954
 -0.06568737 -0.05014519 -0.03460302 -0.01906084 -0.00351867]
[  89  116  198  333  559  925 1301 2046 3044 3539] [-0.15894042 -0.14339824 -0.12785607 -0.11231389 -0.09677172 -0.08122954
 -0.06568737 -0.05014519 -0.03460302 -0.01906084 -0.00351867]
-4.27199
5.47901
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.248715 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.00368
Epoch 1, cost is  0.949449
Epoch 2, cost is  0.920474
Epoch 3, cost is  0.901766
Epoch 4, cost is  0.885808
Training took 0.169693 minutes
Weight histogram
[3930 2129 2032 1997 2109 1150  423  180  112  113] [-0.15389389 -0.13885637 -0.12381884 -0.10878132 -0.0937438  -0.07870628
 -0.06366876 -0.04863124 -0.03359371 -0.01855619 -0.00351867]
[ 183  217  376  631 1083 1540 1390 2048 2852 3855] [-0.15389389 -0.13885637 -0.12381884 -0.10878132 -0.0937438  -0.07870628
 -0.06366876 -0.04863124 -0.03359371 -0.01855619 -0.00351867]
-4.4177
5.56814
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.44399
Epoch 1, cost is  1.95724
Epoch 2, cost is  1.66532
Epoch 3, cost is  1.52278
Epoch 4, cost is  1.43406
Training took 0.120252 minutes
Weight histogram
[2992 2718 1865 1548  889  600  428  309  402  399] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[ 788  540  579  723  851 1077 1351 1674 2085 2482] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.68512
4.06771
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.086015 minutes
Epoch 0
Fine tuning took 0.083047 minutes
Epoch 0
Fine tuning took 0.092580 minutes
{'zero': {0: [0.13669950738916256, 0.24261083743842365, 0.24630541871921183, 0.28694581280788178], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.51724137931034486, 0.48029556650246308, 0.45320197044334976], 5: [0.13177339901477833, 0.24014778325123154, 0.27339901477832512, 0.25985221674876846], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13669950738916256, 0.046798029556650245, 0.04064039408866995, 0.050492610837438424], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.8645320197044335, 0.89162561576354682, 0.87068965517241381], 5: [0.13177339901477833, 0.088669950738916259, 0.067733990147783252, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13669950738916256, 0.050492610837438424, 0.056650246305418719, 0.056650246305418719], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.85591133004926112, 0.83497536945812811, 0.83251231527093594], 5: [0.13177339901477833, 0.093596059113300489, 0.10837438423645321, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13669950738916256, 0.017241379310344827, 0.012315270935960592, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.92980295566502458, 0.93103448275862066, 0.92610837438423643], 5: [0.13177339901477833, 0.05295566502463054, 0.056650246305418719, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234561 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.02662
Epoch 1, cost is  0.973005
Epoch 2, cost is  0.945204
Epoch 3, cost is  0.926775
Epoch 4, cost is  0.907786
Training took 0.170875 minutes
Weight histogram
[3933 2123 2037 1959  927  717  249   95   56   54] [-0.15894042 -0.14339824 -0.12785607 -0.11231389 -0.09677172 -0.08122954
 -0.06568737 -0.05014519 -0.03460302 -0.01906084 -0.00351867]
[  89  116  198  333  559  925 1301 2046 3044 3539] [-0.15894042 -0.14339824 -0.12785607 -0.11231389 -0.09677172 -0.08122954
 -0.06568737 -0.05014519 -0.03460302 -0.01906084 -0.00351867]
-4.27199
5.47901
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.261595 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.00368
Epoch 1, cost is  0.949449
Epoch 2, cost is  0.920474
Epoch 3, cost is  0.901766
Epoch 4, cost is  0.885808
Training took 0.196061 minutes
Weight histogram
[3930 2129 2032 1997 2109 1150  423  180  112  113] [-0.15389389 -0.13885637 -0.12381884 -0.10878132 -0.0937438  -0.07870628
 -0.06366876 -0.04863124 -0.03359371 -0.01855619 -0.00351867]
[ 183  217  376  631 1083 1540 1390 2048 2852 3855] [-0.15389389 -0.13885637 -0.12381884 -0.10878132 -0.0937438  -0.07870628
 -0.06366876 -0.04863124 -0.03359371 -0.01855619 -0.00351867]
-4.4177
5.56814
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.25112
Epoch 1, cost is  1.66044
Epoch 2, cost is  1.30833
Epoch 3, cost is  1.13361
Epoch 4, cost is  1.0212
Training took 0.115737 minutes
Weight histogram
[3298 2593 1844 1329  957  664  403  283  460  319] [-0.15568438 -0.1405115  -0.12533861 -0.11016573 -0.09499285 -0.07981996
 -0.06464708 -0.04947419 -0.03430131 -0.01912843 -0.00395554]
[ 810  528  568  715  881 1119 1362 1674 2056 2437] [-0.15568438 -0.1405115  -0.12533861 -0.11016573 -0.09499285 -0.07981996
 -0.06464708 -0.04947419 -0.03430131 -0.01912843 -0.00395554]
-3.11941
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083321 minutes
Epoch 0
Fine tuning took 0.087711 minutes
Epoch 0
Fine tuning took 0.084017 minutes
{'zero': {0: [0.1268472906403941, 0.23645320197044334, 0.31650246305418717, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.48891625615763545, 0.45320197044334976, 0.37807881773399016], 5: [0.11945812807881774, 0.27463054187192121, 0.23029556650246305, 0.28448275862068967], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1268472906403941, 0.10960591133004927, 0.093596059113300489, 0.084975369458128072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.74753694581280783, 0.78325123152709364, 0.77216748768472909], 5: [0.11945812807881774, 0.14285714285714285, 0.12315270935960591, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1268472906403941, 0.1206896551724138, 0.1268472906403941, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.69827586206896552, 0.73399014778325122, 0.72044334975369462], 5: [0.11945812807881774, 0.18103448275862069, 0.13916256157635468, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1268472906403941, 0.056650246305418719, 0.071428571428571425, 0.054187192118226604], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.84975369458128081, 0.80172413793103448, 0.8214285714285714], 5: [0.11945812807881774, 0.093596059113300489, 0.1268472906403941, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.250549 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.02662
Epoch 1, cost is  0.973005
Epoch 2, cost is  0.945204
Epoch 3, cost is  0.926775
Epoch 4, cost is  0.907786
Training took 0.168446 minutes
Weight histogram
[3933 2123 2037 1959  927  717  249   95   56   54] [-0.15894042 -0.14339824 -0.12785607 -0.11231389 -0.09677172 -0.08122954
 -0.06568737 -0.05014519 -0.03460302 -0.01906084 -0.00351867]
[  89  116  198  333  559  925 1301 2046 3044 3539] [-0.15894042 -0.14339824 -0.12785607 -0.11231389 -0.09677172 -0.08122954
 -0.06568737 -0.05014519 -0.03460302 -0.01906084 -0.00351867]
-4.27199
5.47901
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.248613 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.00368
Epoch 1, cost is  0.949449
Epoch 2, cost is  0.920474
Epoch 3, cost is  0.901766
Epoch 4, cost is  0.885808
Training took 0.167297 minutes
Weight histogram
[3930 2129 2032 1997 2109 1150  423  180  112  113] [-0.15389389 -0.13885637 -0.12381884 -0.10878132 -0.0937438  -0.07870628
 -0.06366876 -0.04863124 -0.03359371 -0.01855619 -0.00351867]
[ 183  217  376  631 1083 1540 1390 2048 2852 3855] [-0.15389389 -0.13885637 -0.12381884 -0.10878132 -0.0937438  -0.07870628
 -0.06366876 -0.04863124 -0.03359371 -0.01855619 -0.00351867]
-4.4177
5.56814
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.05118
Epoch 1, cost is  1.40706
Epoch 2, cost is  1.01019
Epoch 3, cost is  0.826484
Epoch 4, cost is  0.718763
Training took 0.171260 minutes
Weight histogram
[3396 2549 1801 1236  932  671  509  339  474  243] [-0.10722177 -0.09687931 -0.08653685 -0.07619439 -0.06585193 -0.05550947
 -0.04516701 -0.03482455 -0.02448209 -0.01413963 -0.00379718]
[ 856  474  551  708  872 1079 1349 1681 2112 2468] [-0.10722177 -0.09687931 -0.08653685 -0.07619439 -0.06585193 -0.05550947
 -0.04516701 -0.03482455 -0.02448209 -0.01413963 -0.00379718]
-2.01397
2.92356
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088399 minutes
Epoch 0
Fine tuning took 0.089927 minutes
Epoch 0
Fine tuning took 0.090193 minutes
{'zero': {0: [0.14655172413793102, 0.30049261083743845, 0.32635467980295568, 0.39039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.45073891625615764, 0.44211822660098521, 0.4211822660098522], 5: [0.13916256157635468, 0.24876847290640394, 0.23152709359605911, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14655172413793102, 0.27709359605911332, 0.34359605911330049, 0.3460591133004926], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.45566502463054187, 0.41995073891625617, 0.41995073891625617], 5: [0.13916256157635468, 0.26724137931034481, 0.23645320197044334, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14655172413793102, 0.27586206896551724, 0.30172413793103448, 0.31280788177339902], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.47044334975369456, 0.46551724137931033, 0.45320197044334976], 5: [0.13916256157635468, 0.2536945812807882, 0.23275862068965517, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14655172413793102, 0.31650246305418717, 0.33743842364532017, 0.37192118226600984], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.44088669950738918, 0.41625615763546797, 0.40886699507389163], 5: [0.13916256157635468, 0.24261083743842365, 0.24630541871921183, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226623 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.1497
Epoch 1, cost is  1.11978
Epoch 2, cost is  1.10091
Epoch 3, cost is  1.08545
Epoch 4, cost is  1.07229
Training took 0.152487 minutes
Weight histogram
[3827 2403 2148 1565  629  544  337  273  234  190] [-0.07547334 -0.06795093 -0.06042851 -0.0529061  -0.04538369 -0.03786127
 -0.03033886 -0.02281645 -0.01529403 -0.00777162 -0.00024921]
[ 454  348  449  629  854 1054 1432 1828 2291 2811] [-0.07547334 -0.06795093 -0.06042851 -0.0529061  -0.04538369 -0.03786127
 -0.03033886 -0.02281645 -0.01529403 -0.00777162 -0.00024921]
-1.92506
2.45819
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227144 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.12897
Epoch 1, cost is  1.10049
Epoch 2, cost is  1.08141
Epoch 3, cost is  1.06738
Epoch 4, cost is  1.05326
Training took 0.150450 minutes
Weight histogram
[3787 2329 1969 1596  958 1203  774  607  510  442] [-0.0782176  -0.07042076 -0.06262392 -0.05482708 -0.04703024 -0.0392334
 -0.03143656 -0.02363972 -0.01584289 -0.00804605 -0.00024921]
[ 954  705  909 1233  860  996 1400 1828 2353 2937] [-0.0782176  -0.07042076 -0.06262392 -0.05482708 -0.04703024 -0.0392334
 -0.03143656 -0.02363972 -0.01584289 -0.00804605 -0.00024921]
-1.83891
2.08476
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.48489
Epoch 1, cost is  5.74306
Epoch 2, cost is  4.80855
Epoch 3, cost is  4.10391
Epoch 4, cost is  3.71642
Training took 0.092380 minutes
Weight histogram
[1039 1121 1038  886  928  842 1019 1304 2074 1899] [-0.06757683 -0.06085222 -0.05412761 -0.047403   -0.04067839 -0.03395379
 -0.02722918 -0.02050457 -0.01377996 -0.00705535 -0.00033074]
[2977 1407  978  846  872  937  999 1081 1208  845] [-0.06757683 -0.06085222 -0.05412761 -0.047403   -0.04067839 -0.03395379
 -0.02722918 -0.02050457 -0.01377996 -0.00705535 -0.00033074]
-0.829642
1.34971
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081724 minutes
Epoch 0
Fine tuning took 0.081330 minutes
Epoch 0
Fine tuning took 0.082712 minutes
{'zero': {0: [0.22536945812807882, 0.0036945812807881772, 0.030788177339901478, 0.033251231527093597], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.95073891625615758, 0.90394088669950734, 0.81403940886699511], 5: [0.16009852216748768, 0.045566502463054187, 0.065270935960591137, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22536945812807882, 0.0024630541871921183, 0.023399014778325122, 0.01600985221674877], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.97413793103448276, 0.90640394088669951, 0.88177339901477836], 5: [0.16009852216748768, 0.023399014778325122, 0.070197044334975367, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22536945812807882, 0.0012315270935960591, 0.02832512315270936, 0.030788177339901478], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.96798029556650245, 0.90640394088669951, 0.84605911330049266], 5: [0.16009852216748768, 0.030788177339901478, 0.065270935960591137, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22536945812807882, 0.0012315270935960591, 0.022167487684729065, 0.012315270935960592], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.97044334975369462, 0.9211822660098522, 0.89408866995073888], 5: [0.16009852216748768, 0.02832512315270936, 0.056650246305418719, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226321 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.1497
Epoch 1, cost is  1.11978
Epoch 2, cost is  1.10091
Epoch 3, cost is  1.08545
Epoch 4, cost is  1.07229
Training took 0.152134 minutes
Weight histogram
[3827 2403 2148 1565  629  544  337  273  234  190] [-0.07547334 -0.06795093 -0.06042851 -0.0529061  -0.04538369 -0.03786127
 -0.03033886 -0.02281645 -0.01529403 -0.00777162 -0.00024921]
[ 454  348  449  629  854 1054 1432 1828 2291 2811] [-0.07547334 -0.06795093 -0.06042851 -0.0529061  -0.04538369 -0.03786127
 -0.03033886 -0.02281645 -0.01529403 -0.00777162 -0.00024921]
-1.92506
2.45819
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226778 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.12897
Epoch 1, cost is  1.10049
Epoch 2, cost is  1.08141
Epoch 3, cost is  1.06738
Epoch 4, cost is  1.05326
Training took 0.152027 minutes
Weight histogram
[3787 2329 1969 1596  958 1203  774  607  510  442] [-0.0782176  -0.07042076 -0.06262392 -0.05482708 -0.04703024 -0.0392334
 -0.03143656 -0.02363972 -0.01584289 -0.00804605 -0.00024921]
[ 954  705  909 1233  860  996 1400 1828 2353 2937] [-0.0782176  -0.07042076 -0.06262392 -0.05482708 -0.04703024 -0.0392334
 -0.03143656 -0.02363972 -0.01584289 -0.00804605 -0.00024921]
-1.83891
2.08476
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.41302
Epoch 1, cost is  5.67304
Epoch 2, cost is  4.64609
Epoch 3, cost is  3.82585
Epoch 4, cost is  3.36318
Training took 0.107644 minutes
Weight histogram
[ 825 1267 1143  966 1017  874 1241 1635 3031  151] [-0.04952583 -0.0446076  -0.03968936 -0.03477112 -0.02985288 -0.02493464
 -0.0200164  -0.01509816 -0.01017992 -0.00526168 -0.00034344]
[2985 1605  865  801  850  888  962 1055 1196  943] [-0.04952583 -0.0446076  -0.03968936 -0.03477112 -0.02985288 -0.02493464
 -0.0200164  -0.01509816 -0.01017992 -0.00526168 -0.00034344]
-0.634967
1.21137
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082259 minutes
Epoch 0
Fine tuning took 0.083141 minutes
Epoch 0
Fine tuning took 0.083968 minutes
{'zero': {0: [0.23399014778325122, 0.022167487684729065, 0.073891625615763554, 0.083743842364532015], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.87931034482758619, 0.82389162561576357, 0.7857142857142857], 5: [0.15270935960591134, 0.098522167487684734, 0.10221674876847291, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23399014778325122, 0.0049261083743842365, 0.022167487684729065, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.93103448275862066, 0.90394088669950734, 0.88669950738916259], 5: [0.15270935960591134, 0.064039408866995079, 0.073891625615763554, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23399014778325122, 0.0049261083743842365, 0.036945812807881777, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.90886699507389157, 0.86576354679802958, 0.83743842364532017], 5: [0.15270935960591134, 0.086206896551724144, 0.097290640394088676, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23399014778325122, 0.0061576354679802959, 0.030788177339901478, 0.045566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.93719211822660098, 0.89532019704433496, 0.88300492610837433], 5: [0.15270935960591134, 0.056650246305418719, 0.073891625615763554, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226226 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.1497
Epoch 1, cost is  1.11978
Epoch 2, cost is  1.10091
Epoch 3, cost is  1.08545
Epoch 4, cost is  1.07229
Training took 0.152106 minutes
Weight histogram
[3827 2403 2148 1565  629  544  337  273  234  190] [-0.07547334 -0.06795093 -0.06042851 -0.0529061  -0.04538369 -0.03786127
 -0.03033886 -0.02281645 -0.01529403 -0.00777162 -0.00024921]
[ 454  348  449  629  854 1054 1432 1828 2291 2811] [-0.07547334 -0.06795093 -0.06042851 -0.0529061  -0.04538369 -0.03786127
 -0.03033886 -0.02281645 -0.01529403 -0.00777162 -0.00024921]
-1.92506
2.45819
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226950 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.12897
Epoch 1, cost is  1.10049
Epoch 2, cost is  1.08141
Epoch 3, cost is  1.06738
Epoch 4, cost is  1.05326
Training took 0.150547 minutes
Weight histogram
[3787 2329 1969 1596  958 1203  774  607  510  442] [-0.0782176  -0.07042076 -0.06262392 -0.05482708 -0.04703024 -0.0392334
 -0.03143656 -0.02363972 -0.01584289 -0.00804605 -0.00024921]
[ 954  705  909 1233  860  996 1400 1828 2353 2937] [-0.0782176  -0.07042076 -0.06262392 -0.05482708 -0.04703024 -0.0392334
 -0.03143656 -0.02363972 -0.01584289 -0.00804605 -0.00024921]
-1.83891
2.08476
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.22377
Epoch 1, cost is  5.51324
Epoch 2, cost is  4.4713
Epoch 3, cost is  3.56247
Epoch 4, cost is  2.9971
Training took 0.158130 minutes
Weight histogram
[ 867 1544 1160 1077 1062 1395 2470 2233  287   55] [-0.03077359 -0.02772865 -0.02468371 -0.02163877 -0.01859383 -0.01554889
 -0.01250395 -0.00945901 -0.00641407 -0.00336913 -0.00032419]
[3459 1447  763  774  786  859  935 1022 1103 1002] [-0.03077359 -0.02772865 -0.02468371 -0.02163877 -0.01859383 -0.01554889
 -0.01250395 -0.00945901 -0.00641407 -0.00336913 -0.00032419]
-0.558863
1.02393
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087907 minutes
Epoch 0
Fine tuning took 0.087816 minutes
Epoch 0
Fine tuning took 0.088644 minutes
{'zero': {0: [0.19827586206896552, 0.13793103448275862, 0.17733990147783252, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.71059113300492616, 0.64039408866995073, 0.62931034482758619], 5: [0.17733990147783252, 0.15147783251231528, 0.18226600985221675, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19827586206896552, 0.075123152709359611, 0.13423645320197045, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.83620689655172409, 0.7426108374384236, 0.72290640394088668], 5: [0.17733990147783252, 0.088669950738916259, 0.12315270935960591, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19827586206896552, 0.075123152709359611, 0.17241379310344829, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.80788177339901479, 0.66995073891625612, 0.68842364532019706], 5: [0.17733990147783252, 0.11699507389162561, 0.15763546798029557, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19827586206896552, 0.044334975369458129, 0.13793103448275862, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.85837438423645318, 0.74630541871921185, 0.7142857142857143], 5: [0.17733990147783252, 0.097290640394088676, 0.11576354679802955, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226508 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.61564
Epoch 1, cost is  2.55105
Epoch 2, cost is  2.49554
Epoch 3, cost is  2.44736
Epoch 4, cost is  2.39823
Training took 0.151671 minutes
Weight histogram
[1783 1568 1365 1236 1197  949  982 1048 1901  121] [ -3.96004319e-02  -3.56326150e-02  -3.16647980e-02  -2.76969810e-02
  -2.37291640e-02  -1.97613471e-02  -1.57935301e-02  -1.18257131e-02
  -7.85789617e-03  -3.89007920e-03   7.77377718e-05]
[2314  812  862  952  976 1065 1117 1229 1349 1474] [ -3.96004319e-02  -3.56326150e-02  -3.16647980e-02  -2.76969810e-02
  -2.37291640e-02  -1.97613471e-02  -1.57935301e-02  -1.18257131e-02
  -7.85789617e-03  -3.89007920e-03   7.77377718e-05]
-0.544254
1.1189
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224626 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.71774
Epoch 1, cost is  2.64712
Epoch 2, cost is  2.58499
Epoch 3, cost is  2.53104
Epoch 4, cost is  2.47948
Training took 0.152966 minutes
Weight histogram
[1641 1435 1287 1307 1114 1102 1010 1561 3506  212] [ -3.71515416e-02  -3.34286136e-02  -2.97056857e-02  -2.59827578e-02
  -2.22598298e-02  -1.85369019e-02  -1.48139740e-02  -1.10910460e-02
  -7.36811809e-03  -3.64519016e-03   7.77377718e-05]
[4544  915  900  924  965 1036 1089 1162 1283 1357] [ -3.71515416e-02  -3.34286136e-02  -2.97056857e-02  -2.59827578e-02
  -2.22598298e-02  -1.85369019e-02  -1.48139740e-02  -1.10910460e-02
  -7.36811809e-03  -3.64519016e-03   7.77377718e-05]
-0.535003
0.897826
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8335
Epoch 1, cost is  6.72868
Epoch 2, cost is  6.63858
Epoch 3, cost is  6.55317
Epoch 4, cost is  6.46983
Training took 0.090385 minutes
Weight histogram
[1365 4013 4571  871  474  303  209  146  110   88] [ -8.89928360e-03  -8.01256373e-03  -7.12584385e-03  -6.23912398e-03
  -5.35240411e-03  -4.46568424e-03  -3.57896437e-03  -2.69224450e-03
  -1.80552463e-03  -9.18804762e-04  -3.20848922e-05]
[4666 1971 1498 1257 1104  743  497  211   99  104] [ -8.89928360e-03  -8.01256373e-03  -7.12584385e-03  -6.23912398e-03
  -5.35240411e-03  -4.46568424e-03  -3.57896437e-03  -2.69224450e-03
  -1.80552463e-03  -9.18804762e-04  -3.20848922e-05]
-0.0589014
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081103 minutes
Epoch 0
Fine tuning took 0.081773 minutes
Epoch 0
Fine tuning took 0.080290 minutes
{'zero': {0: [0.044334975369458129, 0.02832512315270936, 0.22783251231527094, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90147783251231528, 0.88916256157635465, 0.55418719211822665, 0.4605911330049261], 5: [0.054187192118226604, 0.082512315270935957, 0.21798029556650247, 0.3251231527093596], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.044334975369458129, 0.059113300492610835, 0.24753694581280788, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90147783251231528, 0.84605911330049266, 0.53940886699507384, 0.47906403940886699], 5: [0.054187192118226604, 0.094827586206896547, 0.21305418719211822, 0.33497536945812806], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.044334975369458129, 0.043103448275862072, 0.20443349753694581, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90147783251231528, 0.86206896551724133, 0.6071428571428571, 0.43226600985221675], 5: [0.054187192118226604, 0.094827586206896547, 0.18842364532019704, 0.35467980295566504], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.044334975369458129, 0.033251231527093597, 0.27216748768472904, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90147783251231528, 0.86699507389162567, 0.52709359605911332, 0.46551724137931033], 5: [0.054187192118226604, 0.099753694581280791, 0.20073891625615764, 0.31157635467980294], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226235 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.61564
Epoch 1, cost is  2.55105
Epoch 2, cost is  2.49554
Epoch 3, cost is  2.44736
Epoch 4, cost is  2.39823
Training took 0.151739 minutes
Weight histogram
[1783 1568 1365 1236 1197  949  982 1048 1901  121] [ -3.96004319e-02  -3.56326150e-02  -3.16647980e-02  -2.76969810e-02
  -2.37291640e-02  -1.97613471e-02  -1.57935301e-02  -1.18257131e-02
  -7.85789617e-03  -3.89007920e-03   7.77377718e-05]
[2314  812  862  952  976 1065 1117 1229 1349 1474] [ -3.96004319e-02  -3.56326150e-02  -3.16647980e-02  -2.76969810e-02
  -2.37291640e-02  -1.97613471e-02  -1.57935301e-02  -1.18257131e-02
  -7.85789617e-03  -3.89007920e-03   7.77377718e-05]
-0.544254
1.1189
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224464 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.71774
Epoch 1, cost is  2.64712
Epoch 2, cost is  2.58499
Epoch 3, cost is  2.53104
Epoch 4, cost is  2.47948
Training took 0.153817 minutes
Weight histogram
[1641 1435 1287 1307 1114 1102 1010 1561 3506  212] [ -3.71515416e-02  -3.34286136e-02  -2.97056857e-02  -2.59827578e-02
  -2.22598298e-02  -1.85369019e-02  -1.48139740e-02  -1.10910460e-02
  -7.36811809e-03  -3.64519016e-03   7.77377718e-05]
[4544  915  900  924  965 1036 1089 1162 1283 1357] [ -3.71515416e-02  -3.34286136e-02  -2.97056857e-02  -2.59827578e-02
  -2.22598298e-02  -1.85369019e-02  -1.48139740e-02  -1.10910460e-02
  -7.36811809e-03  -3.64519016e-03   7.77377718e-05]
-0.535003
0.897826
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78278
Epoch 1, cost is  6.65831
Epoch 2, cost is  6.5649
Epoch 3, cost is  6.47631
Epoch 4, cost is  6.38927
Training took 0.106324 minutes
Weight histogram
[1228 2990 5361 1094  544  338  227  160  116   92] [ -9.23346821e-03  -8.31561328e-03  -7.39775835e-03  -6.47990342e-03
  -5.56204849e-03  -4.64419356e-03  -3.72633863e-03  -2.80848370e-03
  -1.89062876e-03  -9.72773834e-04  -5.49189026e-05]
[4474 1952 1511 1277 1140  800  519  260  107  110] [ -9.23346821e-03  -8.31561328e-03  -7.39775835e-03  -6.47990342e-03
  -5.56204849e-03  -4.64419356e-03  -3.72633863e-03  -2.80848370e-03
  -1.89062876e-03  -9.72773834e-04  -5.49189026e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084025 minutes
Epoch 0
Fine tuning took 0.082254 minutes
Epoch 0
Fine tuning took 0.083835 minutes
{'zero': {0: [0.050492610837438424, 0.032019704433497539, 0.22906403940886699, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89408866995073888, 0.85344827586206895, 0.53940886699507384, 0.51231527093596063], 5: [0.055418719211822662, 0.1145320197044335, 0.23152709359605911, 0.30295566502463056], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.050492610837438424, 0.044334975369458129, 0.22044334975369459, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89408866995073888, 0.83620689655172409, 0.57019704433497542, 0.50123152709359609], 5: [0.055418719211822662, 0.11945812807881774, 0.20935960591133004, 0.30172413793103448], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.050492610837438424, 0.029556650246305417, 0.2229064039408867, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89408866995073888, 0.87438423645320196, 0.58004926108374388, 0.46305418719211822], 5: [0.055418719211822662, 0.096059113300492605, 0.19704433497536947, 0.33743842364532017], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.050492610837438424, 0.030788177339901478, 0.23152709359605911, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89408866995073888, 0.86822660098522164, 0.53940886699507384, 0.45443349753694579], 5: [0.055418719211822662, 0.10098522167487685, 0.22906403940886699, 0.36330049261083741], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225813 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2725 3628  952] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 679  696  670  751  985 1100 1380 1574 1972 2343] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.578195
0.81742
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.61564
Epoch 1, cost is  2.55105
Epoch 2, cost is  2.49554
Epoch 3, cost is  2.44736
Epoch 4, cost is  2.39823
Training took 0.153624 minutes
Weight histogram
[1783 1568 1365 1236 1197  949  982 1048 1901  121] [ -3.96004319e-02  -3.56326150e-02  -3.16647980e-02  -2.76969810e-02
  -2.37291640e-02  -1.97613471e-02  -1.57935301e-02  -1.18257131e-02
  -7.85789617e-03  -3.89007920e-03   7.77377718e-05]
[2314  812  862  952  976 1065 1117 1229 1349 1474] [ -3.96004319e-02  -3.56326150e-02  -3.16647980e-02  -2.76969810e-02
  -2.37291640e-02  -1.97613471e-02  -1.57935301e-02  -1.18257131e-02
  -7.85789617e-03  -3.89007920e-03   7.77377718e-05]
-0.544254
1.1189
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227273 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3098 3562  532] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1022 1369 1261  809  885 1142 1275 1831 2126 2455] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.698606
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.71774
Epoch 1, cost is  2.64712
Epoch 2, cost is  2.58499
Epoch 3, cost is  2.53104
Epoch 4, cost is  2.47948
Training took 0.152820 minutes
Weight histogram
[1641 1435 1287 1307 1114 1102 1010 1561 3506  212] [ -3.71515416e-02  -3.34286136e-02  -2.97056857e-02  -2.59827578e-02
  -2.22598298e-02  -1.85369019e-02  -1.48139740e-02  -1.10910460e-02
  -7.36811809e-03  -3.64519016e-03   7.77377718e-05]
[4544  915  900  924  965 1036 1089 1162 1283 1357] [ -3.71515416e-02  -3.34286136e-02  -2.97056857e-02  -2.59827578e-02
  -2.22598298e-02  -1.85369019e-02  -1.48139740e-02  -1.10910460e-02
  -7.36811809e-03  -3.64519016e-03   7.77377718e-05]
-0.535003
0.897826
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.63238
Epoch 1, cost is  6.45692
Epoch 2, cost is  6.35173
Epoch 3, cost is  6.25521
Epoch 4, cost is  6.16741
Training took 0.157989 minutes
Weight histogram
[ 929 1955 5324 2022  773  438  282  192  133  102] [ -1.00774989e-02  -9.07389618e-03  -8.07029351e-03  -7.06669084e-03
  -6.06308817e-03  -5.05948550e-03  -4.05588283e-03  -3.05228016e-03
  -2.04867749e-03  -1.04507482e-03  -4.14721471e-05]
[4049 1870 1494 1328 1202  986  596  370  125  130] [ -1.00774989e-02  -9.07389618e-03  -8.07029351e-03  -7.06669084e-03
  -6.06308817e-03  -5.05948550e-03  -4.05588283e-03  -3.05228016e-03
  -2.04867749e-03  -1.04507482e-03  -4.14721471e-05]
-0.0601954
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087505 minutes
Epoch 0
Fine tuning took 0.089242 minutes
Epoch 0
Fine tuning took 0.088885 minutes
{'zero': {0: [0.054187192118226604, 0.050492610837438424, 0.23152709359605911, 0.21182266009852216], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89039408866995073, 0.86699507389162567, 0.53817733990147787, 0.46798029556650245], 5: [0.055418719211822662, 0.082512315270935957, 0.23029556650246305, 0.32019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.054187192118226604, 0.057881773399014777, 0.23891625615763548, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89039408866995073, 0.85591133004926112, 0.56773399014778325, 0.44827586206896552], 5: [0.055418719211822662, 0.086206896551724144, 0.19334975369458129, 0.32758620689655171], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.054187192118226604, 0.055418719211822662, 0.22167487684729065, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89039408866995073, 0.85591133004926112, 0.57389162561576357, 0.44581280788177341], 5: [0.055418719211822662, 0.088669950738916259, 0.20443349753694581, 0.29556650246305421], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.054187192118226604, 0.060344827586206899, 0.24630541871921183, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89039408866995073, 0.8645320197044335, 0.55665024630541871, 0.41379310344827586], 5: [0.055418719211822662, 0.075123152709359611, 0.19704433497536947, 0.34113300492610837], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227564 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.934983
Epoch 1, cost is  0.887191
Epoch 2, cost is  0.861026
Epoch 3, cost is  0.841833
Epoch 4, cost is  0.825422
Training took 0.151038 minutes
Weight histogram
[4019 3280 2698 1632  964 1030  327  110   59   56] [-0.16738968 -0.15100258 -0.13461547 -0.11822837 -0.10184127 -0.08545417
 -0.06906707 -0.05267997 -0.03629287 -0.01990577 -0.00351867]
[  93  125  214  376  639 1096 1410 2583 3221 4418] [-0.16738968 -0.15100258 -0.13461547 -0.11822837 -0.10184127 -0.08545417
 -0.06906707 -0.05267997 -0.03629287 -0.01990577 -0.00351867]
-4.68881
5.61004
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226065 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.91627
Epoch 1, cost is  0.869273
Epoch 2, cost is  0.845003
Epoch 3, cost is  0.825628
Epoch 4, cost is  0.808304
Training took 0.152662 minutes
Weight histogram
[4032 3261 2729 1929 1668 1610  528  205  122  116] [-0.16188303 -0.14604659 -0.13021016 -0.11437372 -0.09853728 -0.08270085
 -0.06686441 -0.05102798 -0.03519154 -0.01935511 -0.00351867]
[ 191  234  404  714 1233 1631 1432 2460 3345 4556] [-0.16188303 -0.14604659 -0.13021016 -0.11437372 -0.09853728 -0.08270085
 -0.06686441 -0.05102798 -0.03519154 -0.01935511 -0.00351867]
-4.81714
5.56814
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.43454
Epoch 1, cost is  1.94459
Epoch 2, cost is  1.64719
Epoch 3, cost is  1.50424
Epoch 4, cost is  1.41846
Training took 0.088535 minutes
Weight histogram
[3482 3141 2197 1803 1051  706  498  360  471  466] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[ 925  631  680  843  995 1259 1571 1947 2425 2899] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.68512
4.06771
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082746 minutes
Epoch 0
Fine tuning took 0.081776 minutes
Epoch 0
Fine tuning took 0.085190 minutes
{'zero': {0: [0.11945812807881774, 0.29802955665024633, 0.39408866995073893, 0.38669950738916259], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74753694581280783, 0.43226600985221675, 0.39162561576354682, 0.39285714285714285], 5: [0.13300492610837439, 0.26970443349753692, 0.21428571428571427, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11945812807881774, 0.059113300492610835, 0.071428571428571425, 0.041871921182266007], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74753694581280783, 0.88054187192118227, 0.90024630541871919, 0.88793103448275867], 5: [0.13300492610837439, 0.060344827586206899, 0.02832512315270936, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11945812807881774, 0.080049261083743842, 0.078817733990147784, 0.049261083743842367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74753694581280783, 0.83004926108374388, 0.83990147783251234, 0.84852216748768472], 5: [0.13300492610837439, 0.089901477832512317, 0.081280788177339899, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11945812807881774, 0.019704433497536946, 0.020935960591133004, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74753694581280783, 0.94088669950738912, 0.95320197044334976, 0.94581280788177335], 5: [0.13300492610837439, 0.039408866995073892, 0.025862068965517241, 0.034482758620689655], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226951 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.934983
Epoch 1, cost is  0.887191
Epoch 2, cost is  0.861026
Epoch 3, cost is  0.841833
Epoch 4, cost is  0.825422
Training took 0.156955 minutes
Weight histogram
[4019 3280 2698 1632  964 1030  327  110   59   56] [-0.16738968 -0.15100258 -0.13461547 -0.11822837 -0.10184127 -0.08545417
 -0.06906707 -0.05267997 -0.03629287 -0.01990577 -0.00351867]
[  93  125  214  376  639 1096 1410 2583 3221 4418] [-0.16738968 -0.15100258 -0.13461547 -0.11822837 -0.10184127 -0.08545417
 -0.06906707 -0.05267997 -0.03629287 -0.01990577 -0.00351867]
-4.68881
5.61004
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.256718 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.91627
Epoch 1, cost is  0.869273
Epoch 2, cost is  0.845003
Epoch 3, cost is  0.825628
Epoch 4, cost is  0.808304
Training took 0.171413 minutes
Weight histogram
[4032 3261 2729 1929 1668 1610  528  205  122  116] [-0.16188303 -0.14604659 -0.13021016 -0.11437372 -0.09853728 -0.08270085
 -0.06686441 -0.05102798 -0.03519154 -0.01935511 -0.00351867]
[ 191  234  404  714 1233 1631 1432 2460 3345 4556] [-0.16188303 -0.14604659 -0.13021016 -0.11437372 -0.09853728 -0.08270085
 -0.06686441 -0.05102798 -0.03519154 -0.01935511 -0.00351867]
-4.81714
5.56814
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.24307
Epoch 1, cost is  1.65447
Epoch 2, cost is  1.30118
Epoch 3, cost is  1.1215
Epoch 4, cost is  1.01421
Training took 0.115383 minutes
Weight histogram
[3811 3011 2161 1559 1135  785  472  326  542  373] [-0.15621416 -0.1409883  -0.12576244 -0.11053658 -0.09531072 -0.08008485
 -0.06485899 -0.04963313 -0.03440727 -0.01918141 -0.00395554]
[ 949  618  668  837 1030 1310 1591 1945 2389 2838] [-0.15621416 -0.1409883  -0.12576244 -0.11053658 -0.09531072 -0.08008485
 -0.06485899 -0.04963313 -0.03440727 -0.01918141 -0.00395554]
-3.18745
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.091243 minutes
Epoch 0
Fine tuning took 0.084368 minutes
Epoch 0
Fine tuning took 0.085565 minutes
{'zero': {0: [0.1354679802955665, 0.40763546798029554, 0.40147783251231528, 0.34482758620689657], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.35837438423645318, 0.36206896551724138, 0.38054187192118227], 5: [0.14039408866995073, 0.23399014778325122, 0.23645320197044334, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1354679802955665, 0.15886699507389163, 0.12315270935960591, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.69827586206896552, 0.75862068965517238, 0.74876847290640391], 5: [0.14039408866995073, 0.14285714285714285, 0.11822660098522167, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1354679802955665, 0.18596059113300492, 0.14901477832512317, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.67980295566502458, 0.72536945812807885, 0.70443349753694584], 5: [0.14039408866995073, 0.13423645320197045, 0.12561576354679804, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1354679802955665, 0.12931034482758622, 0.11576354679802955, 0.05295566502463054], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.72906403940886699, 0.81403940886699511, 0.79926108374384242], 5: [0.14039408866995073, 0.14162561576354679, 0.070197044334975367, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227247 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.934983
Epoch 1, cost is  0.887191
Epoch 2, cost is  0.861026
Epoch 3, cost is  0.841833
Epoch 4, cost is  0.825422
Training took 0.152161 minutes
Weight histogram
[4019 3280 2698 1632  964 1030  327  110   59   56] [-0.16738968 -0.15100258 -0.13461547 -0.11822837 -0.10184127 -0.08545417
 -0.06906707 -0.05267997 -0.03629287 -0.01990577 -0.00351867]
[  93  125  214  376  639 1096 1410 2583 3221 4418] [-0.16738968 -0.15100258 -0.13461547 -0.11822837 -0.10184127 -0.08545417
 -0.06906707 -0.05267997 -0.03629287 -0.01990577 -0.00351867]
-4.68881
5.61004
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.249639 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.91627
Epoch 1, cost is  0.869273
Epoch 2, cost is  0.845003
Epoch 3, cost is  0.825628
Epoch 4, cost is  0.808304
Training took 0.169749 minutes
Weight histogram
[4032 3261 2729 1929 1668 1610  528  205  122  116] [-0.16188303 -0.14604659 -0.13021016 -0.11437372 -0.09853728 -0.08270085
 -0.06686441 -0.05102798 -0.03519154 -0.01935511 -0.00351867]
[ 191  234  404  714 1233 1631 1432 2460 3345 4556] [-0.16188303 -0.14604659 -0.13021016 -0.11437372 -0.09853728 -0.08270085
 -0.06686441 -0.05102798 -0.03519154 -0.01935511 -0.00351867]
-4.81714
5.56814
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.04614
Epoch 1, cost is  1.41363
Epoch 2, cost is  1.01002
Epoch 3, cost is  0.823759
Epoch 4, cost is  0.710956
Training took 0.196006 minutes
Weight histogram
[3743 3081 2063 1531 1097  822  600  394  558  286] [-0.10819522 -0.09775542 -0.08731561 -0.07687581 -0.066436   -0.0559962
 -0.04555639 -0.03511659 -0.02467678 -0.01423698 -0.00379718]
[1002  557  651  828 1020 1262 1569 1957 2452 2877] [-0.10819522 -0.09775542 -0.08731561 -0.07687581 -0.066436   -0.0559962
 -0.04555639 -0.03511659 -0.02467678 -0.01423698 -0.00379718]
-2.01397
2.92356
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.092259 minutes
Epoch 0
Fine tuning took 0.090119 minutes
Epoch 0
Fine tuning took 0.090866 minutes
{'zero': {0: [0.1748768472906404, 0.39655172413793105, 0.49137931034482757, 0.34729064039408869], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.40024630541871919, 0.27339901477832512, 0.34359605911330049], 5: [0.1206896551724138, 0.20320197044334976, 0.23522167487684728, 0.30911330049261082], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1748768472906404, 0.38423645320197042, 0.46674876847290642, 0.36206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.39655172413793105, 0.33128078817733991, 0.30418719211822659], 5: [0.1206896551724138, 0.21921182266009853, 0.2019704433497537, 0.33374384236453203], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1748768472906404, 0.33251231527093594, 0.44334975369458129, 0.32266009852216748], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.45566502463054187, 0.33866995073891626, 0.35098522167487683], 5: [0.1206896551724138, 0.21182266009852216, 0.21798029556650247, 0.32635467980295568], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1748768472906404, 0.40024630541871919, 0.50123152709359609, 0.3682266009852217], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.36945812807881773, 0.29064039408866993, 0.31403940886699505], 5: [0.1206896551724138, 0.23029556650246305, 0.20812807881773399, 0.31773399014778325], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227396 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.03701
Epoch 1, cost is  1.01522
Epoch 2, cost is  0.998826
Epoch 3, cost is  0.987007
Epoch 4, cost is  0.975437
Training took 0.152055 minutes
Weight histogram
[4006 3573 2400 1635  786  638  378  308  248  203] [-0.07955445 -0.07162392 -0.0636934  -0.05576287 -0.04783235 -0.03990183
 -0.0319713  -0.02404078 -0.01611026 -0.00817973 -0.00024921]
[ 474  373  509  708  965 1274 1612 2143 2794 3323] [-0.07955445 -0.07162392 -0.0636934  -0.05576287 -0.04783235 -0.03990183
 -0.0319713  -0.02404078 -0.01611026 -0.00817973 -0.00024921]
-2.10047
2.67904
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225484 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.02403
Epoch 1, cost is  1.00064
Epoch 2, cost is  0.985491
Epoch 3, cost is  0.974236
Epoch 4, cost is  0.961741
Training took 0.150604 minutes
Weight histogram
[4015 3578 2311 1485  837 1463  855  656  528  472] [-0.0818873  -0.07372349 -0.06555968 -0.05739587 -0.04923206 -0.04106825
 -0.03290444 -0.02474063 -0.01657683 -0.00841302 -0.00024921]
[ 997  754 1011 1270  896 1157 1626 2166 2881 3442] [-0.0818873  -0.07372349 -0.06555968 -0.05739587 -0.04923206 -0.04106825
 -0.03290444 -0.02474063 -0.01657683 -0.00841302 -0.00024921]
-1.95991
2.28014
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.48911
Epoch 1, cost is  5.75031
Epoch 2, cost is  4.81046
Epoch 3, cost is  4.11163
Epoch 4, cost is  3.73195
Training took 0.090679 minutes
Weight histogram
[1145 1316 1231 1036 1086  981 1171 1575 2346 2288] [-0.06757683 -0.06085222 -0.05412761 -0.047403   -0.04067839 -0.03395379
 -0.02722918 -0.02050457 -0.01377996 -0.00705535 -0.00033074]
[3430 1675 1137  982 1002 1085 1161 1261 1406 1036] [-0.06757683 -0.06085222 -0.05412761 -0.047403   -0.04067839 -0.03395379
 -0.02722918 -0.02050457 -0.01377996 -0.00705535 -0.00033074]
-0.829642
1.34971
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080492 minutes
Epoch 0
Fine tuning took 0.082339 minutes
Epoch 0
Fine tuning took 0.080584 minutes
{'zero': {0: [0.23645320197044334, 0.013546798029556651, 0.023399014778325122, 0.050492610837438424], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.9285714285714286, 0.91625615763546797, 0.88300492610837433], 5: [0.2857142857142857, 0.057881773399014777, 0.060344827586206899, 0.066502463054187194], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23645320197044334, 0.0012315270935960591, 0.012315270935960592, 0.01600985221674877], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.95812807881773399, 0.93103448275862066, 0.94088669950738912], 5: [0.2857142857142857, 0.04064039408866995, 0.056650246305418719, 0.043103448275862072], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23645320197044334, 0.014778325123152709, 0.023399014778325122, 0.025862068965517241], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.93349753694581283, 0.91995073891625612, 0.92241379310344829], 5: [0.2857142857142857, 0.051724137931034482, 0.056650246305418719, 0.051724137931034482], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23645320197044334, 0.0, 0.0086206896551724137, 0.012315270935960592], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47783251231527096, 0.96551724137931039, 0.94581280788177335, 0.9568965517241379], 5: [0.2857142857142857, 0.034482758620689655, 0.045566502463054187, 0.030788177339901478], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226982 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.03701
Epoch 1, cost is  1.01522
Epoch 2, cost is  0.998826
Epoch 3, cost is  0.987007
Epoch 4, cost is  0.975437
Training took 0.151092 minutes
Weight histogram
[4006 3573 2400 1635  786  638  378  308  248  203] [-0.07955445 -0.07162392 -0.0636934  -0.05576287 -0.04783235 -0.03990183
 -0.0319713  -0.02404078 -0.01611026 -0.00817973 -0.00024921]
[ 474  373  509  708  965 1274 1612 2143 2794 3323] [-0.07955445 -0.07162392 -0.0636934  -0.05576287 -0.04783235 -0.03990183
 -0.0319713  -0.02404078 -0.01611026 -0.00817973 -0.00024921]
-2.10047
2.67904
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225017 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.02403
Epoch 1, cost is  1.00064
Epoch 2, cost is  0.985491
Epoch 3, cost is  0.974236
Epoch 4, cost is  0.961741
Training took 0.152094 minutes
Weight histogram
[4015 3578 2311 1485  837 1463  855  656  528  472] [-0.0818873  -0.07372349 -0.06555968 -0.05739587 -0.04923206 -0.04106825
 -0.03290444 -0.02474063 -0.01657683 -0.00841302 -0.00024921]
[ 997  754 1011 1270  896 1157 1626 2166 2881 3442] [-0.0818873  -0.07372349 -0.06555968 -0.05739587 -0.04923206 -0.04106825
 -0.03290444 -0.02474063 -0.01657683 -0.00841302 -0.00024921]
-1.95991
2.28014
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.41825
Epoch 1, cost is  5.68243
Epoch 2, cost is  4.64903
Epoch 3, cost is  3.82814
Epoch 4, cost is  3.37239
Training took 0.106169 minutes
Weight histogram
[ 918 1495 1332 1130 1170 1036 1446 1941 3529  178] [-0.04952583 -0.0446076  -0.03968936 -0.03477112 -0.02985288 -0.02493464
 -0.0200164  -0.01509816 -0.01017992 -0.00526168 -0.00034344]
[3460 1898 1017  940  992 1042 1133 1243 1413 1037] [-0.04952583 -0.0446076  -0.03968936 -0.03477112 -0.02985288 -0.02493464
 -0.0200164  -0.01509816 -0.01017992 -0.00526168 -0.00034344]
-0.634967
1.21137
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083718 minutes
Epoch 0
Fine tuning took 0.082644 minutes
Epoch 0
Fine tuning took 0.084551 minutes
{'zero': {0: [0.22167487684729065, 0.024630541871921183, 0.083743842364532015, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.86699507389162567, 0.81896551724137934, 0.76847290640394084], 5: [0.25123152709359609, 0.10837438423645321, 0.097290640394088676, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22167487684729065, 0.009852216748768473, 0.029556650246305417, 0.043103448275862072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.93349753694581283, 0.93472906403940892, 0.8780788177339901], 5: [0.25123152709359609, 0.056650246305418719, 0.035714285714285712, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22167487684729065, 0.0073891625615763543, 0.030788177339901478, 0.051724137931034482], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.9211822660098522, 0.90517241379310343, 0.84113300492610843], 5: [0.25123152709359609, 0.071428571428571425, 0.064039408866995079, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22167487684729065, 0.012315270935960592, 0.030788177339901478, 0.045566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.94334975369458129, 0.92487684729064035, 0.8854679802955665], 5: [0.25123152709359609, 0.044334975369458129, 0.044334975369458129, 0.068965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226545 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.03701
Epoch 1, cost is  1.01522
Epoch 2, cost is  0.998826
Epoch 3, cost is  0.987007
Epoch 4, cost is  0.975437
Training took 0.150767 minutes
Weight histogram
[4006 3573 2400 1635  786  638  378  308  248  203] [-0.07955445 -0.07162392 -0.0636934  -0.05576287 -0.04783235 -0.03990183
 -0.0319713  -0.02404078 -0.01611026 -0.00817973 -0.00024921]
[ 474  373  509  708  965 1274 1612 2143 2794 3323] [-0.07955445 -0.07162392 -0.0636934  -0.05576287 -0.04783235 -0.03990183
 -0.0319713  -0.02404078 -0.01611026 -0.00817973 -0.00024921]
-2.10047
2.67904
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225156 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.02403
Epoch 1, cost is  1.00064
Epoch 2, cost is  0.985491
Epoch 3, cost is  0.974236
Epoch 4, cost is  0.961741
Training took 0.152682 minutes
Weight histogram
[4015 3578 2311 1485  837 1463  855  656  528  472] [-0.0818873  -0.07372349 -0.06555968 -0.05739587 -0.04923206 -0.04106825
 -0.03290444 -0.02474063 -0.01657683 -0.00841302 -0.00024921]
[ 997  754 1011 1270  896 1157 1626 2166 2881 3442] [-0.0818873  -0.07372349 -0.06555968 -0.05739587 -0.04923206 -0.04106825
 -0.03290444 -0.02474063 -0.01657683 -0.00841302 -0.00024921]
-1.95991
2.28014
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.23033
Epoch 1, cost is  5.52796
Epoch 2, cost is  4.48732
Epoch 3, cost is  3.57349
Epoch 4, cost is  3.01825
Training took 0.157664 minutes
Weight histogram
[ 937 1818 1363 1266 1243 1608 2930 2597  348   65] [-0.03077359 -0.02772865 -0.02468371 -0.02163877 -0.01859383 -0.01554889
 -0.01250395 -0.00945901 -0.00641407 -0.00336913 -0.00032419]
[4006 1711  898  903  919 1005 1096 1196 1289 1152] [-0.03077359 -0.02772865 -0.02468371 -0.02163877 -0.01859383 -0.01554889
 -0.01250395 -0.00945901 -0.00641407 -0.00336913 -0.00032419]
-0.558863
1.02393
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089842 minutes
Epoch 0
Fine tuning took 0.088957 minutes
Epoch 0
Fine tuning took 0.088667 minutes
{'zero': {0: [0.19334975369458129, 0.1354679802955665, 0.17241379310344829, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.69704433497536944, 0.63054187192118227, 0.57389162561576357], 5: [0.23399014778325122, 0.16748768472906403, 0.19704433497536947, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19334975369458129, 0.043103448275862072, 0.078817733990147784, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.86083743842364535, 0.78078817733990147, 0.68965517241379315], 5: [0.23399014778325122, 0.096059113300492605, 0.14039408866995073, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19334975369458129, 0.093596059113300489, 0.13300492610837439, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.79187192118226601, 0.70566502463054193, 0.61822660098522164], 5: [0.23399014778325122, 0.1145320197044335, 0.16133004926108374, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19334975369458129, 0.044334975369458129, 0.098522167487684734, 0.11945812807881774], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.8719211822660099, 0.76847290640394084, 0.70566502463054193], 5: [0.23399014778325122, 0.083743842364532015, 0.13300492610837439, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227076 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.31204
Epoch 1, cost is  2.26356
Epoch 2, cost is  2.22589
Epoch 3, cost is  2.18842
Epoch 4, cost is  2.15496
Training took 0.153069 minutes
Weight histogram
[2305 1793 1745 1473 1268 1220 1027 1110 2087  147] [ -4.33831848e-02  -3.90370926e-02  -3.46910003e-02  -3.03449080e-02
  -2.59988158e-02  -2.16527235e-02  -1.73066313e-02  -1.29605390e-02
  -8.61444675e-03  -4.26835449e-03   7.77377718e-05]
[2417  914 1015 1082 1153 1228 1367 1502 1675 1822] [ -4.33831848e-02  -3.90370926e-02  -3.46910003e-02  -3.03449080e-02
  -2.59988158e-02  -2.16527235e-02  -1.73066313e-02  -1.29605390e-02
  -8.61444675e-03  -4.26835449e-03   7.77377718e-05]
-0.62088
1.26697
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225851 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.38821
Epoch 1, cost is  2.33809
Epoch 2, cost is  2.29051
Epoch 3, cost is  2.2488
Epoch 4, cost is  2.20922
Training took 0.152678 minutes
Weight histogram
[1999 1798 1592 1431 1302 1318 1135 1239 4121  265] [ -4.13735211e-02  -3.72283952e-02  -3.30832693e-02  -2.89381434e-02
  -2.47930175e-02  -2.06478917e-02  -1.65027658e-02  -1.23576399e-02
  -8.21251400e-03  -4.06738811e-03   7.77377718e-05]
[4672 1023 1031 1056 1150 1201 1320 1451 1574 1722] [ -4.13735211e-02  -3.72283952e-02  -3.30832693e-02  -2.89381434e-02
  -2.47930175e-02  -2.06478917e-02  -1.65027658e-02  -1.23576399e-02
  -8.21251400e-03  -4.06738811e-03   7.77377718e-05]
-0.557086
0.94873
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.83686
Epoch 1, cost is  6.73598
Epoch 2, cost is  6.64932
Epoch 3, cost is  6.5663
Epoch 4, cost is  6.48665
Training took 0.090555 minutes
Weight histogram
[1365 4013 6101 1094  579  365  249  175  130  104] [ -8.89928360e-03  -8.01256373e-03  -7.12584385e-03  -6.23912398e-03
  -5.35240411e-03  -4.46568424e-03  -3.57896437e-03  -2.69224450e-03
  -1.80552463e-03  -9.18804762e-04  -3.20848922e-05]
[5581 2360 1791 1516 1273  743  497  211   99  104] [ -8.89928360e-03  -8.01256373e-03  -7.12584385e-03  -6.23912398e-03
  -5.35240411e-03  -4.46568424e-03  -3.57896437e-03  -2.69224450e-03
  -1.80552463e-03  -9.18804762e-04  -3.20848922e-05]
-0.0589014
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080315 minutes
Epoch 0
Fine tuning took 0.081222 minutes
Epoch 0
Fine tuning took 0.080339 minutes
{'zero': {0: [0.0049261083743842365, 0.057881773399014777, 0.15517241379310345, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.76108374384236455, 0.45935960591133007, 0.47783251231527096], 5: [0.030788177339901478, 0.18103448275862069, 0.3854679802955665, 0.29310344827586204], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.0049261083743842365, 0.067733990147783252, 0.14532019704433496, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.73152709359605916, 0.4963054187192118, 0.45197044334975367], 5: [0.030788177339901478, 0.20073891625615764, 0.35837438423645318, 0.29926108374384236], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.0049261083743842365, 0.070197044334975367, 0.15763546798029557, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.73029556650246308, 0.47044334975369456, 0.47906403940886699], 5: [0.030788177339901478, 0.19950738916256158, 0.37192118226600984, 0.28694581280788178], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.0049261083743842365, 0.065270935960591137, 0.11822660098522167, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.74014778325123154, 0.51108374384236455, 0.48645320197044334], 5: [0.030788177339901478, 0.19458128078817735, 0.37068965517241381, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226938 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.31204
Epoch 1, cost is  2.26356
Epoch 2, cost is  2.22589
Epoch 3, cost is  2.18842
Epoch 4, cost is  2.15496
Training took 0.152578 minutes
Weight histogram
[2305 1793 1745 1473 1268 1220 1027 1110 2087  147] [ -4.33831848e-02  -3.90370926e-02  -3.46910003e-02  -3.03449080e-02
  -2.59988158e-02  -2.16527235e-02  -1.73066313e-02  -1.29605390e-02
  -8.61444675e-03  -4.26835449e-03   7.77377718e-05]
[2417  914 1015 1082 1153 1228 1367 1502 1675 1822] [ -4.33831848e-02  -3.90370926e-02  -3.46910003e-02  -3.03449080e-02
  -2.59988158e-02  -2.16527235e-02  -1.73066313e-02  -1.29605390e-02
  -8.61444675e-03  -4.26835449e-03   7.77377718e-05]
-0.62088
1.26697
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226049 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.38821
Epoch 1, cost is  2.33809
Epoch 2, cost is  2.29051
Epoch 3, cost is  2.2488
Epoch 4, cost is  2.20922
Training took 0.151532 minutes
Weight histogram
[1999 1798 1592 1431 1302 1318 1135 1239 4121  265] [ -4.13735211e-02  -3.72283952e-02  -3.30832693e-02  -2.89381434e-02
  -2.47930175e-02  -2.06478917e-02  -1.65027658e-02  -1.23576399e-02
  -8.21251400e-03  -4.06738811e-03   7.77377718e-05]
[4672 1023 1031 1056 1150 1201 1320 1451 1574 1722] [ -4.13735211e-02  -3.72283952e-02  -3.30832693e-02  -2.89381434e-02
  -2.47930175e-02  -2.06478917e-02  -1.65027658e-02  -1.23576399e-02
  -8.21251400e-03  -4.06738811e-03   7.77377718e-05]
-0.557086
0.94873
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78837
Epoch 1, cost is  6.66836
Epoch 2, cost is  6.57862
Epoch 3, cost is  6.49357
Epoch 4, cost is  6.40948
Training took 0.107266 minutes
Weight histogram
[1228 2990 6772 1403  666  407  272  191  137  109] [ -9.23346821e-03  -8.31561328e-03  -7.39775835e-03  -6.47990342e-03
  -5.56204849e-03  -4.64419356e-03  -3.72633863e-03  -2.80848370e-03
  -1.89062876e-03  -9.72773834e-04  -5.49189026e-05]
[5357 2336 1809 1547 1330  800  519  260  107  110] [ -9.23346821e-03  -8.31561328e-03  -7.39775835e-03  -6.47990342e-03
  -5.56204849e-03  -4.64419356e-03  -3.72633863e-03  -2.80848370e-03
  -1.89062876e-03  -9.72773834e-04  -5.49189026e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083773 minutes
Epoch 0
Fine tuning took 0.084230 minutes
Epoch 0
Fine tuning took 0.083110 minutes
{'zero': {0: [0.0024630541871921183, 0.080049261083743842, 0.17857142857142858, 0.22044334975369459], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.98152709359605916, 0.71182266009852213, 0.46551724137931033, 0.43596059113300495], 5: [0.01600985221674877, 0.20812807881773399, 0.35591133004926107, 0.34359605911330049], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.0024630541871921183, 0.080049261083743842, 0.15517241379310345, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.98152709359605916, 0.68349753694581283, 0.45443349753694579, 0.39039408866995073], 5: [0.01600985221674877, 0.23645320197044334, 0.39039408866995073, 0.37192118226600984], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.0024630541871921183, 0.084975369458128072, 0.17980295566502463, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.98152709359605916, 0.68596059113300489, 0.45566502463054187, 0.39655172413793105], 5: [0.01600985221674877, 0.22906403940886699, 0.3645320197044335, 0.34113300492610837], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.0024630541871921183, 0.080049261083743842, 0.18596059113300492, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.98152709359605916, 0.73152709359605916, 0.46674876847290642, 0.37807881773399016], 5: [0.01600985221674877, 0.18842364532019704, 0.34729064039408869, 0.39778325123152708], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226168 minutes
Weight histogram
[  38  202  577  605  882 1045 1496 2783 4651 1896] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
[ 713  764  754  877  998 1328 1622 1822 2414 2883] [ -1.34912640e-04  -3.11314914e-05   7.26496568e-05   1.76430805e-04
   2.80211953e-04   3.83993101e-04   4.87774250e-04   5.91555398e-04
   6.95336546e-04   7.99117694e-04   9.02898842e-04]
-0.582558
0.854486
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.31204
Epoch 1, cost is  2.26356
Epoch 2, cost is  2.22589
Epoch 3, cost is  2.18842
Epoch 4, cost is  2.15496
Training took 0.151658 minutes
Weight histogram
[2305 1793 1745 1473 1268 1220 1027 1110 2087  147] [ -4.33831848e-02  -3.90370926e-02  -3.46910003e-02  -3.03449080e-02
  -2.59988158e-02  -2.16527235e-02  -1.73066313e-02  -1.29605390e-02
  -8.61444675e-03  -4.26835449e-03   7.77377718e-05]
[2417  914 1015 1082 1153 1228 1367 1502 1675 1822] [ -4.33831848e-02  -3.90370926e-02  -3.46910003e-02  -3.03449080e-02
  -2.59988158e-02  -2.16527235e-02  -1.73066313e-02  -1.29605390e-02
  -8.61444675e-03  -4.26835449e-03   7.77377718e-05]
-0.62088
1.26697
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227266 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1655 3543 4913  761] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1102 1458 1219  864 1038 1289 1618 2005 2554 3053] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.746965
0.473893
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.38821
Epoch 1, cost is  2.33809
Epoch 2, cost is  2.29051
Epoch 3, cost is  2.2488
Epoch 4, cost is  2.20922
Training took 0.152563 minutes
Weight histogram
[1999 1798 1592 1431 1302 1318 1135 1239 4121  265] [ -4.13735211e-02  -3.72283952e-02  -3.30832693e-02  -2.89381434e-02
  -2.47930175e-02  -2.06478917e-02  -1.65027658e-02  -1.23576399e-02
  -8.21251400e-03  -4.06738811e-03   7.77377718e-05]
[4672 1023 1031 1056 1150 1201 1320 1451 1574 1722] [ -4.13735211e-02  -3.72283952e-02  -3.30832693e-02  -2.89381434e-02
  -2.47930175e-02  -2.06478917e-02  -1.65027658e-02  -1.23576399e-02
  -8.21251400e-03  -4.06738811e-03   7.77377718e-05]
-0.557086
0.94873
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.64307
Epoch 1, cost is  6.47489
Epoch 2, cost is  6.37458
Epoch 3, cost is  6.28243
Epoch 4, cost is  6.19905
Training took 0.156696 minutes
Weight histogram
[ 929 1955 6184 2771  959  531  339  229  157  121] [ -1.00774989e-02  -9.07389618e-03  -8.07029351e-03  -7.06669084e-03
  -6.06308817e-03  -5.05948550e-03  -4.05588283e-03  -3.05228016e-03
  -2.04867749e-03  -1.04507482e-03  -4.14721471e-05]
[4852 2244 1811 1611 1447  989  596  370  125  130] [ -1.00774989e-02  -9.07389618e-03  -8.07029351e-03  -7.06669084e-03
  -6.06308817e-03  -5.05948550e-03  -4.05588283e-03  -3.05228016e-03
  -2.04867749e-03  -1.04507482e-03  -4.14721471e-05]
-0.0601954
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089124 minutes
Epoch 0
Fine tuning took 0.088333 minutes
Epoch 0
Fine tuning took 0.088391 minutes
{'zero': {0: [0.0061576354679802959, 0.099753694581280791, 0.30418719211822659, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9568965517241379, 0.77216748768472909, 0.34482758620689657, 0.57758620689655171], 5: [0.036945812807881777, 0.12807881773399016, 0.35098522167487683, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.0061576354679802959, 0.10467980295566502, 0.30049261083743845, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9568965517241379, 0.72783251231527091, 0.33004926108374383, 0.55172413793103448], 5: [0.036945812807881777, 0.16748768472906403, 0.36945812807881773, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.0061576354679802959, 0.089901477832512317, 0.28325123152709358, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9568965517241379, 0.7573891625615764, 0.34236453201970446, 0.55788177339901479], 5: [0.036945812807881777, 0.15270935960591134, 0.37438423645320196, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.0061576354679802959, 0.091133004926108374, 0.30788177339901479, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9568965517241379, 0.76477832512315269, 0.33251231527093594, 0.57266009852216748], 5: [0.036945812807881777, 0.14408866995073891, 0.35960591133004927, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227765 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.87385
Epoch 1, cost is  0.827976
Epoch 2, cost is  0.803874
Epoch 3, cost is  0.784112
Epoch 4, cost is  0.770797
Training took 0.152415 minutes
Weight histogram
[4428 3640 2698 2189 1249 1345  383  150   60   58] [-0.17499919 -0.15785114 -0.14070309 -0.12355504 -0.10640698 -0.08925893
 -0.07211088 -0.05496283 -0.03781477 -0.02066672 -0.00351867]
[  96  135  239  407  690 1266 1810 2693 3887 4977] [-0.17499919 -0.15785114 -0.14070309 -0.12355504 -0.10640698 -0.08925893
 -0.07211088 -0.05496283 -0.03781477 -0.02066672 -0.00351867]
-5.08326
5.8314
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224203 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.857705
Epoch 1, cost is  0.813829
Epoch 2, cost is  0.790357
Epoch 3, cost is  0.770935
Epoch 4, cost is  0.753963
Training took 0.152752 minutes
Weight histogram
[5065 3014 2987 2236 1642 2142  657  233  130  119] [-0.16812907 -0.15166803 -0.13520699 -0.11874595 -0.10228491 -0.08582387
 -0.06936283 -0.05290179 -0.03644075 -0.01997971 -0.00351867]
[ 198  252  441  785 1366 1670 1780 2680 3976 5077] [-0.16812907 -0.15166803 -0.13520699 -0.11874595 -0.10228491 -0.08582387
 -0.06936283 -0.05290179 -0.03644075 -0.01997971 -0.00351867]
-5.14181
5.82728
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.43001
Epoch 1, cost is  1.94674
Epoch 2, cost is  1.65508
Epoch 3, cost is  1.51384
Epoch 4, cost is  1.42399
Training took 0.089316 minutes
Weight histogram
[3929 3619 2525 2052 1218  805  568  410  541  533] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[1062  722  780  966 1141 1443 1798 2227 2778 3283] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.68512
4.06771
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081101 minutes
Epoch 0
Fine tuning took 0.082469 minutes
Epoch 0
Fine tuning took 0.081444 minutes
{'zero': {0: [0.1748768472906404, 0.33990147783251229, 0.27586206896551724, 0.33251231527093594], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69704433497536944, 0.41995073891625617, 0.40517241379310343, 0.43842364532019706], 5: [0.12807881773399016, 0.24014778325123154, 0.31896551724137934, 0.22906403940886699], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1748768472906404, 0.072660098522167482, 0.038177339901477834, 0.062807881773399021], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69704433497536944, 0.83497536945812811, 0.8928571428571429, 0.87068965517241381], 5: [0.12807881773399016, 0.092364532019704432, 0.068965517241379309, 0.066502463054187194], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1748768472906404, 0.089901477832512317, 0.060344827586206899, 0.035714285714285712], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69704433497536944, 0.78078817733990147, 0.85960591133004927, 0.85221674876847286], 5: [0.12807881773399016, 0.12931034482758622, 0.080049261083743842, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1748768472906404, 0.04064039408866995, 0.014778325123152709, 0.032019704433497539], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69704433497536944, 0.89039408866995073, 0.93719211822660098, 0.92364532019704437], 5: [0.12807881773399016, 0.068965517241379309, 0.048029556650246302, 0.044334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227660 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.87385
Epoch 1, cost is  0.827976
Epoch 2, cost is  0.803874
Epoch 3, cost is  0.784112
Epoch 4, cost is  0.770797
Training took 0.153053 minutes
Weight histogram
[4428 3640 2698 2189 1249 1345  383  150   60   58] [-0.17499919 -0.15785114 -0.14070309 -0.12355504 -0.10640698 -0.08925893
 -0.07211088 -0.05496283 -0.03781477 -0.02066672 -0.00351867]
[  96  135  239  407  690 1266 1810 2693 3887 4977] [-0.17499919 -0.15785114 -0.14070309 -0.12355504 -0.10640698 -0.08925893
 -0.07211088 -0.05496283 -0.03781477 -0.02066672 -0.00351867]
-5.08326
5.8314
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227199 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.857705
Epoch 1, cost is  0.813829
Epoch 2, cost is  0.790357
Epoch 3, cost is  0.770935
Epoch 4, cost is  0.753963
Training took 0.152639 minutes
Weight histogram
[5065 3014 2987 2236 1642 2142  657  233  130  119] [-0.16812907 -0.15166803 -0.13520699 -0.11874595 -0.10228491 -0.08582387
 -0.06936283 -0.05290179 -0.03644075 -0.01997971 -0.00351867]
[ 198  252  441  785 1366 1670 1780 2680 3976 5077] [-0.16812907 -0.15166803 -0.13520699 -0.11874595 -0.10228491 -0.08582387
 -0.06936283 -0.05290179 -0.03644075 -0.01997971 -0.00351867]
-5.14181
5.82728
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.24468
Epoch 1, cost is  1.65455
Epoch 2, cost is  1.29982
Epoch 3, cost is  1.12264
Epoch 4, cost is  1.01395
Training took 0.106816 minutes
Weight histogram
[4155 3530 2497 1830 1313  904  551  369  624  427] [-0.15717396 -0.14185212 -0.12653028 -0.11120844 -0.09588659 -0.08056475
 -0.06524291 -0.04992107 -0.03459923 -0.01927739 -0.00395554]
[1089  710  769  959 1182 1500 1824 2219 2732 3216] [-0.15717396 -0.14185212 -0.12653028 -0.11120844 -0.09588659 -0.08056475
 -0.06524291 -0.04992107 -0.03459923 -0.01927739 -0.00395554]
-3.18745
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083252 minutes
Epoch 0
Fine tuning took 0.083353 minutes
Epoch 0
Fine tuning took 0.084107 minutes
{'zero': {0: [0.13423645320197045, 0.38793103448275862, 0.34482758620689657, 0.39408866995073893], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.39039408866995073, 0.33620689655172414, 0.39901477832512317], 5: [0.11083743842364532, 0.22167487684729065, 0.31896551724137934, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13423645320197045, 0.17857142857142858, 0.11822660098522167, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.6785714285714286, 0.68349753694581283, 0.73645320197044339], 5: [0.11083743842364532, 0.14285714285714285, 0.19827586206896552, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13423645320197045, 0.15024630541871922, 0.15270935960591134, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.70197044334975367, 0.66502463054187189, 0.69581280788177335], 5: [0.11083743842364532, 0.14778325123152711, 0.18226600985221675, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13423645320197045, 0.14162561576354679, 0.1268472906403941, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.69827586206896552, 0.68349753694581283, 0.78940886699507384], 5: [0.11083743842364532, 0.16009852216748768, 0.18965517241379309, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226615 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.87385
Epoch 1, cost is  0.827976
Epoch 2, cost is  0.803874
Epoch 3, cost is  0.784112
Epoch 4, cost is  0.770797
Training took 0.151555 minutes
Weight histogram
[4428 3640 2698 2189 1249 1345  383  150   60   58] [-0.17499919 -0.15785114 -0.14070309 -0.12355504 -0.10640698 -0.08925893
 -0.07211088 -0.05496283 -0.03781477 -0.02066672 -0.00351867]
[  96  135  239  407  690 1266 1810 2693 3887 4977] [-0.17499919 -0.15785114 -0.14070309 -0.12355504 -0.10640698 -0.08925893
 -0.07211088 -0.05496283 -0.03781477 -0.02066672 -0.00351867]
-5.08326
5.8314
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224500 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.857705
Epoch 1, cost is  0.813829
Epoch 2, cost is  0.790357
Epoch 3, cost is  0.770935
Epoch 4, cost is  0.753963
Training took 0.154282 minutes
Weight histogram
[5065 3014 2987 2236 1642 2142  657  233  130  119] [-0.16812907 -0.15166803 -0.13520699 -0.11874595 -0.10228491 -0.08582387
 -0.06936283 -0.05290179 -0.03644075 -0.01997971 -0.00351867]
[ 198  252  441  785 1366 1670 1780 2680 3976 5077] [-0.16812907 -0.15166803 -0.13520699 -0.11874595 -0.10228491 -0.08582387
 -0.06936283 -0.05290179 -0.03644075 -0.01997971 -0.00351867]
-5.14181
5.82728
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.04636
Epoch 1, cost is  1.41639
Epoch 2, cost is  1.01056
Epoch 3, cost is  0.820608
Epoch 4, cost is  0.708879
Training took 0.158147 minutes
Weight histogram
[4272 3495 2363 1753 1252  960  687  451  639  328] [-0.10850771 -0.09803665 -0.0875656  -0.07709455 -0.06662349 -0.05615244
 -0.04568139 -0.03521034 -0.02473928 -0.01426823 -0.00379718]
[1149  643  754  949 1175 1452 1797 2245 2801 3235] [-0.10850771 -0.09803665 -0.0875656  -0.07709455 -0.06662349 -0.05615244
 -0.04568139 -0.03521034 -0.02473928 -0.01426823 -0.00379718]
-2.01397
3.03885
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088998 minutes
Epoch 0
Fine tuning took 0.089578 minutes
Epoch 0
Fine tuning took 0.089972 minutes
{'zero': {0: [0.21798029556650247, 0.36699507389162561, 0.36576354679802958, 0.35098522167487683], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62684729064039413, 0.43965517241379309, 0.2894088669950739, 0.41009852216748771], 5: [0.15517241379310345, 0.19334975369458129, 0.34482758620689657, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21798029556650247, 0.34729064039408869, 0.37684729064039407, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62684729064039413, 0.47536945812807879, 0.33374384236453203, 0.4211822660098522], 5: [0.15517241379310345, 0.17733990147783252, 0.2894088669950739, 0.26724137931034481], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21798029556650247, 0.31896551724137934, 0.37315270935960593, 0.31650246305418717], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62684729064039413, 0.48152709359605911, 0.3460591133004926, 0.42733990147783252], 5: [0.15517241379310345, 0.19950738916256158, 0.28078817733990147, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21798029556650247, 0.3608374384236453, 0.39408866995073893, 0.30788177339901479], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62684729064039413, 0.41748768472906406, 0.26970443349753692, 0.41625615763546797], 5: [0.15517241379310345, 0.22167487684729065, 0.33620689655172414, 0.27586206896551724], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226555 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.960481
Epoch 1, cost is  0.939911
Epoch 2, cost is  0.92766
Epoch 3, cost is  0.915904
Epoch 4, cost is  0.904987
Training took 0.151620 minutes
Weight histogram
[4659 3431 3170 1969  952  782  449  319  250  219] [-0.08305027 -0.07477016 -0.06649005 -0.05820995 -0.04992984 -0.04164974
 -0.03336963 -0.02508953 -0.01680942 -0.00852931 -0.00024921]
[ 491  398  558  810 1037 1450 1904 2470 3124 3958] [-0.08305027 -0.07477016 -0.06649005 -0.05820995 -0.04992984 -0.04164974
 -0.03336963 -0.02508953 -0.01680942 -0.00852931 -0.00024921]
-2.21326
2.8478
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226908 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.953251
Epoch 1, cost is  0.932939
Epoch 2, cost is  0.918677
Epoch 3, cost is  0.908501
Epoch 4, cost is  0.897423
Training took 0.150121 minutes
Weight histogram
[5039 3080 3119 1686 1189 1430  943  689  548  502] [-0.08552448 -0.07699695 -0.06846942 -0.0599419  -0.05141437 -0.04288684
 -0.03435932 -0.02583179 -0.01730426 -0.00877673 -0.00024921]
[1030  807 1100 1256  982 1378 1863 2513 3198 4098] [-0.08552448 -0.07699695 -0.06846942 -0.0599419  -0.05141437 -0.04288684
 -0.03435932 -0.02583179 -0.01730426 -0.00877673 -0.00024921]
-2.08829
2.5371
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.49057
Epoch 1, cost is  5.75596
Epoch 2, cost is  4.81499
Epoch 3, cost is  4.11665
Epoch 4, cost is  3.74146
Training took 0.090743 minutes
Weight histogram
[1241 1526 1416 1189 1234 1121 1332 1842 2630 2669] [-0.06757683 -0.0608519  -0.05412697 -0.04740204 -0.04067711 -0.03395218
 -0.02722725 -0.02050232 -0.01377739 -0.00705246 -0.00032753]
[3879 1939 1293 1114 1137 1225 1324 1435 1607 1247] [-0.06757683 -0.0608519  -0.05412697 -0.04740204 -0.04067711 -0.03395218
 -0.02722725 -0.02050232 -0.01377739 -0.00705246 -0.00032753]
-0.860581
1.34971
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082463 minutes
Epoch 0
Fine tuning took 0.081725 minutes
Epoch 0
Fine tuning took 0.082572 minutes
{'zero': {0: [0.42733990147783252, 0.013546798029556651, 0.029556650246305417, 0.055418719211822662], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44950738916256155, 0.93349753694581283, 0.90640394088669951, 0.82389162561576357], 5: [0.12315270935960591, 0.05295566502463054, 0.064039408866995079, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.42733990147783252, 0.012315270935960592, 0.009852216748768473, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44950738916256155, 0.94950738916256161, 0.96059113300492616, 0.9285714285714286], 5: [0.12315270935960591, 0.038177339901477834, 0.029556650246305417, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.42733990147783252, 0.0086206896551724137, 0.022167487684729065, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44950738916256155, 0.96305418719211822, 0.94211822660098521, 0.8780788177339901], 5: [0.12315270935960591, 0.02832512315270936, 0.035714285714285712, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.42733990147783252, 0.0049261083743842365, 0.013546798029556651, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44950738916256155, 0.96921182266009853, 0.9568965517241379, 0.92733990147783252], 5: [0.12315270935960591, 0.025862068965517241, 0.029556650246305417, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226314 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.960481
Epoch 1, cost is  0.939911
Epoch 2, cost is  0.92766
Epoch 3, cost is  0.915904
Epoch 4, cost is  0.904987
Training took 0.150673 minutes
Weight histogram
[4659 3431 3170 1969  952  782  449  319  250  219] [-0.08305027 -0.07477016 -0.06649005 -0.05820995 -0.04992984 -0.04164974
 -0.03336963 -0.02508953 -0.01680942 -0.00852931 -0.00024921]
[ 491  398  558  810 1037 1450 1904 2470 3124 3958] [-0.08305027 -0.07477016 -0.06649005 -0.05820995 -0.04992984 -0.04164974
 -0.03336963 -0.02508953 -0.01680942 -0.00852931 -0.00024921]
-2.21326
2.8478
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225215 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.953251
Epoch 1, cost is  0.932939
Epoch 2, cost is  0.918677
Epoch 3, cost is  0.908501
Epoch 4, cost is  0.897423
Training took 0.152128 minutes
Weight histogram
[5039 3080 3119 1686 1189 1430  943  689  548  502] [-0.08552448 -0.07699695 -0.06846942 -0.0599419  -0.05141437 -0.04288684
 -0.03435932 -0.02583179 -0.01730426 -0.00877673 -0.00024921]
[1030  807 1100 1256  982 1378 1863 2513 3198 4098] [-0.08552448 -0.07699695 -0.06846942 -0.0599419  -0.05141437 -0.04288684
 -0.03435932 -0.02583179 -0.01730426 -0.00877673 -0.00024921]
-2.08829
2.5371
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.41961
Epoch 1, cost is  5.68898
Epoch 2, cost is  4.65751
Epoch 3, cost is  3.84077
Epoch 4, cost is  3.38471
Training took 0.108283 minutes
Weight histogram
[ 991 1735 1521 1303 1320 1194 1654 2244 4034  204] [-0.04952583 -0.04460718 -0.03968853 -0.03476988 -0.02985122 -0.02493257
 -0.02001392 -0.01509526 -0.01017661 -0.00525796 -0.00033931]
[3909 2195 1165 1069 1126 1182 1287 1415 1608 1244] [-0.04952583 -0.04460718 -0.03968853 -0.03476988 -0.02985122 -0.02493257
 -0.02001392 -0.01509526 -0.01017661 -0.00525796 -0.00033931]
-0.634967
1.22653
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083873 minutes
Epoch 0
Fine tuning took 0.084617 minutes
Epoch 0
Fine tuning took 0.082684 minutes
{'zero': {0: [0.37068965517241381, 0.060344827586206899, 0.068965517241379309, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49507389162561577, 0.85098522167487689, 0.83497536945812811, 0.68472906403940892], 5: [0.13423645320197045, 0.088669950738916259, 0.096059113300492605, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37068965517241381, 0.019704433497536946, 0.017241379310344827, 0.054187192118226604], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49507389162561577, 0.92980295566502458, 0.93103448275862066, 0.83990147783251234], 5: [0.13423645320197045, 0.050492610837438424, 0.051724137931034482, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37068965517241381, 0.02832512315270936, 0.033251231527093597, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49507389162561577, 0.91871921182266014, 0.90270935960591137, 0.77339901477832518], 5: [0.13423645320197045, 0.05295566502463054, 0.064039408866995079, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37068965517241381, 0.018472906403940888, 0.029556650246305417, 0.072660098522167482], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49507389162561577, 0.94211822660098521, 0.91625615763546797, 0.81157635467980294], 5: [0.13423645320197045, 0.039408866995073892, 0.054187192118226604, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225390 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.960481
Epoch 1, cost is  0.939911
Epoch 2, cost is  0.92766
Epoch 3, cost is  0.915904
Epoch 4, cost is  0.904987
Training took 0.150677 minutes
Weight histogram
[4659 3431 3170 1969  952  782  449  319  250  219] [-0.08305027 -0.07477016 -0.06649005 -0.05820995 -0.04992984 -0.04164974
 -0.03336963 -0.02508953 -0.01680942 -0.00852931 -0.00024921]
[ 491  398  558  810 1037 1450 1904 2470 3124 3958] [-0.08305027 -0.07477016 -0.06649005 -0.05820995 -0.04992984 -0.04164974
 -0.03336963 -0.02508953 -0.01680942 -0.00852931 -0.00024921]
-2.21326
2.8478
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224683 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.953251
Epoch 1, cost is  0.932939
Epoch 2, cost is  0.918677
Epoch 3, cost is  0.908501
Epoch 4, cost is  0.897423
Training took 0.149864 minutes
Weight histogram
[5039 3080 3119 1686 1189 1430  943  689  548  502] [-0.08552448 -0.07699695 -0.06846942 -0.0599419  -0.05141437 -0.04288684
 -0.03435932 -0.02583179 -0.01730426 -0.00877673 -0.00024921]
[1030  807 1100 1256  982 1378 1863 2513 3198 4098] [-0.08552448 -0.07699695 -0.06846942 -0.0599419  -0.05141437 -0.04288684
 -0.03435932 -0.02583179 -0.01730426 -0.00877673 -0.00024921]
-2.08829
2.5371
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.23389
Epoch 1, cost is  5.53489
Epoch 2, cost is  4.49855
Epoch 3, cost is  3.59427
Epoch 4, cost is  3.03562
Training took 0.158011 minutes
Weight histogram
[ 967 2119 1579 1453 1426 1806 3401 2965  409   75] [-0.03077359 -0.02772842 -0.02468324 -0.02163807 -0.0185929  -0.01554773
 -0.01250255 -0.00945738 -0.00641221 -0.00336704 -0.00032187]
[4540 1982 1029 1028 1047 1147 1253 1362 1471 1341] [-0.03077359 -0.02772842 -0.02468324 -0.02163807 -0.0185929  -0.01554773
 -0.01250255 -0.00945738 -0.00641221 -0.00336704 -0.00032187]
-0.558863
1.04317
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089778 minutes
Epoch 0
Fine tuning took 0.089011 minutes
Epoch 0
Fine tuning took 0.088755 minutes
{'zero': {0: [0.28325123152709358, 0.14901477832512317, 0.19458128078817735, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.67487684729064035, 0.61945812807881773, 0.56650246305418717], 5: [0.16995073891625614, 0.17610837438423646, 0.18596059113300492, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28325123152709358, 0.062807881773399021, 0.093596059113300489, 0.089901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.84113300492610843, 0.75985221674876846, 0.71551724137931039], 5: [0.16995073891625614, 0.096059113300492605, 0.14655172413793102, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28325123152709358, 0.087438423645320201, 0.15763546798029557, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.77832512315270941, 0.69458128078817738, 0.61576354679802958], 5: [0.16995073891625614, 0.13423645320197045, 0.14778325123152711, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28325123152709358, 0.043103448275862072, 0.098522167487684734, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54679802955665024, 0.87684729064039413, 0.76724137931034486, 0.68226600985221675], 5: [0.16995073891625614, 0.080049261083743842, 0.13423645320197045, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226578 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.09577
Epoch 1, cost is  2.05775
Epoch 2, cost is  2.02664
Epoch 3, cost is  1.99894
Epoch 4, cost is  1.97031
Training took 0.151866 minutes
Weight histogram
[2905 2260 1886 1633 1412 1431 1106 1144 2249  174] [ -4.66245562e-02  -4.19543268e-02  -3.72840974e-02  -3.26138680e-02
  -2.79436386e-02  -2.32734092e-02  -1.86031798e-02  -1.39329504e-02
  -9.26272102e-03  -4.59249162e-03   7.77377718e-05]
[2508 1022 1137 1194 1321 1424 1560 1821 1991 2222] [ -4.66245562e-02  -4.19543268e-02  -3.72840974e-02  -3.26138680e-02
  -2.79436386e-02  -2.32734092e-02  -1.86031798e-02  -1.39329504e-02
  -9.26272102e-03  -4.59249162e-03   7.77377718e-05]
-0.691858
1.37913
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225560 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.14205
Epoch 1, cost is  2.10772
Epoch 2, cost is  2.0676
Epoch 3, cost is  2.03295
Epoch 4, cost is  2.00136
Training took 0.151220 minutes
Weight histogram
[2553 2083 1801 1630 1589 1384 1230 1330 4305  320] [ -4.49430496e-02  -4.04409708e-02  -3.59388921e-02  -3.14368134e-02
  -2.69347346e-02  -2.24326559e-02  -1.79305772e-02  -1.34284984e-02
  -8.92641969e-03  -4.42434096e-03   7.77377718e-05]
[4787 1122 1150 1205 1304 1401 1551 1703 1907 2095] [ -4.49430496e-02  -4.04409708e-02  -3.59388921e-02  -3.14368134e-02
  -2.69347346e-02  -2.24326559e-02  -1.79305772e-02  -1.34284984e-02
  -8.92641969e-03  -4.42434096e-03   7.77377718e-05]
-0.584364
0.987724
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.84041
Epoch 1, cost is  6.74317
Epoch 2, cost is  6.65939
Epoch 3, cost is  6.57943
Epoch 4, cost is  6.50241
Training took 0.092533 minutes
Weight histogram
[1365 4013 7590 1349  688  429  291  204  150  121] [ -8.89928360e-03  -8.01252927e-03  -7.12577495e-03  -6.23902063e-03
  -5.35226631e-03  -4.46551199e-03  -3.57875766e-03  -2.69200334e-03
  -1.80524902e-03  -9.18494698e-04  -3.17403756e-05]
[6507 2758 2085 1777 1419  743  497  211   99  104] [ -8.89928360e-03  -8.01252927e-03  -7.12577495e-03  -6.23902063e-03
  -5.35226631e-03  -4.46551199e-03  -3.57875766e-03  -2.69200334e-03
  -1.80524902e-03  -9.18494698e-04  -3.17403756e-05]
-0.0589014
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081993 minutes
Epoch 0
Fine tuning took 0.081903 minutes
Epoch 0
Fine tuning took 0.081230 minutes
{'zero': {0: [0.011083743842364532, 0.0024630541871921183, 0.17857142857142858, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.96551724137931039, 0.60837438423645318, 0.73029556650246308], 5: [0.20443349753694581, 0.032019704433497539, 0.21305418719211822, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.011083743842364532, 0.009852216748768473, 0.15886699507389163, 0.12807881773399016], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.97167487684729059, 0.5923645320197044, 0.72906403940886699], 5: [0.20443349753694581, 0.018472906403940888, 0.24876847290640394, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.011083743842364532, 0.0049261083743842365, 0.18472906403940886, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.96798029556650245, 0.62561576354679804, 0.7573891625615764], 5: [0.20443349753694581, 0.027093596059113302, 0.18965517241379309, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.011083743842364532, 0.0061576354679802959, 0.15886699507389163, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78448275862068961, 0.97413793103448276, 0.64408866995073888, 0.74630541871921185], 5: [0.20443349753694581, 0.019704433497536946, 0.19704433497536947, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227451 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.09577
Epoch 1, cost is  2.05775
Epoch 2, cost is  2.02664
Epoch 3, cost is  1.99894
Epoch 4, cost is  1.97031
Training took 0.152959 minutes
Weight histogram
[2905 2260 1886 1633 1412 1431 1106 1144 2249  174] [ -4.66245562e-02  -4.19543268e-02  -3.72840974e-02  -3.26138680e-02
  -2.79436386e-02  -2.32734092e-02  -1.86031798e-02  -1.39329504e-02
  -9.26272102e-03  -4.59249162e-03   7.77377718e-05]
[2508 1022 1137 1194 1321 1424 1560 1821 1991 2222] [ -4.66245562e-02  -4.19543268e-02  -3.72840974e-02  -3.26138680e-02
  -2.79436386e-02  -2.32734092e-02  -1.86031798e-02  -1.39329504e-02
  -9.26272102e-03  -4.59249162e-03   7.77377718e-05]
-0.691858
1.37913
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225387 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.14205
Epoch 1, cost is  2.10772
Epoch 2, cost is  2.0676
Epoch 3, cost is  2.03295
Epoch 4, cost is  2.00136
Training took 0.152231 minutes
Weight histogram
[2553 2083 1801 1630 1589 1384 1230 1330 4305  320] [ -4.49430496e-02  -4.04409708e-02  -3.59388921e-02  -3.14368134e-02
  -2.69347346e-02  -2.24326559e-02  -1.79305772e-02  -1.34284984e-02
  -8.92641969e-03  -4.42434096e-03   7.77377718e-05]
[4787 1122 1150 1205 1304 1401 1551 1703 1907 2095] [ -4.49430496e-02  -4.04409708e-02  -3.59388921e-02  -3.14368134e-02
  -2.69347346e-02  -2.24326559e-02  -1.79305772e-02  -1.34284984e-02
  -8.92641969e-03  -4.42434096e-03   7.77377718e-05]
-0.584364
0.987724
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79335
Epoch 1, cost is  6.67809
Epoch 2, cost is  6.59148
Epoch 3, cost is  6.50972
Epoch 4, cost is  6.42912
Training took 0.106276 minutes
Weight histogram
[1228 2990 8115 1766  796  479  318  223  159  126] [ -9.23346821e-03  -8.31560346e-03  -7.39773870e-03  -6.47987394e-03
  -5.56200919e-03  -4.64414443e-03  -3.72627968e-03  -2.80841492e-03
  -1.89055016e-03  -9.72685408e-04  -5.48206517e-05]
[6250 2726 2113 1821 1494  800  519  260  107  110] [ -9.23346821e-03  -8.31560346e-03  -7.39773870e-03  -6.47987394e-03
  -5.56200919e-03  -4.64414443e-03  -3.72627968e-03  -2.80841492e-03
  -1.89055016e-03  -9.72685408e-04  -5.48206517e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084080 minutes
Epoch 0
Fine tuning took 0.084200 minutes
Epoch 0
Fine tuning took 0.084035 minutes
{'zero': {0: [0.0086206896551724137, 0.0061576354679802959, 0.19704433497536947, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80541871921182262, 0.96059113300492616, 0.63793103448275867, 0.68472906403940892], 5: [0.18596059113300492, 0.033251231527093597, 0.16502463054187191, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.0086206896551724137, 0.0036945812807881772, 0.20073891625615764, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80541871921182262, 0.97044334975369462, 0.6219211822660099, 0.67241379310344829], 5: [0.18596059113300492, 0.025862068965517241, 0.17733990147783252, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.0086206896551724137, 0.0086206896551724137, 0.19088669950738915, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80541871921182262, 0.96551724137931039, 0.64039408866995073, 0.69458128078817738], 5: [0.18596059113300492, 0.025862068965517241, 0.16871921182266009, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.0086206896551724137, 0.0012315270935960591, 0.19458128078817735, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80541871921182262, 0.97044334975369462, 0.65270935960591137, 0.66009852216748766], 5: [0.18596059113300492, 0.02832512315270936, 0.15270935960591134, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226484 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3255 6270 1264] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 739  778  833  966 1166 1499 1752 2274 2829 3364] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.598268
0.879126
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.09577
Epoch 1, cost is  2.05775
Epoch 2, cost is  2.02664
Epoch 3, cost is  1.99894
Epoch 4, cost is  1.97031
Training took 0.154938 minutes
Weight histogram
[2905 2260 1886 1633 1412 1431 1106 1144 2249  174] [ -4.66245562e-02  -4.19543268e-02  -3.72840974e-02  -3.26138680e-02
  -2.79436386e-02  -2.32734092e-02  -1.86031798e-02  -1.39329504e-02
  -9.26272102e-03  -4.59249162e-03   7.77377718e-05]
[2508 1022 1137 1194 1321 1424 1560 1821 1991 2222] [ -4.66245562e-02  -4.19543268e-02  -3.72840974e-02  -3.26138680e-02
  -2.79436386e-02  -2.32734092e-02  -1.86031798e-02  -1.39329504e-02
  -9.26272102e-03  -4.59249162e-03   7.77377718e-05]
-0.691858
1.37913
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226259 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 3946 6360  930] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1166 1533 1237  938 1200 1413 1910 2367 2960 3501] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.766725
0.488094
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.14205
Epoch 1, cost is  2.10772
Epoch 2, cost is  2.0676
Epoch 3, cost is  2.03295
Epoch 4, cost is  2.00136
Training took 0.154415 minutes
Weight histogram
[2553 2083 1801 1630 1589 1384 1230 1330 4305  320] [ -4.49430496e-02  -4.04409708e-02  -3.59388921e-02  -3.14368134e-02
  -2.69347346e-02  -2.24326559e-02  -1.79305772e-02  -1.34284984e-02
  -8.92641969e-03  -4.42434096e-03   7.77377718e-05]
[4787 1122 1150 1205 1304 1401 1551 1703 1907 2095] [ -4.49430496e-02  -4.04409708e-02  -3.59388921e-02  -3.14368134e-02
  -2.69347346e-02  -2.24326559e-02  -1.79305772e-02  -1.34284984e-02
  -8.92641969e-03  -4.42434096e-03   7.77377718e-05]
-0.584364
0.987724
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.6535
Epoch 1, cost is  6.4922
Epoch 2, cost is  6.39607
Epoch 3, cost is  6.30773
Epoch 4, cost is  6.22766
Training took 0.158955 minutes
Weight histogram
[ 929 1955 6840 3701 1161  628  397  267  182  140] [ -1.00774989e-02  -9.07386434e-03  -8.07022983e-03  -7.06659531e-03
  -6.06296080e-03  -5.05932629e-03  -4.05569177e-03  -3.05205726e-03
  -2.04842275e-03  -1.04478823e-03  -4.11537185e-05]
[5662 2627 2128 1905 1668  989  596  370  125  130] [ -1.00774989e-02  -9.07386434e-03  -8.07022983e-03  -7.06659531e-03
  -6.06296080e-03  -5.05932629e-03  -4.05569177e-03  -3.05205726e-03
  -2.04842275e-03  -1.04478823e-03  -4.11537185e-05]
-0.0601954
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088557 minutes
Epoch 0
Fine tuning took 0.087490 minutes
Epoch 0
Fine tuning took 0.088946 minutes
{'zero': {0: [0.023399014778325122, 0.012315270935960592, 0.15024630541871922, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79187192118226601, 0.93103448275862066, 0.68226600985221675, 0.77216748768472909], 5: [0.18472906403940886, 0.056650246305418719, 0.16748768472906403, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.023399014778325122, 0.012315270935960592, 0.15640394088669951, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79187192118226601, 0.94704433497536944, 0.6576354679802956, 0.7931034482758621], 5: [0.18472906403940886, 0.04064039408866995, 0.18596059113300492, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.023399014778325122, 0.0086206896551724137, 0.1539408866995074, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79187192118226601, 0.93226600985221675, 0.6785714285714286, 0.76724137931034486], 5: [0.18472906403940886, 0.059113300492610835, 0.16748768472906403, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.023399014778325122, 0.012315270935960592, 0.15640394088669951, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79187192118226601, 0.92241379310344829, 0.66748768472906406, 0.77339901477832518], 5: [0.18472906403940886, 0.065270935960591137, 0.17610837438423646, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.228091 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.813949
Epoch 1, cost is  0.772058
Epoch 2, cost is  0.747924
Epoch 3, cost is  0.730471
Epoch 4, cost is  0.718374
Training took 0.151751 minutes
Weight histogram
[5684 3489 2951 2086 1977 1252  477  181   69   59] [-0.18205062 -0.16419742 -0.14634423 -0.12849103 -0.11063784 -0.09278464
 -0.07493145 -0.05707825 -0.03922506 -0.02137186 -0.00351867]
[ 100  142  255  451  771 1350 2047 2840 4144 6125] [-0.18205062 -0.16419742 -0.14634423 -0.12849103 -0.11063784 -0.09278464
 -0.07493145 -0.05707825 -0.03922506 -0.02137186 -0.00351867]
-5.44092
6.07225
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224922 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.797048
Epoch 1, cost is  0.754719
Epoch 2, cost is  0.730872
Epoch 3, cost is  0.713687
Epoch 4, cost is  0.700325
Training took 0.152682 minutes
Weight histogram
[5957 3922 2262 2458 1654 2714  740  285  134  124] [-0.17458509 -0.15747845 -0.14037181 -0.12326516 -0.10615852 -0.08905188
 -0.07194524 -0.0548386  -0.03773195 -0.02062531 -0.00351867]
[ 204  267  479  866 1518 1641 2040 2836 4550 5849] [-0.17458509 -0.15747845 -0.14037181 -0.12326516 -0.10615852 -0.08905188
 -0.07194524 -0.0548386  -0.03773195 -0.02062531 -0.00351867]
-5.19842
6.2593
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.42332
Epoch 1, cost is  1.94903
Epoch 2, cost is  1.66955
Epoch 3, cost is  1.52879
Epoch 4, cost is  1.43565
Training took 0.088816 minutes
Weight histogram
[4406 4063 2869 2312 1367  910  627  460  612  599] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[1198  813  882 1087 1289 1629 2033 2509 3136 3649] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.68512
4.06771
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082446 minutes
Epoch 0
Fine tuning took 0.082218 minutes
Epoch 0
Fine tuning took 0.081660 minutes
{'zero': {0: [0.15886699507389163, 0.27709359605911332, 0.32635467980295568, 0.31403940886699505], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.42364532019704432, 0.41009852216748771, 0.43719211822660098], 5: [0.13916256157635468, 0.29926108374384236, 0.26354679802955666, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15886699507389163, 0.064039408866995079, 0.068965517241379309, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.84113300492610843, 0.86576354679802958, 0.8571428571428571], 5: [0.13916256157635468, 0.094827586206896547, 0.065270935960591137, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15886699507389163, 0.088669950738916259, 0.11083743842364532, 0.049261083743842367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.78448275862068961, 0.79433497536945807, 0.80788177339901479], 5: [0.13916256157635468, 0.1268472906403941, 0.094827586206896547, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15886699507389163, 0.049261083743842367, 0.020935960591133004, 0.024630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.87315270935960587, 0.93103448275862066, 0.90024630541871919], 5: [0.13916256157635468, 0.077586206896551727, 0.048029556650246302, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226414 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.813949
Epoch 1, cost is  0.772058
Epoch 2, cost is  0.747924
Epoch 3, cost is  0.730471
Epoch 4, cost is  0.718374
Training took 0.152422 minutes
Weight histogram
[5684 3489 2951 2086 1977 1252  477  181   69   59] [-0.18205062 -0.16419742 -0.14634423 -0.12849103 -0.11063784 -0.09278464
 -0.07493145 -0.05707825 -0.03922506 -0.02137186 -0.00351867]
[ 100  142  255  451  771 1350 2047 2840 4144 6125] [-0.18205062 -0.16419742 -0.14634423 -0.12849103 -0.11063784 -0.09278464
 -0.07493145 -0.05707825 -0.03922506 -0.02137186 -0.00351867]
-5.44092
6.07225
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225102 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.797048
Epoch 1, cost is  0.754719
Epoch 2, cost is  0.730872
Epoch 3, cost is  0.713687
Epoch 4, cost is  0.700325
Training took 0.152767 minutes
Weight histogram
[5957 3922 2262 2458 1654 2714  740  285  134  124] [-0.17458509 -0.15747845 -0.14037181 -0.12326516 -0.10615852 -0.08905188
 -0.07194524 -0.0548386  -0.03773195 -0.02062531 -0.00351867]
[ 204  267  479  866 1518 1641 2040 2836 4550 5849] [-0.17458509 -0.15747845 -0.14037181 -0.12326516 -0.10615852 -0.08905188
 -0.07194524 -0.0548386  -0.03773195 -0.02062531 -0.00351867]
-5.19842
6.2593
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.23963
Epoch 1, cost is  1.66266
Epoch 2, cost is  1.3123
Epoch 3, cost is  1.13509
Epoch 4, cost is  1.02768
Training took 0.107459 minutes
Weight histogram
[4640 4002 2805 2063 1479 1022  617  415  702  480] [-0.15717396 -0.14185212 -0.12653028 -0.11120844 -0.09588659 -0.08056475
 -0.06524291 -0.04992107 -0.03459923 -0.01927739 -0.00395554]
[1228  802  870 1080 1337 1692 2058 2493 3074 3591] [-0.15717396 -0.14185212 -0.12653028 -0.11120844 -0.09588659 -0.08056475
 -0.06524291 -0.04992107 -0.03459923 -0.01927739 -0.00395554]
-3.18745
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083280 minutes
Epoch 0
Fine tuning took 0.083600 minutes
Epoch 0
Fine tuning took 0.082886 minutes
{'zero': {0: [0.14285714285714285, 0.30295566502463056, 0.41995073891625617, 0.36330049261083741], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.41748768472906406, 0.33743842364532017, 0.39655172413793105], 5: [0.13916256157635468, 0.27955665024630544, 0.24261083743842365, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14285714285714285, 0.15886699507389163, 0.19950738916256158, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.63423645320197042, 0.64408866995073888, 0.7068965517241379], 5: [0.13916256157635468, 0.20689655172413793, 0.15640394088669951, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14285714285714285, 0.19334975369458129, 0.20443349753694581, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.6145320197044335, 0.65517241379310343, 0.71305418719211822], 5: [0.13916256157635468, 0.19211822660098521, 0.14039408866995073, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14285714285714285, 0.1145320197044335, 0.15147783251231528, 0.075123152709359611], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.69334975369458129, 0.71551724137931039, 0.76724137931034486], 5: [0.13916256157635468, 0.19211822660098521, 0.13300492610837439, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225628 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.813949
Epoch 1, cost is  0.772058
Epoch 2, cost is  0.747924
Epoch 3, cost is  0.730471
Epoch 4, cost is  0.718374
Training took 0.152581 minutes
Weight histogram
[5684 3489 2951 2086 1977 1252  477  181   69   59] [-0.18205062 -0.16419742 -0.14634423 -0.12849103 -0.11063784 -0.09278464
 -0.07493145 -0.05707825 -0.03922506 -0.02137186 -0.00351867]
[ 100  142  255  451  771 1350 2047 2840 4144 6125] [-0.18205062 -0.16419742 -0.14634423 -0.12849103 -0.11063784 -0.09278464
 -0.07493145 -0.05707825 -0.03922506 -0.02137186 -0.00351867]
-5.44092
6.07225
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225702 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.797048
Epoch 1, cost is  0.754719
Epoch 2, cost is  0.730872
Epoch 3, cost is  0.713687
Epoch 4, cost is  0.700325
Training took 0.151991 minutes
Weight histogram
[5957 3922 2262 2458 1654 2714  740  285  134  124] [-0.17458509 -0.15747845 -0.14037181 -0.12326516 -0.10615852 -0.08905188
 -0.07194524 -0.0548386  -0.03773195 -0.02062531 -0.00351867]
[ 204  267  479  866 1518 1641 2040 2836 4550 5849] [-0.17458509 -0.15747845 -0.14037181 -0.12326516 -0.10615852 -0.08905188
 -0.07194524 -0.0548386  -0.03773195 -0.02062531 -0.00351867]
-5.19842
6.2593
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.04488
Epoch 1, cost is  1.41736
Epoch 2, cost is  1.01296
Epoch 3, cost is  0.821369
Epoch 4, cost is  0.710642
Training took 0.157032 minutes
Weight histogram
[4858 3893 2657 1958 1407 1085  773  507  719  368] [-0.10850771 -0.09803665 -0.0875656  -0.07709455 -0.06662349 -0.05615244
 -0.04568139 -0.03521034 -0.02473928 -0.01426823 -0.00379718]
[1296  728  853 1072 1320 1639 2016 2523 3140 3638] [-0.10850771 -0.09803665 -0.0875656  -0.07709455 -0.06662349 -0.05615244
 -0.04568139 -0.03521034 -0.02473928 -0.01426823 -0.00379718]
-2.01397
3.03885
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088470 minutes
Epoch 0
Fine tuning took 0.089686 minutes
Epoch 0
Fine tuning took 0.088799 minutes
{'zero': {0: [0.22044334975369459, 0.31157635467980294, 0.37561576354679804, 0.41009852216748771], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.38793103448275862, 0.34482758620689657, 0.31650246305418717], 5: [0.1206896551724138, 0.30049261083743845, 0.27955665024630544, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22044334975369459, 0.31034482758620691, 0.3854679802955665, 0.41871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.39162561576354682, 0.3460591133004926, 0.31034482758620691], 5: [0.1206896551724138, 0.29802955665024633, 0.26847290640394089, 0.27093596059113301], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22044334975369459, 0.29433497536945813, 0.36699507389162561, 0.38300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.40147783251231528, 0.36576354679802958, 0.34482758620689657], 5: [0.1206896551724138, 0.30418719211822659, 0.26724137931034481, 0.27216748768472904], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22044334975369459, 0.30049261083743845, 0.34852216748768472, 0.43596059113300495], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.34482758620689657, 0.33743842364532017, 0.29064039408866993], 5: [0.1206896551724138, 0.35467980295566504, 0.31403940886699505, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226936 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.890199
Epoch 1, cost is  0.872102
Epoch 2, cost is  0.860233
Epoch 3, cost is  0.849902
Epoch 4, cost is  0.841541
Training took 0.150913 minutes
Weight histogram
[5570 3999 2780 2445 1355  765  482  342  256  231] [-0.08612578 -0.07753813 -0.06895047 -0.06036281 -0.05177515 -0.0431875
 -0.03459984 -0.02601218 -0.01742452 -0.00883687 -0.00024921]
[ 506  428  598  883 1145 1614 2137 2759 3610 4545] [-0.08612578 -0.07753813 -0.06895047 -0.06036281 -0.05177515 -0.0431875
 -0.03459984 -0.02601218 -0.01742452 -0.00883687 -0.00024921]
-2.34335
2.91943
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223679 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.883576
Epoch 1, cost is  0.863796
Epoch 2, cost is  0.851139
Epoch 3, cost is  0.842768
Epoch 4, cost is  0.833977
Training took 0.151952 minutes
Weight histogram
[5637 4014 2673 2197 1515 1321 1059  746  565  523] [-0.08885174 -0.07999149 -0.07113124 -0.06227098 -0.05341073 -0.04455048
 -0.03569022 -0.02682997 -0.01796971 -0.00910946 -0.00024921]
[1058  861 1184 1245 1064 1552 2106 2819 3694 4667] [-0.08885174 -0.07999149 -0.07113124 -0.06227098 -0.05341073 -0.04455048
 -0.03569022 -0.02682997 -0.01796971 -0.00910946 -0.00024921]
-2.19483
2.77786
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.49302
Epoch 1, cost is  5.75901
Epoch 2, cost is  4.82479
Epoch 3, cost is  4.12149
Epoch 4, cost is  3.74115
Training took 0.090437 minutes
Weight histogram
[1305 1735 1612 1351 1391 1261 1498 2112 2910 3050] [-0.06757683 -0.06085158 -0.05412634 -0.0474011  -0.04067586 -0.03395062
 -0.02722538 -0.02050014 -0.01377489 -0.00704965 -0.00032441]
[4332 2223 1457 1256 1277 1381 1496 1635 1809 1359] [-0.06757683 -0.06085158 -0.05412634 -0.0474011  -0.04067586 -0.03395062
 -0.02722538 -0.02050014 -0.01377489 -0.00704965 -0.00032441]
-0.860581
1.34971
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.080683 minutes
Epoch 0
Fine tuning took 0.080906 minutes
Epoch 0
Fine tuning took 0.081252 minutes
{'zero': {0: [0.30172413793103448, 0.024630541871921183, 0.045566502463054187, 0.029556650246305417], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.29310344827586204, 0.92487684729064035, 0.88300492610837433, 0.87068965517241381], 5: [0.40517241379310343, 0.050492610837438424, 0.071428571428571425, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30172413793103448, 0.009852216748768473, 0.01600985221674877, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.29310344827586204, 0.95197044334975367, 0.94950738916256161, 0.94088669950738912], 5: [0.40517241379310343, 0.038177339901477834, 0.034482758620689655, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30172413793103448, 0.012315270935960592, 0.038177339901477834, 0.024630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.29310344827586204, 0.94581280788177335, 0.91871921182266014, 0.89532019704433496], 5: [0.40517241379310343, 0.041871921182266007, 0.043103448275862072, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30172413793103448, 0.0086206896551724137, 0.01600985221674877, 0.0073891625615763543], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.29310344827586204, 0.95073891625615758, 0.95812807881773399, 0.93842364532019706], 5: [0.40517241379310343, 0.04064039408866995, 0.025862068965517241, 0.054187192118226604], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226071 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.890199
Epoch 1, cost is  0.872102
Epoch 2, cost is  0.860233
Epoch 3, cost is  0.849902
Epoch 4, cost is  0.841541
Training took 0.152512 minutes
Weight histogram
[5570 3999 2780 2445 1355  765  482  342  256  231] [-0.08612578 -0.07753813 -0.06895047 -0.06036281 -0.05177515 -0.0431875
 -0.03459984 -0.02601218 -0.01742452 -0.00883687 -0.00024921]
[ 506  428  598  883 1145 1614 2137 2759 3610 4545] [-0.08612578 -0.07753813 -0.06895047 -0.06036281 -0.05177515 -0.0431875
 -0.03459984 -0.02601218 -0.01742452 -0.00883687 -0.00024921]
-2.34335
2.91943
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226745 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.883576
Epoch 1, cost is  0.863796
Epoch 2, cost is  0.851139
Epoch 3, cost is  0.842768
Epoch 4, cost is  0.833977
Training took 0.151848 minutes
Weight histogram
[5637 4014 2673 2197 1515 1321 1059  746  565  523] [-0.08885174 -0.07999149 -0.07113124 -0.06227098 -0.05341073 -0.04455048
 -0.03569022 -0.02682997 -0.01796971 -0.00910946 -0.00024921]
[1058  861 1184 1245 1064 1552 2106 2819 3694 4667] [-0.08885174 -0.07999149 -0.07113124 -0.06227098 -0.05341073 -0.04455048
 -0.03569022 -0.02682997 -0.01796971 -0.00910946 -0.00024921]
-2.19483
2.77786
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.4226
Epoch 1, cost is  5.69178
Epoch 2, cost is  4.66552
Epoch 3, cost is  3.85122
Epoch 4, cost is  3.39769
Training took 0.108576 minutes
Weight histogram
[1057 1969 1720 1471 1468 1363 1860 2559 4527  231] [-0.04952583 -0.04460697 -0.0396881  -0.03476923 -0.02985037 -0.0249315
 -0.02001263 -0.01509376 -0.0101749  -0.00525603 -0.00033716]
[4352 2499 1314 1200 1260 1324 1449 1588 1810 1429] [-0.04952583 -0.04460697 -0.0396881  -0.03476923 -0.02985037 -0.0249315
 -0.02001263 -0.01509376 -0.0101749  -0.00525603 -0.00033716]
-0.634967
1.28427
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083017 minutes
Epoch 0
Fine tuning took 0.082560 minutes
Epoch 0
Fine tuning took 0.084619 minutes
{'zero': {0: [0.28325123152709358, 0.060344827586206899, 0.10714285714285714, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37192118226600984, 0.86699507389162567, 0.75985221674876846, 0.66995073891625612], 5: [0.34482758620689657, 0.072660098522167482, 0.13300492610837439, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28325123152709358, 0.034482758620689655, 0.051724137931034482, 0.038177339901477834], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37192118226600984, 0.93349753694581283, 0.8854679802955665, 0.86576354679802958], 5: [0.34482758620689657, 0.032019704433497539, 0.062807881773399021, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28325123152709358, 0.024630541871921183, 0.048029556650246302, 0.065270935960591137], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37192118226600984, 0.92980295566502458, 0.87438423645320196, 0.81527093596059108], 5: [0.34482758620689657, 0.045566502463054187, 0.077586206896551727, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28325123152709358, 0.030788177339901478, 0.045566502463054187, 0.045566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37192118226600984, 0.93965517241379315, 0.89901477832512311, 0.86330049261083741], 5: [0.34482758620689657, 0.029556650246305417, 0.055418719211822662, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225664 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.890199
Epoch 1, cost is  0.872102
Epoch 2, cost is  0.860233
Epoch 3, cost is  0.849902
Epoch 4, cost is  0.841541
Training took 0.154730 minutes
Weight histogram
[5570 3999 2780 2445 1355  765  482  342  256  231] [-0.08612578 -0.07753813 -0.06895047 -0.06036281 -0.05177515 -0.0431875
 -0.03459984 -0.02601218 -0.01742452 -0.00883687 -0.00024921]
[ 506  428  598  883 1145 1614 2137 2759 3610 4545] [-0.08612578 -0.07753813 -0.06895047 -0.06036281 -0.05177515 -0.0431875
 -0.03459984 -0.02601218 -0.01742452 -0.00883687 -0.00024921]
-2.34335
2.91943
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225484 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.883576
Epoch 1, cost is  0.863796
Epoch 2, cost is  0.851139
Epoch 3, cost is  0.842768
Epoch 4, cost is  0.833977
Training took 0.150413 minutes
Weight histogram
[5637 4014 2673 2197 1515 1321 1059  746  565  523] [-0.08885174 -0.07999149 -0.07113124 -0.06227098 -0.05341073 -0.04455048
 -0.03569022 -0.02682997 -0.01796971 -0.00910946 -0.00024921]
[1058  861 1184 1245 1064 1552 2106 2819 3694 4667] [-0.08885174 -0.07999149 -0.07113124 -0.06227098 -0.05341073 -0.04455048
 -0.03569022 -0.02682997 -0.01796971 -0.00910946 -0.00024921]
-2.19483
2.77786
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.23869
Epoch 1, cost is  5.5419
Epoch 2, cost is  4.50954
Epoch 3, cost is  3.60908
Epoch 4, cost is  3.05352
Training took 0.160312 minutes
Weight histogram
[1002 2400 1811 1639 1612 2025 3850 3333  469   84] [-0.03077359 -0.02772761 -0.02468163 -0.02163565 -0.01858968 -0.0155437
 -0.01249772 -0.00945174 -0.00640576 -0.00335979 -0.00031381]
[5059 2260 1164 1155 1175 1287 1410 1530 1651 1534] [-0.03077359 -0.02772761 -0.02468163 -0.02163565 -0.01858968 -0.0155437
 -0.01249772 -0.00945174 -0.00640576 -0.00335979 -0.00031381]
-0.558863
1.04317
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.087701 minutes
Epoch 0
Fine tuning took 0.088802 minutes
Epoch 0
Fine tuning took 0.089637 minutes
{'zero': {0: [0.25123152709359609, 0.1748768472906404, 0.24261083743842365, 0.27709359605911332], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45197044334975367, 0.63300492610837433, 0.56527093596059108, 0.47906403940886699], 5: [0.29679802955665024, 0.19211822660098521, 0.19211822660098521, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25123152709359609, 0.050492610837438424, 0.12192118226600986, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45197044334975367, 0.83128078817733986, 0.74507389162561577, 0.68472906403940892], 5: [0.29679802955665024, 0.11822660098522167, 0.13300492610837439, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25123152709359609, 0.10467980295566502, 0.20443349753694581, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45197044334975367, 0.73768472906403937, 0.63916256157635465, 0.57635467980295563], 5: [0.29679802955665024, 0.15763546798029557, 0.15640394088669951, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25123152709359609, 0.071428571428571425, 0.14162561576354679, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45197044334975367, 0.80418719211822665, 0.72783251231527091, 0.63177339901477836], 5: [0.29679802955665024, 0.12438423645320197, 0.13054187192118227, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226664 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.91106
Epoch 1, cost is  1.88433
Epoch 2, cost is  1.85962
Epoch 3, cost is  1.83444
Epoch 4, cost is  1.81175
Training took 0.152874 minutes
Weight histogram
[3562 2583 2126 1847 1652 1475 1211 1193 2375  201] [ -4.93155941e-02  -4.43762609e-02  -3.94369278e-02  -3.44975946e-02
  -2.95582614e-02  -2.46189282e-02  -1.96795950e-02  -1.47402618e-02
  -9.80092861e-03  -4.86159542e-03   7.77377718e-05]
[2593 1111 1256 1343 1457 1617 1857 2077 2341 2573] [ -4.93155941e-02  -4.43762609e-02  -3.94369278e-02  -3.44975946e-02
  -2.95582614e-02  -2.46189282e-02  -1.96795950e-02  -1.47402618e-02
  -9.80092861e-03  -4.86159542e-03   7.77377718e-05]
-0.743876
1.46827
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226569 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.94725
Epoch 1, cost is  1.91695
Epoch 2, cost is  1.88719
Epoch 3, cost is  1.86235
Epoch 4, cost is  1.83489
Training took 0.151436 minutes
Weight histogram
[3126 2465 2056 1847 1675 1486 1409 1374 4436  376] [ -4.80259843e-02  -4.32156121e-02  -3.84052399e-02  -3.35948677e-02
  -2.87844955e-02  -2.39741233e-02  -1.91637511e-02  -1.43533789e-02
  -9.54300665e-03  -4.73263444e-03   7.77377718e-05]
[4888 1218 1265 1357 1442 1601 1797 2001 2242 2439] [ -4.80259843e-02  -4.32156121e-02  -3.84052399e-02  -3.35948677e-02
  -2.87844955e-02  -2.39741233e-02  -1.91637511e-02  -1.43533789e-02
  -9.54300665e-03  -4.73263444e-03   7.77377718e-05]
-0.610749
1.01267
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8431
Epoch 1, cost is  6.74906
Epoch 2, cost is  6.66874
Epoch 3, cost is  6.59112
Epoch 4, cost is  6.51706
Training took 0.090118 minutes
Weight histogram
[1365 4014 9034 1636  805  494  334  234  171  138] [ -8.89928360e-03  -8.01249542e-03  -7.12570724e-03  -6.23891906e-03
  -5.35213088e-03  -4.46534270e-03  -3.57855452e-03  -2.69176634e-03
  -1.80497816e-03  -9.18189983e-04  -3.14018034e-05]
[7430 3157 2384 2040 1560  743  497  211   99  104] [ -8.89928360e-03  -8.01249542e-03  -7.12570724e-03  -6.23891906e-03
  -5.35213088e-03  -4.46534270e-03  -3.57855452e-03  -2.69176634e-03
  -1.80497816e-03  -9.18189983e-04  -3.14018034e-05]
-0.0589014
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081656 minutes
Epoch 0
Fine tuning took 0.080445 minutes
Epoch 0
Fine tuning took 0.082504 minutes
{'zero': {0: [0.13916256157635468, 0.0012315270935960591, 0.0049261083743842365, 0.013546798029556651], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79064039408866993, 0.99876847290640391, 0.96674876847290636, 0.68103448275862066], 5: [0.070197044334975367, 0.0, 0.02832512315270936, 0.30541871921182268], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13916256157635468, 0.0, 0.0061576354679802959, 0.013546798029556651], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79064039408866993, 0.99876847290640391, 0.97290640394088668, 0.6280788177339901], 5: [0.070197044334975367, 0.0012315270935960591, 0.020935960591133004, 0.35837438423645318], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13916256157635468, 0.0012315270935960591, 0.0012315270935960591, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79064039408866993, 0.99753694581280783, 0.97536945812807885, 0.6711822660098522], 5: [0.070197044334975367, 0.0012315270935960591, 0.023399014778325122, 0.30541871921182268], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13916256157635468, 0.0036945812807881772, 0.0049261083743842365, 0.012315270935960592], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79064039408866993, 0.9926108374384236, 0.97290640394088668, 0.66995073891625612], 5: [0.070197044334975367, 0.0036945812807881772, 0.022167487684729065, 0.31773399014778325], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226943 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.91106
Epoch 1, cost is  1.88433
Epoch 2, cost is  1.85962
Epoch 3, cost is  1.83444
Epoch 4, cost is  1.81175
Training took 0.155105 minutes
Weight histogram
[3562 2583 2126 1847 1652 1475 1211 1193 2375  201] [ -4.93155941e-02  -4.43762609e-02  -3.94369278e-02  -3.44975946e-02
  -2.95582614e-02  -2.46189282e-02  -1.96795950e-02  -1.47402618e-02
  -9.80092861e-03  -4.86159542e-03   7.77377718e-05]
[2593 1111 1256 1343 1457 1617 1857 2077 2341 2573] [ -4.93155941e-02  -4.43762609e-02  -3.94369278e-02  -3.44975946e-02
  -2.95582614e-02  -2.46189282e-02  -1.96795950e-02  -1.47402618e-02
  -9.80092861e-03  -4.86159542e-03   7.77377718e-05]
-0.743876
1.46827
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226042 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.94725
Epoch 1, cost is  1.91695
Epoch 2, cost is  1.88719
Epoch 3, cost is  1.86235
Epoch 4, cost is  1.83489
Training took 0.151352 minutes
Weight histogram
[3126 2465 2056 1847 1675 1486 1409 1374 4436  376] [ -4.80259843e-02  -4.32156121e-02  -3.84052399e-02  -3.35948677e-02
  -2.87844955e-02  -2.39741233e-02  -1.91637511e-02  -1.43533789e-02
  -9.54300665e-03  -4.73263444e-03   7.77377718e-05]
[4888 1218 1265 1357 1442 1601 1797 2001 2242 2439] [ -4.80259843e-02  -4.32156121e-02  -3.84052399e-02  -3.35948677e-02
  -2.87844955e-02  -2.39741233e-02  -1.91637511e-02  -1.43533789e-02
  -9.54300665e-03  -4.73263444e-03   7.77377718e-05]
-0.610749
1.01267
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79806
Epoch 1, cost is  6.68698
Epoch 2, cost is  6.6034
Epoch 3, cost is  6.52433
Epoch 4, cost is  6.44655
Training took 0.108963 minutes
Weight histogram
[1228 2990 9380 2193  934  554  366  255  182  143] [ -9.23346821e-03  -8.31558548e-03  -7.39770276e-03  -6.47982003e-03
  -5.56193730e-03  -4.64405457e-03  -3.72617184e-03  -2.80828911e-03
  -1.89040639e-03  -9.72523657e-04  -5.46409283e-05]
[7135 3122 2421 2098 1653  800  519  260  107  110] [ -9.23346821e-03  -8.31558548e-03  -7.39770276e-03  -6.47982003e-03
  -5.56193730e-03  -4.64405457e-03  -3.72617184e-03  -2.80828911e-03
  -1.89040639e-03  -9.72523657e-04  -5.46409283e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.083312 minutes
Epoch 0
Fine tuning took 0.082359 minutes
Epoch 0
Fine tuning took 0.082827 minutes
{'zero': {0: [0.16009852216748768, 0.0049261083743842365, 0.0012315270935960591, 0.009852216748768473], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76847290640394084, 0.9926108374384236, 0.98522167487684731, 0.68472906403940892], 5: [0.071428571428571425, 0.0024630541871921183, 0.013546798029556651, 0.30541871921182268], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16009852216748768, 0.0024630541871921183, 0.0, 0.009852216748768473], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76847290640394084, 0.99630541871921185, 0.98645320197044339, 0.63423645320197042], 5: [0.071428571428571425, 0.0012315270935960591, 0.013546798029556651, 0.35591133004926107], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16009852216748768, 0.0073891625615763543, 0.0049261083743842365, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76847290640394084, 0.98891625615763545, 0.98891625615763545, 0.66995073891625612], 5: [0.071428571428571425, 0.0036945812807881772, 0.0061576354679802959, 0.31896551724137934], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16009852216748768, 0.0036945812807881772, 0.0036945812807881772, 0.0086206896551724137], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76847290640394084, 0.99630541871921185, 0.98768472906403937, 0.65270935960591137], 5: [0.071428571428571425, 0.0, 0.0086206896551724137, 0.33866995073891626], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226845 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3329 7793 1692] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 800  775  895 1069 1303 1657 2054 2643 3154 3875] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.604992
0.921883
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.91106
Epoch 1, cost is  1.88433
Epoch 2, cost is  1.85962
Epoch 3, cost is  1.83444
Epoch 4, cost is  1.81175
Training took 0.153707 minutes
Weight histogram
[3562 2583 2126 1847 1652 1475 1211 1193 2375  201] [ -4.93155941e-02  -4.43762609e-02  -3.94369278e-02  -3.44975946e-02
  -2.95582614e-02  -2.46189282e-02  -1.96795950e-02  -1.47402618e-02
  -9.80092861e-03  -4.86159542e-03   7.77377718e-05]
[2593 1111 1256 1343 1457 1617 1857 2077 2341 2573] [ -4.93155941e-02  -4.43762609e-02  -3.94369278e-02  -3.44975946e-02
  -2.95582614e-02  -2.46189282e-02  -1.96795950e-02  -1.47402618e-02
  -9.80092861e-03  -4.86159542e-03   7.77377718e-05]
-0.743876
1.46827
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225127 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 4533 7718 1010] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1264 1589 1222 1056 1296 1646 2098 2797 3376 3906] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.789352
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.94725
Epoch 1, cost is  1.91695
Epoch 2, cost is  1.88719
Epoch 3, cost is  1.86235
Epoch 4, cost is  1.83489
Training took 0.151508 minutes
Weight histogram
[3126 2465 2056 1847 1675 1486 1409 1374 4436  376] [ -4.80259843e-02  -4.32156121e-02  -3.84052399e-02  -3.35948677e-02
  -2.87844955e-02  -2.39741233e-02  -1.91637511e-02  -1.43533789e-02
  -9.54300665e-03  -4.73263444e-03   7.77377718e-05]
[4888 1218 1265 1357 1442 1601 1797 2001 2242 2439] [ -4.80259843e-02  -4.32156121e-02  -3.84052399e-02  -3.35948677e-02
  -2.87844955e-02  -2.39741233e-02  -1.91637511e-02  -1.43533789e-02
  -9.54300665e-03  -4.73263444e-03   7.77377718e-05]
-0.610749
1.01267
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.66315
Epoch 1, cost is  6.50776
Epoch 2, cost is  6.41481
Epoch 3, cost is  6.32991
Epoch 4, cost is  6.25225
Training took 0.158123 minutes
Weight histogram
[ 929 1955 7230 4871 1378  732  456  306  208  160] [ -1.00774989e-02  -9.07384788e-03  -8.07019691e-03  -7.06654593e-03
  -6.06289496e-03  -5.05924399e-03  -4.05559302e-03  -3.05194204e-03
  -2.04829107e-03  -1.04464010e-03  -4.09891254e-05]
[6469 3010 2450 2201 1885  989  596  370  125  130] [ -1.00774989e-02  -9.07384788e-03  -8.07019691e-03  -7.06654593e-03
  -6.06289496e-03  -5.05924399e-03  -4.05559302e-03  -3.05194204e-03
  -2.04829107e-03  -1.04464010e-03  -4.09891254e-05]
-0.0601954
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088250 minutes
Epoch 0
Fine tuning took 0.088417 minutes
Epoch 0
Fine tuning took 0.088112 minutes
{'zero': {0: [0.1539408866995074, 0.0, 0.012315270935960592, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76970443349753692, 0.99876847290640391, 0.95320197044334976, 0.67487684729064035], 5: [0.076354679802955669, 0.0012315270935960591, 0.034482758620689655, 0.31034482758620691], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1539408866995074, 0.0036945812807881772, 0.0061576354679802959, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76970443349753692, 0.9926108374384236, 0.96059113300492616, 0.70073891625615758], 5: [0.076354679802955669, 0.0036945812807881772, 0.033251231527093597, 0.28448275862068967], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1539408866995074, 0.0036945812807881772, 0.0061576354679802959, 0.014778325123152709], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76970443349753692, 0.99630541871921185, 0.96059113300492616, 0.70073891625615758], 5: [0.076354679802955669, 0.0, 0.033251231527093597, 0.28448275862068967], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1539408866995074, 0.0024630541871921183, 0.0012315270935960591, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76970443349753692, 0.99137931034482762, 0.96305418719211822, 0.70197044334975367], 5: [0.076354679802955669, 0.0061576354679802959, 0.035714285714285712, 0.28694581280788178], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226379 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.759457
Epoch 1, cost is  0.718313
Epoch 2, cost is  0.696222
Epoch 3, cost is  0.680237
Epoch 4, cost is  0.666542
Training took 0.151663 minutes
Weight histogram
[6024 4065 3829 2260 1998 1173  576  187   78   60] [-0.18798372 -0.16953722 -0.15109071 -0.13264421 -0.1141977  -0.0957512
 -0.07730469 -0.05885819 -0.04041168 -0.02196518 -0.00351867]
[ 102  150  277  488  836 1443 2231 3471 4610 6642] [-0.18798372 -0.16953722 -0.15109071 -0.13264421 -0.1141977  -0.0957512
 -0.07730469 -0.05885819 -0.04041168 -0.02196518 -0.00351867]
-5.49818
6.34826
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224567 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.751479
Epoch 1, cost is  0.709375
Epoch 2, cost is  0.688102
Epoch 3, cost is  0.672053
Epoch 4, cost is  0.659081
Training took 0.152828 minutes
Weight histogram
[6039 4074 3945 2123 2019 2558  912  330  149  126] [-0.18091199 -0.16317266 -0.14543332 -0.12769399 -0.10995466 -0.09221533
 -0.074476   -0.05673667 -0.03899733 -0.021258   -0.00351867]
[ 210  280  517  941 1642 1624 2220 3466 4770 6605] [-0.18091199 -0.16317266 -0.14543332 -0.12769399 -0.10995466 -0.09221533
 -0.074476   -0.05673667 -0.03899733 -0.021258   -0.00351867]
-5.19842
6.26929
... retrieved True_rbm_500-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.41691
Epoch 1, cost is  1.93484
Epoch 2, cost is  1.64255
Epoch 3, cost is  1.50073
Epoch 4, cost is  1.41626
Training took 0.089909 minutes
Weight histogram
[4817 4533 3237 2573 1527 1018  691  507  681  666] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
[1334  906  983 1207 1435 1812 2259 2785 3490 4039] [-0.21557178 -0.19441478 -0.17325779 -0.15210079 -0.1309438  -0.10978681
 -0.08862981 -0.06747282 -0.04631583 -0.02515883 -0.00400184]
-3.68512
4.0784
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.081301 minutes
Epoch 0
Fine tuning took 0.082567 minutes
Epoch 0
Fine tuning took 0.081163 minutes
{'zero': {0: [0.15763546798029557, 0.24507389162561577, 0.34113300492610837, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.49876847290640391, 0.39532019704433496, 0.53694581280788178], 5: [0.18103448275862069, 0.25615763546798032, 0.26354679802955666, 0.25], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.10467980295566502, 0.086206896551724144, 0.060344827586206899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.78940886699507384, 0.81527093596059108, 0.83251231527093594], 5: [0.18103448275862069, 0.10591133004926108, 0.098522167487684734, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.099753694581280791, 0.10344827586206896, 0.064039408866995079], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.79556650246305416, 0.77339901477832518, 0.81773399014778325], 5: [0.18103448275862069, 0.10467980295566502, 0.12315270935960591, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.10960591133004927, 0.070197044334975367, 0.082512315270935957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.75985221674876846, 0.83866995073891626, 0.77709359605911332], 5: [0.18103448275862069, 0.13054187192118227, 0.091133004926108374, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227908 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.759457
Epoch 1, cost is  0.718313
Epoch 2, cost is  0.696222
Epoch 3, cost is  0.680237
Epoch 4, cost is  0.666542
Training took 0.153125 minutes
Weight histogram
[6024 4065 3829 2260 1998 1173  576  187   78   60] [-0.18798372 -0.16953722 -0.15109071 -0.13264421 -0.1141977  -0.0957512
 -0.07730469 -0.05885819 -0.04041168 -0.02196518 -0.00351867]
[ 102  150  277  488  836 1443 2231 3471 4610 6642] [-0.18798372 -0.16953722 -0.15109071 -0.13264421 -0.1141977  -0.0957512
 -0.07730469 -0.05885819 -0.04041168 -0.02196518 -0.00351867]
-5.49818
6.34826
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224998 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.751479
Epoch 1, cost is  0.709375
Epoch 2, cost is  0.688102
Epoch 3, cost is  0.672053
Epoch 4, cost is  0.659081
Training took 0.153979 minutes
Weight histogram
[6039 4074 3945 2123 2019 2558  912  330  149  126] [-0.18091199 -0.16317266 -0.14543332 -0.12769399 -0.10995466 -0.09221533
 -0.074476   -0.05673667 -0.03899733 -0.021258   -0.00351867]
[ 210  280  517  941 1642 1624 2220 3466 4770 6605] [-0.18091199 -0.16317266 -0.14543332 -0.12769399 -0.10995466 -0.09221533
 -0.074476   -0.05673667 -0.03899733 -0.021258   -0.00351867]
-5.19842
6.26929
... retrieved True_rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.2359
Epoch 1, cost is  1.65401
Epoch 2, cost is  1.30251
Epoch 3, cost is  1.1277
Epoch 4, cost is  1.01843
Training took 0.107982 minutes
Weight histogram
[5181 4439 3108 2288 1643 1135  682  460  780  534] [-0.15717396 -0.14185212 -0.12653028 -0.11120844 -0.09588659 -0.08056475
 -0.06524291 -0.04992107 -0.03459923 -0.01927739 -0.00395554]
[1369  892  971 1201 1489 1883 2286 2770 3417 3972] [-0.15717396 -0.14185212 -0.12653028 -0.11120844 -0.09588659 -0.08056475
 -0.06524291 -0.04992107 -0.03459923 -0.01927739 -0.00395554]
-3.18745
3.9888
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084890 minutes
Epoch 0
Fine tuning took 0.082905 minutes
Epoch 0
Fine tuning took 0.083620 minutes
{'zero': {0: [0.13423645320197045, 0.34359605911330049, 0.39162561576354682, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.41256157635467983, 0.37438423645320196, 0.41379310344827586], 5: [0.11206896551724138, 0.24384236453201971, 0.23399014778325122, 0.31157635467980294], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13423645320197045, 0.21798029556650247, 0.20689655172413793, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.6071428571428571, 0.64408866995073888, 0.74014778325123154], 5: [0.11206896551724138, 0.1748768472906404, 0.14901477832512317, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13423645320197045, 0.20443349753694581, 0.17980295566502463, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.62438423645320196, 0.6785714285714286, 0.67364532019704437], 5: [0.11206896551724138, 0.17118226600985223, 0.14162561576354679, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13423645320197045, 0.21305418719211822, 0.1354679802955665, 0.076354679802955669], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.62931034482758619, 0.68719211822660098, 0.80418719211822665], 5: [0.11206896551724138, 0.15763546798029557, 0.17733990147783252, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225531 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.759457
Epoch 1, cost is  0.718313
Epoch 2, cost is  0.696222
Epoch 3, cost is  0.680237
Epoch 4, cost is  0.666542
Training took 0.153274 minutes
Weight histogram
[6024 4065 3829 2260 1998 1173  576  187   78   60] [-0.18798372 -0.16953722 -0.15109071 -0.13264421 -0.1141977  -0.0957512
 -0.07730469 -0.05885819 -0.04041168 -0.02196518 -0.00351867]
[ 102  150  277  488  836 1443 2231 3471 4610 6642] [-0.18798372 -0.16953722 -0.15109071 -0.13264421 -0.1141977  -0.0957512
 -0.07730469 -0.05885819 -0.04041168 -0.02196518 -0.00351867]
-5.49818
6.34826
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224845 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.751479
Epoch 1, cost is  0.709375
Epoch 2, cost is  0.688102
Epoch 3, cost is  0.672053
Epoch 4, cost is  0.659081
Training took 0.151826 minutes
Weight histogram
[6039 4074 3945 2123 2019 2558  912  330  149  126] [-0.18091199 -0.16317266 -0.14543332 -0.12769399 -0.10995466 -0.09221533
 -0.074476   -0.05673667 -0.03899733 -0.021258   -0.00351867]
[ 210  280  517  941 1642 1624 2220 3466 4770 6605] [-0.18091199 -0.16317266 -0.14543332 -0.12769399 -0.10995466 -0.09221533
 -0.074476   -0.05673667 -0.03899733 -0.021258   -0.00351867]
-5.19842
6.26929
... retrieved True_rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  3.04168
Epoch 1, cost is  1.4178
Epoch 2, cost is  1.01111
Epoch 3, cost is  0.818496
Epoch 4, cost is  0.705199
Training took 0.157603 minutes
Weight histogram
[5439 4299 2956 2152 1571 1205  858  563  798  409] [-0.10850771 -0.09803665 -0.0875656  -0.07709455 -0.06662349 -0.05615244
 -0.04568139 -0.03521034 -0.02473928 -0.01426823 -0.00379718]
[1443  813  952 1194 1472 1820 2238 2798 3479 4041] [-0.10850771 -0.09803665 -0.0875656  -0.07709455 -0.06662349 -0.05615244
 -0.04568139 -0.03521034 -0.02473928 -0.01426823 -0.00379718]
-2.01397
3.03885
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.089457 minutes
Epoch 0
Fine tuning took 0.089473 minutes
Epoch 0
Fine tuning took 0.088522 minutes
{'zero': {0: [0.20073891625615764, 0.33990147783251229, 0.39039408866995073, 0.3288177339901478], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.45443349753694579, 0.30788177339901479, 0.33866995073891626], 5: [0.12438423645320197, 0.20566502463054187, 0.30172413793103448, 0.33251231527093594], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20073891625615764, 0.34975369458128081, 0.35591133004926107, 0.33866995073891626], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.40024630541871919, 0.36945812807881773, 0.33497536945812806], 5: [0.12438423645320197, 0.25, 0.27463054187192121, 0.32635467980295568], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20073891625615764, 0.33251231527093594, 0.33497536945812806, 0.2857142857142857], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.43965517241379309, 0.35714285714285715, 0.34975369458128081], 5: [0.12438423645320197, 0.22783251231527094, 0.30788177339901479, 0.3645320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20073891625615764, 0.37684729064039407, 0.33251231527093594, 0.32389162561576357], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.40886699507389163, 0.33990147783251229, 0.30049261083743845], 5: [0.12438423645320197, 0.21428571428571427, 0.32758620689655171, 0.37561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226416 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.827211
Epoch 1, cost is  0.811114
Epoch 2, cost is  0.797766
Epoch 3, cost is  0.789205
Epoch 4, cost is  0.780435
Training took 0.152303 minutes
Weight histogram
[5866 4214 3876 2234 1892  760  532  366  265  245] [-0.08948763 -0.08056379 -0.07163995 -0.06271611 -0.05379226 -0.04486842
 -0.03594458 -0.02702074 -0.01809689 -0.00917305 -0.00024921]
[ 519  452  639  958 1252 1759 2377 3165 3989 5140] [-0.08948763 -0.08056379 -0.07163995 -0.06271611 -0.05379226 -0.04486842
 -0.03594458 -0.02702074 -0.01809689 -0.00917305 -0.00024921]
-2.46555
3.0024
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225146 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.828662
Epoch 1, cost is  0.811322
Epoch 2, cost is  0.800965
Epoch 3, cost is  0.791817
Epoch 4, cost is  0.783491
Training took 0.150652 minutes
Weight histogram
[5933 4164 3913 2153 1737 1253 1202  793  578  549] [-0.09198776 -0.0828139  -0.07364005 -0.06446619 -0.05529234 -0.04611848
 -0.03694463 -0.02777077 -0.01859692 -0.00942306 -0.00024921]
[1085  906 1265 1226 1167 1706 2344 3238 4097 5241] [-0.09198776 -0.0828139  -0.07364005 -0.06446619 -0.05529234 -0.04611848
 -0.03694463 -0.02777077 -0.01859692 -0.00942306 -0.00024921]
-2.32734
3.0172
... retrieved True_rbm_500-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.49139
Epoch 1, cost is  5.75728
Epoch 2, cost is  4.8269
Epoch 3, cost is  4.12129
Epoch 4, cost is  3.73364
Training took 0.091628 minutes
Weight histogram
[1325 1974 1804 1523 1556 1397 1654 2413 3173 3431] [-0.06757683 -0.06085158 -0.05412634 -0.0474011  -0.04067586 -0.03395062
 -0.02722538 -0.02050014 -0.01377489 -0.00704965 -0.00032441]
[4770 2513 1627 1390 1417 1531 1673 1819 2019 1491] [-0.06757683 -0.06085158 -0.05412634 -0.0474011  -0.04067586 -0.03395062
 -0.02722538 -0.02050014 -0.01377489 -0.00704965 -0.00032441]
-0.860581
1.34971
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082578 minutes
Epoch 0
Fine tuning took 0.080887 minutes
Epoch 0
Fine tuning took 0.081786 minutes
{'zero': {0: [0.3817733990147783, 0.036945812807881777, 0.041871921182266007, 0.05295566502463054], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4211822660098522, 0.9076354679802956, 0.88916256157635465, 0.83374384236453203], 5: [0.19704433497536947, 0.055418719211822662, 0.068965517241379309, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3817733990147783, 0.022167487684729065, 0.033251231527093597, 0.013546798029556651], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4211822660098522, 0.93349753694581283, 0.93596059113300489, 0.92980295566502458], 5: [0.19704433497536947, 0.044334975369458129, 0.030788177339901478, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3817733990147783, 0.023399014778325122, 0.034482758620689655, 0.029556650246305417], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4211822660098522, 0.93965517241379315, 0.92487684729064035, 0.87438423645320196], 5: [0.19704433497536947, 0.036945812807881777, 0.04064039408866995, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3817733990147783, 0.018472906403940888, 0.033251231527093597, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4211822660098522, 0.95812807881773399, 0.94334975369458129, 0.95197044334975367], 5: [0.19704433497536947, 0.023399014778325122, 0.023399014778325122, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225872 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.827211
Epoch 1, cost is  0.811114
Epoch 2, cost is  0.797766
Epoch 3, cost is  0.789205
Epoch 4, cost is  0.780435
Training took 0.154081 minutes
Weight histogram
[5866 4214 3876 2234 1892  760  532  366  265  245] [-0.08948763 -0.08056379 -0.07163995 -0.06271611 -0.05379226 -0.04486842
 -0.03594458 -0.02702074 -0.01809689 -0.00917305 -0.00024921]
[ 519  452  639  958 1252 1759 2377 3165 3989 5140] [-0.08948763 -0.08056379 -0.07163995 -0.06271611 -0.05379226 -0.04486842
 -0.03594458 -0.02702074 -0.01809689 -0.00917305 -0.00024921]
-2.46555
3.0024
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224762 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.828662
Epoch 1, cost is  0.811322
Epoch 2, cost is  0.800965
Epoch 3, cost is  0.791817
Epoch 4, cost is  0.783491
Training took 0.151515 minutes
Weight histogram
[5933 4164 3913 2153 1737 1253 1202  793  578  549] [-0.09198776 -0.0828139  -0.07364005 -0.06446619 -0.05529234 -0.04611848
 -0.03694463 -0.02777077 -0.01859692 -0.00942306 -0.00024921]
[1085  906 1265 1226 1167 1706 2344 3238 4097 5241] [-0.09198776 -0.0828139  -0.07364005 -0.06446619 -0.05529234 -0.04611848
 -0.03694463 -0.02777077 -0.01859692 -0.00942306 -0.00024921]
-2.32734
3.0172
... retrieved True_rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.42077
Epoch 1, cost is  5.68971
Epoch 2, cost is  4.67359
Epoch 3, cost is  3.85584
Epoch 4, cost is  3.40099
Training took 0.110603 minutes
Weight histogram
[1110 2210 1921 1639 1618 1524 2078 2881 5011  258] [-0.04952583 -0.04460697 -0.0396881  -0.03476923 -0.02985037 -0.0249315
 -0.02001263 -0.01509376 -0.0101749  -0.00525603 -0.00033716]
[4778 2817 1468 1334 1395 1468 1599 1764 2011 1616] [-0.04952583 -0.04460697 -0.0396881  -0.03476923 -0.02985037 -0.0249315
 -0.02001263 -0.01509376 -0.0101749  -0.00525603 -0.00033716]
-0.634967
1.28427
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084125 minutes
Epoch 0
Fine tuning took 0.082688 minutes
Epoch 0
Fine tuning took 0.084458 minutes
{'zero': {0: [0.37931034482758619, 0.057881773399014777, 0.13300492610837439, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41256157635467983, 0.81650246305418717, 0.74753694581280783, 0.73029556650246308], 5: [0.20812807881773399, 0.12561576354679804, 0.11945812807881774, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37931034482758619, 0.027093596059113302, 0.046798029556650245, 0.030788177339901478], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41256157635467983, 0.89408866995073888, 0.89778325123152714, 0.88916256157635465], 5: [0.20812807881773399, 0.078817733990147784, 0.055418719211822662, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37931034482758619, 0.038177339901477834, 0.075123152709359611, 0.060344827586206899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41256157635467983, 0.87315270935960587, 0.8214285714285714, 0.83004926108374388], 5: [0.20812807881773399, 0.088669950738916259, 0.10344827586206896, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37931034482758619, 0.01600985221674877, 0.041871921182266007, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41256157635467983, 0.91871921182266014, 0.89655172413793105, 0.86206896551724133], 5: [0.20812807881773399, 0.065270935960591137, 0.061576354679802957, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.228174 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.827211
Epoch 1, cost is  0.811114
Epoch 2, cost is  0.797766
Epoch 3, cost is  0.789205
Epoch 4, cost is  0.780435
Training took 0.151414 minutes
Weight histogram
[5866 4214 3876 2234 1892  760  532  366  265  245] [-0.08948763 -0.08056379 -0.07163995 -0.06271611 -0.05379226 -0.04486842
 -0.03594458 -0.02702074 -0.01809689 -0.00917305 -0.00024921]
[ 519  452  639  958 1252 1759 2377 3165 3989 5140] [-0.08948763 -0.08056379 -0.07163995 -0.06271611 -0.05379226 -0.04486842
 -0.03594458 -0.02702074 -0.01809689 -0.00917305 -0.00024921]
-2.46555
3.0024
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224638 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.828662
Epoch 1, cost is  0.811322
Epoch 2, cost is  0.800965
Epoch 3, cost is  0.791817
Epoch 4, cost is  0.783491
Training took 0.151301 minutes
Weight histogram
[5933 4164 3913 2153 1737 1253 1202  793  578  549] [-0.09198776 -0.0828139  -0.07364005 -0.06446619 -0.05529234 -0.04611848
 -0.03694463 -0.02777077 -0.01859692 -0.00942306 -0.00024921]
[1085  906 1265 1226 1167 1706 2344 3238 4097 5241] [-0.09198776 -0.0828139  -0.07364005 -0.06446619 -0.05529234 -0.04611848
 -0.03694463 -0.02777077 -0.01859692 -0.00942306 -0.00024921]
-2.32734
3.0172
... retrieved True_rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.23546
Epoch 1, cost is  5.54168
Epoch 2, cost is  4.52006
Epoch 3, cost is  3.619
Epoch 4, cost is  3.06045
Training took 0.160320 minutes
Weight histogram
[1010 2691 2053 1822 1801 2248 4309 3689  533   94] [-0.03077359 -0.0277276  -0.02468162 -0.02163563 -0.01858965 -0.01554366
 -0.01249768 -0.00945169 -0.00640571 -0.00335972 -0.00031373]
[5562 2555 1301 1281 1304 1428 1567 1697 1830 1725] [-0.03077359 -0.0277276  -0.02468162 -0.02163563 -0.01858965 -0.01554366
 -0.01249768 -0.00945169 -0.00640571 -0.00335972 -0.00031373]
-0.558863
1.04317
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088900 minutes
Epoch 0
Fine tuning took 0.088517 minutes
Epoch 0
Fine tuning took 0.088940 minutes
{'zero': {0: [0.27709359605911332, 0.15763546798029557, 0.21674876847290642, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51477832512315269, 0.67610837438423643, 0.58374384236453203, 0.50985221674876846], 5: [0.20812807881773399, 0.16625615763546797, 0.19950738916256158, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27709359605911332, 0.071428571428571425, 0.11083743842364532, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51477832512315269, 0.8288177339901478, 0.74876847290640391, 0.65270935960591137], 5: [0.20812807881773399, 0.099753694581280791, 0.14039408866995073, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27709359605911332, 0.10714285714285714, 0.18226600985221675, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51477832512315269, 0.76231527093596063, 0.66256157635467983, 0.55049261083743839], 5: [0.20812807881773399, 0.13054187192118227, 0.15517241379310345, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27709359605911332, 0.076354679802955669, 0.11206896551724138, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51477832512315269, 0.84852216748768472, 0.80049261083743839, 0.65640394088669951], 5: [0.20812807881773399, 0.075123152709359611, 0.087438423645320201, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225624 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.76384
Epoch 1, cost is  1.74271
Epoch 2, cost is  1.72061
Epoch 3, cost is  1.70057
Epoch 4, cost is  1.68157
Training took 0.152781 minutes
Weight histogram
[4198 3115 2399 1983 1758 1534 1298 1279 2460  226] [ -5.16128615e-02  -4.64438015e-02  -4.12747416e-02  -3.61056817e-02
  -3.09366218e-02  -2.57675618e-02  -2.05985019e-02  -1.54294420e-02
  -1.02603821e-02  -5.09132215e-03   7.77377718e-05]
[2673 1206 1367 1473 1619 1791 2125 2390 2679 2927] [ -5.16128615e-02  -4.64438015e-02  -4.12747416e-02  -3.61056817e-02
  -3.09366218e-02  -2.57675618e-02  -2.05985019e-02  -1.54294420e-02
  -1.02603821e-02  -5.09132215e-03   7.77377718e-05]
-0.790643
1.53425
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226005 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.7877
Epoch 1, cost is  1.76175
Epoch 2, cost is  1.73904
Epoch 3, cost is  1.71895
Epoch 4, cost is  1.69539
Training took 0.151414 minutes
Weight histogram
[3758 2849 2356 1997 1790 1585 1563 1396 4549  432] [ -5.05660772e-02  -4.55016957e-02  -4.04373142e-02  -3.53729327e-02
  -3.03085512e-02  -2.52441697e-02  -2.01797882e-02  -1.51154067e-02
  -1.00510252e-02  -4.98664373e-03   7.77377718e-05]
[4986 1319 1368 1482 1617 1800 2047 2298 2554 2804] [ -5.05660772e-02  -4.55016957e-02  -4.04373142e-02  -3.53729327e-02
  -3.03085512e-02  -2.52441697e-02  -2.01797882e-02  -1.51154067e-02
  -1.00510252e-02  -4.98664373e-03   7.77377718e-05]
-0.631252
1.04093
... retrieved True_rbm_500-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.84612
Epoch 1, cost is  6.75523
Epoch 2, cost is  6.67708
Epoch 3, cost is  6.60237
Epoch 4, cost is  6.53054
Training took 0.089798 minutes
Weight histogram
[ 1365  4014 10413  1977   927   563   379   264   193   155] [ -8.89928360e-03  -8.01249084e-03  -7.12569808e-03  -6.23890532e-03
  -5.35211256e-03  -4.46531980e-03  -3.57852704e-03  -2.69173428e-03
  -1.80494152e-03  -9.18148764e-04  -3.13560049e-05]
[8326 3549 2680 2305 1736  743  497  211   99  104] [ -8.89928360e-03  -8.01249084e-03  -7.12569808e-03  -6.23890532e-03
  -5.35211256e-03  -4.46531980e-03  -3.57852704e-03  -2.69173428e-03
  -1.80494152e-03  -9.18148764e-04  -3.13560049e-05]
-0.0589014
0.0931744
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.082298 minutes
Epoch 0
Fine tuning took 0.082213 minutes
Epoch 0
Fine tuning took 0.081354 minutes
{'zero': {0: [0.18472906403940886, 0.0024630541871921183, 0.0, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.99384236453201968, 0.98152709359605916, 0.81280788177339902], 5: [0.060344827586206899, 0.0036945812807881772, 0.018472906403940888, 0.066502463054187194], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18472906403940886, 0.0036945812807881772, 0.0, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.98152709359605916, 0.98522167487684731, 0.80665024630541871], 5: [0.060344827586206899, 0.014778325123152709, 0.014778325123152709, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18472906403940886, 0.0036945812807881772, 0.0, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.99137931034482762, 0.98891625615763545, 0.80049261083743839], 5: [0.060344827586206899, 0.0049261083743842365, 0.011083743842364532, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18472906403940886, 0.0036945812807881772, 0.0012315270935960591, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75492610837438423, 0.99014778325123154, 0.98768472906403937, 0.80295566502463056], 5: [0.060344827586206899, 0.0061576354679802959, 0.011083743842364532, 0.062807881773399021], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.226729 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.76384
Epoch 1, cost is  1.74271
Epoch 2, cost is  1.72061
Epoch 3, cost is  1.70057
Epoch 4, cost is  1.68157
Training took 0.152599 minutes
Weight histogram
[4198 3115 2399 1983 1758 1534 1298 1279 2460  226] [ -5.16128615e-02  -4.64438015e-02  -4.12747416e-02  -3.61056817e-02
  -3.09366218e-02  -2.57675618e-02  -2.05985019e-02  -1.54294420e-02
  -1.02603821e-02  -5.09132215e-03   7.77377718e-05]
[2673 1206 1367 1473 1619 1791 2125 2390 2679 2927] [ -5.16128615e-02  -4.64438015e-02  -4.12747416e-02  -3.61056817e-02
  -3.09366218e-02  -2.57675618e-02  -2.05985019e-02  -1.54294420e-02
  -1.02603821e-02  -5.09132215e-03   7.77377718e-05]
-0.790643
1.53425
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224767 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.7877
Epoch 1, cost is  1.76175
Epoch 2, cost is  1.73904
Epoch 3, cost is  1.71895
Epoch 4, cost is  1.69539
Training took 0.153575 minutes
Weight histogram
[3758 2849 2356 1997 1790 1585 1563 1396 4549  432] [ -5.05660772e-02  -4.55016957e-02  -4.04373142e-02  -3.53729327e-02
  -3.03085512e-02  -2.52441697e-02  -2.01797882e-02  -1.51154067e-02
  -1.00510252e-02  -4.98664373e-03   7.77377718e-05]
[4986 1319 1368 1482 1617 1800 2047 2298 2554 2804] [ -5.05660772e-02  -4.55016957e-02  -4.04373142e-02  -3.53729327e-02
  -3.03085512e-02  -2.52441697e-02  -2.01797882e-02  -1.51154067e-02
  -1.00510252e-02  -4.98664373e-03   7.77377718e-05]
-0.631252
1.04093
... retrieved True_rbm_500-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80215
Epoch 1, cost is  6.69492
Epoch 2, cost is  6.61381
Epoch 3, cost is  6.53748
Epoch 4, cost is  6.46198
Training took 0.106369 minutes
Weight histogram
[ 1228  2991 10504  2744  1081   633   415   288   205   161] [ -9.23346821e-03  -8.31552690e-03  -7.39758559e-03  -6.47964428e-03
  -5.56170297e-03  -4.64376166e-03  -3.72582035e-03  -2.80787903e-03
  -1.88993772e-03  -9.71996412e-04  -5.40551009e-05]
[7992 3511 2725 2376 1850  800  519  260  107  110] [ -9.23346821e-03  -8.31552690e-03  -7.39758559e-03  -6.47964428e-03
  -5.56170297e-03  -4.64376166e-03  -3.72582035e-03  -2.80787903e-03
  -1.88993772e-03  -9.71996412e-04  -5.40551009e-05]
-0.0624371
0.0838582
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.084016 minutes
Epoch 0
Fine tuning took 0.083353 minutes
Epoch 0
Fine tuning took 0.082953 minutes
{'zero': {0: [0.16133004926108374, 0.0061576354679802959, 0.0, 0.041871921182266007], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.99014778325123154, 0.97413793103448276, 0.88054187192118227], 5: [0.066502463054187194, 0.0036945812807881772, 0.025862068965517241, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.0, 0.0012315270935960591, 0.04064039408866995], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.99507389162561577, 0.97906403940886699, 0.8854679802955665], 5: [0.066502463054187194, 0.0049261083743842365, 0.019704433497536946, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.0036945812807881772, 0.0, 0.046798029556650245], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.9926108374384236, 0.98522167487684731, 0.87931034482758619], 5: [0.066502463054187194, 0.0036945812807881772, 0.014778325123152709, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.0012315270935960591, 0.0012315270935960591, 0.041871921182266007], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.99384236453201968, 0.98029556650246308, 0.8854679802955665], 5: [0.066502463054187194, 0.0049261083743842365, 0.018472906403940888, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.227667 minutes
Weight histogram
[  41  227  599  644  943 1176 1781 3333 8780 2726] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
[ 835  809  980 1146 1410 1844 2224 2903 3747 4352] [ -1.34912640e-04  -2.74149381e-05   8.00827635e-05   1.87580465e-04
   2.95078167e-04   4.02575868e-04   5.10073570e-04   6.17571271e-04
   7.25068973e-04   8.32566674e-04   9.40064376e-04]
-0.624658
0.954219
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.76384
Epoch 1, cost is  1.74271
Epoch 2, cost is  1.72061
Epoch 3, cost is  1.70057
Epoch 4, cost is  1.68157
Training took 0.151714 minutes
Weight histogram
[4198 3115 2399 1983 1758 1534 1298 1279 2460  226] [ -5.16128615e-02  -4.64438015e-02  -4.12747416e-02  -3.61056817e-02
  -3.09366218e-02  -2.57675618e-02  -2.05985019e-02  -1.54294420e-02
  -1.02603821e-02  -5.09132215e-03   7.77377718e-05]
[2673 1206 1367 1473 1619 1791 2125 2390 2679 2927] [ -5.16128615e-02  -4.64438015e-02  -4.12747416e-02  -3.61056817e-02
  -3.09366218e-02  -2.57675618e-02  -2.05985019e-02  -1.54294420e-02
  -1.02603821e-02  -5.09132215e-03   7.77377718e-05]
-0.790643
1.53425
training layer 0, rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225630 minutes
Weight histogram
[  66  446 1076 1289 1349 1102 1661 5141 9081 1064] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
[1300 1673 1213 1108 1425 1793 2422 2880 3792 4669] [ -1.34912640e-04  -3.27810441e-05   6.93505513e-05   1.71482147e-04
   2.73613742e-04   3.75745338e-04   4.77876933e-04   5.80008529e-04
   6.82140124e-04   7.84271720e-04   8.86403315e-04]
-0.821542
0.513878
training layer 1, rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_500-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.7877
Epoch 1, cost is  1.76175
Epoch 2, cost is  1.73904
Epoch 3, cost is  1.71895
Epoch 4, cost is  1.69539
Training took 0.151221 minutes
Weight histogram
[3758 2849 2356 1997 1790 1585 1563 1396 4549  432] [ -5.05660772e-02  -4.55016957e-02  -4.04373142e-02  -3.53729327e-02
  -3.03085512e-02  -2.52441697e-02  -2.01797882e-02  -1.51154067e-02
  -1.00510252e-02  -4.98664373e-03   7.77377718e-05]
[4986 1319 1368 1482 1617 1800 2047 2298 2554 2804] [ -5.05660772e-02  -4.55016957e-02  -4.04373142e-02  -3.53729327e-02
  -3.03085512e-02  -2.52441697e-02  -2.01797882e-02  -1.51154067e-02
  -1.00510252e-02  -4.98664373e-03   7.77377718e-05]
-0.631252
1.04093
... retrieved True_rbm_500-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN6/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.67168
Epoch 1, cost is  6.52247
Epoch 2, cost is  6.43204
Epoch 3, cost is  6.35011
Epoch 4, cost is  6.27535
Training took 0.159575 minutes
Weight histogram
[ 930 1955 7357 6274 1615  842  518  345  235  179] [ -1.00774989e-02  -9.07374729e-03  -8.06999572e-03  -7.06624415e-03
  -6.06249259e-03  -5.05874102e-03  -4.05498945e-03  -3.05123788e-03
  -2.04748632e-03  -1.04373475e-03  -3.99831843e-05]
[7257 3387 2770 2488 2134  993  596  370  125  130] [ -1.00774989e-02  -9.07374729e-03  -8.06999572e-03  -7.06624415e-03
  -6.06249259e-03  -5.05874102e-03  -4.05498945e-03  -3.05123788e-03
  -2.04748632e-03  -1.04373475e-03  -3.99831843e-05]
-0.0601954
0.0720263
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.088912 minutes
Epoch 0
Fine tuning took 0.088959 minutes
Epoch 0
Fine tuning took 0.088219 minutes
{'zero': {0: [0.16379310344827586, 0.0012315270935960591, 0.0012315270935960591, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.99753694581280783, 0.98891625615763545, 0.92487684729064035], 5: [0.087438423645320201, 0.0012315270935960591, 0.009852216748768473, 0.055418719211822662], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16379310344827586, 0.0, 0.0, 0.02832512315270936], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 1.0, 0.99507389162561577, 0.91625615763546797], 5: [0.087438423645320201, 0.0, 0.0049261083743842365, 0.055418719211822662], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16379310344827586, 0.0, 0.0012315270935960591, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.99630541871921185, 0.98029556650246308, 0.91009852216748766], 5: [0.087438423645320201, 0.0036945812807881772, 0.018472906403940888, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16379310344827586, 0.0, 0.0012315270935960591, 0.033251231527093597], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.99876847290640391, 0.98522167487684731, 0.88793103448275867], 5: [0.087438423645320201, 0.0012315270935960591, 0.013546798029556651, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}}
