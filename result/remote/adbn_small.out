Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentADBN4
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185336 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.81998
Epoch 1, cost is  3.06361
Epoch 2, cost is  3.26116
Epoch 3, cost is  3.5852
Epoch 4, cost is  3.90961
Training took 0.095562 minutes
Weight histogram
[330 293 287 302 257 236 146  89  47  38] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
[ 78 109 154 195 208 236 250 261 258 276] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
-6.84086
8.7727
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.186374 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.00124
Epoch 1, cost is  3.21506
Epoch 2, cost is  3.40916
Epoch 3, cost is  3.77041
Epoch 4, cost is  4.20479
Training took 0.097174 minutes
Weight histogram
[544 571 620 609 510 509 317 181  97  92] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
[172 235 341 410 438 492 517 526 543 376] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
-6.84086
8.7727
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  5.01822
Epoch 1, cost is  4.24623
Epoch 2, cost is  4.61626
Epoch 3, cost is  5.11224
Epoch 4, cost is  5.64224
Training took 0.083874 minutes
Weight histogram
[255 332 360 285 215 231 115  90  58  84] [-0.29092243 -0.26201267 -0.23310291 -0.20419315 -0.17528338 -0.14637362
 -0.11746386 -0.0885541  -0.05964433 -0.03073457 -0.00182481]
[122 103 140 186 210 240 252 258 255 259] [-0.29092243 -0.26201267 -0.23310291 -0.20419315 -0.17528338 -0.14637362
 -0.11746386 -0.0885541  -0.05964433 -0.03073457 -0.00182481]
-11.6165
10.8345
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042010 minutes
Epoch 0
Fine tuning took 0.043764 minutes
Epoch 0
Fine tuning took 0.042617 minutes
{'zero': {0: [0.12192118226600986, 0.15517241379310345, 0.17241379310344829, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.48029556650246308, 0.46305418719211822, 0.43965517241379309], 5: [0.11699507389162561, 0.3645320197044335, 0.3645320197044335, 0.37931034482758619], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12192118226600986, 0.11206896551724138, 0.10960591133004927, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.77955665024630538, 0.76231527093596063, 0.73029556650246308], 5: [0.11699507389162561, 0.10837438423645321, 0.12807881773399016, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12192118226600986, 0.11576354679802955, 0.11699507389162561, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.70320197044334976, 0.68596059113300489, 0.66871921182266014], 5: [0.11699507389162561, 0.18103448275862069, 0.19704433497536947, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12192118226600986, 0.072660098522167482, 0.078817733990147784, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.87684729064039413, 0.84975369458128081, 0.8645320197044335], 5: [0.11699507389162561, 0.050492610837438424, 0.071428571428571425, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.186211 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.81998
Epoch 1, cost is  3.06361
Epoch 2, cost is  3.26116
Epoch 3, cost is  3.5852
Epoch 4, cost is  3.90961
Training took 0.097782 minutes
Weight histogram
[330 293 287 302 257 236 146  89  47  38] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
[ 78 109 154 195 208 236 250 261 258 276] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
-6.84086
8.7727
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184421 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.00124
Epoch 1, cost is  3.21506
Epoch 2, cost is  3.40916
Epoch 3, cost is  3.77041
Epoch 4, cost is  4.20479
Training took 0.096641 minutes
Weight histogram
[544 571 620 609 510 509 317 181  97  92] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
[172 235 341 410 438 492 517 526 543 376] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
-6.84086
8.7727
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.64005
Epoch 1, cost is  3.43746
Epoch 2, cost is  3.52715
Epoch 3, cost is  3.83339
Epoch 4, cost is  4.22721
Training took 0.093359 minutes
Weight histogram
[278 288 272 260 287 241 160  96  62  81] [-0.23938569 -0.21564747 -0.19190924 -0.16817102 -0.14443279 -0.12069457
 -0.09695634 -0.07321811 -0.04947989 -0.02574166 -0.00200344]
[129 115 154 179 217 231 245 250 250 255] [-0.23938569 -0.21564747 -0.19190924 -0.16817102 -0.14443279 -0.12069457
 -0.09695634 -0.07321811 -0.04947989 -0.02574166 -0.00200344]
-7.37177
8.63829
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041699 minutes
Epoch 0
Fine tuning took 0.044170 minutes
Epoch 0
Fine tuning took 0.044714 minutes
{'zero': {0: [0.092364532019704432, 0.2536945812807882, 0.19088669950738915, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.27216748768472904, 0.39778325123152708, 0.60837438423645318], 5: [0.17857142857142858, 0.47413793103448276, 0.41133004926108374, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.092364532019704432, 0.11083743842364532, 0.11330049261083744, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.71798029556650245, 0.71059113300492616, 0.70197044334975367], 5: [0.17857142857142858, 0.17118226600985223, 0.17610837438423646, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.092364532019704432, 0.14285714285714285, 0.11699507389162561, 0.096059113300492605], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.66379310344827591, 0.73522167487684731, 0.72290640394088668], 5: [0.17857142857142858, 0.19334975369458129, 0.14778325123152711, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.092364532019704432, 0.051724137931034482, 0.060344827586206899, 0.04064039408866995], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.82266009852216748, 0.76600985221674878, 0.70073891625615758], 5: [0.17857142857142858, 0.12561576354679804, 0.17364532019704434, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184199 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.81998
Epoch 1, cost is  3.06361
Epoch 2, cost is  3.26116
Epoch 3, cost is  3.5852
Epoch 4, cost is  3.90961
Training took 0.093671 minutes
Weight histogram
[330 293 287 302 257 236 146  89  47  38] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
[ 78 109 154 195 208 236 250 261 258 276] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
-6.84086
8.7727
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185092 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.00124
Epoch 1, cost is  3.21506
Epoch 2, cost is  3.40916
Epoch 3, cost is  3.77041
Epoch 4, cost is  4.20479
Training took 0.093665 minutes
Weight histogram
[544 571 620 609 510 509 317 181  97  92] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
[172 235 341 410 438 492 517 526 543 376] [-0.24809891 -0.22349044 -0.19888196 -0.17427349 -0.14966502 -0.12505654
 -0.10044807 -0.07583959 -0.05123112 -0.02662265 -0.00201417]
-6.84086
8.7727
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  4.33042
Epoch 1, cost is  2.53309
Epoch 2, cost is  2.26745
Epoch 3, cost is  2.29147
Epoch 4, cost is  2.34871
Training took 0.120373 minutes
Weight histogram
[268 340 294 285 243 216 144 110  73  52] [-0.14670672 -0.13222895 -0.11775119 -0.10327343 -0.08879567 -0.07431791
 -0.05984015 -0.04536239 -0.03088463 -0.01640687 -0.00192911]
[138 100 141 175 200 236 245 253 262 275] [-0.14670672 -0.13222895 -0.11775119 -0.10327343 -0.08879567 -0.07431791
 -0.05984015 -0.04536239 -0.03088463 -0.01640687 -0.00192911]
-5.76717
6.48031
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045885 minutes
Epoch 0
Fine tuning took 0.045003 minutes
Epoch 0
Fine tuning took 0.044521 minutes
{'zero': {0: [0.1268472906403941, 0.30049261083743845, 0.25615763546798032, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76231527093596063, 0.45443349753694579, 0.4248768472906404, 0.61576354679802958], 5: [0.11083743842364532, 0.24507389162561577, 0.31896551724137934, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1268472906403941, 0.055418719211822662, 0.050492610837438424, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76231527093596063, 0.83620689655172409, 0.80049261083743839, 0.77709359605911332], 5: [0.11083743842364532, 0.10837438423645321, 0.14901477832512317, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1268472906403941, 0.096059113300492605, 0.071428571428571425, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76231527093596063, 0.78817733990147787, 0.80541871921182262, 0.77216748768472909], 5: [0.11083743842364532, 0.11576354679802955, 0.12315270935960591, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1268472906403941, 0.046798029556650245, 0.041871921182266007, 0.033251231527093597], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76231527093596063, 0.83743842364532017, 0.76108374384236455, 0.75615763546798032], 5: [0.11083743842364532, 0.11576354679802955, 0.19704433497536947, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184852 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.10777
Epoch 1, cost is  4.51674
Epoch 2, cost is  3.79081
Epoch 3, cost is  3.44232
Epoch 4, cost is  3.22332
Training took 0.095249 minutes
Weight histogram
[377 344 276 189 206 138 149 127 104 115] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
[240 138 131 150 163 197 212 242 254 298] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
-1.58852
1.94712
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185362 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.34715
Epoch 1, cost is  4.72401
Epoch 2, cost is  3.94546
Epoch 3, cost is  3.58969
Epoch 4, cost is  3.39639
Training took 0.094974 minutes
Weight histogram
[423 711 658 409 417 294 288 256 369 225] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
[532 264 249 290 332 384 428 482 506 583] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
-1.58852
1.94712
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  6.7069
Epoch 1, cost is  6.24797
Epoch 2, cost is  5.28662
Epoch 3, cost is  4.58891
Epoch 4, cost is  4.23763
Training took 0.081272 minutes
Weight histogram
[198 225 133 144 163 103 152 159 704  44] [ -8.42979699e-02  -7.58773946e-02  -6.74568193e-02  -5.90362440e-02
  -5.06156687e-02  -4.21950934e-02  -3.37745181e-02  -2.53539428e-02
  -1.69333675e-02  -8.51279221e-03  -9.22169056e-05]
[581 159 124 124 136 148 155 180 199 219] [ -8.42979699e-02  -7.58773946e-02  -6.74568193e-02  -5.90362440e-02
  -5.06156687e-02  -4.21950934e-02  -3.37745181e-02  -2.53539428e-02
  -1.69333675e-02  -8.51279221e-03  -9.22169056e-05]
-1.39914
1.99187
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044152 minutes
Epoch 0
Fine tuning took 0.041249 minutes
Epoch 0
Fine tuning took 0.040812 minutes
{'zero': {0: [0.15640394088669951, 0.10837438423645321, 0.12315270935960591, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.68596059113300489, 0.69581280788177335, 0.72536945812807885], 5: [0.10221674876847291, 0.20566502463054187, 0.18103448275862069, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15640394088669951, 0.10837438423645321, 0.13669950738916256, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.72536945812807885, 0.66379310344827591, 0.74753694581280783], 5: [0.10221674876847291, 0.16625615763546797, 0.19950738916256158, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15640394088669951, 0.12438423645320197, 0.15024630541871922, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.72536945812807885, 0.6576354679802956, 0.76724137931034486], 5: [0.10221674876847291, 0.15024630541871922, 0.19211822660098521, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15640394088669951, 0.10098522167487685, 0.13300492610837439, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.72044334975369462, 0.6711822660098522, 0.73029556650246308], 5: [0.10221674876847291, 0.17857142857142858, 0.19581280788177341, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185058 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.10777
Epoch 1, cost is  4.51674
Epoch 2, cost is  3.79081
Epoch 3, cost is  3.44232
Epoch 4, cost is  3.22332
Training took 0.093983 minutes
Weight histogram
[377 344 276 189 206 138 149 127 104 115] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
[240 138 131 150 163 197 212 242 254 298] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
-1.58852
1.94712
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.182928 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.34715
Epoch 1, cost is  4.72401
Epoch 2, cost is  3.94546
Epoch 3, cost is  3.58969
Epoch 4, cost is  3.39639
Training took 0.096981 minutes
Weight histogram
[423 711 658 409 417 294 288 256 369 225] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
[532 264 249 290 332 384 428 482 506 583] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
-1.58852
1.94712
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.66477
Epoch 1, cost is  6.19664
Epoch 2, cost is  5.06642
Epoch 3, cost is  4.23517
Epoch 4, cost is  3.74737
Training took 0.092999 minutes
Weight histogram
[214 211 185 177 150 142 135 590 196  25] [-0.06149432 -0.05536452 -0.04923471 -0.0431049  -0.03697509 -0.03084529
 -0.02471548 -0.01858567 -0.01245586 -0.00632606 -0.00019625]
[566 183 118 128 149 145 161 169 195 211] [-0.06149432 -0.05536452 -0.04923471 -0.0431049  -0.03697509 -0.03084529
 -0.02471548 -0.01858567 -0.01245586 -0.00632606 -0.00019625]
-1.11792
1.59546
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043403 minutes
Epoch 0
Fine tuning took 0.043274 minutes
Epoch 0
Fine tuning took 0.045082 minutes
{'zero': {0: [0.10467980295566502, 0.12807881773399016, 0.10714285714285714, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.75862068965517238, 0.74384236453201968, 0.70935960591133007], 5: [0.11699507389162561, 0.11330049261083744, 0.14901477832512317, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10467980295566502, 0.11576354679802955, 0.10467980295566502, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.77093596059113301, 0.73768472906403937, 0.72290640394088668], 5: [0.11699507389162561, 0.11330049261083744, 0.15763546798029557, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10467980295566502, 0.12315270935960591, 0.094827586206896547, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.78448275862068961, 0.7426108374384236, 0.73152709359605916], 5: [0.11699507389162561, 0.092364532019704432, 0.1625615763546798, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10467980295566502, 0.10467980295566502, 0.098522167487684734, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.78694581280788178, 0.77463054187192115, 0.76108374384236455], 5: [0.11699507389162561, 0.10837438423645321, 0.1268472906403941, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185561 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.10777
Epoch 1, cost is  4.51674
Epoch 2, cost is  3.79081
Epoch 3, cost is  3.44232
Epoch 4, cost is  3.22332
Training took 0.095871 minutes
Weight histogram
[377 344 276 189 206 138 149 127 104 115] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
[240 138 131 150 163 197 212 242 254 298] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
-1.58852
1.94712
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184970 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.34715
Epoch 1, cost is  4.72401
Epoch 2, cost is  3.94546
Epoch 3, cost is  3.58969
Epoch 4, cost is  3.39639
Training took 0.095449 minutes
Weight histogram
[423 711 658 409 417 294 288 256 369 225] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
[532 264 249 290 332 384 428 482 506 583] [-0.07366906 -0.06631852 -0.05896798 -0.05161744 -0.04426691 -0.03691637
 -0.02956583 -0.02221529 -0.01486475 -0.00751421 -0.00016367]
-1.58852
1.94712
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.55343
Epoch 1, cost is  6.09979
Epoch 2, cost is  4.92971
Epoch 3, cost is  3.87363
Epoch 4, cost is  3.19043
Training took 0.118546 minutes
Weight histogram
[267 224 218 191 219 348 438  81  25  14] [-0.03940029 -0.03548288 -0.03156546 -0.02764805 -0.02373063 -0.01981322
 -0.0158958  -0.01197839 -0.00806098 -0.00414356 -0.00022615]
[550 223 121 137 136 141 159 168 188 202] [-0.03940029 -0.03548288 -0.03156546 -0.02764805 -0.02373063 -0.01981322
 -0.0158958  -0.01197839 -0.00806098 -0.00414356 -0.00022615]
-1.1016
1.48701
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043861 minutes
Epoch 0
Fine tuning took 0.045158 minutes
Epoch 0
Fine tuning took 0.044967 minutes
{'zero': {0: [0.10467980295566502, 0.16379310344827586, 0.14408866995073891, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72783251231527091, 0.58374384236453203, 0.64162561576354682, 0.52093596059113301], 5: [0.16748768472906403, 0.25246305418719212, 0.21428571428571427, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10467980295566502, 0.14532019704433496, 0.10591133004926108, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72783251231527091, 0.70073891625615758, 0.70073891625615758, 0.65024630541871919], 5: [0.16748768472906403, 0.1539408866995074, 0.19334975369458129, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10467980295566502, 0.12931034482758622, 0.11206896551724138, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72783251231527091, 0.67980295566502458, 0.68472906403940892, 0.64901477832512311], 5: [0.16748768472906403, 0.19088669950738915, 0.20320197044334976, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10467980295566502, 0.11330049261083744, 0.097290640394088676, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72783251231527091, 0.72413793103448276, 0.72660098522167482, 0.65394088669950734], 5: [0.16748768472906403, 0.1625615763546798, 0.17610837438423646, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185176 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.87425
Epoch 1, cost is  6.79951
Epoch 2, cost is  6.673
Epoch 3, cost is  6.43119
Epoch 4, cost is  6.18125
Training took 0.093552 minutes
Weight histogram
[ 125  122  114  129 1154  144   90   64   46   37] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
[897 217 145 115 106  96 109 108 113 119] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
-0.28023
0.321673
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.183393 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.88134
Epoch 1, cost is  6.82712
Epoch 2, cost is  6.77338
Epoch 3, cost is  6.68889
Epoch 4, cost is  6.53272
Training took 0.094206 minutes
Weight histogram
[ 125  122  114  844 2083  285  182  130   93   72] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
[2111  529  334  263  237  127  109  108  113  119] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
-0.28023
0.321673
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  6.82663
Epoch 1, cost is  6.68336
Epoch 2, cost is  6.56334
Epoch 3, cost is  6.437
Epoch 4, cost is  6.28343
Training took 0.081330 minutes
Weight histogram
[1155  441  110   83   64   48   41   33   26   24] [ -1.41098574e-02  -1.26903145e-02  -1.12707715e-02  -9.85122862e-03
  -8.43168570e-03  -7.01214278e-03  -5.59259986e-03  -4.17305694e-03
  -2.75351402e-03  -1.33397110e-03   8.55718172e-05]
[564 287 227 191 166 143 131 117  99 100] [ -1.41098574e-02  -1.26903145e-02  -1.12707715e-02  -9.85122862e-03
  -8.43168570e-03  -7.01214278e-03  -5.59259986e-03  -4.17305694e-03
  -2.75351402e-03  -1.33397110e-03   8.55718172e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043295 minutes
Epoch 0
Fine tuning took 0.040923 minutes
Epoch 0
Fine tuning took 0.042138 minutes
{'zero': {0: [0.21798029556650247, 0.2105911330049261, 0.25615763546798032, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.52216748768472909, 0.47783251231527096, 0.54926108374384242], 5: [0.20689655172413793, 0.26724137931034481, 0.26600985221674878, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21798029556650247, 0.22783251231527094, 0.25615763546798032, 0.29187192118226601], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.50615763546798032, 0.49014778325123154, 0.49876847290640391], 5: [0.20689655172413793, 0.26600985221674878, 0.2536945812807882, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21798029556650247, 0.23522167487684728, 0.26477832512315269, 0.28448275862068967], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.51231527093596063, 0.47413793103448276, 0.53817733990147787], 5: [0.20689655172413793, 0.25246305418719212, 0.26108374384236455, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21798029556650247, 0.22906403940886699, 0.2413793103448276, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.5073891625615764, 0.47660098522167488, 0.52216748768472909], 5: [0.20689655172413793, 0.26354679802955666, 0.28201970443349755, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.184787 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.87425
Epoch 1, cost is  6.79951
Epoch 2, cost is  6.673
Epoch 3, cost is  6.43119
Epoch 4, cost is  6.18125
Training took 0.093464 minutes
Weight histogram
[ 125  122  114  129 1154  144   90   64   46   37] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
[897 217 145 115 106  96 109 108 113 119] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
-0.28023
0.321673
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.185459 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.88134
Epoch 1, cost is  6.82712
Epoch 2, cost is  6.77338
Epoch 3, cost is  6.68889
Epoch 4, cost is  6.53272
Training took 0.093546 minutes
Weight histogram
[ 125  122  114  844 2083  285  182  130   93   72] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
[2111  529  334  263  237  127  109  108  113  119] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
-0.28023
0.321673
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.76998
Epoch 1, cost is  6.58191
Epoch 2, cost is  6.43483
Epoch 3, cost is  6.28725
Epoch 4, cost is  6.10657
Training took 0.091094 minutes
Weight histogram
[1354  201  127   91   69   52   44   34   28   25] [ -1.45929288e-02  -1.31350761e-02  -1.16772234e-02  -1.02193706e-02
  -8.76151792e-03  -7.30366519e-03  -5.84581247e-03  -4.38795975e-03
  -2.93010702e-03  -1.47225430e-03  -1.44015758e-05]
[514 269 220 191 166 154 143 125 125 118] [ -1.45929288e-02  -1.31350761e-02  -1.16772234e-02  -1.02193706e-02
  -8.76151792e-03  -7.30366519e-03  -5.84581247e-03  -4.38795975e-03
  -2.93010702e-03  -1.47225430e-03  -1.44015758e-05]
-0.08559
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043803 minutes
Epoch 0
Fine tuning took 0.042488 minutes
Epoch 0
Fine tuning took 0.042099 minutes
{'zero': {0: [0.21551724137931033, 0.21921182266009853, 0.27463054187192121, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.49261083743842365, 0.4605911330049261, 0.55788177339901479], 5: [0.2019704433497537, 0.28817733990147781, 0.26477832512315269, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21551724137931033, 0.24261083743842365, 0.26108374384236455, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.4963054187192118, 0.48522167487684731, 0.50615763546798032], 5: [0.2019704433497537, 0.26108374384236455, 0.2536945812807882, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21551724137931033, 0.21674876847290642, 0.2857142857142857, 0.27709359605911332], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.53448275862068961, 0.47783251231527096, 0.51108374384236455], 5: [0.2019704433497537, 0.24876847290640394, 0.23645320197044334, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21551724137931033, 0.23275862068965517, 0.25862068965517243, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58251231527093594, 0.49261083743842365, 0.49876847290640391, 0.49876847290640391], 5: [0.2019704433497537, 0.27463054187192121, 0.24261083743842365, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.183767 minutes
Weight histogram
[ 31  25 182 218 244 402 477 262 132  52] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
[ 83  72  94 106 138 158 215 265 368 526] [ -3.91909212e-04  -2.25204491e-04  -5.84997702e-05   1.08204951e-04
   2.74909672e-04   4.41614393e-04   6.08319114e-04   7.75023835e-04
   9.41728556e-04   1.10843328e-03   1.27513800e-03]
-0.731216
0.549615
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.87425
Epoch 1, cost is  6.79951
Epoch 2, cost is  6.673
Epoch 3, cost is  6.43119
Epoch 4, cost is  6.18125
Training took 0.093818 minutes
Weight histogram
[ 125  122  114  129 1154  144   90   64   46   37] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
[897 217 145 115 106  96 109 108 113 119] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
-0.28023
0.321673
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.183632 minutes
Weight histogram
[ 31  44 303 470 610 852 949 511 215  65] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
[171 152 201 237 294 315 488 576 920 696] [ -3.91909212e-04  -2.25072919e-04  -5.82366250e-05   1.08599669e-04
   2.75435962e-04   4.42272256e-04   6.09108550e-04   7.75944843e-04
   9.42781137e-04   1.10961743e-03   1.27645372e-03]
-0.819588
0.592671
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.88134
Epoch 1, cost is  6.82712
Epoch 2, cost is  6.77338
Epoch 3, cost is  6.68889
Epoch 4, cost is  6.53272
Training took 0.093386 minutes
Weight histogram
[ 125  122  114  844 2083  285  182  130   93   72] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
[2111  529  334  263  237  127  109  108  113  119] [ -1.27348276e-02  -1.14592069e-02  -1.01835863e-02  -8.90796568e-03
  -7.63234504e-03  -6.35672441e-03  -5.08110377e-03  -3.80548314e-03
  -2.52986250e-03  -1.25424186e-03   2.13787716e-05]
-0.28023
0.321673
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.60846
Epoch 1, cost is  6.3056
Epoch 2, cost is  6.10206
Epoch 3, cost is  5.92002
Epoch 4, cost is  5.74398
Training took 0.117317 minutes
Weight histogram
[527 634 301 180 117  86  63  49  38  30] [ -1.79279987e-02  -1.61406883e-02  -1.43533779e-02  -1.25660675e-02
  -1.07787572e-02  -8.99144678e-03  -7.20413641e-03  -5.41682603e-03
  -3.62951565e-03  -1.84220527e-03  -5.48948919e-05]
[450 237 201 180 169 160 153 160 152 163] [ -1.79279987e-02  -1.61406883e-02  -1.43533779e-02  -1.25660675e-02
  -1.07787572e-02  -8.99144678e-03  -7.20413641e-03  -5.41682603e-03
  -3.62951565e-03  -1.84220527e-03  -5.48948919e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045458 minutes
Epoch 0
Fine tuning took 0.046896 minutes
Epoch 0
Fine tuning took 0.045347 minutes
{'zero': {0: [0.22536945812807882, 0.23522167487684728, 0.25123152709359609, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.48029556650246308, 0.47660098522167488, 0.55788177339901479], 5: [0.22413793103448276, 0.28448275862068967, 0.27216748768472904, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22536945812807882, 0.24876847290640394, 0.27586206896551724, 0.25], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.48522167487684731, 0.47044334975369456, 0.54064039408866993], 5: [0.22413793103448276, 0.26600985221674878, 0.2536945812807882, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22536945812807882, 0.23152709359605911, 0.25985221674876846, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.5357142857142857, 0.48768472906403942, 0.53325123152709364], 5: [0.22413793103448276, 0.23275862068965517, 0.25246305418719212, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22536945812807882, 0.2229064039408867, 0.25862068965517243, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.48768472906403942, 0.49261083743842365, 0.5357142857142857], 5: [0.22413793103448276, 0.2894088669950739, 0.24876847290640394, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148716 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.13127
Epoch 1, cost is  4.28331
Epoch 2, cost is  4.56754
Epoch 3, cost is  4.91163
Epoch 4, cost is  5.26478
Training took 0.089472 minutes
Weight histogram
[483 631 577 601 489 451 384 260 119  55] [-0.37460089 -0.33734222 -0.30008354 -0.26282487 -0.2255662  -0.18830753
 -0.15104886 -0.11379019 -0.07653152 -0.03927285 -0.00201417]
[150 265 347 411 430 471 523 491 480 482] [-0.37460089 -0.33734222 -0.30008354 -0.26282487 -0.2255662  -0.18830753
 -0.15104886 -0.11379019 -0.07653152 -0.03927285 -0.00201417]
-12.6936
14.4045
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149597 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.7045
Epoch 1, cost is  4.88636
Epoch 2, cost is  5.23335
Epoch 3, cost is  5.64511
Epoch 4, cost is  6.06571
Training took 0.086408 minutes
Weight histogram
[ 445  561  502  664  978 1031  817  683  249  145] [-0.39434484 -0.35511177 -0.3158787  -0.27664564 -0.23741257 -0.19817951
 -0.15894644 -0.11971337 -0.08048031 -0.04124724 -0.00201417]
[335 608 753 891 926 670 492 472 464 464] [-0.39434484 -0.35511177 -0.3158787  -0.27664564 -0.23741257 -0.19817951
 -0.15894644 -0.11971337 -0.08048031 -0.04124724 -0.00201417]
-13.406
14.0143
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.05879
Epoch 1, cost is  4.33427
Epoch 2, cost is  4.71115
Epoch 3, cost is  5.22668
Epoch 4, cost is  5.88813
Training took 0.084453 minutes
Weight histogram
[461 658 701 576 496 458 247 173 116 164] [-0.29092243 -0.2620054  -0.23308836 -0.20417133 -0.17525429 -0.14633726
 -0.11742022 -0.08850319 -0.05958615 -0.03066912 -0.00175208]
[241 208 283 374 426 483 499 510 512 514] [-0.29092243 -0.2620054  -0.23308836 -0.20417133 -0.17525429 -0.14633726
 -0.11742022 -0.08850319 -0.05958615 -0.03066912 -0.00175208]
-11.6165
10.8345
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042465 minutes
Epoch 0
Fine tuning took 0.043406 minutes
Epoch 0
Fine tuning took 0.041043 minutes
{'zero': {0: [0.20689655172413793, 0.26231527093596058, 0.27463054187192121, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.55295566502463056, 0.44088669950738918, 0.73645320197044339], 5: [0.16748768472906403, 0.18472906403940886, 0.28448275862068967, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20689655172413793, 0.17364532019704434, 0.18596059113300492, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.68965517241379315, 0.68596059113300489, 0.71798029556650245], 5: [0.16748768472906403, 0.13669950738916256, 0.12807881773399016, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20689655172413793, 0.20073891625615764, 0.19581280788177341, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.65394088669950734, 0.61206896551724133, 0.68226600985221675], 5: [0.16748768472906403, 0.14532019704433496, 0.19211822660098521, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20689655172413793, 0.11083743842364532, 0.17118226600985223, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.76477832512315269, 0.73275862068965514, 0.73891625615763545], 5: [0.16748768472906403, 0.12438423645320197, 0.096059113300492605, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148988 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.13127
Epoch 1, cost is  4.28331
Epoch 2, cost is  4.56754
Epoch 3, cost is  4.91163
Epoch 4, cost is  5.26478
Training took 0.086226 minutes
Weight histogram
[483 631 577 601 489 451 384 260 119  55] [-0.37460089 -0.33734222 -0.30008354 -0.26282487 -0.2255662  -0.18830753
 -0.15104886 -0.11379019 -0.07653152 -0.03927285 -0.00201417]
[150 265 347 411 430 471 523 491 480 482] [-0.37460089 -0.33734222 -0.30008354 -0.26282487 -0.2255662  -0.18830753
 -0.15104886 -0.11379019 -0.07653152 -0.03927285 -0.00201417]
-12.6936
14.4045
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149997 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.7045
Epoch 1, cost is  4.88636
Epoch 2, cost is  5.23335
Epoch 3, cost is  5.64511
Epoch 4, cost is  6.06571
Training took 0.086201 minutes
Weight histogram
[ 445  561  502  664  978 1031  817  683  249  145] [-0.39434484 -0.35511177 -0.3158787  -0.27664564 -0.23741257 -0.19817951
 -0.15894644 -0.11971337 -0.08048031 -0.04124724 -0.00201417]
[335 608 753 891 926 670 492 472 464 464] [-0.39434484 -0.35511177 -0.3158787  -0.27664564 -0.23741257 -0.19817951
 -0.15894644 -0.11971337 -0.08048031 -0.04124724 -0.00201417]
-13.406
14.0143
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.71643
Epoch 1, cost is  3.5425
Epoch 2, cost is  3.63356
Epoch 3, cost is  3.90414
Epoch 4, cost is  4.30668
Training took 0.090866 minutes
Weight histogram
[537 552 556 544 535 486 359 203 117 161] [-0.23938569 -0.21564071 -0.19189572 -0.16815073 -0.14440575 -0.12066076
 -0.09691577 -0.07317079 -0.0494258  -0.02568081 -0.00193582]
[259 242 319 369 435 471 484 506 499 466] [-0.23938569 -0.21564071 -0.19189572 -0.16815073 -0.14440575 -0.12066076
 -0.09691577 -0.07317079 -0.0494258  -0.02568081 -0.00193582]
-7.90666
8.85636
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044825 minutes
Epoch 0
Fine tuning took 0.045366 minutes
Epoch 0
Fine tuning took 0.043789 minutes
{'zero': {0: [0.17364532019704434, 0.27093596059113301, 0.2229064039408867, 0.29187192118226601], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.38300492610837439, 0.44211822660098521, 0.40147783251231528], 5: [0.1145320197044335, 0.3460591133004926, 0.33497536945812806, 0.30665024630541871], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17364532019704434, 0.16009852216748768, 0.15517241379310345, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.70320197044334976, 0.71674876847290636, 0.72783251231527091], 5: [0.1145320197044335, 0.13669950738916256, 0.12807881773399016, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17364532019704434, 0.15270935960591134, 0.13669950738916256, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.67610837438423643, 0.69581280788177335, 0.70935960591133007], 5: [0.1145320197044335, 0.17118226600985223, 0.16748768472906403, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17364532019704434, 0.078817733990147784, 0.087438423645320201, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.75, 0.75615763546798032, 0.77463054187192115], 5: [0.1145320197044335, 0.17118226600985223, 0.15640394088669951, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148819 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.13127
Epoch 1, cost is  4.28331
Epoch 2, cost is  4.56754
Epoch 3, cost is  4.91163
Epoch 4, cost is  5.26478
Training took 0.087755 minutes
Weight histogram
[483 631 577 601 489 451 384 260 119  55] [-0.37460089 -0.33734222 -0.30008354 -0.26282487 -0.2255662  -0.18830753
 -0.15104886 -0.11379019 -0.07653152 -0.03927285 -0.00201417]
[150 265 347 411 430 471 523 491 480 482] [-0.37460089 -0.33734222 -0.30008354 -0.26282487 -0.2255662  -0.18830753
 -0.15104886 -0.11379019 -0.07653152 -0.03927285 -0.00201417]
-12.6936
14.4045
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148264 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.7045
Epoch 1, cost is  4.88636
Epoch 2, cost is  5.23335
Epoch 3, cost is  5.64511
Epoch 4, cost is  6.06571
Training took 0.086378 minutes
Weight histogram
[ 445  561  502  664  978 1031  817  683  249  145] [-0.39434484 -0.35511177 -0.3158787  -0.27664564 -0.23741257 -0.19817951
 -0.15894644 -0.11971337 -0.08048031 -0.04124724 -0.00201417]
[335 608 753 891 926 670 492 472 464 464] [-0.39434484 -0.35511177 -0.3158787  -0.27664564 -0.23741257 -0.19817951
 -0.15894644 -0.11971337 -0.08048031 -0.04124724 -0.00201417]
-13.406
14.0143
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.43999
Epoch 1, cost is  2.68141
Epoch 2, cost is  2.43434
Epoch 3, cost is  2.41595
Epoch 4, cost is  2.51404
Training took 0.119331 minutes
Weight histogram
[491 671 562 588 504 457 305 230 141 101] [-0.14670672 -0.13222366 -0.1177406  -0.10325755 -0.08877449 -0.07429144
 -0.05980838 -0.04532533 -0.03084227 -0.01635922 -0.00187616]
[280 218 298 362 412 482 489 507 533 469] [-0.14670672 -0.13222366 -0.1177406  -0.10325755 -0.08877449 -0.07429144
 -0.05980838 -0.04532533 -0.03084227 -0.01635922 -0.00187616]
-5.76717
6.96092
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045328 minutes
Epoch 0
Fine tuning took 0.047100 minutes
Epoch 0
Fine tuning took 0.047025 minutes
{'zero': {0: [0.10714285714285714, 0.23645320197044334, 0.21921182266009853, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.40147783251231528, 0.61206896551724133, 0.49876847290640391], 5: [0.1206896551724138, 0.36206896551724138, 0.16871921182266009, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10714285714285714, 0.12931034482758622, 0.10098522167487685, 0.087438423645320201], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.80172413793103448, 0.81403940886699511, 0.84605911330049266], 5: [0.1206896551724138, 0.068965517241379309, 0.084975369458128072, 0.066502463054187194], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10714285714285714, 0.15763546798029557, 0.10714285714285714, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.73275862068965514, 0.77216748768472909, 0.73768472906403937], 5: [0.1206896551724138, 0.10960591133004927, 0.1206896551724138, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10714285714285714, 0.11699507389162561, 0.078817733990147784, 0.043103448275862072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.76600985221674878, 0.85467980295566504, 0.89039408866995073], 5: [0.1206896551724138, 0.11699507389162561, 0.066502463054187194, 0.066502463054187194], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149401 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.19697
Epoch 1, cost is  3.08039
Epoch 2, cost is  3.01257
Epoch 3, cost is  2.95767
Epoch 4, cost is  2.9186
Training took 0.088622 minutes
Weight histogram
[901 779 550 500 336 261 212 204 130 177] [-0.09927407 -0.08936303 -0.07945199 -0.06954095 -0.05962991 -0.04971887
 -0.03980783 -0.02989679 -0.01998575 -0.01007471 -0.00016367]
[317 205 237 298 342 396 505 551 591 608] [-0.09927407 -0.08936303 -0.07945199 -0.06954095 -0.05962991 -0.04971887
 -0.03980783 -0.02989679 -0.01998575 -0.01007471 -0.00016367]
-2.45238
3.27331
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150446 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.35454
Epoch 1, cost is  3.22853
Epoch 2, cost is  3.14916
Epoch 3, cost is  3.09911
Epoch 4, cost is  3.06599
Training took 0.086909 minutes
Weight histogram
[ 844  767 1007  866  656  494  371  341  308  421] [-0.09231309 -0.08309815 -0.0738832  -0.06466826 -0.05545332 -0.04623838
 -0.03702344 -0.0278085  -0.01859355 -0.00937861 -0.00016367]
[689 400 490 613 719 830 659 537 557 581] [-0.09231309 -0.08309815 -0.0738832  -0.06466826 -0.05545332 -0.04623838
 -0.03702344 -0.0278085  -0.01859355 -0.00937861 -0.00016367]
-2.14014
2.47723
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.71985
Epoch 1, cost is  6.26485
Epoch 2, cost is  5.38784
Epoch 3, cost is  4.71176
Epoch 4, cost is  4.33454
Training took 0.083609 minutes
Weight histogram
[ 230  454  344  310  316  262  315  335 1394   90] [ -8.42979699e-02  -7.58760804e-02  -6.74541910e-02  -5.90323015e-02
  -5.06104120e-02  -4.21885225e-02  -3.37666330e-02  -2.53447435e-02
  -1.69228540e-02  -8.50096454e-03  -7.90750491e-05]
[1140  323  260  260  283  306  313  358  393  414] [ -8.42979699e-02  -7.58760804e-02  -6.74541910e-02  -5.90323015e-02
  -5.06104120e-02  -4.21885225e-02  -3.37666330e-02  -2.53447435e-02
  -1.69228540e-02  -8.50096454e-03  -7.90750491e-05]
-1.58135
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041803 minutes
Epoch 0
Fine tuning took 0.042757 minutes
Epoch 0
Fine tuning took 0.043460 minutes
{'zero': {0: [0.1206896551724138, 0.16502463054187191, 0.19950738916256158, 0.089901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78325123152709364, 0.67733990147783252, 0.61576354679802958, 0.71551724137931039], 5: [0.096059113300492605, 0.15763546798029557, 0.18472906403940886, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1206896551724138, 0.17364532019704434, 0.19827586206896552, 0.10467980295566502], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78325123152709364, 0.6711822660098522, 0.58743842364532017, 0.69950738916256161], 5: [0.096059113300492605, 0.15517241379310345, 0.21428571428571427, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1206896551724138, 0.14901477832512317, 0.2019704433497537, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78325123152709364, 0.68226600985221675, 0.60344827586206895, 0.67364532019704437], 5: [0.096059113300492605, 0.16871921182266009, 0.19458128078817735, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1206896551724138, 0.15886699507389163, 0.19211822660098521, 0.096059113300492605], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78325123152709364, 0.64532019704433496, 0.58374384236453203, 0.69458128078817738], 5: [0.096059113300492605, 0.19581280788177341, 0.22413793103448276, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150576 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.19697
Epoch 1, cost is  3.08039
Epoch 2, cost is  3.01257
Epoch 3, cost is  2.95767
Epoch 4, cost is  2.9186
Training took 0.086613 minutes
Weight histogram
[901 779 550 500 336 261 212 204 130 177] [-0.09927407 -0.08936303 -0.07945199 -0.06954095 -0.05962991 -0.04971887
 -0.03980783 -0.02989679 -0.01998575 -0.01007471 -0.00016367]
[317 205 237 298 342 396 505 551 591 608] [-0.09927407 -0.08936303 -0.07945199 -0.06954095 -0.05962991 -0.04971887
 -0.03980783 -0.02989679 -0.01998575 -0.01007471 -0.00016367]
-2.45238
3.27331
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147977 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.35454
Epoch 1, cost is  3.22853
Epoch 2, cost is  3.14916
Epoch 3, cost is  3.09911
Epoch 4, cost is  3.06599
Training took 0.085932 minutes
Weight histogram
[ 844  767 1007  866  656  494  371  341  308  421] [-0.09231309 -0.08309815 -0.0738832  -0.06466826 -0.05545332 -0.04623838
 -0.03702344 -0.0278085  -0.01859355 -0.00937861 -0.00016367]
[689 400 490 613 719 830 659 537 557 581] [-0.09231309 -0.08309815 -0.0738832  -0.06466826 -0.05545332 -0.04623838
 -0.03702344 -0.0278085  -0.01859355 -0.00937861 -0.00016367]
-2.14014
2.47723
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.67936
Epoch 1, cost is  6.21934
Epoch 2, cost is  5.1806
Epoch 3, cost is  4.36113
Epoch 4, cost is  3.86094
Training took 0.093130 minutes
Weight histogram
[271 462 379 385 332 296 301 928 645  51] [-0.06149432 -0.05536353 -0.04923274 -0.04310195 -0.03697116 -0.03084036
 -0.02470957 -0.01857878 -0.01244799 -0.0063172  -0.0001864 ]
[1107  381  244  268  306  295  326  336  388  399] [-0.06149432 -0.05536353 -0.04923274 -0.04310195 -0.03697116 -0.03084036
 -0.02470957 -0.01857878 -0.01244799 -0.0063172  -0.0001864 ]
-1.12971
1.6344
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044627 minutes
Epoch 0
Fine tuning took 0.042763 minutes
Epoch 0
Fine tuning took 0.042285 minutes
{'zero': {0: [0.16133004926108374, 0.17364532019704434, 0.24753694581280788, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.66009852216748766, 0.57266009852216748, 0.63916256157635465], 5: [0.11330049261083744, 0.16625615763546797, 0.17980295566502463, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.16502463054187191, 0.21428571428571427, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.68103448275862066, 0.59482758620689657, 0.65394088669950734], 5: [0.11330049261083744, 0.1539408866995074, 0.19088669950738915, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.14532019704433496, 0.18719211822660098, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.69827586206896552, 0.60837438423645318, 0.66256157635467983], 5: [0.11330049261083744, 0.15640394088669951, 0.20443349753694581, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.12807881773399016, 0.16502463054187191, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.68842364532019706, 0.6280788177339901, 0.68842364532019706], 5: [0.11330049261083744, 0.18349753694581281, 0.20689655172413793, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149451 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.19697
Epoch 1, cost is  3.08039
Epoch 2, cost is  3.01257
Epoch 3, cost is  2.95767
Epoch 4, cost is  2.9186
Training took 0.087663 minutes
Weight histogram
[901 779 550 500 336 261 212 204 130 177] [-0.09927407 -0.08936303 -0.07945199 -0.06954095 -0.05962991 -0.04971887
 -0.03980783 -0.02989679 -0.01998575 -0.01007471 -0.00016367]
[317 205 237 298 342 396 505 551 591 608] [-0.09927407 -0.08936303 -0.07945199 -0.06954095 -0.05962991 -0.04971887
 -0.03980783 -0.02989679 -0.01998575 -0.01007471 -0.00016367]
-2.45238
3.27331
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149493 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.35454
Epoch 1, cost is  3.22853
Epoch 2, cost is  3.14916
Epoch 3, cost is  3.09911
Epoch 4, cost is  3.06599
Training took 0.086376 minutes
Weight histogram
[ 844  767 1007  866  656  494  371  341  308  421] [-0.09231309 -0.08309815 -0.0738832  -0.06466826 -0.05545332 -0.04623838
 -0.03702344 -0.0278085  -0.01859355 -0.00937861 -0.00016367]
[689 400 490 613 719 830 659 537 557 581] [-0.09231309 -0.08309815 -0.0738832  -0.06466826 -0.05545332 -0.04623838
 -0.03702344 -0.0278085  -0.01859355 -0.00937861 -0.00016367]
-2.14014
2.47723
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.5743
Epoch 1, cost is  6.14782
Epoch 2, cost is  5.05847
Epoch 3, cost is  4.02621
Epoch 4, cost is  3.32583
Training took 0.118370 minutes
Weight histogram
[337 499 442 439 468 665 939 180  53  28] [-0.03940029 -0.03548095 -0.03156161 -0.02764227 -0.02372293 -0.0198036
 -0.01588426 -0.01196492 -0.00804558 -0.00412624 -0.0002069 ]
[1089  458  247  283  282  286  312  336  382  375] [-0.03940029 -0.03548095 -0.03156161 -0.02764227 -0.02372293 -0.0198036
 -0.01588426 -0.01196492 -0.00804558 -0.00412624 -0.0002069 ]
-1.16325
1.48701
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046551 minutes
Epoch 0
Fine tuning took 0.045543 minutes
Epoch 0
Fine tuning took 0.045503 minutes
{'zero': {0: [0.16748768472906403, 0.20689655172413793, 0.26231527093596058, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.56280788177339902, 0.50246305418719217, 0.46674876847290642], 5: [0.13177339901477833, 0.23029556650246305, 0.23522167487684728, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16748768472906403, 0.16502463054187191, 0.20566502463054187, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.68842364532019706, 0.63177339901477836, 0.66995073891625612], 5: [0.13177339901477833, 0.14655172413793102, 0.1625615763546798, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16748768472906403, 0.16379310344827586, 0.21921182266009853, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.66009852216748766, 0.61699507389162567, 0.61083743842364535], 5: [0.13177339901477833, 0.17610837438423646, 0.16379310344827586, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16748768472906403, 0.17118226600985223, 0.2105911330049261, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.68596059113300489, 0.6280788177339901, 0.66625615763546797], 5: [0.13177339901477833, 0.14285714285714285, 0.16133004926108374, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150012 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.9843
Epoch 1, cost is  5.78611
Epoch 2, cost is  5.58046
Epoch 3, cost is  5.39161
Epoch 4, cost is  5.21103
Training took 0.086646 minutes
Weight histogram
[ 463  433  506  397  271  229  480 1046  145   80] [ -2.47908980e-02  -2.23096703e-02  -1.98284426e-02  -1.73472149e-02
  -1.48659873e-02  -1.23847596e-02  -9.90353192e-03  -7.42230425e-03
  -4.94107658e-03  -2.45984890e-03   2.13787716e-05]
[1249  308  309  360  337  315  291  298  289  294] [ -2.47908980e-02  -2.23096703e-02  -1.98284426e-02  -1.73472149e-02
  -1.48659873e-02  -1.23847596e-02  -9.90353192e-03  -7.42230425e-03
  -4.94107658e-03  -2.45984890e-03   2.13787716e-05]
-0.434877
0.610179
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150810 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.33421
Epoch 1, cost is  6.13143
Epoch 2, cost is  5.91806
Epoch 3, cost is  5.69973
Epoch 4, cost is  5.49061
Training took 0.086764 minutes
Weight histogram
[ 316  375  493  512  467 3010  448  217  140   97] [ -1.65290572e-02  -1.48740136e-02  -1.32189700e-02  -1.15639264e-02
  -9.90888280e-03  -8.25383920e-03  -6.59879561e-03  -4.94375201e-03
  -3.28870842e-03  -1.63366482e-03   2.13787716e-05]
[2817  651  554  558  274  255  250  239  235  242] [ -1.65290572e-02  -1.48740136e-02  -1.32189700e-02  -1.15639264e-02
  -9.90888280e-03  -8.25383920e-03  -6.59879561e-03  -4.94375201e-03
  -3.28870842e-03  -1.63366482e-03   2.13787716e-05]
-0.420353
0.535119
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.86253
Epoch 1, cost is  6.77112
Epoch 2, cost is  6.69637
Epoch 3, cost is  6.62487
Epoch 4, cost is  6.5489
Training took 0.084120 minutes
Weight histogram
[1489 1477  319  209  158  117   93   75   61   52] [ -1.41098574e-02  -1.26896693e-02  -1.12694812e-02  -9.84929311e-03
  -8.42910502e-03  -7.00891693e-03  -5.58872884e-03  -4.16854075e-03
  -2.74835266e-03  -1.32816457e-03   9.20235252e-05]
[1282  668  540  466  407  240  131  117   99  100] [ -1.41098574e-02  -1.26896693e-02  -1.12694812e-02  -9.84929311e-03
  -8.42910502e-03  -7.00891693e-03  -5.58872884e-03  -4.16854075e-03
  -2.74835266e-03  -1.32816457e-03   9.20235252e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041680 minutes
Epoch 0
Fine tuning took 0.041057 minutes
Epoch 0
Fine tuning took 0.042631 minutes
{'zero': {0: [0.26354679802955666, 0.20443349753694581, 0.34359605911330049, 0.25], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.41748768472906406, 0.39901477832512317, 0.53448275862068961], 5: [0.23275862068965517, 0.37807881773399016, 0.25738916256157635, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26354679802955666, 0.21798029556650247, 0.34113300492610837, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.42241379310344829, 0.37438423645320196, 0.56280788177339902], 5: [0.23275862068965517, 0.35960591133004927, 0.28448275862068967, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26354679802955666, 0.23152709359605911, 0.35960591133004927, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.43349753694581283, 0.37192118226600984, 0.59729064039408863], 5: [0.23275862068965517, 0.33497536945812806, 0.26847290640394089, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26354679802955666, 0.23275862068965517, 0.32019704433497537, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50369458128078815, 0.40640394088669951, 0.40147783251231528, 0.5431034482758621], 5: [0.23275862068965517, 0.3608374384236453, 0.27832512315270935, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148847 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.9843
Epoch 1, cost is  5.78611
Epoch 2, cost is  5.58046
Epoch 3, cost is  5.39161
Epoch 4, cost is  5.21103
Training took 0.089559 minutes
Weight histogram
[ 463  433  506  397  271  229  480 1046  145   80] [ -2.47908980e-02  -2.23096703e-02  -1.98284426e-02  -1.73472149e-02
  -1.48659873e-02  -1.23847596e-02  -9.90353192e-03  -7.42230425e-03
  -4.94107658e-03  -2.45984890e-03   2.13787716e-05]
[1249  308  309  360  337  315  291  298  289  294] [ -2.47908980e-02  -2.23096703e-02  -1.98284426e-02  -1.73472149e-02
  -1.48659873e-02  -1.23847596e-02  -9.90353192e-03  -7.42230425e-03
  -4.94107658e-03  -2.45984890e-03   2.13787716e-05]
-0.434877
0.610179
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151027 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.33421
Epoch 1, cost is  6.13143
Epoch 2, cost is  5.91806
Epoch 3, cost is  5.69973
Epoch 4, cost is  5.49061
Training took 0.086641 minutes
Weight histogram
[ 316  375  493  512  467 3010  448  217  140   97] [ -1.65290572e-02  -1.48740136e-02  -1.32189700e-02  -1.15639264e-02
  -9.90888280e-03  -8.25383920e-03  -6.59879561e-03  -4.94375201e-03
  -3.28870842e-03  -1.63366482e-03   2.13787716e-05]
[2817  651  554  558  274  255  250  239  235  242] [ -1.65290572e-02  -1.48740136e-02  -1.32189700e-02  -1.15639264e-02
  -9.90888280e-03  -8.25383920e-03  -6.59879561e-03  -4.94375201e-03
  -3.28870842e-03  -1.63366482e-03   2.13787716e-05]
-0.420353
0.535119
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8256
Epoch 1, cost is  6.70798
Epoch 2, cost is  6.62062
Epoch 3, cost is  6.53853
Epoch 4, cost is  6.45312
Training took 0.093745 minutes
Weight histogram
[1367 1473  376  236  172  127  101   79   65   54] [ -1.45929288e-02  -1.31344350e-02  -1.16759413e-02  -1.02174475e-02
  -8.75895370e-03  -7.30045992e-03  -5.84196614e-03  -4.38347236e-03
  -2.92497858e-03  -1.46648480e-03  -7.99102236e-06]
[1174  626  525  457  408  349  143  125  125  118] [ -1.45929288e-02  -1.31344350e-02  -1.16759413e-02  -1.02174475e-02
  -8.75895370e-03  -7.30045992e-03  -5.84196614e-03  -4.38347236e-03
  -2.92497858e-03  -1.46648480e-03  -7.99102236e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043953 minutes
Epoch 0
Fine tuning took 0.044369 minutes
Epoch 0
Fine tuning took 0.041561 minutes
{'zero': {0: [0.27216748768472904, 0.2229064039408867, 0.33004926108374383, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49384236453201968, 0.41009852216748771, 0.3608374384236453, 0.55541871921182262], 5: [0.23399014778325122, 0.36699507389162561, 0.30911330049261082, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27216748768472904, 0.22167487684729065, 0.34113300492610837, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49384236453201968, 0.41871921182266009, 0.37438423645320196, 0.57758620689655171], 5: [0.23399014778325122, 0.35960591133004927, 0.28448275862068967, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27216748768472904, 0.23152709359605911, 0.36945812807881773, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49384236453201968, 0.42857142857142855, 0.35098522167487683, 0.57758620689655171], 5: [0.23399014778325122, 0.33990147783251229, 0.27955665024630544, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27216748768472904, 0.23645320197044334, 0.30788177339901479, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49384236453201968, 0.41871921182266009, 0.3645320197044335, 0.53817733990147787], 5: [0.23399014778325122, 0.34482758620689657, 0.32758620689655171, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151266 minutes
Weight histogram
[  39  100  311  340  632  417  509 1000  614   88] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
[ 102  106  131  173  216  336  434  698  741 1113] [ -3.91909212e-04  -1.71336281e-04   4.92366496e-05   2.69809581e-04
   4.90382512e-04   7.10955443e-04   9.31528374e-04   1.15210130e-03
   1.37267424e-03   1.59324717e-03   1.81382010e-03]
-0.912365
0.663572
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.9843
Epoch 1, cost is  5.78611
Epoch 2, cost is  5.58046
Epoch 3, cost is  5.39161
Epoch 4, cost is  5.21103
Training took 0.089915 minutes
Weight histogram
[ 463  433  506  397  271  229  480 1046  145   80] [ -2.47908980e-02  -2.23096703e-02  -1.98284426e-02  -1.73472149e-02
  -1.48659873e-02  -1.23847596e-02  -9.90353192e-03  -7.42230425e-03
  -4.94107658e-03  -2.45984890e-03   2.13787716e-05]
[1249  308  309  360  337  315  291  298  289  294] [ -2.47908980e-02  -2.23096703e-02  -1.98284426e-02  -1.73472149e-02
  -1.48659873e-02  -1.23847596e-02  -9.90353192e-03  -7.42230425e-03
  -4.94107658e-03  -2.45984890e-03   2.13787716e-05]
-0.434877
0.610179
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148837 minutes
Weight histogram
[  34   65  376  539  760 1067 1154 1245  627  208] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
[ 206  218  265  376  442  644  970 1011  727 1216] [ -3.91909212e-04  -2.11227691e-04  -3.05461697e-05   1.50135352e-04
   3.30816873e-04   5.11498394e-04   6.92179915e-04   8.72861437e-04
   1.05354296e-03   1.23422448e-03   1.41490600e-03]
-0.978764
0.710655
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.33421
Epoch 1, cost is  6.13143
Epoch 2, cost is  5.91806
Epoch 3, cost is  5.69973
Epoch 4, cost is  5.49061
Training took 0.087041 minutes
Weight histogram
[ 316  375  493  512  467 3010  448  217  140   97] [ -1.65290572e-02  -1.48740136e-02  -1.32189700e-02  -1.15639264e-02
  -9.90888280e-03  -8.25383920e-03  -6.59879561e-03  -4.94375201e-03
  -3.28870842e-03  -1.63366482e-03   2.13787716e-05]
[2817  651  554  558  274  255  250  239  235  242] [ -1.65290572e-02  -1.48740136e-02  -1.32189700e-02  -1.15639264e-02
  -9.90888280e-03  -8.25383920e-03  -6.59879561e-03  -4.94375201e-03
  -3.28870842e-03  -1.63366482e-03   2.13787716e-05]
-0.420353
0.535119
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.72138
Epoch 1, cost is  6.53365
Epoch 2, cost is  6.41812
Epoch 3, cost is  6.31246
Epoch 4, cost is  6.21684
Training took 0.118035 minutes
Weight histogram
[ 527  900 1145  532  312  214  152  114   86   68] [ -1.79279987e-02  -1.61400391e-02  -1.43520796e-02  -1.25641200e-02
  -1.07761604e-02  -8.98820087e-03  -7.20024131e-03  -5.41228175e-03
  -3.62432219e-03  -1.83636263e-03  -4.84030643e-05]
[1029  558  469  434  420  391  274  160  152  163] [ -1.79279987e-02  -1.61400391e-02  -1.43520796e-02  -1.25641200e-02
  -1.07761604e-02  -8.98820087e-03  -7.20024131e-03  -5.41228175e-03
  -3.62432219e-03  -1.83636263e-03  -4.84030643e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046257 minutes
Epoch 0
Fine tuning took 0.044930 minutes
Epoch 0
Fine tuning took 0.046640 minutes
{'zero': {0: [0.26231527093596058, 0.22783251231527094, 0.34975369458128081, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.45566502463054187, 0.3817733990147783, 0.54064039408866993], 5: [0.24630541871921183, 0.31650246305418717, 0.26847290640394089, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26231527093596058, 0.22660098522167488, 0.34113300492610837, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.43103448275862066, 0.40270935960591131, 0.5431034482758621], 5: [0.24630541871921183, 0.34236453201970446, 0.25615763546798032, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26231527093596058, 0.21428571428571427, 0.3817733990147783, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.46182266009852219, 0.37931034482758619, 0.53940886699507384], 5: [0.24630541871921183, 0.32389162561576357, 0.23891625615763548, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26231527093596058, 0.21798029556650247, 0.34359605911330049, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.45320197044334976, 0.37807881773399016, 0.55788177339901479], 5: [0.24630541871921183, 0.3288177339901478, 0.27832512315270935, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148283 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.31179
Epoch 1, cost is  6.22219
Epoch 2, cost is  6.43423
Epoch 3, cost is  6.7073
Epoch 4, cost is  7.03158
Training took 0.089749 minutes
Weight histogram
[688 763 572 806 817 823 668 555 283 100] [-0.53463131 -0.4813696  -0.42810788 -0.37484617 -0.32158446 -0.26832274
 -0.21506103 -0.16179932 -0.1085376  -0.05527589 -0.00201417]
[246 452 576 625 728 688 676 721 692 671] [-0.53463131 -0.4813696  -0.42810788 -0.37484617 -0.32158446 -0.26832274
 -0.21506103 -0.16179932 -0.1085376  -0.05527589 -0.00201417]
-19.231
22.7764
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149713 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.6756
Epoch 1, cost is  6.62636
Epoch 2, cost is  6.89359
Epoch 3, cost is  7.2264
Epoch 4, cost is  7.5946
Training took 0.086688 minutes
Weight histogram
[ 767  788  598  723  779 1087 1303 1193  643  219] [-0.5404107  -0.48657105 -0.43273139 -0.37889174 -0.32505209 -0.27121244
 -0.21737278 -0.16353313 -0.10969348 -0.05585383 -0.00201417]
[ 548  992 1237 1209  691  659  649  732  703  680] [-0.5404107  -0.48657105 -0.43273139 -0.37889174 -0.32505209 -0.27121244
 -0.21737278 -0.16353313 -0.10969348 -0.05585383 -0.00201417]
-20.4685
25.1581
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.08492
Epoch 1, cost is  4.3996
Epoch 2, cost is  4.75988
Epoch 3, cost is  5.3218
Epoch 4, cost is  5.95805
Training took 0.080821 minutes
Weight histogram
[ 672  964 1029  878  774  682  391  265  175  245] [-0.29092243 -0.26200098 -0.23307953 -0.20415807 -0.17523662 -0.14631517
 -0.11739372 -0.08847226 -0.05955081 -0.03062936 -0.0017079 ]
[359 313 430 566 638 722 737 770 761 779] [-0.29092243 -0.26200098 -0.23307953 -0.20415807 -0.17523662 -0.14631517
 -0.11739372 -0.08847226 -0.05955081 -0.03062936 -0.0017079 ]
-11.6165
12.6309
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040504 minutes
Epoch 0
Fine tuning took 0.043431 minutes
Epoch 0
Fine tuning took 0.041321 minutes
{'zero': {0: [0.16502463054187191, 0.32142857142857145, 0.14655172413793102, 0.096059113300492605], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.44581280788177341, 0.54064039408866993, 0.55418719211822665], 5: [0.10344827586206896, 0.23275862068965517, 0.31280788177339902, 0.34975369458128081], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16502463054187191, 0.22167487684729065, 0.20443349753694581, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.67364532019704437, 0.66995073891625612, 0.70443349753694584], 5: [0.10344827586206896, 0.10467980295566502, 0.12561576354679804, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16502463054187191, 0.22536945812807882, 0.18472906403940886, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.66009852216748766, 0.67733990147783252, 0.67610837438423643], 5: [0.10344827586206896, 0.1145320197044335, 0.13793103448275862, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16502463054187191, 0.19704433497536947, 0.18349753694581281, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.66871921182266014, 0.68349753694581283, 0.6576354679802956], 5: [0.10344827586206896, 0.13423645320197045, 0.13300492610837439, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150292 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.31179
Epoch 1, cost is  6.22219
Epoch 2, cost is  6.43423
Epoch 3, cost is  6.7073
Epoch 4, cost is  7.03158
Training took 0.089567 minutes
Weight histogram
[688 763 572 806 817 823 668 555 283 100] [-0.53463131 -0.4813696  -0.42810788 -0.37484617 -0.32158446 -0.26832274
 -0.21506103 -0.16179932 -0.1085376  -0.05527589 -0.00201417]
[246 452 576 625 728 688 676 721 692 671] [-0.53463131 -0.4813696  -0.42810788 -0.37484617 -0.32158446 -0.26832274
 -0.21506103 -0.16179932 -0.1085376  -0.05527589 -0.00201417]
-19.231
22.7764
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150652 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.6756
Epoch 1, cost is  6.62636
Epoch 2, cost is  6.89359
Epoch 3, cost is  7.2264
Epoch 4, cost is  7.5946
Training took 0.087129 minutes
Weight histogram
[ 767  788  598  723  779 1087 1303 1193  643  219] [-0.5404107  -0.48657105 -0.43273139 -0.37889174 -0.32505209 -0.27121244
 -0.21737278 -0.16353313 -0.10969348 -0.05585383 -0.00201417]
[ 548  992 1237 1209  691  659  649  732  703  680] [-0.5404107  -0.48657105 -0.43273139 -0.37889174 -0.32505209 -0.27121244
 -0.21737278 -0.16353313 -0.10969348 -0.05585383 -0.00201417]
-20.4685
25.1581
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.75275
Epoch 1, cost is  3.61192
Epoch 2, cost is  3.72932
Epoch 3, cost is  4.05024
Epoch 4, cost is  4.45724
Training took 0.093719 minutes
Weight histogram
[665 882 803 894 793 736 584 306 171 241] [-0.23938569 -0.21563449 -0.19188328 -0.16813207 -0.14438086 -0.12062965
 -0.09687845 -0.07312724 -0.04937603 -0.02562482 -0.00187362]
[387 365 485 559 647 707 734 760 751 680] [-0.23938569 -0.21563449 -0.19188328 -0.16813207 -0.14438086 -0.12062965
 -0.09687845 -0.07312724 -0.04937603 -0.02562482 -0.00187362]
-7.90666
8.85636
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041962 minutes
Epoch 0
Fine tuning took 0.042738 minutes
Epoch 0
Fine tuning took 0.043989 minutes
{'zero': {0: [0.10344827586206896, 0.42733990147783252, 0.11576354679802955, 0.12807881773399016], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79802955665024633, 0.42857142857142855, 0.68965517241379315, 0.60221674876847286], 5: [0.098522167487684734, 0.14408866995073891, 0.19458128078817735, 0.26970443349753692], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10344827586206896, 0.15270935960591134, 0.12807881773399016, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79802955665024633, 0.71674876847290636, 0.70320197044334976, 0.72660098522167482], 5: [0.098522167487684734, 0.13054187192118227, 0.16871921182266009, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10344827586206896, 0.19950738916256158, 0.17118226600985223, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79802955665024633, 0.66133004926108374, 0.69581280788177335, 0.69704433497536944], 5: [0.098522167487684734, 0.13916256157635468, 0.13300492610837439, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10344827586206896, 0.15517241379310345, 0.10837438423645321, 0.054187192118226604], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79802955665024633, 0.77339901477832518, 0.84113300492610843, 0.89778325123152714], 5: [0.098522167487684734, 0.071428571428571425, 0.050492610837438424, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150226 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.31179
Epoch 1, cost is  6.22219
Epoch 2, cost is  6.43423
Epoch 3, cost is  6.7073
Epoch 4, cost is  7.03158
Training took 0.089439 minutes
Weight histogram
[688 763 572 806 817 823 668 555 283 100] [-0.53463131 -0.4813696  -0.42810788 -0.37484617 -0.32158446 -0.26832274
 -0.21506103 -0.16179932 -0.1085376  -0.05527589 -0.00201417]
[246 452 576 625 728 688 676 721 692 671] [-0.53463131 -0.4813696  -0.42810788 -0.37484617 -0.32158446 -0.26832274
 -0.21506103 -0.16179932 -0.1085376  -0.05527589 -0.00201417]
-19.231
22.7764
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148720 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.6756
Epoch 1, cost is  6.62636
Epoch 2, cost is  6.89359
Epoch 3, cost is  7.2264
Epoch 4, cost is  7.5946
Training took 0.087911 minutes
Weight histogram
[ 767  788  598  723  779 1087 1303 1193  643  219] [-0.5404107  -0.48657105 -0.43273139 -0.37889174 -0.32505209 -0.27121244
 -0.21737278 -0.16353313 -0.10969348 -0.05585383 -0.00201417]
[ 548  992 1237 1209  691  659  649  732  703  680] [-0.5404107  -0.48657105 -0.43273139 -0.37889174 -0.32505209 -0.27121244
 -0.21737278 -0.16353313 -0.10969348 -0.05585383 -0.00201417]
-20.4685
25.1581
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.4982
Epoch 1, cost is  2.74395
Epoch 2, cost is  2.49269
Epoch 3, cost is  2.48099
Epoch 4, cost is  2.56627
Training took 0.121178 minutes
Weight histogram
[702 996 866 851 769 693 489 351 210 148] [-0.14670672 -0.13222266 -0.11773861 -0.10325456 -0.0887705  -0.07428645
 -0.0598024  -0.04531835 -0.03083429 -0.01635024 -0.00186619]
[420 331 452 544 617 711 728 750 791 731] [-0.14670672 -0.13222266 -0.11773861 -0.10325456 -0.0887705  -0.07428645
 -0.0598024  -0.04531835 -0.03083429 -0.01635024 -0.00186619]
-5.76717
6.96092
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046644 minutes
Epoch 0
Fine tuning took 0.045251 minutes
Epoch 0
Fine tuning took 0.047405 minutes
{'zero': {0: [0.1206896551724138, 0.20566502463054187, 0.1539408866995074, 0.084975369458128072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.62684729064039413, 0.65024630541871919, 0.63177339901477836], 5: [0.086206896551724144, 0.16748768472906403, 0.19581280788177341, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1206896551724138, 0.1354679802955665, 0.1268472906403941, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.76847290640394084, 0.76354679802955661, 0.72906403940886699], 5: [0.086206896551724144, 0.096059113300492605, 0.10960591133004927, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1206896551724138, 0.18719211822660098, 0.13300492610837439, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.68719211822660098, 0.71674876847290636, 0.70566502463054193], 5: [0.086206896551724144, 0.12561576354679804, 0.15024630541871922, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1206896551724138, 0.054187192118226604, 0.027093596059113302, 0.072660098522167482], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.89532019704433496, 0.86822660098522164, 0.82635467980295563], 5: [0.086206896551724144, 0.050492610837438424, 0.10467980295566502, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148949 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.97248
Epoch 1, cost is  2.89558
Epoch 2, cost is  2.86716
Epoch 3, cost is  2.84699
Epoch 4, cost is  2.83916
Training took 0.089316 minutes
Weight histogram
[1441 1013 1070  708  530  420  279  244  172  198] [-0.1184347  -0.10660759 -0.09478049 -0.08295339 -0.07112629 -0.05929918
 -0.04747208 -0.03564498 -0.02381788 -0.01199077 -0.00016367]
[376 280 358 449 547 700 754 823 882 906] [-0.1184347  -0.10660759 -0.09478049 -0.08295339 -0.07112629 -0.05929918
 -0.04747208 -0.03564498 -0.02381788 -0.01199077 -0.00016367]
-3.28662
3.87803
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148562 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.17554
Epoch 1, cost is  3.09245
Epoch 2, cost is  3.06203
Epoch 3, cost is  3.04518
Epoch 4, cost is  3.03908
Training took 0.090006 minutes
Weight histogram
[1242  854 1108 1184 1066  750  574  458  378  486] [-0.11274777 -0.10148936 -0.09023095 -0.07897254 -0.06771413 -0.05645572
 -0.04519731 -0.0339389  -0.02268049 -0.01142208 -0.00016367]
[ 814  566  769  952 1071  693  729  807  848  851] [-0.11274777 -0.10148936 -0.09023095 -0.07897254 -0.06771413 -0.05645572
 -0.04519731 -0.0339389  -0.02268049 -0.01142208 -0.00016367]
-3.15642
3.49961
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.72899
Epoch 1, cost is  6.2937
Epoch 2, cost is  5.44346
Epoch 3, cost is  4.79572
Epoch 4, cost is  4.44141
Training took 0.085894 minutes
Weight histogram
[ 241  663  578  432  513  429  460  539 2081  139] [ -8.42979699e-02  -7.58750975e-02  -6.74522250e-02  -5.90293525e-02
  -5.06064801e-02  -4.21836076e-02  -3.37607351e-02  -2.53378627e-02
  -1.69149902e-02  -8.49211774e-03  -6.92452741e-05]
[1701  491  396  398  435  464  478  546  598  568] [ -8.42979699e-02  -7.58750975e-02  -6.74522250e-02  -5.90293525e-02
  -5.06064801e-02  -4.21836076e-02  -3.37607351e-02  -2.53378627e-02
  -1.69149902e-02  -8.49211774e-03  -6.92452741e-05]
-1.58199
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042905 minutes
Epoch 0
Fine tuning took 0.042920 minutes
Epoch 0
Fine tuning took 0.044132 minutes
{'zero': {0: [0.16502463054187191, 0.15640394088669951, 0.23152709359605911, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69088669950738912, 0.55788177339901479, 0.58128078817733986, 0.57266009852216748], 5: [0.14408866995073891, 0.2857142857142857, 0.18719211822660098, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16502463054187191, 0.16133004926108374, 0.21798029556650247, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69088669950738912, 0.55418719211822665, 0.57019704433497542, 0.49507389162561577], 5: [0.14408866995073891, 0.28448275862068967, 0.21182266009852216, 0.30049261083743845], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16502463054187191, 0.16009852216748768, 0.21182266009852216, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69088669950738912, 0.54556650246305416, 0.58004926108374388, 0.55295566502463056], 5: [0.14408866995073891, 0.29433497536945813, 0.20812807881773399, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16502463054187191, 0.15517241379310345, 0.22660098522167488, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69088669950738912, 0.56034482758620685, 0.56896551724137934, 0.53078817733990147], 5: [0.14408866995073891, 0.28448275862068967, 0.20443349753694581, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147899 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.97248
Epoch 1, cost is  2.89558
Epoch 2, cost is  2.86716
Epoch 3, cost is  2.84699
Epoch 4, cost is  2.83916
Training took 0.086004 minutes
Weight histogram
[1441 1013 1070  708  530  420  279  244  172  198] [-0.1184347  -0.10660759 -0.09478049 -0.08295339 -0.07112629 -0.05929918
 -0.04747208 -0.03564498 -0.02381788 -0.01199077 -0.00016367]
[376 280 358 449 547 700 754 823 882 906] [-0.1184347  -0.10660759 -0.09478049 -0.08295339 -0.07112629 -0.05929918
 -0.04747208 -0.03564498 -0.02381788 -0.01199077 -0.00016367]
-3.28662
3.87803
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148467 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.17554
Epoch 1, cost is  3.09245
Epoch 2, cost is  3.06203
Epoch 3, cost is  3.04518
Epoch 4, cost is  3.03908
Training took 0.088134 minutes
Weight histogram
[1242  854 1108 1184 1066  750  574  458  378  486] [-0.11274777 -0.10148936 -0.09023095 -0.07897254 -0.06771413 -0.05645572
 -0.04519731 -0.0339389  -0.02268049 -0.01142208 -0.00016367]
[ 814  566  769  952 1071  693  729  807  848  851] [-0.11274777 -0.10148936 -0.09023095 -0.07897254 -0.06771413 -0.05645572
 -0.04519731 -0.0339389  -0.02268049 -0.01142208 -0.00016367]
-3.15642
3.49961
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.69003
Epoch 1, cost is  6.25499
Epoch 2, cost is  5.2417
Epoch 3, cost is  4.46056
Epoch 4, cost is  3.99061
Training took 0.092020 minutes
Weight histogram
[ 296  712  568  624  491  458  461 1295 1092   78] [-0.06149432 -0.05536212 -0.04922992 -0.04309772 -0.03696551 -0.03083331
 -0.02470111 -0.0185689  -0.0124367  -0.0063045  -0.0001723 ]
[1654  581  373  411  460  454  497  513  587  545] [-0.06149432 -0.05536212 -0.04922992 -0.04309772 -0.03696551 -0.03083331
 -0.02470111 -0.0185689  -0.0124367  -0.0063045  -0.0001723 ]
-1.34371
1.63867
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042517 minutes
Epoch 0
Fine tuning took 0.043459 minutes
Epoch 0
Fine tuning took 0.041893 minutes
{'zero': {0: [0.15270935960591134, 0.17364532019704434, 0.12315270935960591, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.6428571428571429, 0.68719211822660098, 0.73275862068965514], 5: [0.16748768472906403, 0.18349753694581281, 0.18965517241379309, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15270935960591134, 0.16625615763546797, 0.16502463054187191, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.63916256157635465, 0.63793103448275867, 0.69581280788177335], 5: [0.16748768472906403, 0.19458128078817735, 0.19704433497536947, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15270935960591134, 0.17118226600985223, 0.14039408866995073, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.62931034482758619, 0.68103448275862066, 0.71182266009852213], 5: [0.16748768472906403, 0.19950738916256158, 0.17857142857142858, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15270935960591134, 0.16871921182266009, 0.13300492610837439, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67980295566502458, 0.62931034482758619, 0.69827586206896552, 0.6711822660098522], 5: [0.16748768472906403, 0.2019704433497537, 0.16871921182266009, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150063 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.97248
Epoch 1, cost is  2.89558
Epoch 2, cost is  2.86716
Epoch 3, cost is  2.84699
Epoch 4, cost is  2.83916
Training took 0.090053 minutes
Weight histogram
[1441 1013 1070  708  530  420  279  244  172  198] [-0.1184347  -0.10660759 -0.09478049 -0.08295339 -0.07112629 -0.05929918
 -0.04747208 -0.03564498 -0.02381788 -0.01199077 -0.00016367]
[376 280 358 449 547 700 754 823 882 906] [-0.1184347  -0.10660759 -0.09478049 -0.08295339 -0.07112629 -0.05929918
 -0.04747208 -0.03564498 -0.02381788 -0.01199077 -0.00016367]
-3.28662
3.87803
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147355 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.17554
Epoch 1, cost is  3.09245
Epoch 2, cost is  3.06203
Epoch 3, cost is  3.04518
Epoch 4, cost is  3.03908
Training took 0.088740 minutes
Weight histogram
[1242  854 1108 1184 1066  750  574  458  378  486] [-0.11274777 -0.10148936 -0.09023095 -0.07897254 -0.06771413 -0.05645572
 -0.04519731 -0.0339389  -0.02268049 -0.01142208 -0.00016367]
[ 814  566  769  952 1071  693  729  807  848  851] [-0.11274777 -0.10148936 -0.09023095 -0.07897254 -0.06771413 -0.05645572
 -0.04519731 -0.0339389  -0.02268049 -0.01142208 -0.00016367]
-3.15642
3.49961
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.58915
Epoch 1, cost is  6.18303
Epoch 2, cost is  5.13052
Epoch 3, cost is  4.17537
Epoch 4, cost is  3.48382
Training took 0.119472 minutes
Weight histogram
[ 342  781  707  690  692  997 1455  286   82   43] [-0.03940029 -0.03548017 -0.03156005 -0.02763992 -0.0237198  -0.01979968
 -0.01587956 -0.01195943 -0.00803931 -0.00411919 -0.00019906]
[1635  693  376  426  433  440  477  516  581  498] [-0.03940029 -0.03548017 -0.03156005 -0.02763992 -0.0237198  -0.01979968
 -0.01587956 -0.01195943 -0.00803931 -0.00411919 -0.00019906]
-1.25848
1.49735
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044289 minutes
Epoch 0
Fine tuning took 0.044827 minutes
Epoch 0
Fine tuning took 0.046803 minutes
{'zero': {0: [0.16625615763546797, 0.10098522167487685, 0.14778325123152711, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.75862068965517238, 0.66748768472906406, 0.73275862068965514], 5: [0.11945812807881774, 0.14039408866995073, 0.18472906403940886, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16625615763546797, 0.14532019704433496, 0.13793103448275862, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.71674876847290636, 0.69211822660098521, 0.72536945812807885], 5: [0.11945812807881774, 0.13793103448275862, 0.16995073891625614, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16625615763546797, 0.15886699507389163, 0.14778325123152711, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.7142857142857143, 0.67733990147783252, 0.71305418719211822], 5: [0.11945812807881774, 0.1268472906403941, 0.1748768472906404, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16625615763546797, 0.14285714285714285, 0.15886699507389163, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.72044334975369462, 0.68472906403940892, 0.75369458128078815], 5: [0.11945812807881774, 0.13669950738916256, 0.15640394088669951, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150287 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.07059
Epoch 1, cost is  4.91659
Epoch 2, cost is  4.77472
Epoch 3, cost is  4.65413
Epoch 4, cost is  4.54155
Training took 0.089813 minutes
Weight histogram
[736 673 601 682 643 577 331 897 803 132] [ -3.55666615e-02  -3.20078575e-02  -2.84490535e-02  -2.48902494e-02
  -2.13314454e-02  -1.77726414e-02  -1.42138373e-02  -1.06550333e-02
  -7.09622928e-03  -3.53742526e-03   2.13787716e-05]
[1450  511  567  504  477  474  506  515  522  549] [ -3.55666615e-02  -3.20078575e-02  -2.84490535e-02  -2.48902494e-02
  -2.13314454e-02  -1.77726414e-02  -1.42138373e-02  -1.06550333e-02
  -7.09622928e-03  -3.53742526e-03   2.13787716e-05]
-0.657232
0.80678
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148262 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.31854
Epoch 1, cost is  5.14117
Epoch 2, cost is  4.98807
Epoch 3, cost is  4.8579
Epoch 4, cost is  4.7427
Training took 0.088080 minutes
Weight histogram
[ 542  519  469  450  552  800  816 3377  387  188] [ -2.80236937e-02  -2.52191865e-02  -2.24146792e-02  -1.96101720e-02
  -1.68056647e-02  -1.40011575e-02  -1.11966502e-02  -8.39214297e-03
  -5.58763572e-03  -2.78312848e-03   2.13787716e-05]
[3334  983  619  440  421  433  442  453  472  503] [ -2.80236937e-02  -2.52191865e-02  -2.24146792e-02  -1.96101720e-02
  -1.68056647e-02  -1.40011575e-02  -1.11966502e-02  -8.39214297e-03
  -5.58763572e-03  -2.78312848e-03   2.13787716e-05]
-0.618081
0.780159
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.87633
Epoch 1, cost is  6.80476
Epoch 2, cost is  6.74773
Epoch 3, cost is  6.69564
Epoch 4, cost is  6.64296
Training took 0.083065 minutes
Weight histogram
[2317 1946  547  354  262  194  151  122   99   83] [ -1.41098574e-02  -1.26894365e-02  -1.12690156e-02  -9.84859464e-03
  -8.42817373e-03  -7.00775282e-03  -5.58733191e-03  -4.16691099e-03
  -2.74649008e-03  -1.32606917e-03   9.43517443e-05]
[2387 1254  874  466  406  241  131  117   99  100] [ -1.41098574e-02  -1.26894365e-02  -1.12690156e-02  -9.84859464e-03
  -8.42817373e-03  -7.00775282e-03  -5.58733191e-03  -4.16691099e-03
  -2.74649008e-03  -1.32606917e-03   9.43517443e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040350 minutes
Epoch 0
Fine tuning took 0.042082 minutes
Epoch 0
Fine tuning took 0.043617 minutes
{'zero': {0: [0.16871921182266009, 0.29064039408866993, 0.21305418719211822, 0.28325123152709358], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5073891625615764, 0.37561576354679804, 0.53201970443349755, 0.5431034482758621], 5: [0.32389162561576357, 0.33374384236453203, 0.25492610837438423, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16871921182266009, 0.30295566502463056, 0.18596059113300492, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5073891625615764, 0.35221674876847292, 0.54556650246305416, 0.51600985221674878], 5: [0.32389162561576357, 0.34482758620689657, 0.26847290640394089, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16871921182266009, 0.30541871921182268, 0.18472906403940886, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5073891625615764, 0.33990147783251229, 0.56157635467980294, 0.49507389162561577], 5: [0.32389162561576357, 0.35467980295566504, 0.2536945812807882, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16871921182266009, 0.33128078817733991, 0.22044334975369459, 0.29433497536945813], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5073891625615764, 0.32635467980295568, 0.50985221674876846, 0.52463054187192115], 5: [0.32389162561576357, 0.34236453201970446, 0.26970443349753692, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148570 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.07059
Epoch 1, cost is  4.91659
Epoch 2, cost is  4.77472
Epoch 3, cost is  4.65413
Epoch 4, cost is  4.54155
Training took 0.089379 minutes
Weight histogram
[736 673 601 682 643 577 331 897 803 132] [ -3.55666615e-02  -3.20078575e-02  -2.84490535e-02  -2.48902494e-02
  -2.13314454e-02  -1.77726414e-02  -1.42138373e-02  -1.06550333e-02
  -7.09622928e-03  -3.53742526e-03   2.13787716e-05]
[1450  511  567  504  477  474  506  515  522  549] [ -3.55666615e-02  -3.20078575e-02  -2.84490535e-02  -2.48902494e-02
  -2.13314454e-02  -1.77726414e-02  -1.42138373e-02  -1.06550333e-02
  -7.09622928e-03  -3.53742526e-03   2.13787716e-05]
-0.657232
0.80678
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149022 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.31854
Epoch 1, cost is  5.14117
Epoch 2, cost is  4.98807
Epoch 3, cost is  4.8579
Epoch 4, cost is  4.7427
Training took 0.086714 minutes
Weight histogram
[ 542  519  469  450  552  800  816 3377  387  188] [ -2.80236937e-02  -2.52191865e-02  -2.24146792e-02  -1.96101720e-02
  -1.68056647e-02  -1.40011575e-02  -1.11966502e-02  -8.39214297e-03
  -5.58763572e-03  -2.78312848e-03   2.13787716e-05]
[3334  983  619  440  421  433  442  453  472  503] [ -2.80236937e-02  -2.52191865e-02  -2.24146792e-02  -1.96101720e-02
  -1.68056647e-02  -1.40011575e-02  -1.11966502e-02  -8.39214297e-03
  -5.58763572e-03  -2.78312848e-03   2.13787716e-05]
-0.618081
0.780159
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.84722
Epoch 1, cost is  6.75726
Epoch 2, cost is  6.6937
Epoch 3, cost is  6.63787
Epoch 4, cost is  6.58144
Training took 0.092949 minutes
Weight histogram
[1953 2094  644  401  287  212  164  129  104   87] [ -1.45929288e-02  -1.31342226e-02  -1.16755164e-02  -1.02168102e-02
  -8.75810398e-03  -7.29939777e-03  -5.84069156e-03  -4.38198536e-03
  -2.92327915e-03  -1.46457294e-03  -5.86673241e-06]
[2184 1183  983  457  408  349  143  125  125  118] [ -1.45929288e-02  -1.31342226e-02  -1.16755164e-02  -1.02168102e-02
  -8.75810398e-03  -7.29939777e-03  -5.84069156e-03  -4.38198536e-03
  -2.92327915e-03  -1.46457294e-03  -5.86673241e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043161 minutes
Epoch 0
Fine tuning took 0.042898 minutes
Epoch 0
Fine tuning took 0.043339 minutes
{'zero': {0: [0.1748768472906404, 0.2857142857142857, 0.23029556650246305, 0.37438423645320196], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49384236453201968, 0.34236453201970446, 0.54433497536945807, 0.43472906403940886], 5: [0.33128078817733991, 0.37192118226600984, 0.22536945812807882, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1748768472906404, 0.30541871921182268, 0.27463054187192121, 0.34852216748768472], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49384236453201968, 0.34113300492610837, 0.51354679802955661, 0.47536945812807879], 5: [0.33128078817733991, 0.35344827586206895, 0.21182266009852216, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1748768472906404, 0.30295566502463056, 0.23891625615763548, 0.33374384236453203], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49384236453201968, 0.3682266009852217, 0.56527093596059108, 0.48522167487684731], 5: [0.33128078817733991, 0.3288177339901478, 0.19581280788177341, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1748768472906404, 0.27832512315270935, 0.27586206896551724, 0.36699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49384236453201968, 0.39408866995073893, 0.51600985221674878, 0.44581280788177341], 5: [0.33128078817733991, 0.32758620689655171, 0.20812807881773399, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148886 minutes
Weight histogram
[  42  169  337  486  664  425 1211 1828  843   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 112  123  165  212  288  396  726  800 1379 1874] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-0.995879
0.787248
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.07059
Epoch 1, cost is  4.91659
Epoch 2, cost is  4.77472
Epoch 3, cost is  4.65413
Epoch 4, cost is  4.54155
Training took 0.091570 minutes
Weight histogram
[736 673 601 682 643 577 331 897 803 132] [ -3.55666615e-02  -3.20078575e-02  -2.84490535e-02  -2.48902494e-02
  -2.13314454e-02  -1.77726414e-02  -1.42138373e-02  -1.06550333e-02
  -7.09622928e-03  -3.53742526e-03   2.13787716e-05]
[1450  511  567  504  477  474  506  515  522  549] [ -3.55666615e-02  -3.20078575e-02  -2.84490535e-02  -2.48902494e-02
  -2.13314454e-02  -1.77726414e-02  -1.42138373e-02  -1.06550333e-02
  -7.09622928e-03  -3.53742526e-03   2.13787716e-05]
-0.657232
0.80678
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149033 minutes
Weight histogram
[  39  146  572  796 1211 1395 1355  653 1094  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 229  246  347  402  620  898 1179  823 1425 1931] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.09266
0.928317
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.31854
Epoch 1, cost is  5.14117
Epoch 2, cost is  4.98807
Epoch 3, cost is  4.8579
Epoch 4, cost is  4.7427
Training took 0.086636 minutes
Weight histogram
[ 542  519  469  450  552  800  816 3377  387  188] [ -2.80236937e-02  -2.52191865e-02  -2.24146792e-02  -1.96101720e-02
  -1.68056647e-02  -1.40011575e-02  -1.11966502e-02  -8.39214297e-03
  -5.58763572e-03  -2.78312848e-03   2.13787716e-05]
[3334  983  619  440  421  433  442  453  472  503] [ -2.80236937e-02  -2.52191865e-02  -2.24146792e-02  -1.96101720e-02
  -1.68056647e-02  -1.40011575e-02  -1.11966502e-02  -8.39214297e-03
  -5.58763572e-03  -2.78312848e-03   2.13787716e-05]
-0.618081
0.780159
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.76336
Epoch 1, cost is  6.62264
Epoch 2, cost is  6.54345
Epoch 3, cost is  6.47473
Epoch 4, cost is  6.41525
Training took 0.120416 minutes
Weight histogram
[ 527  900 2122  945  537  357  252  186  140  109] [ -1.79279987e-02  -1.61397788e-02  -1.43515589e-02  -1.25633390e-02
  -1.07751191e-02  -8.98689917e-03  -7.19867927e-03  -5.41045936e-03
  -3.62223946e-03  -1.83401956e-03  -4.57996575e-05]
[1922 1058  912  623  420  391  274  160  152  163] [ -1.79279987e-02  -1.61397788e-02  -1.43515589e-02  -1.25633390e-02
  -1.07751191e-02  -8.98689917e-03  -7.19867927e-03  -5.41045936e-03
  -3.62223946e-03  -1.83401956e-03  -4.57996575e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045172 minutes
Epoch 0
Fine tuning took 0.047123 minutes
Epoch 0
Fine tuning took 0.047191 minutes
{'zero': {0: [0.17733990147783252, 0.27709359605911332, 0.3288177339901478, 0.32266009852216748], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.40147783251231528, 0.49384236453201968, 0.46182266009852219], 5: [0.34975369458128081, 0.32142857142857145, 0.17733990147783252, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17733990147783252, 0.27955665024630544, 0.32635467980295568, 0.30541871921182268], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.39285714285714285, 0.51724137931034486, 0.50862068965517238], 5: [0.34975369458128081, 0.32758620689655171, 0.15640394088669951, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17733990147783252, 0.30172413793103448, 0.30172413793103448, 0.31650246305418717], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.39408866995073893, 0.52339901477832518, 0.47536945812807879], 5: [0.34975369458128081, 0.30418719211822659, 0.1748768472906404, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17733990147783252, 0.29556650246305421, 0.33251231527093594, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.39778325123152708, 0.46798029556650245, 0.48891625615763545], 5: [0.34975369458128081, 0.30665024630541871, 0.19950738916256158, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149709 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.84407
Epoch 1, cost is  8.61008
Epoch 2, cost is  8.81431
Epoch 3, cost is  9.12092
Epoch 4, cost is  9.46154
Training took 0.086922 minutes
Weight histogram
[ 874  913  849  962  823 1060 1058  830  592  139] [-0.69593436 -0.62654234 -0.55715032 -0.4877583  -0.41836628 -0.34897426
 -0.27958225 -0.21019023 -0.14079821 -0.07140619 -0.00201417]
[365 677 810 932 896 926 896 894 862 842] [-0.69593436 -0.62654234 -0.55715032 -0.4877583  -0.41836628 -0.34897426
 -0.27958225 -0.21019023 -0.14079821 -0.07140619 -0.00201417]
-20.794
29.7299
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148653 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.87565
Epoch 1, cost is  9.70533
Epoch 2, cost is  9.96352
Epoch 3, cost is  10.303
Epoch 4, cost is  10.7042
Training took 0.086689 minutes
Weight histogram
[ 815  842  809 1003  953  940 1345 1792 1273  353] [-0.72696084 -0.65446617 -0.58197151 -0.50947684 -0.43698217 -0.36448751
 -0.29199284 -0.21949817 -0.14700351 -0.07450884 -0.00201417]
[ 828 1473 1649  897  862  927  923  892  846  828] [-0.72696084 -0.65446617 -0.58197151 -0.50947684 -0.43698217 -0.36448751
 -0.29199284 -0.21949817 -0.14700351 -0.07450884 -0.00201417]
-26.2941
25.9586
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.09696
Epoch 1, cost is  4.41067
Epoch 2, cost is  4.78629
Epoch 3, cost is  5.27309
Epoch 4, cost is  5.86922
Training took 0.083556 minutes
Weight histogram
[ 832 1311 1346 1172 1065  916  544  355  236  323] [-0.29092243 -0.26199895 -0.23307547 -0.20415199 -0.17522851 -0.14630503
 -0.11738155 -0.08845807 -0.05953459 -0.03061111 -0.00168763]
[ 476  418  576  756  852  963  983 1019 1020 1037] [-0.29092243 -0.26199895 -0.23307547 -0.20415199 -0.17522851 -0.14630503
 -0.11738155 -0.08845807 -0.05953459 -0.03061111 -0.00168763]
-11.6165
13.7547
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041242 minutes
Epoch 0
Fine tuning took 0.042358 minutes
Epoch 0
Fine tuning took 0.042198 minutes
{'zero': {0: [0.18719211822660098, 0.20073891625615764, 0.3251231527093596, 0.29187192118226601], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.27216748768472904, 0.40640394088669951, 0.42857142857142855], 5: [0.23891625615763548, 0.52709359605911332, 0.26847290640394089, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18719211822660098, 0.19088669950738915, 0.19950738916256158, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.59113300492610843, 0.61330049261083741, 0.58004926108374388], 5: [0.23891625615763548, 0.21798029556650247, 0.18719211822660098, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18719211822660098, 0.19704433497536947, 0.2105911330049261, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.61330049261083741, 0.60098522167487689, 0.61083743842364535], 5: [0.23891625615763548, 0.18965517241379309, 0.18842364532019704, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18719211822660098, 0.18965517241379309, 0.17857142857142858, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.59113300492610843, 0.58990147783251234, 0.64039408866995073], 5: [0.23891625615763548, 0.21921182266009853, 0.23152709359605911, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148065 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.84407
Epoch 1, cost is  8.61008
Epoch 2, cost is  8.81431
Epoch 3, cost is  9.12092
Epoch 4, cost is  9.46154
Training took 0.089650 minutes
Weight histogram
[ 874  913  849  962  823 1060 1058  830  592  139] [-0.69593436 -0.62654234 -0.55715032 -0.4877583  -0.41836628 -0.34897426
 -0.27958225 -0.21019023 -0.14079821 -0.07140619 -0.00201417]
[365 677 810 932 896 926 896 894 862 842] [-0.69593436 -0.62654234 -0.55715032 -0.4877583  -0.41836628 -0.34897426
 -0.27958225 -0.21019023 -0.14079821 -0.07140619 -0.00201417]
-20.794
29.7299
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149664 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.87565
Epoch 1, cost is  9.70533
Epoch 2, cost is  9.96352
Epoch 3, cost is  10.303
Epoch 4, cost is  10.7042
Training took 0.086904 minutes
Weight histogram
[ 815  842  809 1003  953  940 1345 1792 1273  353] [-0.72696084 -0.65446617 -0.58197151 -0.50947684 -0.43698217 -0.36448751
 -0.29199284 -0.21949817 -0.14700351 -0.07450884 -0.00201417]
[ 828 1473 1649  897  862  927  923  892  846  828] [-0.72696084 -0.65446617 -0.58197151 -0.50947684 -0.43698217 -0.36448751
 -0.29199284 -0.21949817 -0.14700351 -0.07450884 -0.00201417]
-26.2941
25.9586
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.76172
Epoch 1, cost is  3.68178
Epoch 2, cost is  3.81057
Epoch 3, cost is  4.13768
Epoch 4, cost is  4.55285
Training took 0.091215 minutes
Weight histogram
[ 798 1224 1059 1203 1090  958  796  419  231  322] [-0.23938569 -0.21563268 -0.19187966 -0.16812664 -0.14437362 -0.1206206
 -0.09686758 -0.07311456 -0.04936154 -0.02560852 -0.00185551]
[ 513  487  649  748  857  944  975 1013  993  921] [-0.23938569 -0.21563268 -0.19187966 -0.16812664 -0.14437362 -0.1206206
 -0.09686758 -0.07311456 -0.04936154 -0.02560852 -0.00185551]
-7.90666
8.85636
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043811 minutes
Epoch 0
Fine tuning took 0.042510 minutes
Epoch 0
Fine tuning took 0.042499 minutes
{'zero': {0: [0.21921182266009853, 0.2413793103448276, 0.30172413793103448, 0.22783251231527094], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62315270935960587, 0.21551724137931033, 0.39655172413793105, 0.43719211822660098], 5: [0.15763546798029557, 0.5431034482758621, 0.30172413793103448, 0.33497536945812806], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21921182266009853, 0.18719211822660098, 0.17733990147783252, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62315270935960587, 0.63177339901477836, 0.66379310344827591, 0.63054187192118227], 5: [0.15763546798029557, 0.18103448275862069, 0.15886699507389163, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21921182266009853, 0.17364532019704434, 0.20320197044334976, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62315270935960587, 0.63793103448275867, 0.64408866995073888, 0.64162561576354682], 5: [0.15763546798029557, 0.18842364532019704, 0.15270935960591134, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21921182266009853, 0.10714285714285714, 0.14408866995073891, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62315270935960587, 0.68103448275862066, 0.70197044334975367, 0.63916256157635465], 5: [0.15763546798029557, 0.21182266009852216, 0.1539408866995074, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.154008 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.84407
Epoch 1, cost is  8.61008
Epoch 2, cost is  8.81431
Epoch 3, cost is  9.12092
Epoch 4, cost is  9.46154
Training took 0.089271 minutes
Weight histogram
[ 874  913  849  962  823 1060 1058  830  592  139] [-0.69593436 -0.62654234 -0.55715032 -0.4877583  -0.41836628 -0.34897426
 -0.27958225 -0.21019023 -0.14079821 -0.07140619 -0.00201417]
[365 677 810 932 896 926 896 894 862 842] [-0.69593436 -0.62654234 -0.55715032 -0.4877583  -0.41836628 -0.34897426
 -0.27958225 -0.21019023 -0.14079821 -0.07140619 -0.00201417]
-20.794
29.7299
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149289 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.87565
Epoch 1, cost is  9.70533
Epoch 2, cost is  9.96352
Epoch 3, cost is  10.303
Epoch 4, cost is  10.7042
Training took 0.090425 minutes
Weight histogram
[ 815  842  809 1003  953  940 1345 1792 1273  353] [-0.72696084 -0.65446617 -0.58197151 -0.50947684 -0.43698217 -0.36448751
 -0.29199284 -0.21949817 -0.14700351 -0.07450884 -0.00201417]
[ 828 1473 1649  897  862  927  923  892  846  828] [-0.72696084 -0.65446617 -0.58197151 -0.50947684 -0.43698217 -0.36448751
 -0.29199284 -0.21949817 -0.14700351 -0.07450884 -0.00201417]
-26.2941
25.9586
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.49948
Epoch 1, cost is  2.77049
Epoch 2, cost is  2.53146
Epoch 3, cost is  2.50862
Epoch 4, cost is  2.62695
Training took 0.118185 minutes
Weight histogram
[ 917 1308 1173 1129 1025  924  679  470  278  197] [-0.14670672 -0.13221754 -0.11772837 -0.1032392  -0.08875003 -0.07426085
 -0.05977168 -0.04528251 -0.03079334 -0.01630417 -0.00181499]
[ 562  448  611  733  832  949  969 1005 1056  935] [-0.14670672 -0.13221754 -0.11772837 -0.1032392  -0.08875003 -0.07426085
 -0.05977168 -0.04528251 -0.03079334 -0.01630417 -0.00181499]
-5.96692
6.96092
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046584 minutes
Epoch 0
Fine tuning took 0.044088 minutes
Epoch 0
Fine tuning took 0.045517 minutes
{'zero': {0: [0.13669950738916256, 0.31280788177339902, 0.39285714285714285, 0.61206896551724133], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.43472906403940886, 0.47290640394088668, 0.29433497536945813], 5: [0.15886699507389163, 0.25246305418719212, 0.13423645320197045, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13669950738916256, 0.18103448275862069, 0.15270935960591134, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.65886699507389157, 0.61330049261083741, 0.63054187192118227], 5: [0.15886699507389163, 0.16009852216748768, 0.23399014778325122, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13669950738916256, 0.17980295566502463, 0.18842364532019704, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.66995073891625612, 0.64532019704433496, 0.70812807881773399], 5: [0.15886699507389163, 0.15024630541871922, 0.16625615763546797, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13669950738916256, 0.20320197044334976, 0.20320197044334976, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.67364532019704437, 0.5357142857142857, 0.64655172413793105], 5: [0.15886699507389163, 0.12315270935960591, 0.26108374384236455, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150638 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.07569
Epoch 1, cost is  3.01875
Epoch 2, cost is  3.00348
Epoch 3, cost is  3.00931
Epoch 4, cost is  3.02227
Training took 0.086644 minutes
Weight histogram
[1499 1620 1311 1170  794  543  413  307  227  216] [-0.13630638 -0.1226921  -0.10907783 -0.09546356 -0.08184929 -0.06823502
 -0.05462075 -0.04100648 -0.02739221 -0.01377794 -0.00016367]
[ 439  369  506  658  858  942 1079 1103 1070 1076] [-0.13630638 -0.1226921  -0.10907783 -0.09546356 -0.08184929 -0.06823502
 -0.05462075 -0.04100648 -0.02739221 -0.01377794 -0.00016367]
-4.12259
5.09713
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147862 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.23615
Epoch 1, cost is  3.17898
Epoch 2, cost is  3.17822
Epoch 3, cost is  3.18351
Epoch 4, cost is  3.19569
Training took 0.086348 minutes
Weight histogram
[1452 1418 1216 1169 1310 1240  742  574  462  542] [-0.13098727 -0.11790491 -0.10482255 -0.09174019 -0.07865783 -0.06557547
 -0.05249311 -0.03941075 -0.02632839 -0.01324603 -0.00016367]
[ 935  769 1082 1337  855  914 1038 1045 1066 1084] [-0.13098727 -0.11790491 -0.10482255 -0.09174019 -0.07865783 -0.06557547
 -0.05249311 -0.03941075 -0.02632839 -0.01324603 -0.00016367]
-3.66207
4.20152
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.73459
Epoch 1, cost is  6.29497
Epoch 2, cost is  5.43076
Epoch 3, cost is  4.80044
Epoch 4, cost is  4.46231
Training took 0.083611 minutes
Weight histogram
[ 241  828  843  576  711  590  614  729 2779  189] [ -8.42979699e-02  -7.58748007e-02  -6.74516315e-02  -5.90284623e-02
  -5.06052931e-02  -4.21821239e-02  -3.37589547e-02  -2.53357855e-02
  -1.69126162e-02  -8.48944704e-03  -6.62778257e-05]
[2249  661  530  534  585  624  642  734  798  743] [ -8.42979699e-02  -7.58748007e-02  -6.74516315e-02  -5.90284623e-02
  -5.06052931e-02  -4.21821239e-02  -3.37589547e-02  -2.53357855e-02
  -1.69126162e-02  -8.48944704e-03  -6.62778257e-05]
-1.58199
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040567 minutes
Epoch 0
Fine tuning took 0.041370 minutes
Epoch 0
Fine tuning took 0.042102 minutes
{'zero': {0: [0.20566502463054187, 0.19950738916256158, 0.19458128078817735, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.6145320197044335, 0.62561576354679804, 0.73522167487684731], 5: [0.15886699507389163, 0.18596059113300492, 0.17980295566502463, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20566502463054187, 0.20812807881773399, 0.21798029556650247, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.61945812807881773, 0.59359605911330049, 0.66256157635467983], 5: [0.15886699507389163, 0.17241379310344829, 0.18842364532019704, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20566502463054187, 0.22413793103448276, 0.18103448275862069, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.60098522167487689, 0.65640394088669951, 0.71674876847290636], 5: [0.15886699507389163, 0.1748768472906404, 0.1625615763546798, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20566502463054187, 0.22167487684729065, 0.20689655172413793, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.58004926108374388, 0.5788177339901478, 0.67487684729064035], 5: [0.15886699507389163, 0.19827586206896552, 0.21428571428571427, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148549 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.07569
Epoch 1, cost is  3.01875
Epoch 2, cost is  3.00348
Epoch 3, cost is  3.00931
Epoch 4, cost is  3.02227
Training took 0.086050 minutes
Weight histogram
[1499 1620 1311 1170  794  543  413  307  227  216] [-0.13630638 -0.1226921  -0.10907783 -0.09546356 -0.08184929 -0.06823502
 -0.05462075 -0.04100648 -0.02739221 -0.01377794 -0.00016367]
[ 439  369  506  658  858  942 1079 1103 1070 1076] [-0.13630638 -0.1226921  -0.10907783 -0.09546356 -0.08184929 -0.06823502
 -0.05462075 -0.04100648 -0.02739221 -0.01377794 -0.00016367]
-4.12259
5.09713
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147790 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.23615
Epoch 1, cost is  3.17898
Epoch 2, cost is  3.17822
Epoch 3, cost is  3.18351
Epoch 4, cost is  3.19569
Training took 0.087907 minutes
Weight histogram
[1452 1418 1216 1169 1310 1240  742  574  462  542] [-0.13098727 -0.11790491 -0.10482255 -0.09174019 -0.07865783 -0.06557547
 -0.05249311 -0.03941075 -0.02632839 -0.01324603 -0.00016367]
[ 935  769 1082 1337  855  914 1038 1045 1066 1084] [-0.13098727 -0.11790491 -0.10482255 -0.09174019 -0.07865783 -0.06557547
 -0.05249311 -0.03941075 -0.02632839 -0.01324603 -0.00016367]
-3.66207
4.20152
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.69596
Epoch 1, cost is  6.26026
Epoch 2, cost is  5.22889
Epoch 3, cost is  4.4581
Epoch 4, cost is  4.00011
Training took 0.093452 minutes
Weight histogram
[ 302  987  740  845  678  619  597 1666 1560  106] [-0.06149432 -0.05536212 -0.04922992 -0.04309772 -0.03696551 -0.03083331
 -0.02470111 -0.0185689  -0.0124367  -0.0063045  -0.0001723 ]
[2191  782  497  550  615  611  671  687  789  707] [-0.06149432 -0.05536212 -0.04922992 -0.04309772 -0.03696551 -0.03083331
 -0.02470111 -0.0185689  -0.0124367  -0.0063045  -0.0001723 ]
-1.34818
1.63867
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044142 minutes
Epoch 0
Fine tuning took 0.042526 minutes
Epoch 0
Fine tuning took 0.045076 minutes
{'zero': {0: [0.21182266009852216, 0.19827586206896552, 0.18349753694581281, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6785714285714286, 0.67487684729064035, 0.65024630541871919, 0.72906403940886699], 5: [0.10960591133004927, 0.1268472906403941, 0.16625615763546797, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21182266009852216, 0.19334975369458129, 0.19458128078817735, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6785714285714286, 0.67610837438423643, 0.62684729064039413, 0.70443349753694584], 5: [0.10960591133004927, 0.13054187192118227, 0.17857142857142858, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21182266009852216, 0.20073891625615764, 0.17733990147783252, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6785714285714286, 0.64778325123152714, 0.66625615763546797, 0.66871921182266014], 5: [0.10960591133004927, 0.15147783251231528, 0.15640394088669951, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21182266009852216, 0.24753694581280788, 0.18226600985221675, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6785714285714286, 0.61576354679802958, 0.65394088669950734, 0.72536945812807885], 5: [0.10960591133004927, 0.13669950738916256, 0.16379310344827586, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150089 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.07569
Epoch 1, cost is  3.01875
Epoch 2, cost is  3.00348
Epoch 3, cost is  3.00931
Epoch 4, cost is  3.02227
Training took 0.086551 minutes
Weight histogram
[1499 1620 1311 1170  794  543  413  307  227  216] [-0.13630638 -0.1226921  -0.10907783 -0.09546356 -0.08184929 -0.06823502
 -0.05462075 -0.04100648 -0.02739221 -0.01377794 -0.00016367]
[ 439  369  506  658  858  942 1079 1103 1070 1076] [-0.13630638 -0.1226921  -0.10907783 -0.09546356 -0.08184929 -0.06823502
 -0.05462075 -0.04100648 -0.02739221 -0.01377794 -0.00016367]
-4.12259
5.09713
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149985 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.23615
Epoch 1, cost is  3.17898
Epoch 2, cost is  3.17822
Epoch 3, cost is  3.18351
Epoch 4, cost is  3.19569
Training took 0.086539 minutes
Weight histogram
[1452 1418 1216 1169 1310 1240  742  574  462  542] [-0.13098727 -0.11790491 -0.10482255 -0.09174019 -0.07865783 -0.06557547
 -0.05249311 -0.03941075 -0.02632839 -0.01324603 -0.00016367]
[ 935  769 1082 1337  855  914 1038 1045 1066 1084] [-0.13098727 -0.11790491 -0.10482255 -0.09174019 -0.07865783 -0.06557547
 -0.05249311 -0.03941075 -0.02632839 -0.01324603 -0.00016367]
-3.66207
4.20152
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.59714
Epoch 1, cost is  6.19019
Epoch 2, cost is  5.13181
Epoch 3, cost is  4.19884
Epoch 4, cost is  3.52157
Training took 0.119859 minutes
Weight histogram
[ 342 1017  997  959  915 1322 1955  423  112   58] [-0.03940029 -0.03548017 -0.03156005 -0.02763992 -0.0237198  -0.01979968
 -0.01587956 -0.01195943 -0.00803931 -0.00411919 -0.00019906]
[2168  929  503  575  584  595  645  694  781  626] [-0.03940029 -0.03548017 -0.03156005 -0.02763992 -0.0237198  -0.01979968
 -0.01587956 -0.01195943 -0.00803931 -0.00411919 -0.00019906]
-1.26809
1.49735
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047298 minutes
Epoch 0
Fine tuning took 0.046140 minutes
Epoch 0
Fine tuning took 0.045708 minutes
{'zero': {0: [0.22536945812807882, 0.093596059113300489, 0.17857142857142858, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.75492610837438423, 0.6785714285714286, 0.72044334975369462], 5: [0.16133004926108374, 0.15147783251231528, 0.14285714285714285, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22536945812807882, 0.14778325123152711, 0.13669950738916256, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.73645320197044339, 0.71059113300492616, 0.75615763546798032], 5: [0.16133004926108374, 0.11576354679802955, 0.15270935960591134, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22536945812807882, 0.16009852216748768, 0.1625615763546798, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.72167487684729059, 0.68842364532019706, 0.71551724137931039], 5: [0.16133004926108374, 0.11822660098522167, 0.14901477832512317, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22536945812807882, 0.1539408866995074, 0.14655172413793102, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.75, 0.73399014778325122, 0.75492610837438423], 5: [0.16133004926108374, 0.096059113300492605, 0.11945812807881774, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148485 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.50896
Epoch 1, cost is  4.41315
Epoch 2, cost is  4.32278
Epoch 3, cost is  4.23998
Epoch 4, cost is  4.17533
Training took 0.090052 minutes
Weight histogram
[ 980 1106  940  791  733  837  639  413 1476  185] [ -4.41550873e-02  -3.97374407e-02  -3.53197941e-02  -3.09021475e-02
  -2.64845009e-02  -2.20668543e-02  -1.76492077e-02  -1.32315611e-02
  -8.81391445e-03  -4.39626784e-03   2.13787716e-05]
[1608  742  688  637  658  687  697  745  792  846] [ -4.41550873e-02  -3.97374407e-02  -3.53197941e-02  -3.09021475e-02
  -2.64845009e-02  -2.20668543e-02  -1.76492077e-02  -1.32315611e-02
  -8.81391445e-03  -4.39626784e-03   2.13787716e-05]
-0.82316
0.974496
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150359 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.69489
Epoch 1, cost is  4.59523
Epoch 2, cost is  4.50298
Epoch 3, cost is  4.42041
Epoch 4, cost is  4.34791
Training took 0.086660 minutes
Weight histogram
[ 850  765  767  689  608  672  998 1983 2507  286] [ -3.74615900e-02  -3.37132931e-02  -2.99649963e-02  -2.62166994e-02
  -2.24684025e-02  -1.87201056e-02  -1.49718087e-02  -1.12235119e-02
  -7.47521499e-03  -3.72691811e-03   2.13787716e-05]
[3694 1118  612  574  609  617  672  706  743  780] [ -3.74615900e-02  -3.37132931e-02  -2.99649963e-02  -2.62166994e-02
  -2.24684025e-02  -1.87201056e-02  -1.49718087e-02  -1.12235119e-02
  -7.47521499e-03  -3.72691811e-03   2.13787716e-05]
-0.738221
0.996551
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.88394
Epoch 1, cost is  6.82306
Epoch 2, cost is  6.77506
Epoch 3, cost is  6.73075
Epoch 4, cost is  6.68729
Training took 0.080833 minutes
Weight histogram
[3014 2471  803  516  374  279  214  173  139  117] [ -1.41098574e-02  -1.26892624e-02  -1.12686673e-02  -9.84807233e-03
  -8.42747732e-03  -7.00688230e-03  -5.58628728e-03  -4.16569227e-03
  -2.74509725e-03  -1.32450224e-03   9.60927791e-05]
[3834 1830  876  466  406  241  131  116  100  100] [ -1.41098574e-02  -1.26892624e-02  -1.12686673e-02  -9.84807233e-03
  -8.42747732e-03  -7.00688230e-03  -5.58628728e-03  -4.16569227e-03
  -2.74509725e-03  -1.32450224e-03   9.60927791e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040371 minutes
Epoch 0
Fine tuning took 0.044079 minutes
Epoch 0
Fine tuning took 0.040709 minutes
{'zero': {0: [0.29433497536945813, 0.35714285714285715, 0.2894088669950739, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.46305418719211822, 0.52216748768472909, 0.40024630541871919], 5: [0.16871921182266009, 0.17980295566502463, 0.18842364532019704, 0.4039408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29433497536945813, 0.37315270935960593, 0.29556650246305421, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.45443349753694579, 0.52832512315270941, 0.41379310344827586], 5: [0.16871921182266009, 0.17241379310344829, 0.17610837438423646, 0.37931034482758619], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29433497536945813, 0.37561576354679804, 0.27339901477832512, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.44704433497536944, 0.56403940886699511, 0.38423645320197042], 5: [0.16871921182266009, 0.17733990147783252, 0.1625615763546798, 0.43103448275862066], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29433497536945813, 0.36945812807881773, 0.2857142857142857, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.44704433497536944, 0.5357142857142857, 0.4039408866995074], 5: [0.16871921182266009, 0.18349753694581281, 0.17857142857142858, 0.39285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151135 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.50896
Epoch 1, cost is  4.41315
Epoch 2, cost is  4.32278
Epoch 3, cost is  4.23998
Epoch 4, cost is  4.17533
Training took 0.086710 minutes
Weight histogram
[ 980 1106  940  791  733  837  639  413 1476  185] [ -4.41550873e-02  -3.97374407e-02  -3.53197941e-02  -3.09021475e-02
  -2.64845009e-02  -2.20668543e-02  -1.76492077e-02  -1.32315611e-02
  -8.81391445e-03  -4.39626784e-03   2.13787716e-05]
[1608  742  688  637  658  687  697  745  792  846] [ -4.41550873e-02  -3.97374407e-02  -3.53197941e-02  -3.09021475e-02
  -2.64845009e-02  -2.20668543e-02  -1.76492077e-02  -1.32315611e-02
  -8.81391445e-03  -4.39626784e-03   2.13787716e-05]
-0.82316
0.974496
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148111 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.69489
Epoch 1, cost is  4.59523
Epoch 2, cost is  4.50298
Epoch 3, cost is  4.42041
Epoch 4, cost is  4.34791
Training took 0.089415 minutes
Weight histogram
[ 850  765  767  689  608  672  998 1983 2507  286] [ -3.74615900e-02  -3.37132931e-02  -2.99649963e-02  -2.62166994e-02
  -2.24684025e-02  -1.87201056e-02  -1.49718087e-02  -1.12235119e-02
  -7.47521499e-03  -3.72691811e-03   2.13787716e-05]
[3694 1118  612  574  609  617  672  706  743  780] [ -3.74615900e-02  -3.37132931e-02  -2.99649963e-02  -2.62166994e-02
  -2.24684025e-02  -1.87201056e-02  -1.49718087e-02  -1.12235119e-02
  -7.47521499e-03  -3.72691811e-03   2.13787716e-05]
-0.738221
0.996551
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8589
Epoch 1, cost is  6.78292
Epoch 2, cost is  6.72967
Epoch 3, cost is  6.68403
Epoch 4, cost is  6.63838
Training took 0.092448 minutes
Weight histogram
[2342 2819  951  585  413  305  233  183  147  122] [ -1.45929288e-02  -1.31340252e-02  -1.16751216e-02  -1.02162180e-02
  -8.75731436e-03  -7.29841074e-03  -5.83950713e-03  -4.38060352e-03
  -2.92169990e-03  -1.46279629e-03  -3.89267780e-06]
[3506 1886  983  457  408  349  143  125  125  118] [ -1.45929288e-02  -1.31340252e-02  -1.16751216e-02  -1.02162180e-02
  -8.75731436e-03  -7.29841074e-03  -5.83950713e-03  -4.38060352e-03
  -2.92169990e-03  -1.46279629e-03  -3.89267780e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044699 minutes
Epoch 0
Fine tuning took 0.045114 minutes
Epoch 0
Fine tuning took 0.044303 minutes
{'zero': {0: [0.29187192118226601, 0.33497536945812806, 0.28448275862068967, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52339901477832518, 0.45812807881773399, 0.49384236453201968, 0.36945812807881773], 5: [0.18472906403940886, 0.20689655172413793, 0.22167487684729065, 0.35837438423645318], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29187192118226601, 0.33128078817733991, 0.26108374384236455, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52339901477832518, 0.47536945812807879, 0.53201970443349755, 0.37438423645320196], 5: [0.18472906403940886, 0.19334975369458129, 0.20689655172413793, 0.35837438423645318], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29187192118226601, 0.33743842364532017, 0.26231527093596058, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52339901477832518, 0.47783251231527096, 0.55049261083743839, 0.35837438423645318], 5: [0.18472906403940886, 0.18472906403940886, 0.18719211822660098, 0.39285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29187192118226601, 0.33128078817733991, 0.29433497536945813, 0.29187192118226601], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52339901477832518, 0.46921182266009853, 0.46798029556650245, 0.34236453201970446], 5: [0.18472906403940886, 0.19950738916256158, 0.2376847290640394, 0.36576354679802958], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148546 minutes
Weight histogram
[  42  169  337  486  664  480 1887 2966  999   70] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
[ 118  133  186  221  355  502  831 1006 1788 2960] [ -3.91909212e-04  -1.48318621e-04   9.52719711e-05   3.38862563e-04
   5.82453154e-04   8.26043746e-04   1.06963434e-03   1.31322493e-03
   1.55681552e-03   1.80040611e-03   2.04399670e-03]
-1.4104
0.846529
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.50896
Epoch 1, cost is  4.41315
Epoch 2, cost is  4.32278
Epoch 3, cost is  4.23998
Epoch 4, cost is  4.17533
Training took 0.086687 minutes
Weight histogram
[ 980 1106  940  791  733  837  639  413 1476  185] [ -4.41550873e-02  -3.97374407e-02  -3.53197941e-02  -3.09021475e-02
  -2.64845009e-02  -2.20668543e-02  -1.76492077e-02  -1.32315611e-02
  -8.81391445e-03  -4.39626784e-03   2.13787716e-05]
[1608  742  688  637  658  687  697  745  792  846] [ -4.41550873e-02  -3.97374407e-02  -3.53197941e-02  -3.09021475e-02
  -2.64845009e-02  -2.20668543e-02  -1.76492077e-02  -1.32315611e-02
  -8.81391445e-03  -4.39626784e-03   2.13787716e-05]
-0.82316
0.974496
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149944 minutes
Weight histogram
[  39  146  589  822 1442 2043 2113  984 1108  839] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
[ 243  275  377  456  731 1113 1102 1045 1897 2886] [ -3.91909212e-04  -1.77830242e-04   3.62487277e-05   2.50327698e-04
   4.64406668e-04   6.78485638e-04   8.92564608e-04   1.10664358e-03
   1.32072255e-03   1.53480152e-03   1.74888049e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.69489
Epoch 1, cost is  4.59523
Epoch 2, cost is  4.50298
Epoch 3, cost is  4.42041
Epoch 4, cost is  4.34791
Training took 0.086857 minutes
Weight histogram
[ 850  765  767  689  608  672  998 1983 2507  286] [ -3.74615900e-02  -3.37132931e-02  -2.99649963e-02  -2.62166994e-02
  -2.24684025e-02  -1.87201056e-02  -1.49718087e-02  -1.12235119e-02
  -7.47521499e-03  -3.72691811e-03   2.13787716e-05]
[3694 1118  612  574  609  617  672  706  743  780] [ -3.74615900e-02  -3.37132931e-02  -2.99649963e-02  -2.62166994e-02
  -2.24684025e-02  -1.87201056e-02  -1.49718087e-02  -1.12235119e-02
  -7.47521499e-03  -3.72691811e-03   2.13787716e-05]
-0.738221
0.996551
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78659
Epoch 1, cost is  6.66798
Epoch 2, cost is  6.60393
Epoch 3, cost is  6.54929
Epoch 4, cost is  6.50322
Training took 0.117020 minutes
Weight histogram
[ 527  900 2959 1427  793  517  361  265  198  153] [ -1.79279987e-02  -1.61396231e-02  -1.43512475e-02  -1.25628719e-02
  -1.07744964e-02  -8.98612078e-03  -7.19774520e-03  -5.40936962e-03
  -3.62099404e-03  -1.83261846e-03  -4.42428791e-05]
[3090 1722 1105  623  420  391  274  160  152  163] [ -1.79279987e-02  -1.61396231e-02  -1.43512475e-02  -1.25628719e-02
  -1.07744964e-02  -8.98612078e-03  -7.19774520e-03  -5.40936962e-03
  -3.62099404e-03  -1.83261846e-03  -4.42428791e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043694 minutes
Epoch 0
Fine tuning took 0.046244 minutes
Epoch 0
Fine tuning took 0.046197 minutes
{'zero': {0: [0.27339901477832512, 0.3460591133004926, 0.19704433497536947, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.49137931034482757, 0.6280788177339901, 0.40763546798029554], 5: [0.19458128078817735, 0.1625615763546798, 0.1748768472906404, 0.35098522167487683], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27339901477832512, 0.32019704433497537, 0.2105911330049261, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.48275862068965519, 0.61699507389162567, 0.41009852216748771], 5: [0.19458128078817735, 0.19704433497536947, 0.17241379310344829, 0.35960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27339901477832512, 0.30049261083743845, 0.21921182266009853, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.51970443349753692, 0.64039408866995073, 0.40763546798029554], 5: [0.19458128078817735, 0.17980295566502463, 0.14039408866995073, 0.34113300492610837], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27339901477832512, 0.3288177339901478, 0.21305418719211822, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.47660098522167488, 0.61206896551724133, 0.41379310344827586], 5: [0.19458128078817735, 0.19458128078817735, 0.1748768472906404, 0.33866995073891626], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150432 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  11.0852
Epoch 1, cost is  10.5045
Epoch 2, cost is  10.5423
Epoch 3, cost is  10.7144
Epoch 4, cost is  10.9643
Training took 0.088325 minutes
Weight histogram
[1277  862 1152 1019 1195 1027 1389 1129  859  216] [-0.85273033 -0.76765872 -0.6825871  -0.59751549 -0.51244387 -0.42737225
 -0.34230064 -0.25722902 -0.17215741 -0.08708579 -0.00201417]
[ 494  874 1066 1088 1109 1077 1068 1025 1165 1159] [-0.85273033 -0.76765872 -0.6825871  -0.59751549 -0.51244387 -0.42737225
 -0.34230064 -0.25722902 -0.17215741 -0.08708579 -0.00201417]
-26.0366
31.9422
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147793 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.0612
Epoch 1, cost is  11.5806
Epoch 2, cost is  11.6637
Epoch 3, cost is  11.8923
Epoch 4, cost is  12.1627
Training took 0.089769 minutes
Weight histogram
[1257 1151 1031  851 1247 1051 1198 2030 1833  501] [-0.86666226 -0.78019745 -0.69373265 -0.60726784 -0.52080303 -0.43433822
 -0.34787341 -0.2614086  -0.17494379 -0.08847898 -0.00201417]
[1084 1875 1524 1038 1092 1098 1056  999 1188 1196] [-0.86666226 -0.78019745 -0.69373265 -0.60726784 -0.52080303 -0.43433822
 -0.34787341 -0.2614086  -0.17494379 -0.08847898 -0.00201417]
-31.978
30.1722
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.10016
Epoch 1, cost is  4.44135
Epoch 2, cost is  4.82656
Epoch 3, cost is  5.32743
Epoch 4, cost is  5.94449
Training took 0.082848 minutes
Weight histogram
[1009 1650 1648 1485 1348 1157  689  447  293  399] [-0.29092243 -0.26199638 -0.23307033 -0.20414428 -0.17521823 -0.14629218
 -0.11736613 -0.08844008 -0.05951403 -0.03058798 -0.00166192]
[ 591  525  726  947 1078 1200 1240 1284 1292 1242] [-0.29092243 -0.26199638 -0.23307033 -0.20414428 -0.17521823 -0.14629218
 -0.11736613 -0.08844008 -0.05951403 -0.03058798 -0.00166192]
-11.6165
14.2751
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040068 minutes
Epoch 0
Fine tuning took 0.042078 minutes
Epoch 0
Fine tuning took 0.042962 minutes
{'zero': {0: [0.27093596059113301, 0.34236453201970446, 0.18842364532019704, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.44088669950738918, 0.49876847290640391, 0.5923645320197044], 5: [0.21551724137931033, 0.21674876847290642, 0.31280788177339902, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27093596059113301, 0.21428571428571427, 0.22783251231527094, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.59975369458128081, 0.64532019704433496, 0.59605911330049266], 5: [0.21551724137931033, 0.18596059113300492, 0.1268472906403941, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27093596059113301, 0.27339901477832512, 0.28817733990147781, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.53817733990147787, 0.52832512315270941, 0.53940886699507384], 5: [0.21551724137931033, 0.18842364532019704, 0.18349753694581281, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27093596059113301, 0.20689655172413793, 0.20320197044334976, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.68472906403940892, 0.68226600985221675, 0.62315270935960587], 5: [0.21551724137931033, 0.10837438423645321, 0.1145320197044335, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148548 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  11.0852
Epoch 1, cost is  10.5045
Epoch 2, cost is  10.5423
Epoch 3, cost is  10.7144
Epoch 4, cost is  10.9643
Training took 0.086375 minutes
Weight histogram
[1277  862 1152 1019 1195 1027 1389 1129  859  216] [-0.85273033 -0.76765872 -0.6825871  -0.59751549 -0.51244387 -0.42737225
 -0.34230064 -0.25722902 -0.17215741 -0.08708579 -0.00201417]
[ 494  874 1066 1088 1109 1077 1068 1025 1165 1159] [-0.85273033 -0.76765872 -0.6825871  -0.59751549 -0.51244387 -0.42737225
 -0.34230064 -0.25722902 -0.17215741 -0.08708579 -0.00201417]
-26.0366
31.9422
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148743 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.0612
Epoch 1, cost is  11.5806
Epoch 2, cost is  11.6637
Epoch 3, cost is  11.8923
Epoch 4, cost is  12.1627
Training took 0.086635 minutes
Weight histogram
[1257 1151 1031  851 1247 1051 1198 2030 1833  501] [-0.86666226 -0.78019745 -0.69373265 -0.60726784 -0.52080303 -0.43433822
 -0.34787341 -0.2614086  -0.17494379 -0.08847898 -0.00201417]
[1084 1875 1524 1038 1092 1098 1056  999 1188 1196] [-0.86666226 -0.78019745 -0.69373265 -0.60726784 -0.52080303 -0.43433822
 -0.34787341 -0.2614086  -0.17494379 -0.08847898 -0.00201417]
-31.978
30.1722
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.77023
Epoch 1, cost is  3.67646
Epoch 2, cost is  3.82241
Epoch 3, cost is  4.19035
Epoch 4, cost is  4.63741
Training took 0.092239 minutes
Weight histogram
[ 960 1535 1302 1516 1373 1216 1011  525  285  402] [-0.23938569 -0.21563007 -0.19187444 -0.16811882 -0.14436319 -0.12060756
 -0.09685194 -0.07309631 -0.04934069 -0.02558506 -0.00182944]
[ 638  611  813  940 1069 1181 1216 1266 1240 1151] [-0.23938569 -0.21563007 -0.19187444 -0.16811882 -0.14436319 -0.12060756
 -0.09685194 -0.07309631 -0.04934069 -0.02558506 -0.00182944]
-7.90666
8.85636
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043847 minutes
Epoch 0
Fine tuning took 0.043566 minutes
Epoch 0
Fine tuning took 0.043461 minutes
{'zero': {0: [0.14285714285714285, 0.31896551724137934, 0.10467980295566502, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.55665024630541871, 0.64778325123152714, 0.58743842364532017], 5: [0.18965517241379309, 0.12438423645320197, 0.24753694581280788, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14285714285714285, 0.13177339901477833, 0.17857142857142858, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.73275862068965514, 0.68842364532019706, 0.67241379310344829], 5: [0.18965517241379309, 0.1354679802955665, 0.13300492610837439, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14285714285714285, 0.19827586206896552, 0.17857142857142858, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.67610837438423643, 0.67487684729064035, 0.6711822660098522], 5: [0.18965517241379309, 0.12561576354679804, 0.14655172413793102, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14285714285714285, 0.19334975369458129, 0.22413793103448276, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.67487684729064035, 0.6711822660098522, 0.5923645320197044], 5: [0.18965517241379309, 0.13177339901477833, 0.10467980295566502, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149976 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  11.0852
Epoch 1, cost is  10.5045
Epoch 2, cost is  10.5423
Epoch 3, cost is  10.7144
Epoch 4, cost is  10.9643
Training took 0.087078 minutes
Weight histogram
[1277  862 1152 1019 1195 1027 1389 1129  859  216] [-0.85273033 -0.76765872 -0.6825871  -0.59751549 -0.51244387 -0.42737225
 -0.34230064 -0.25722902 -0.17215741 -0.08708579 -0.00201417]
[ 494  874 1066 1088 1109 1077 1068 1025 1165 1159] [-0.85273033 -0.76765872 -0.6825871  -0.59751549 -0.51244387 -0.42737225
 -0.34230064 -0.25722902 -0.17215741 -0.08708579 -0.00201417]
-26.0366
31.9422
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148899 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.0612
Epoch 1, cost is  11.5806
Epoch 2, cost is  11.6637
Epoch 3, cost is  11.8923
Epoch 4, cost is  12.1627
Training took 0.086691 minutes
Weight histogram
[1257 1151 1031  851 1247 1051 1198 2030 1833  501] [-0.86666226 -0.78019745 -0.69373265 -0.60726784 -0.52080303 -0.43433822
 -0.34787341 -0.2614086  -0.17494379 -0.08847898 -0.00201417]
[1084 1875 1524 1038 1092 1098 1056  999 1188 1196] [-0.86666226 -0.78019745 -0.69373265 -0.60726784 -0.52080303 -0.43433822
 -0.34787341 -0.2614086  -0.17494379 -0.08847898 -0.00201417]
-31.978
30.1722
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.51629
Epoch 1, cost is  2.80314
Epoch 2, cost is  2.56301
Epoch 3, cost is  2.56273
Epoch 4, cost is  2.65474
Training took 0.119235 minutes
Weight histogram
[1111 1626 1463 1410 1309 1159  866  588  349  244] [-0.14670672 -0.13221594 -0.11772516 -0.10323438 -0.0887436  -0.07425282
 -0.05976204 -0.04527126 -0.03078048 -0.0162897  -0.00179893]
[ 701  562  766  916 1037 1183 1206 1250 1313 1191] [-0.14670672 -0.13221594 -0.11772516 -0.10323438 -0.0887436  -0.07425282
 -0.05976204 -0.04527126 -0.03078048 -0.0162897  -0.00179893]
-6.06737
7.36821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043618 minutes
Epoch 0
Fine tuning took 0.043944 minutes
Epoch 0
Fine tuning took 0.046212 minutes
{'zero': {0: [0.12561576354679804, 0.30911330049261082, 0.044334975369458129, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.59605911330049266, 0.88669950738916259, 0.7857142857142857], 5: [0.17118226600985223, 0.094827586206896547, 0.068965517241379309, 0.030788177339901478], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12561576354679804, 0.1206896551724138, 0.19704433497536947, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.75492610837438423, 0.67980295566502458, 0.69458128078817738], 5: [0.17118226600985223, 0.12438423645320197, 0.12315270935960591, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12561576354679804, 0.16748768472906403, 0.15024630541871922, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.70812807881773399, 0.70073891625615758, 0.69458128078817738], 5: [0.17118226600985223, 0.12438423645320197, 0.14901477832512317, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12561576354679804, 0.20935960591133004, 0.30665024630541871, 0.3288177339901478], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.63916256157635465, 0.59852216748768472, 0.57266009852216748], 5: [0.17118226600985223, 0.15147783251231528, 0.094827586206896547, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150856 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.18308
Epoch 1, cost is  3.06851
Epoch 2, cost is  3.04326
Epoch 3, cost is  3.04027
Epoch 4, cost is  3.04526
Training took 0.086538 minutes
Weight histogram
[1963 1290 1864 1479 1260  829  552  351  309  228] [-0.15636441 -0.14074434 -0.12512426 -0.10950419 -0.09388412 -0.07826404
 -0.06264397 -0.04702389 -0.03140382 -0.01578375 -0.00016367]
[ 487  453  634  885 1055 1198 1262 1220 1380 1551] [-0.15636441 -0.14074434 -0.12512426 -0.10950419 -0.09388412 -0.07826404
 -0.06264397 -0.04702389 -0.03140382 -0.01578375 -0.00016367]
-4.80489
5.39524
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149139 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.22197
Epoch 1, cost is  3.12344
Epoch 2, cost is  3.10444
Epoch 3, cost is  3.09982
Epoch 4, cost is  3.10793
Training took 0.086355 minutes
Weight histogram
[1987 1329 1610 1501 1254 1534 1076  707  558  594] [-0.14803952 -0.13325193 -0.11846435 -0.10367677 -0.08888918 -0.0741016
 -0.05931401 -0.04452643 -0.02973884 -0.01495126 -0.00016367]
[1028  960 1340 1277 1021 1156 1197 1218 1372 1581] [-0.14803952 -0.13325193 -0.11846435 -0.10367677 -0.08888918 -0.0741016
 -0.05931401 -0.04452643 -0.02973884 -0.01495126 -0.00016367]
-4.03031
4.77388
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.73534
Epoch 1, cost is  6.29899
Epoch 2, cost is  5.41408
Epoch 3, cost is  4.79612
Epoch 4, cost is  4.47612
Training took 0.084544 minutes
Weight histogram
[ 241  995 1121  707  916  747  758  898 3502  240] [ -8.42979699e-02  -7.58748007e-02  -6.74516315e-02  -5.90284623e-02
  -5.06052931e-02  -4.21821239e-02  -3.37589547e-02  -2.53357855e-02
  -1.69126162e-02  -8.48944704e-03  -6.62778257e-05]
[2800  830  658  668  734  783  805  926 1002  919] [ -8.42979699e-02  -7.58748007e-02  -6.74516315e-02  -5.90284623e-02
  -5.06052931e-02  -4.21821239e-02  -3.37589547e-02  -2.53357855e-02
  -1.69126162e-02  -8.48944704e-03  -6.62778257e-05]
-1.58199
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042634 minutes
Epoch 0
Fine tuning took 0.042035 minutes
Epoch 0
Fine tuning took 0.043917 minutes
{'zero': {0: [0.23399014778325122, 0.11330049261083744, 0.13054187192118227, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.72783251231527091, 0.69827586206896552, 0.66256157635467983], 5: [0.13054187192118227, 0.15886699507389163, 0.17118226600985223, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23399014778325122, 0.16625615763546797, 0.18596059113300492, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.65270935960591137, 0.65517241379310343, 0.59482758620689657], 5: [0.13054187192118227, 0.18103448275862069, 0.15886699507389163, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23399014778325122, 0.15147783251231528, 0.16009852216748768, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.66625615763546797, 0.66995073891625612, 0.60467980295566504], 5: [0.13054187192118227, 0.18226600985221675, 0.16995073891625614, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23399014778325122, 0.16625615763546797, 0.18226600985221675, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.67364532019704437, 0.6280788177339901, 0.57266009852216748], 5: [0.13054187192118227, 0.16009852216748768, 0.18965517241379309, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150039 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.18308
Epoch 1, cost is  3.06851
Epoch 2, cost is  3.04326
Epoch 3, cost is  3.04027
Epoch 4, cost is  3.04526
Training took 0.087929 minutes
Weight histogram
[1963 1290 1864 1479 1260  829  552  351  309  228] [-0.15636441 -0.14074434 -0.12512426 -0.10950419 -0.09388412 -0.07826404
 -0.06264397 -0.04702389 -0.03140382 -0.01578375 -0.00016367]
[ 487  453  634  885 1055 1198 1262 1220 1380 1551] [-0.15636441 -0.14074434 -0.12512426 -0.10950419 -0.09388412 -0.07826404
 -0.06264397 -0.04702389 -0.03140382 -0.01578375 -0.00016367]
-4.80489
5.39524
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147715 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.22197
Epoch 1, cost is  3.12344
Epoch 2, cost is  3.10444
Epoch 3, cost is  3.09982
Epoch 4, cost is  3.10793
Training took 0.086313 minutes
Weight histogram
[1987 1329 1610 1501 1254 1534 1076  707  558  594] [-0.14803952 -0.13325193 -0.11846435 -0.10367677 -0.08888918 -0.0741016
 -0.05931401 -0.04452643 -0.02973884 -0.01495126 -0.00016367]
[1028  960 1340 1277 1021 1156 1197 1218 1372 1581] [-0.14803952 -0.13325193 -0.11846435 -0.10367677 -0.08888918 -0.0741016
 -0.05931401 -0.04452643 -0.02973884 -0.01495126 -0.00016367]
-4.03031
4.77388
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.69652
Epoch 1, cost is  6.26397
Epoch 2, cost is  5.23295
Epoch 3, cost is  4.48346
Epoch 4, cost is  4.03982
Training took 0.091605 minutes
Weight histogram
[ 309 1246  930 1073  855  773  740 2020 2045  134] [-0.06149432 -0.05536212 -0.04922992 -0.04309772 -0.03696551 -0.03083331
 -0.02470111 -0.0185689  -0.0124367  -0.0063045  -0.0001723 ]
[2726  985  618  687  775  772  847  868  998  849] [-0.06149432 -0.05536212 -0.04922992 -0.04309772 -0.03696551 -0.03083331
 -0.02470111 -0.0185689  -0.0124367  -0.0063045  -0.0001723 ]
-1.35531
1.63867
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043565 minutes
Epoch 0
Fine tuning took 0.042768 minutes
Epoch 0
Fine tuning took 0.044734 minutes
{'zero': {0: [0.20566502463054187, 0.14901477832512317, 0.10098522167487685, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68596059113300489, 0.66995073891625612, 0.70812807881773399, 0.72413793103448276], 5: [0.10837438423645321, 0.18103448275862069, 0.19088669950738915, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20566502463054187, 0.2105911330049261, 0.16995073891625614, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68596059113300489, 0.61945812807881773, 0.66009852216748766, 0.61083743842364535], 5: [0.10837438423645321, 0.16995073891625614, 0.16995073891625614, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20566502463054187, 0.20566502463054187, 0.1539408866995074, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68596059113300489, 0.6219211822660099, 0.68472906403940892, 0.6219211822660099], 5: [0.10837438423645321, 0.17241379310344829, 0.16133004926108374, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20566502463054187, 0.19088669950738915, 0.16995073891625614, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68596059113300489, 0.61945812807881773, 0.63916256157635465, 0.58990147783251234], 5: [0.10837438423645321, 0.18965517241379309, 0.19088669950738915, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149072 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.18308
Epoch 1, cost is  3.06851
Epoch 2, cost is  3.04326
Epoch 3, cost is  3.04027
Epoch 4, cost is  3.04526
Training took 0.088114 minutes
Weight histogram
[1963 1290 1864 1479 1260  829  552  351  309  228] [-0.15636441 -0.14074434 -0.12512426 -0.10950419 -0.09388412 -0.07826404
 -0.06264397 -0.04702389 -0.03140382 -0.01578375 -0.00016367]
[ 487  453  634  885 1055 1198 1262 1220 1380 1551] [-0.15636441 -0.14074434 -0.12512426 -0.10950419 -0.09388412 -0.07826404
 -0.06264397 -0.04702389 -0.03140382 -0.01578375 -0.00016367]
-4.80489
5.39524
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148306 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.22197
Epoch 1, cost is  3.12344
Epoch 2, cost is  3.10444
Epoch 3, cost is  3.09982
Epoch 4, cost is  3.10793
Training took 0.088237 minutes
Weight histogram
[1987 1329 1610 1501 1254 1534 1076  707  558  594] [-0.14803952 -0.13325193 -0.11846435 -0.10367677 -0.08888918 -0.0741016
 -0.05931401 -0.04452643 -0.02973884 -0.01495126 -0.00016367]
[1028  960 1340 1277 1021 1156 1197 1218 1372 1581] [-0.14803952 -0.13325193 -0.11846435 -0.10367677 -0.08888918 -0.0741016
 -0.05931401 -0.04452643 -0.02973884 -0.01495126 -0.00016367]
-4.03031
4.77388
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.59779
Epoch 1, cost is  6.19407
Epoch 2, cost is  5.14939
Epoch 3, cost is  4.24252
Epoch 4, cost is  3.58007
Training took 0.119225 minutes
Weight histogram
[ 342 1238 1312 1198 1155 1649 2452  563  143   73] [-0.03940029 -0.03548003 -0.03155977 -0.02763952 -0.02371926 -0.019799
 -0.01587874 -0.01195848 -0.00803822 -0.00411797 -0.00019771]
[2698 1167  633  723  737  754  818  881  984  730] [-0.03940029 -0.03548003 -0.03155977 -0.02763952 -0.02371926 -0.019799
 -0.01587874 -0.01195848 -0.00803822 -0.00411797 -0.00019771]
-1.2812
1.49735
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047474 minutes
Epoch 0
Fine tuning took 0.046627 minutes
Epoch 0
Fine tuning took 0.047228 minutes
{'zero': {0: [0.20320197044334976, 0.059113300492610835, 0.099753694581280791, 0.051724137931034482], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65640394088669951, 0.7857142857142857, 0.75615763546798032, 0.78694581280788178], 5: [0.14039408866995073, 0.15517241379310345, 0.14408866995073891, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20320197044334976, 0.099753694581280791, 0.11083743842364532, 0.093596059113300489], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65640394088669951, 0.73645320197044339, 0.72413793103448276, 0.75369458128078815], 5: [0.14039408866995073, 0.16379310344827586, 0.16502463054187191, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20320197044334976, 0.11576354679802955, 0.12931034482758622, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65640394088669951, 0.73029556650246308, 0.71551724137931039, 0.75615763546798032], 5: [0.14039408866995073, 0.1539408866995074, 0.15517241379310345, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20320197044334976, 0.099753694581280791, 0.11576354679802955, 0.076354679802955669], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65640394088669951, 0.79556650246305416, 0.74014778325123154, 0.7857142857142857], 5: [0.14039408866995073, 0.10467980295566502, 0.14408866995073891, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148715 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.04552
Epoch 1, cost is  3.96211
Epoch 2, cost is  3.90305
Epoch 3, cost is  3.84804
Epoch 4, cost is  3.79575
Training took 0.089347 minutes
Weight histogram
[1364  828 1161 1295 1059  911 1011  637 1589  270] [ -5.45551516e-02  -4.90974986e-02  -4.36398455e-02  -3.81821925e-02
  -3.27245395e-02  -2.72668864e-02  -2.18092334e-02  -1.63515803e-02
  -1.08939273e-02  -5.43627427e-03   2.13787716e-05]
[1736  878  770  758  805  835  885  958 1197 1303] [ -5.45551516e-02  -4.90974986e-02  -4.36398455e-02  -3.81821925e-02
  -3.27245395e-02  -2.72668864e-02  -2.18092334e-02  -1.63515803e-02
  -1.08939273e-02  -5.43627427e-03   2.13787716e-05]
-0.970524
1.17069
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148425 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.26287
Epoch 1, cost is  4.18024
Epoch 2, cost is  4.11153
Epoch 3, cost is  4.05427
Epoch 4, cost is  3.99791
Training took 0.089569 minutes
Weight histogram
[1190  816 1022  994  839  830  872 1349 3824  414] [ -4.71111238e-02  -4.23978735e-02  -3.76846233e-02  -3.29713730e-02
  -2.82581228e-02  -2.35448725e-02  -1.88316223e-02  -1.41183720e-02
  -9.40512174e-03  -4.69187149e-03   2.13787716e-05]
[3996 1087  717  729  754  828  878  937 1072 1152] [ -4.71111238e-02  -4.23978735e-02  -3.76846233e-02  -3.29713730e-02
  -2.82581228e-02  -2.35448725e-02  -1.88316223e-02  -1.41183720e-02
  -9.40512174e-03  -4.69187149e-03   2.13787716e-05]
-0.863118
1.1293
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8833
Epoch 1, cost is  6.82118
Epoch 2, cost is  6.77288
Epoch 3, cost is  6.72762
Epoch 4, cost is  6.68306
Training took 0.083745 minutes
Weight histogram
[3723 2992 1057  675  486  363  277  223  179  150] [ -1.41098574e-02  -1.26892624e-02  -1.12686673e-02  -9.84807233e-03
  -8.42747732e-03  -7.00688230e-03  -5.58628728e-03  -4.16569227e-03
  -2.74509725e-03  -1.32450224e-03   9.60927791e-05]
[5293 2396  876  466  406  241  131  116  100  100] [ -1.41098574e-02  -1.26892624e-02  -1.12686673e-02  -9.84807233e-03
  -8.42747732e-03  -7.00688230e-03  -5.58628728e-03  -4.16569227e-03
  -2.74509725e-03  -1.32450224e-03   9.60927791e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042256 minutes
Epoch 0
Fine tuning took 0.042971 minutes
Epoch 0
Fine tuning took 0.041395 minutes
{'zero': {0: [0.2229064039408867, 0.18842364532019704, 0.26231527093596058, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.44211822660098521, 0.61699507389162567, 0.65147783251231528], 5: [0.15147783251231528, 0.36945812807881773, 0.1206896551724138, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2229064039408867, 0.21428571428571427, 0.24384236453201971, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.44211822660098521, 0.6280788177339901, 0.61699507389162567], 5: [0.15147783251231528, 0.34359605911330049, 0.12807881773399016, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2229064039408867, 0.21182266009852216, 0.25738916256157635, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.43349753694581283, 0.61206896551724133, 0.61699507389162567], 5: [0.15147783251231528, 0.35467980295566504, 0.13054187192118227, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2229064039408867, 0.2019704433497537, 0.26354679802955666, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.43965517241379309, 0.6219211822660099, 0.62068965517241381], 5: [0.15147783251231528, 0.35837438423645318, 0.1145320197044335, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150099 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.04552
Epoch 1, cost is  3.96211
Epoch 2, cost is  3.90305
Epoch 3, cost is  3.84804
Epoch 4, cost is  3.79575
Training took 0.086778 minutes
Weight histogram
[1364  828 1161 1295 1059  911 1011  637 1589  270] [ -5.45551516e-02  -4.90974986e-02  -4.36398455e-02  -3.81821925e-02
  -3.27245395e-02  -2.72668864e-02  -2.18092334e-02  -1.63515803e-02
  -1.08939273e-02  -5.43627427e-03   2.13787716e-05]
[1736  878  770  758  805  835  885  958 1197 1303] [ -5.45551516e-02  -4.90974986e-02  -4.36398455e-02  -3.81821925e-02
  -3.27245395e-02  -2.72668864e-02  -2.18092334e-02  -1.63515803e-02
  -1.08939273e-02  -5.43627427e-03   2.13787716e-05]
-0.970524
1.17069
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149688 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.26287
Epoch 1, cost is  4.18024
Epoch 2, cost is  4.11153
Epoch 3, cost is  4.05427
Epoch 4, cost is  3.99791
Training took 0.088472 minutes
Weight histogram
[1190  816 1022  994  839  830  872 1349 3824  414] [ -4.71111238e-02  -4.23978735e-02  -3.76846233e-02  -3.29713730e-02
  -2.82581228e-02  -2.35448725e-02  -1.88316223e-02  -1.41183720e-02
  -9.40512174e-03  -4.69187149e-03   2.13787716e-05]
[3996 1087  717  729  754  828  878  937 1072 1152] [ -4.71111238e-02  -4.23978735e-02  -3.76846233e-02  -3.29713730e-02
  -2.82581228e-02  -2.35448725e-02  -1.88316223e-02  -1.41183720e-02
  -9.40512174e-03  -4.69187149e-03   2.13787716e-05]
-0.863118
1.1293
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.85788
Epoch 1, cost is  6.78064
Epoch 2, cost is  6.7272
Epoch 3, cost is  6.68018
Epoch 4, cost is  6.63369
Training took 0.092733 minutes
Weight histogram
[2724 3559 1256  767  539  396  302  236  189  157] [ -1.45929288e-02  -1.31340252e-02  -1.16751216e-02  -1.02162180e-02
  -8.75731436e-03  -7.29841074e-03  -5.83950713e-03  -4.38060352e-03
  -2.92169990e-03  -1.46279629e-03  -3.89267780e-06]
[4824 2592  984  457  408  349  143  125  125  118] [ -1.45929288e-02  -1.31340252e-02  -1.16751216e-02  -1.02162180e-02
  -8.75731436e-03  -7.29841074e-03  -5.83950713e-03  -4.38060352e-03
  -2.92169990e-03  -1.46279629e-03  -3.89267780e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045163 minutes
Epoch 0
Fine tuning took 0.043483 minutes
Epoch 0
Fine tuning took 0.043521 minutes
{'zero': {0: [0.2413793103448276, 0.16748768472906403, 0.30911330049261082, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60098522167487689, 0.4963054187192118, 0.54926108374384242, 0.58743842364532017], 5: [0.15763546798029557, 0.33620689655172414, 0.14162561576354679, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2413793103448276, 0.18103448275862069, 0.26847290640394089, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60098522167487689, 0.50246305418719217, 0.5788177339901478, 0.5788177339901478], 5: [0.15763546798029557, 0.31650246305418717, 0.15270935960591134, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2413793103448276, 0.16625615763546797, 0.24630541871921183, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60098522167487689, 0.52832512315270941, 0.58620689655172409, 0.60098522167487689], 5: [0.15763546798029557, 0.30541871921182268, 0.16748768472906403, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2413793103448276, 0.19950738916256158, 0.27709359605911332, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60098522167487689, 0.49014778325123154, 0.57019704433497542, 0.59852216748768472], 5: [0.15763546798029557, 0.31034482758620691, 0.15270935960591134, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148631 minutes
Weight histogram
[  47  255  399  748  529 1705 3358 1620 1172  292] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
[ 120  136  190  239  375  560  816 1095 1988 4606] [-0.00039191 -0.00011373  0.00016444  0.00044262  0.00072079  0.00099897
  0.00127715  0.00155532  0.0018335   0.00211167  0.00238985]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.04552
Epoch 1, cost is  3.96211
Epoch 2, cost is  3.90305
Epoch 3, cost is  3.84804
Epoch 4, cost is  3.79575
Training took 0.091391 minutes
Weight histogram
[1364  828 1161 1295 1059  911 1011  637 1589  270] [ -5.45551516e-02  -4.90974986e-02  -4.36398455e-02  -3.81821925e-02
  -3.27245395e-02  -2.72668864e-02  -2.18092334e-02  -1.63515803e-02
  -1.08939273e-02  -5.43627427e-03   2.13787716e-05]
[1736  878  770  758  805  835  885  958 1197 1303] [ -5.45551516e-02  -4.90974986e-02  -4.36398455e-02  -3.81821925e-02
  -3.27245395e-02  -2.72668864e-02  -2.18092334e-02  -1.63515803e-02
  -1.08939273e-02  -5.43627427e-03   2.13787716e-05]
-0.970524
1.17069
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148947 minutes
Weight histogram
[  39  185  637  884 1635 2202 2074 1655 2316  523] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 245  276  381  458  744 1193 1035 1072 1987 4759] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
0.947496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.26287
Epoch 1, cost is  4.18024
Epoch 2, cost is  4.11153
Epoch 3, cost is  4.05427
Epoch 4, cost is  3.99791
Training took 0.086418 minutes
Weight histogram
[1190  816 1022  994  839  830  872 1349 3824  414] [ -4.71111238e-02  -4.23978735e-02  -3.76846233e-02  -3.29713730e-02
  -2.82581228e-02  -2.35448725e-02  -1.88316223e-02  -1.41183720e-02
  -9.40512174e-03  -4.69187149e-03   2.13787716e-05]
[3996 1087  717  729  754  828  878  937 1072 1152] [ -4.71111238e-02  -4.23978735e-02  -3.76846233e-02  -3.29713730e-02
  -2.82581228e-02  -2.35448725e-02  -1.88316223e-02  -1.41183720e-02
  -9.40512174e-03  -4.69187149e-03   2.13787716e-05]
-0.863118
1.1293
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78411
Epoch 1, cost is  6.66482
Epoch 2, cost is  6.59984
Epoch 3, cost is  6.54426
Epoch 4, cost is  6.49687
Training took 0.118850 minutes
Weight histogram
[ 527  900 3802 1910 1046  676  469  343  255  197] [ -1.79279987e-02  -1.61396231e-02  -1.43512475e-02  -1.25628719e-02
  -1.07744964e-02  -8.98612078e-03  -7.19774520e-03  -5.40936962e-03
  -3.62099404e-03  -1.83261846e-03  -4.42428791e-05]
[4253 2382 1307  623  420  391  274  160  152  163] [ -1.79279987e-02  -1.61396231e-02  -1.43512475e-02  -1.25628719e-02
  -1.07744964e-02  -8.98612078e-03  -7.19774520e-03  -5.40936962e-03
  -3.62099404e-03  -1.83261846e-03  -4.42428791e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045178 minutes
Epoch 0
Fine tuning took 0.046937 minutes
Epoch 0
Fine tuning took 0.044784 minutes
{'zero': {0: [0.2229064039408867, 0.20566502463054187, 0.26970443349753692, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.43349753694581283, 0.50985221674876846, 0.63916256157635465], 5: [0.16009852216748768, 0.3608374384236453, 0.22044334975369459, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2229064039408867, 0.18596059113300492, 0.24384236453201971, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.45566502463054187, 0.50985221674876846, 0.66379310344827591], 5: [0.16009852216748768, 0.35837438423645318, 0.24630541871921183, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2229064039408867, 0.20935960591133004, 0.24014778325123154, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.44827586206896552, 0.52216748768472909, 0.63423645320197042], 5: [0.16009852216748768, 0.34236453201970446, 0.2376847290640394, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2229064039408867, 0.2105911330049261, 0.27709359605911332, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.42733990147783252, 0.5, 0.64532019704433496], 5: [0.16009852216748768, 0.36206896551724138, 0.2229064039408867, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148580 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  13.1922
Epoch 1, cost is  12.6117
Epoch 2, cost is  12.6623
Epoch 3, cost is  12.8719
Epoch 4, cost is  13.1205
Training took 0.090497 minutes
Weight histogram
[1537 1290 1212 1213 1266 1399 1257 1546 1087  343] [-1.00010848 -0.90029905 -0.80048962 -0.70068019 -0.60087076 -0.50106133
 -0.4012519  -0.30144247 -0.20163304 -0.1018236  -0.00201417]
[ 631 1092 1311 1287 1286 1261 1205 1423 1347 1307] [-1.00010848 -0.90029905 -0.80048962 -0.70068019 -0.60087076 -0.50106133
 -0.4012519  -0.30144247 -0.20163304 -0.1018236  -0.00201417]
-28.8861
40.0102
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149719 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  14.7329
Epoch 1, cost is  14.1918
Epoch 2, cost is  14.2815
Epoch 3, cost is  14.5174
Epoch 4, cost is  14.8244
Training took 0.086774 minutes
Weight histogram
[1313 1188 1573 1219 1119 1452 1263 1846 2422  780] [-1.03493643 -0.9316442  -0.82835198 -0.72505975 -0.62176753 -0.5184753
 -0.41518308 -0.31189085 -0.20859862 -0.1053064  -0.00201417]
[1381 2324 1344 1247 1314 1242 1178 1465 1381 1299] [-1.03493643 -0.9316442  -0.82835198 -0.72505975 -0.62176753 -0.5184753
 -0.41518308 -0.31189085 -0.20859862 -0.1053064  -0.00201417]
-35.3018
38.9324
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.10357
Epoch 1, cost is  4.46745
Epoch 2, cost is  4.86178
Epoch 3, cost is  5.39945
Epoch 4, cost is  6.05476
Training took 0.083347 minutes
Weight histogram
[1108 1979 1984 1821 1655 1411  823  540  351  478] [-0.29092243 -0.26199638 -0.23307033 -0.20414428 -0.17521823 -0.14629218
 -0.11736613 -0.08844008 -0.05951403 -0.03058798 -0.00166192]
[ 705  632  872 1138 1302 1437 1501 1542 1549 1472] [-0.29092243 -0.26199638 -0.23307033 -0.20414428 -0.17521823 -0.14629218
 -0.11736613 -0.08844008 -0.05951403 -0.03058798 -0.00166192]
-11.6165
14.2751
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041658 minutes
Epoch 0
Fine tuning took 0.040005 minutes
Epoch 0
Fine tuning took 0.041686 minutes
{'zero': {0: [0.20689655172413793, 0.23152709359605911, 0.22906403940886699, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.35467980295566504, 0.36945812807881773, 0.5073891625615764], 5: [0.2019704433497537, 0.41379310344827586, 0.40147783251231528, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20689655172413793, 0.2105911330049261, 0.17857142857142858, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.66625615763546797, 0.61206896551724133, 0.6145320197044335], 5: [0.2019704433497537, 0.12315270935960591, 0.20935960591133004, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20689655172413793, 0.2413793103448276, 0.23029556650246305, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.61822660098522164, 0.55295566502463056, 0.57019704433497542], 5: [0.2019704433497537, 0.14039408866995073, 0.21674876847290642, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20689655172413793, 0.14285714285714285, 0.17857142857142858, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.73891625615763545, 0.62315270935960587, 0.66379310344827591], 5: [0.2019704433497537, 0.11822660098522167, 0.19827586206896552, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.166043 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  13.1922
Epoch 1, cost is  12.6117
Epoch 2, cost is  12.6623
Epoch 3, cost is  12.8719
Epoch 4, cost is  13.1205
Training took 0.119596 minutes
Weight histogram
[1537 1290 1212 1213 1266 1399 1257 1546 1087  343] [-1.00010848 -0.90029905 -0.80048962 -0.70068019 -0.60087076 -0.50106133
 -0.4012519  -0.30144247 -0.20163304 -0.1018236  -0.00201417]
[ 631 1092 1311 1287 1286 1261 1205 1423 1347 1307] [-1.00010848 -0.90029905 -0.80048962 -0.70068019 -0.60087076 -0.50106133
 -0.4012519  -0.30144247 -0.20163304 -0.1018236  -0.00201417]
-28.8861
40.0102
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.166219 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  14.7329
Epoch 1, cost is  14.1918
Epoch 2, cost is  14.2815
Epoch 3, cost is  14.5174
Epoch 4, cost is  14.8244
Training took 0.098807 minutes
Weight histogram
[1313 1188 1573 1219 1119 1452 1263 1846 2422  780] [-1.03493643 -0.9316442  -0.82835198 -0.72505975 -0.62176753 -0.5184753
 -0.41518308 -0.31189085 -0.20859862 -0.1053064  -0.00201417]
[1381 2324 1344 1247 1314 1242 1178 1465 1381 1299] [-1.03493643 -0.9316442  -0.82835198 -0.72505975 -0.62176753 -0.5184753
 -0.41518308 -0.31189085 -0.20859862 -0.1053064  -0.00201417]
-35.3018
38.9324
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.76887
Epoch 1, cost is  3.69219
Epoch 2, cost is  3.82239
Epoch 3, cost is  4.18195
Epoch 4, cost is  4.6024
Training took 0.093456 minutes
Weight histogram
[1122 1814 1594 1825 1653 1455 1235  628  341  483] [-0.23938569 -0.21563007 -0.19187444 -0.16811882 -0.14436319 -0.12060756
 -0.09685194 -0.07309631 -0.04934069 -0.02558506 -0.00182944]
[ 762  734  976 1128 1279 1414 1456 1518 1483 1400] [-0.23938569 -0.21563007 -0.19187444 -0.16811882 -0.14436319 -0.12060756
 -0.09685194 -0.07309631 -0.04934069 -0.02558506 -0.00182944]
-8.56558
9.19243
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043481 minutes
Epoch 0
Fine tuning took 0.045725 minutes
Epoch 0
Fine tuning took 0.043552 minutes
{'zero': {0: [0.18842364532019704, 0.34236453201970446, 0.22783251231527094, 0.37315270935960593], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.44458128078817732, 0.46551724137931033, 0.47906403940886699], 5: [0.1625615763546798, 0.21305418719211822, 0.30665024630541871, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18842364532019704, 0.23275862068965517, 0.19704433497536947, 0.25], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.70197044334975367, 0.69581280788177335, 0.63177339901477836], 5: [0.1625615763546798, 0.065270935960591137, 0.10714285714285714, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18842364532019704, 0.26477832512315269, 0.22413793103448276, 0.28325123152709358], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.59482758620689657, 0.64778325123152714, 0.59605911330049266], 5: [0.1625615763546798, 0.14039408866995073, 0.12807881773399016, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18842364532019704, 0.15024630541871922, 0.13300492610837439, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.76354679802955661, 0.71921182266009853, 0.6428571428571429], 5: [0.1625615763546798, 0.086206896551724144, 0.14778325123152711, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148653 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  13.1922
Epoch 1, cost is  12.6117
Epoch 2, cost is  12.6623
Epoch 3, cost is  12.8719
Epoch 4, cost is  13.1205
Training took 0.089454 minutes
Weight histogram
[1537 1290 1212 1213 1266 1399 1257 1546 1087  343] [-1.00010848 -0.90029905 -0.80048962 -0.70068019 -0.60087076 -0.50106133
 -0.4012519  -0.30144247 -0.20163304 -0.1018236  -0.00201417]
[ 631 1092 1311 1287 1286 1261 1205 1423 1347 1307] [-1.00010848 -0.90029905 -0.80048962 -0.70068019 -0.60087076 -0.50106133
 -0.4012519  -0.30144247 -0.20163304 -0.1018236  -0.00201417]
-28.8861
40.0102
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148248 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  14.7329
Epoch 1, cost is  14.1918
Epoch 2, cost is  14.2815
Epoch 3, cost is  14.5174
Epoch 4, cost is  14.8244
Training took 0.093969 minutes
Weight histogram
[1313 1188 1573 1219 1119 1452 1263 1846 2422  780] [-1.03493643 -0.9316442  -0.82835198 -0.72505975 -0.62176753 -0.5184753
 -0.41518308 -0.31189085 -0.20859862 -0.1053064  -0.00201417]
[1381 2324 1344 1247 1314 1242 1178 1465 1381 1299] [-1.03493643 -0.9316442  -0.82835198 -0.72505975 -0.62176753 -0.5184753
 -0.41518308 -0.31189085 -0.20859862 -0.1053064  -0.00201417]
-35.3018
38.9324
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.52886
Epoch 1, cost is  2.83042
Epoch 2, cost is  2.5721
Epoch 3, cost is  2.55917
Epoch 4, cost is  2.6607
Training took 0.127074 minutes
Weight histogram
[1320 1964 1751 1680 1551 1379 1088  706  420  291] [-0.14670672 -0.13221594 -0.11772516 -0.10323438 -0.0887436  -0.07425282
 -0.05976204 -0.04527126 -0.03078048 -0.0162897  -0.00179893]
[ 841  683  930 1123 1245 1424 1451 1501 1576 1376] [-0.14670672 -0.13221594 -0.11772516 -0.10323438 -0.0887436  -0.07425282
 -0.05976204 -0.04527126 -0.03078048 -0.0162897  -0.00179893]
-6.06737
7.36821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045284 minutes
Epoch 0
Fine tuning took 0.045016 minutes
Epoch 0
Fine tuning took 0.047898 minutes
{'zero': {0: [0.16133004926108374, 0.20935960591133004, 0.20566502463054187, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.58374384236453203, 0.58866995073891626, 0.60221674876847286], 5: [0.078817733990147784, 0.20689655172413793, 0.20566502463054187, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.18349753694581281, 0.2019704433497537, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.70812807881773399, 0.7068965517241379, 0.70566502463054193], 5: [0.078817733990147784, 0.10837438423645321, 0.091133004926108374, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.17241379310344829, 0.21428571428571427, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.68226600985221675, 0.69088669950738912, 0.70566502463054193], 5: [0.078817733990147784, 0.14532019704433496, 0.094827586206896547, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.17610837438423646, 0.16009852216748768, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.75615763546798032, 0.77709359605911332, 0.68842364532019706], 5: [0.078817733990147784, 0.067733990147783252, 0.062807881773399021, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.153883 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.27526
Epoch 1, cost is  3.1896
Epoch 2, cost is  3.17858
Epoch 3, cost is  3.18738
Epoch 4, cost is  3.20018
Training took 0.090147 minutes
Weight histogram
[2456 1585 1907 2022 1332 1132  691  440  322  263] [ -1.70702502e-01  -1.53648619e-01  -1.36594736e-01  -1.19540853e-01
  -1.02486970e-01  -8.54330866e-02  -6.83792035e-02  -5.13253204e-02
  -3.42714373e-02  -1.72175542e-02  -1.63671051e-04]
[ 534  573  789 1127 1290 1427 1401 1584 1736 1689] [ -1.70702502e-01  -1.53648619e-01  -1.36594736e-01  -1.19540853e-01
  -1.02486970e-01  -8.54330866e-02  -6.83792035e-02  -5.13253204e-02
  -3.42714373e-02  -1.72175542e-02  -1.63671051e-04]
-5.39512
5.79054
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150514 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.49732
Epoch 1, cost is  3.38967
Epoch 2, cost is  3.36596
Epoch 3, cost is  3.36752
Epoch 4, cost is  3.37814
Training took 0.091844 minutes
Weight histogram
[1971 2043 1680 1731 1516 1592 1449  894  636  663] [ -1.66234747e-01  -1.49627639e-01  -1.33020531e-01  -1.16413424e-01
  -9.98063164e-02  -8.31992088e-02  -6.65921013e-02  -4.99849937e-02
  -3.33778862e-02  -1.67707786e-02  -1.63671051e-04]
[1130 1181 1697 1128 1244 1350 1379 1550 1799 1717] [ -1.66234747e-01  -1.49627639e-01  -1.33020531e-01  -1.16413424e-01
  -9.98063164e-02  -8.31992088e-02  -6.65921013e-02  -4.99849937e-02
  -3.33778862e-02  -1.67707786e-02  -1.63671051e-04]
-4.75422
5.36482
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.73999
Epoch 1, cost is  6.30902
Epoch 2, cost is  5.42848
Epoch 3, cost is  4.82025
Epoch 4, cost is  4.49723
Training took 0.088983 minutes
Weight histogram
[ 241 1190 1362  845 1116  900  895 1062 4247  292] [ -8.42979699e-02  -7.58746839e-02  -6.74513979e-02  -5.90281119e-02
  -5.06048259e-02  -4.21815398e-02  -3.37582538e-02  -2.53349678e-02
  -1.69116818e-02  -8.48839578e-03  -6.51097580e-05]
[3359  991  783  804  884  949  970 1120 1213 1077] [ -8.42979699e-02  -7.58746839e-02  -6.74513979e-02  -5.90281119e-02
  -5.06048259e-02  -4.21815398e-02  -3.37582538e-02  -2.53349678e-02
  -1.69116818e-02  -8.48839578e-03  -6.51097580e-05]
-1.58199
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045229 minutes
Epoch 0
Fine tuning took 0.043778 minutes
Epoch 0
Fine tuning took 0.043138 minutes
{'zero': {0: [0.13793103448275862, 0.18596059113300492, 0.10467980295566502, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77709359605911332, 0.68842364532019706, 0.79064039408866993, 0.75985221674876846], 5: [0.084975369458128072, 0.12561576354679804, 0.10467980295566502, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13793103448275862, 0.20443349753694581, 0.13669950738916256, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77709359605911332, 0.65024630541871919, 0.76108374384236455, 0.76231527093596063], 5: [0.084975369458128072, 0.14532019704433496, 0.10221674876847291, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13793103448275862, 0.2019704433497537, 0.15024630541871922, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77709359605911332, 0.6785714285714286, 0.74876847290640391, 0.75492610837438423], 5: [0.084975369458128072, 0.11945812807881774, 0.10098522167487685, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13793103448275862, 0.20320197044334976, 0.13300492610837439, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77709359605911332, 0.6711822660098522, 0.76231527093596063, 0.77832512315270941], 5: [0.084975369458128072, 0.12561576354679804, 0.10467980295566502, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149195 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.27526
Epoch 1, cost is  3.1896
Epoch 2, cost is  3.17858
Epoch 3, cost is  3.18738
Epoch 4, cost is  3.20018
Training took 0.089177 minutes
Weight histogram
[2456 1585 1907 2022 1332 1132  691  440  322  263] [ -1.70702502e-01  -1.53648619e-01  -1.36594736e-01  -1.19540853e-01
  -1.02486970e-01  -8.54330866e-02  -6.83792035e-02  -5.13253204e-02
  -3.42714373e-02  -1.72175542e-02  -1.63671051e-04]
[ 534  573  789 1127 1290 1427 1401 1584 1736 1689] [ -1.70702502e-01  -1.53648619e-01  -1.36594736e-01  -1.19540853e-01
  -1.02486970e-01  -8.54330866e-02  -6.83792035e-02  -5.13253204e-02
  -3.42714373e-02  -1.72175542e-02  -1.63671051e-04]
-5.39512
5.79054
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148945 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.49732
Epoch 1, cost is  3.38967
Epoch 2, cost is  3.36596
Epoch 3, cost is  3.36752
Epoch 4, cost is  3.37814
Training took 0.086391 minutes
Weight histogram
[1971 2043 1680 1731 1516 1592 1449  894  636  663] [ -1.66234747e-01  -1.49627639e-01  -1.33020531e-01  -1.16413424e-01
  -9.98063164e-02  -8.31992088e-02  -6.65921013e-02  -4.99849937e-02
  -3.33778862e-02  -1.67707786e-02  -1.63671051e-04]
[1130 1181 1697 1128 1244 1350 1379 1550 1799 1717] [ -1.66234747e-01  -1.49627639e-01  -1.33020531e-01  -1.16413424e-01
  -9.98063164e-02  -8.31992088e-02  -6.65921013e-02  -4.99849937e-02
  -3.33778862e-02  -1.67707786e-02  -1.63671051e-04]
-4.75422
5.36482
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.70204
Epoch 1, cost is  6.27393
Epoch 2, cost is  5.2513
Epoch 3, cost is  4.5231
Epoch 4, cost is  4.06992
Training took 0.091671 minutes
Weight histogram
[ 309 1510 1122 1291 1019  950  879 2332 2575  163] [-0.06149432 -0.05536212 -0.04922992 -0.04309772 -0.03696551 -0.03083331
 -0.02470111 -0.0185689  -0.0124367  -0.0063045  -0.0001723 ]
[3266 1183  736  830  934  937 1025 1051 1207  981] [-0.06149432 -0.05536212 -0.04922992 -0.04309772 -0.03696551 -0.03083331
 -0.02470111 -0.0185689  -0.0124367  -0.0063045  -0.0001723 ]
-1.39844
1.63867
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044254 minutes
Epoch 0
Fine tuning took 0.044077 minutes
Epoch 0
Fine tuning took 0.043790 minutes
{'zero': {0: [0.12192118226600986, 0.23645320197044334, 0.14532019704433496, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.58866995073891626, 0.75, 0.75], 5: [0.12438423645320197, 0.1748768472906404, 0.10467980295566502, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12192118226600986, 0.25615763546798032, 0.15763546798029557, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.54187192118226601, 0.72044334975369462, 0.74753694581280783], 5: [0.12438423645320197, 0.2019704433497537, 0.12192118226600986, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12192118226600986, 0.23029556650246305, 0.16995073891625614, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.56157635467980294, 0.70812807881773399, 0.74753694581280783], 5: [0.12438423645320197, 0.20812807881773399, 0.12192118226600986, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12192118226600986, 0.24753694581280788, 0.16995073891625614, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.56773399014778325, 0.69704433497536944, 0.7573891625615764], 5: [0.12438423645320197, 0.18472906403940886, 0.13300492610837439, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149121 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.27526
Epoch 1, cost is  3.1896
Epoch 2, cost is  3.17858
Epoch 3, cost is  3.18738
Epoch 4, cost is  3.20018
Training took 0.086172 minutes
Weight histogram
[2456 1585 1907 2022 1332 1132  691  440  322  263] [ -1.70702502e-01  -1.53648619e-01  -1.36594736e-01  -1.19540853e-01
  -1.02486970e-01  -8.54330866e-02  -6.83792035e-02  -5.13253204e-02
  -3.42714373e-02  -1.72175542e-02  -1.63671051e-04]
[ 534  573  789 1127 1290 1427 1401 1584 1736 1689] [ -1.70702502e-01  -1.53648619e-01  -1.36594736e-01  -1.19540853e-01
  -1.02486970e-01  -8.54330866e-02  -6.83792035e-02  -5.13253204e-02
  -3.42714373e-02  -1.72175542e-02  -1.63671051e-04]
-5.39512
5.79054
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149290 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.49732
Epoch 1, cost is  3.38967
Epoch 2, cost is  3.36596
Epoch 3, cost is  3.36752
Epoch 4, cost is  3.37814
Training took 0.086080 minutes
Weight histogram
[1971 2043 1680 1731 1516 1592 1449  894  636  663] [ -1.66234747e-01  -1.49627639e-01  -1.33020531e-01  -1.16413424e-01
  -9.98063164e-02  -8.31992088e-02  -6.65921013e-02  -4.99849937e-02
  -3.33778862e-02  -1.67707786e-02  -1.63671051e-04]
[1130 1181 1697 1128 1244 1350 1379 1550 1799 1717] [ -1.66234747e-01  -1.49627639e-01  -1.33020531e-01  -1.16413424e-01
  -9.98063164e-02  -8.31992088e-02  -6.65921013e-02  -4.99849937e-02
  -3.33778862e-02  -1.67707786e-02  -1.63671051e-04]
-4.75422
5.36482
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.60529
Epoch 1, cost is  6.2085
Epoch 2, cost is  5.16433
Epoch 3, cost is  4.27633
Epoch 4, cost is  3.61617
Training took 0.120771 minutes
Weight histogram
[ 342 1430 1615 1471 1401 1930 2992  707  174   88] [-0.03940029 -0.03547948 -0.03155866 -0.02763785 -0.02371703 -0.01979622
 -0.0158754  -0.01195459 -0.00803377 -0.00411296 -0.00019214]
[3235 1398  761  873  891  921  990 1069 1183  829] [-0.03940029 -0.03547948 -0.03155866 -0.02763785 -0.02371703 -0.01979622
 -0.0158754  -0.01195459 -0.00803377 -0.00411296 -0.00019214]
-1.28853
1.49735
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046033 minutes
Epoch 0
Fine tuning took 0.047305 minutes
Epoch 0
Fine tuning took 0.046284 minutes
{'zero': {0: [0.14039408866995073, 0.21305418719211822, 0.17364532019704434, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74014778325123154, 0.64532019704433496, 0.66256157635467983, 0.60837438423645318], 5: [0.11945812807881774, 0.14162561576354679, 0.16379310344827586, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14039408866995073, 0.18472906403940886, 0.13669950738916256, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74014778325123154, 0.66379310344827591, 0.75369458128078815, 0.75862068965517238], 5: [0.11945812807881774, 0.15147783251231528, 0.10960591133004927, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14039408866995073, 0.16133004926108374, 0.11576354679802955, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74014778325123154, 0.68842364532019706, 0.73891625615763545, 0.71798029556650245], 5: [0.11945812807881774, 0.15024630541871922, 0.14532019704433496, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14039408866995073, 0.18472906403940886, 0.12807881773399016, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74014778325123154, 0.66133004926108374, 0.73029556650246308, 0.76108374384236455], 5: [0.11945812807881774, 0.1539408866995074, 0.14162561576354679, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148429 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.86712
Epoch 1, cost is  3.80998
Epoch 2, cost is  3.75789
Epoch 3, cost is  3.7232
Epoch 4, cost is  3.68302
Training took 0.087976 minutes
Weight histogram
[2340 1334  982 1366 1216 1052 1116  806 1614  324] [ -5.94445914e-02  -5.34979944e-02  -4.75513974e-02  -4.16048004e-02
  -3.56582033e-02  -2.97116063e-02  -2.37650093e-02  -1.78184123e-02
  -1.18718153e-02  -5.92521825e-03   2.13787716e-05]
[1866 1012  878  917  965 1023 1125 1454 1462 1448] [ -5.94445914e-02  -5.34979944e-02  -4.75513974e-02  -4.16048004e-02
  -3.56582033e-02  -2.97116063e-02  -2.37650093e-02  -1.78184123e-02
  -1.18718153e-02  -5.92521825e-03   2.13787716e-05]
-1.12808
1.2968
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149325 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.00152
Epoch 1, cost is  3.93831
Epoch 2, cost is  3.88932
Epoch 3, cost is  3.84188
Epoch 4, cost is  3.80317
Training took 0.086508 minutes
Weight histogram
[1660 1450  932 1200 1088  936  924 1339 4123  523] [ -5.36115393e-02  -4.82482475e-02  -4.28849557e-02  -3.75216639e-02
  -3.21583721e-02  -2.67950803e-02  -2.14317885e-02  -1.60684967e-02
  -1.07052048e-02  -5.34191304e-03   2.13787716e-05]
[4259 1070  827  868  935 1017 1092 1286 1374 1447] [ -5.36115393e-02  -4.82482475e-02  -4.28849557e-02  -3.75216639e-02
  -3.21583721e-02  -2.67950803e-02  -2.14317885e-02  -1.60684967e-02
  -1.07052048e-02  -5.34191304e-03   2.13787716e-05]
-0.9986
1.25778
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.88578
Epoch 1, cost is  6.82694
Epoch 2, cost is  6.78097
Epoch 3, cost is  6.7382
Epoch 4, cost is  6.69608
Training took 0.081529 minutes
Weight histogram
[4100 3774 1348  851  604  451  342  277  219  184] [ -1.41098574e-02  -1.26891519e-02  -1.12684464e-02  -9.84774087e-03
  -8.42703537e-03  -7.00632986e-03  -5.58562436e-03  -4.16491886e-03
  -2.74421335e-03  -1.32350785e-03   9.71976551e-05]
[6695 3019  876  466  406  241  131  116  100  100] [ -1.41098574e-02  -1.26891519e-02  -1.12684464e-02  -9.84774087e-03
  -8.42703537e-03  -7.00632986e-03  -5.58562436e-03  -4.16491886e-03
  -2.74421335e-03  -1.32350785e-03   9.71976551e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043857 minutes
Epoch 0
Fine tuning took 0.042763 minutes
Epoch 0
Fine tuning took 0.043686 minutes
{'zero': {0: [0.25738916256157635, 0.22536945812807882, 0.072660098522167482, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42241379310344829, 0.53078817733990147, 0.58497536945812811, 0.70566502463054193], 5: [0.32019704433497537, 0.24384236453201971, 0.34236453201970446, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25738916256157635, 0.23522167487684728, 0.071428571428571425, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42241379310344829, 0.50862068965517238, 0.60344827586206895, 0.7142857142857143], 5: [0.32019704433497537, 0.25615763546798032, 0.3251231527093596, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25738916256157635, 0.21305418719211822, 0.073891625615763554, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42241379310344829, 0.54802955665024633, 0.60221674876847286, 0.69827586206896552], 5: [0.32019704433497537, 0.23891625615763548, 0.32389162561576357, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25738916256157635, 0.22044334975369459, 0.064039408866995079, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42241379310344829, 0.52709359605911332, 0.58866995073891626, 0.70443349753694584], 5: [0.32019704433497537, 0.25246305418719212, 0.34729064039408869, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148999 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.86712
Epoch 1, cost is  3.80998
Epoch 2, cost is  3.75789
Epoch 3, cost is  3.7232
Epoch 4, cost is  3.68302
Training took 0.086713 minutes
Weight histogram
[2340 1334  982 1366 1216 1052 1116  806 1614  324] [ -5.94445914e-02  -5.34979944e-02  -4.75513974e-02  -4.16048004e-02
  -3.56582033e-02  -2.97116063e-02  -2.37650093e-02  -1.78184123e-02
  -1.18718153e-02  -5.92521825e-03   2.13787716e-05]
[1866 1012  878  917  965 1023 1125 1454 1462 1448] [ -5.94445914e-02  -5.34979944e-02  -4.75513974e-02  -4.16048004e-02
  -3.56582033e-02  -2.97116063e-02  -2.37650093e-02  -1.78184123e-02
  -1.18718153e-02  -5.92521825e-03   2.13787716e-05]
-1.12808
1.2968
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148958 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.00152
Epoch 1, cost is  3.93831
Epoch 2, cost is  3.88932
Epoch 3, cost is  3.84188
Epoch 4, cost is  3.80317
Training took 0.087978 minutes
Weight histogram
[1660 1450  932 1200 1088  936  924 1339 4123  523] [ -5.36115393e-02  -4.82482475e-02  -4.28849557e-02  -3.75216639e-02
  -3.21583721e-02  -2.67950803e-02  -2.14317885e-02  -1.60684967e-02
  -1.07052048e-02  -5.34191304e-03   2.13787716e-05]
[4259 1070  827  868  935 1017 1092 1286 1374 1447] [ -5.36115393e-02  -4.82482475e-02  -4.28849557e-02  -3.75216639e-02
  -3.21583721e-02  -2.67950803e-02  -2.14317885e-02  -1.60684967e-02
  -1.07052048e-02  -5.34191304e-03   2.13787716e-05]
-0.9986
1.25778
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.86151
Epoch 1, cost is  6.78837
Epoch 2, cost is  6.73769
Epoch 3, cost is  6.69313
Epoch 4, cost is  6.64904
Training took 0.092883 minutes
Weight histogram
[2724 4594 1606  969  673  494  373  292  232  193] [ -1.45929288e-02  -1.31339439e-02  -1.16749590e-02  -1.02159740e-02
  -8.75698912e-03  -7.29800420e-03  -5.83901928e-03  -4.38003435e-03
  -2.92104943e-03  -1.46206451e-03  -3.07958544e-06]
[6093 3287 1045  457  408  349  143  125  125  118] [ -1.45929288e-02  -1.31339439e-02  -1.16749590e-02  -1.02159740e-02
  -8.75698912e-03  -7.29800420e-03  -5.83901928e-03  -4.38003435e-03
  -2.92104943e-03  -1.46206451e-03  -3.07958544e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044907 minutes
Epoch 0
Fine tuning took 0.043539 minutes
Epoch 0
Fine tuning took 0.043011 minutes
{'zero': {0: [0.25862068965517243, 0.27216748768472904, 0.071428571428571425, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41133004926108374, 0.50615763546798032, 0.60467980295566504, 0.70812807881773399], 5: [0.33004926108374383, 0.22167487684729065, 0.32389162561576357, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25862068965517243, 0.24753694581280788, 0.087438423645320201, 0.11945812807881774], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41133004926108374, 0.51724137931034486, 0.61945812807881773, 0.68842364532019706], 5: [0.33004926108374383, 0.23522167487684728, 0.29310344827586204, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25862068965517243, 0.29556650246305421, 0.075123152709359611, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41133004926108374, 0.5073891625615764, 0.63669950738916259, 0.67980295566502458], 5: [0.33004926108374383, 0.19704433497536947, 0.28817733990147781, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25862068965517243, 0.25615763546798032, 0.049261083743842367, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41133004926108374, 0.52955665024630538, 0.66625615763546797, 0.72660098522167482], 5: [0.33004926108374383, 0.21428571428571427, 0.28448275862068967, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149357 minutes
Weight histogram
[  48  282  448  804  587 2443 3144 1716 2334  344] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 122  142  194  252  390  638  848 1221 2597 5746] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.86712
Epoch 1, cost is  3.80998
Epoch 2, cost is  3.75789
Epoch 3, cost is  3.7232
Epoch 4, cost is  3.68302
Training took 0.087906 minutes
Weight histogram
[2340 1334  982 1366 1216 1052 1116  806 1614  324] [ -5.94445914e-02  -5.34979944e-02  -4.75513974e-02  -4.16048004e-02
  -3.56582033e-02  -2.97116063e-02  -2.37650093e-02  -1.78184123e-02
  -1.18718153e-02  -5.92521825e-03   2.13787716e-05]
[1866 1012  878  917  965 1023 1125 1454 1462 1448] [ -5.94445914e-02  -5.34979944e-02  -4.75513974e-02  -4.16048004e-02
  -3.56582033e-02  -2.97116063e-02  -2.37650093e-02  -1.78184123e-02
  -1.18718153e-02  -5.92521825e-03   2.13787716e-05]
-1.12808
1.2968
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148442 minutes
Weight histogram
[  39  185  637  884 1635 2204 2272 2788 2958  573] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 252  293  395  538  800 1386  902 1286 2763 5560] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10121
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.00152
Epoch 1, cost is  3.93831
Epoch 2, cost is  3.88932
Epoch 3, cost is  3.84188
Epoch 4, cost is  3.80317
Training took 0.089324 minutes
Weight histogram
[1660 1450  932 1200 1088  936  924 1339 4123  523] [ -5.36115393e-02  -4.82482475e-02  -4.28849557e-02  -3.75216639e-02
  -3.21583721e-02  -2.67950803e-02  -2.14317885e-02  -1.60684967e-02
  -1.07052048e-02  -5.34191304e-03   2.13787716e-05]
[4259 1070  827  868  935 1017 1092 1286 1374 1447] [ -5.36115393e-02  -4.82482475e-02  -4.28849557e-02  -3.75216639e-02
  -3.21583721e-02  -2.67950803e-02  -2.14317885e-02  -1.60684967e-02
  -1.07052048e-02  -5.34191304e-03   2.13787716e-05]
-0.9986
1.25778
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79164
Epoch 1, cost is  6.67805
Epoch 2, cost is  6.61693
Epoch 3, cost is  6.56473
Epoch 4, cost is  6.5195
Training took 0.121425 minutes
Weight histogram
[ 527  900 4499 2485 1328  846  584  425  315  241] [ -1.79279987e-02  -1.61395036e-02  -1.43510085e-02  -1.25625134e-02
  -1.07740183e-02  -8.98552317e-03  -7.19702807e-03  -5.40853296e-03
  -3.62003786e-03  -1.83154276e-03  -4.30476575e-05]
[5371 3026 1570  623  420  391  274  160  152  163] [ -1.79279987e-02  -1.61395036e-02  -1.43510085e-02  -1.25625134e-02
  -1.07740183e-02  -8.98552317e-03  -7.19702807e-03  -5.40853296e-03
  -3.62003786e-03  -1.83154276e-03  -4.30476575e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045806 minutes
Epoch 0
Fine tuning took 0.046164 minutes
Epoch 0
Fine tuning took 0.047208 minutes
{'zero': {0: [0.23891625615763548, 0.27832512315270935, 0.10344827586206896, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39901477832512317, 0.51724137931034486, 0.61576354679802958, 0.64162561576354682], 5: [0.36206896551724138, 0.20443349753694581, 0.28078817733990147, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23891625615763548, 0.27463054187192121, 0.091133004926108374, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39901477832512317, 0.52955665024630538, 0.65640394088669951, 0.62068965517241381], 5: [0.36206896551724138, 0.19581280788177341, 0.25246305418719212, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23891625615763548, 0.27339901477832512, 0.10467980295566502, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39901477832512317, 0.51354679802955661, 0.63669950738916259, 0.63669950738916259], 5: [0.36206896551724138, 0.21305418719211822, 0.25862068965517243, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23891625615763548, 0.24014778325123154, 0.087438423645320201, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39901477832512317, 0.55788177339901479, 0.65394088669950734, 0.61699507389162567], 5: [0.36206896551724138, 0.2019704433497537, 0.25862068965517243, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149523 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  15.027
Epoch 1, cost is  14.2521
Epoch 2, cost is  14.2024
Epoch 3, cost is  14.3131
Epoch 4, cost is  14.4935
Training took 0.089823 minutes
Weight histogram
[1867 1329 1591 1325 1549 1439 1451 1818 1341  465] [-1.15085423 -1.03597022 -0.92108622 -0.80620221 -0.69131821 -0.5764342
 -0.4615502  -0.34666619 -0.23178218 -0.11689818 -0.00201417]
[ 762 1314 1495 1493 1447 1388 1590 1540 1554 1592] [-1.15085423 -1.03597022 -0.92108622 -0.80620221 -0.69131821 -0.5764342
 -0.4615502  -0.34666619 -0.23178218 -0.11689818 -0.00201417]
-34.3857
41.4592
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148690 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.6736
Epoch 1, cost is  16.8878
Epoch 2, cost is  16.8494
Epoch 3, cost is  16.9953
Epoch 4, cost is  17.2293
Training took 0.087419 minutes
Weight histogram
[1611 1261 1434 1762 1404 1395 1621 1679 2875 1158] [-1.21325636 -1.09213214 -0.97100792 -0.8498837  -0.72875948 -0.60763527
 -0.48651105 -0.36538683 -0.24426261 -0.12313839 -0.00201417]
[1696 2486 1428 1506 1460 1366 1661 1580 1523 1494] [-1.21325636 -1.09213214 -0.97100792 -0.8498837  -0.72875948 -0.60763527
 -0.48651105 -0.36538683 -0.24426261 -0.12313839 -0.00201417]
-41.9436
46.3409
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.12682
Epoch 1, cost is  4.4767
Epoch 2, cost is  4.87016
Epoch 3, cost is  5.45749
Epoch 4, cost is  6.13218
Training took 0.082265 minutes
Weight histogram
[1133 2336 2283 2226 1958 1677  964  631  413  554] [-0.29092243 -0.26199297 -0.23306351 -0.20413404 -0.17520458 -0.14627512
 -0.11734566 -0.08841619 -0.05948673 -0.03055727 -0.0016278 ]
[ 820  737 1021 1334 1527 1679 1765 1801 1818 1673] [-0.29092243 -0.26199297 -0.23306351 -0.20413404 -0.17520458 -0.14627512
 -0.11734566 -0.08841619 -0.05948673 -0.03055727 -0.0016278 ]
-11.6165
14.2751
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042082 minutes
Epoch 0
Fine tuning took 0.040186 minutes
Epoch 0
Fine tuning took 0.042231 minutes
{'zero': {0: [0.23029556650246305, 0.35221674876847292, 0.25123152709359609, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57019704433497542, 0.54926108374384242, 0.50615763546798032, 0.63916256157635465], 5: [0.19950738916256158, 0.098522167487684734, 0.24261083743842365, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23029556650246305, 0.2019704433497537, 0.23522167487684728, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57019704433497542, 0.60344827586206895, 0.61945812807881773, 0.57758620689655171], 5: [0.19950738916256158, 0.19458128078817735, 0.14532019704433496, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23029556650246305, 0.2019704433497537, 0.2229064039408867, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57019704433497542, 0.63054187192118227, 0.59852216748768472, 0.58743842364532017], 5: [0.19950738916256158, 0.16748768472906403, 0.17857142857142858, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23029556650246305, 0.27463054187192121, 0.28078817733990147, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57019704433497542, 0.48891625615763545, 0.54926108374384242, 0.51108374384236455], 5: [0.19950738916256158, 0.23645320197044334, 0.16995073891625614, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149266 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  15.027
Epoch 1, cost is  14.2521
Epoch 2, cost is  14.2024
Epoch 3, cost is  14.3131
Epoch 4, cost is  14.4935
Training took 0.091043 minutes
Weight histogram
[1867 1329 1591 1325 1549 1439 1451 1818 1341  465] [-1.15085423 -1.03597022 -0.92108622 -0.80620221 -0.69131821 -0.5764342
 -0.4615502  -0.34666619 -0.23178218 -0.11689818 -0.00201417]
[ 762 1314 1495 1493 1447 1388 1590 1540 1554 1592] [-1.15085423 -1.03597022 -0.92108622 -0.80620221 -0.69131821 -0.5764342
 -0.4615502  -0.34666619 -0.23178218 -0.11689818 -0.00201417]
-34.3857
41.4592
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148911 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.6736
Epoch 1, cost is  16.8878
Epoch 2, cost is  16.8494
Epoch 3, cost is  16.9953
Epoch 4, cost is  17.2293
Training took 0.086088 minutes
Weight histogram
[1611 1261 1434 1762 1404 1395 1621 1679 2875 1158] [-1.21325636 -1.09213214 -0.97100792 -0.8498837  -0.72875948 -0.60763527
 -0.48651105 -0.36538683 -0.24426261 -0.12313839 -0.00201417]
[1696 2486 1428 1506 1460 1366 1661 1580 1523 1494] [-1.21325636 -1.09213214 -0.97100792 -0.8498837  -0.72875948 -0.60763527
 -0.48651105 -0.36538683 -0.24426261 -0.12313839 -0.00201417]
-41.9436
46.3409
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.80053
Epoch 1, cost is  3.73364
Epoch 2, cost is  3.873
Epoch 3, cost is  4.24983
Epoch 4, cost is  4.6485
Training took 0.093103 minutes
Weight histogram
[1221 2118 1889 2134 1947 1699 1465  740  398  564] [-0.23938569 -0.21562639 -0.19186708 -0.16810777 -0.14434847 -0.12058916
 -0.09682985 -0.07307054 -0.04931124 -0.02555193 -0.00179262]
[ 887  858 1143 1318 1493 1657 1702 1772 1733 1612] [-0.23938569 -0.21562639 -0.19186708 -0.16810777 -0.14434847 -0.12058916
 -0.09682985 -0.07307054 -0.04931124 -0.02555193 -0.00179262]
-8.56558
9.19243
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042984 minutes
Epoch 0
Fine tuning took 0.043071 minutes
Epoch 0
Fine tuning took 0.044282 minutes
{'zero': {0: [0.1748768472906404, 0.084975369458128072, 0.19950738916256158, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.74876847290640391, 0.59605911330049266, 0.66133004926108374], 5: [0.15763546798029557, 0.16625615763546797, 0.20443349753694581, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1748768472906404, 0.11699507389162561, 0.1268472906403941, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.72167487684729059, 0.69704433497536944, 0.68103448275862066], 5: [0.15763546798029557, 0.16133004926108374, 0.17610837438423646, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1748768472906404, 0.13054187192118227, 0.15763546798029557, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.67487684729064035, 0.65640394088669951, 0.68103448275862066], 5: [0.15763546798029557, 0.19458128078817735, 0.18596059113300492, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1748768472906404, 0.057881773399014777, 0.082512315270935957, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.8645320197044335, 0.76231527093596063, 0.77586206896551724], 5: [0.15763546798029557, 0.077586206896551727, 0.15517241379310345, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.165447 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  15.027
Epoch 1, cost is  14.2521
Epoch 2, cost is  14.2024
Epoch 3, cost is  14.3131
Epoch 4, cost is  14.4935
Training took 0.089439 minutes
Weight histogram
[1867 1329 1591 1325 1549 1439 1451 1818 1341  465] [-1.15085423 -1.03597022 -0.92108622 -0.80620221 -0.69131821 -0.5764342
 -0.4615502  -0.34666619 -0.23178218 -0.11689818 -0.00201417]
[ 762 1314 1495 1493 1447 1388 1590 1540 1554 1592] [-1.15085423 -1.03597022 -0.92108622 -0.80620221 -0.69131821 -0.5764342
 -0.4615502  -0.34666619 -0.23178218 -0.11689818 -0.00201417]
-34.3857
41.4592
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150024 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.6736
Epoch 1, cost is  16.8878
Epoch 2, cost is  16.8494
Epoch 3, cost is  16.9953
Epoch 4, cost is  17.2293
Training took 0.087467 minutes
Weight histogram
[1611 1261 1434 1762 1404 1395 1621 1679 2875 1158] [-1.21325636 -1.09213214 -0.97100792 -0.8498837  -0.72875948 -0.60763527
 -0.48651105 -0.36538683 -0.24426261 -0.12313839 -0.00201417]
[1696 2486 1428 1506 1460 1366 1661 1580 1523 1494] [-1.21325636 -1.09213214 -0.97100792 -0.8498837  -0.72875948 -0.60763527
 -0.48651105 -0.36538683 -0.24426261 -0.12313839 -0.00201417]
-41.9436
46.3409
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.55242
Epoch 1, cost is  2.83621
Epoch 2, cost is  2.5882
Epoch 3, cost is  2.57109
Epoch 4, cost is  2.69099
Training took 0.124230 minutes
Weight histogram
[1497 2272 2058 1976 1816 1601 1302  825  491  337] [-0.14670672 -0.13220875 -0.11771079 -0.10321282 -0.08871486 -0.0742169
 -0.05971893 -0.04522097 -0.030723   -0.01622504 -0.00172708]
[ 980  800 1089 1311 1453 1655 1686 1738 1831 1632] [-0.14670672 -0.13220875 -0.11771079 -0.10321282 -0.08871486 -0.0742169
 -0.05971893 -0.04522097 -0.030723   -0.01622504 -0.00172708]
-6.51553
7.36821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045458 minutes
Epoch 0
Fine tuning took 0.043727 minutes
Epoch 0
Fine tuning took 0.044481 minutes
{'zero': {0: [0.14162561576354679, 0.075123152709359611, 0.066502463054187194, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.78078817733990147, 0.78325123152709364, 0.72167487684729059], 5: [0.1145320197044335, 0.14408866995073891, 0.15024630541871922, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14162561576354679, 0.14655172413793102, 0.13793103448275862, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.75, 0.73645320197044339, 0.69088669950738912], 5: [0.1145320197044335, 0.10344827586206896, 0.12561576354679804, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14162561576354679, 0.1625615763546798, 0.16748768472906403, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.68965517241379315, 0.65394088669950734, 0.67241379310344829], 5: [0.1145320197044335, 0.14778325123152711, 0.17857142857142858, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14162561576354679, 0.05295566502463054, 0.086206896551724144, 0.089901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.89901477832512311, 0.81280788177339902, 0.79064039408866993], 5: [0.1145320197044335, 0.048029556650246302, 0.10098522167487685, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150602 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.37852
Epoch 1, cost is  3.25867
Epoch 2, cost is  3.22258
Epoch 3, cost is  3.21542
Epoch 4, cost is  3.21861
Training took 0.088793 minutes
Weight histogram
[2017 2711 1705 2091 1994 1492  901  573  403  288] [ -1.90393358e-01  -1.71370390e-01  -1.52347421e-01  -1.33324452e-01
  -1.14301484e-01  -9.52785148e-02  -7.62555460e-02  -5.72325773e-02
  -3.82096085e-02  -1.91866398e-02  -1.63671051e-04]
[ 583  654  975 1302 1516 1561 1557 1978 1854 2195] [ -1.90393358e-01  -1.71370390e-01  -1.52347421e-01  -1.33324452e-01
  -1.14301484e-01  -9.52785148e-02  -7.62555460e-02  -5.72325773e-02
  -3.82096085e-02  -1.91866398e-02  -1.63671051e-04]
-5.54834
6.18829
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.164567 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.67182
Epoch 1, cost is  3.54511
Epoch 2, cost is  3.52236
Epoch 3, cost is  3.5117
Epoch 4, cost is  3.52409
Training took 0.093630 minutes
Weight histogram
[2067 2116 2049 2157 1766 1593 1871 1154  699  728] [ -1.83707401e-01  -1.65353028e-01  -1.46998655e-01  -1.28644282e-01
  -1.10289909e-01  -9.19355361e-02  -7.35811631e-02  -5.52267901e-02
  -3.68724171e-02  -1.85180441e-02  -1.63671051e-04]
[1233 1393 1748 1274 1460 1516 1575 2017 1908 2076] [ -1.83707401e-01  -1.65353028e-01  -1.46998655e-01  -1.28644282e-01
  -1.10289909e-01  -9.19355361e-02  -7.35811631e-02  -5.52267901e-02
  -3.68724171e-02  -1.85180441e-02  -1.63671051e-04]
-5.15402
5.77615
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.7395
Epoch 1, cost is  6.29734
Epoch 2, cost is  5.43833
Epoch 3, cost is  4.83464
Epoch 4, cost is  4.51272
Training took 0.087112 minutes
Weight histogram
[ 241 1349 1623  999 1316 1064 1037 1245 4956  345] [ -8.42979699e-02  -7.58742939e-02  -6.74506178e-02  -5.90269417e-02
  -5.06032656e-02  -4.21795895e-02  -3.37559134e-02  -2.53322373e-02
  -1.69085612e-02  -8.48488516e-03  -6.12090735e-05]
[3895 1165  915  941 1035 1115 1137 1314 1427 1231] [ -8.42979699e-02  -7.58742939e-02  -6.74506178e-02  -5.90269417e-02
  -5.06032656e-02  -4.21795895e-02  -3.37559134e-02  -2.53322373e-02
  -1.69085612e-02  -8.48488516e-03  -6.12090735e-05]
-1.59026
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043455 minutes
Epoch 0
Fine tuning took 0.043712 minutes
Epoch 0
Fine tuning took 0.044446 minutes
{'zero': {0: [0.24876847290640394, 0.16625615763546797, 0.19088669950738915, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.70197044334975367, 0.65147783251231528, 0.72413793103448276], 5: [0.15270935960591134, 0.13177339901477833, 0.15763546798029557, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24876847290640394, 0.17118226600985223, 0.24753694581280788, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.68965517241379315, 0.5923645320197044, 0.6354679802955665], 5: [0.15270935960591134, 0.13916256157635468, 0.16009852216748768, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24876847290640394, 0.18103448275862069, 0.2413793103448276, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.70197044334975367, 0.58004926108374388, 0.6428571428571429], 5: [0.15270935960591134, 0.11699507389162561, 0.17857142857142858, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24876847290640394, 0.21428571428571427, 0.22660098522167488, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.65270935960591137, 0.59359605911330049, 0.61330049261083741], 5: [0.15270935960591134, 0.13300492610837439, 0.17980295566502463, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148841 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.37852
Epoch 1, cost is  3.25867
Epoch 2, cost is  3.22258
Epoch 3, cost is  3.21542
Epoch 4, cost is  3.21861
Training took 0.086738 minutes
Weight histogram
[2017 2711 1705 2091 1994 1492  901  573  403  288] [ -1.90393358e-01  -1.71370390e-01  -1.52347421e-01  -1.33324452e-01
  -1.14301484e-01  -9.52785148e-02  -7.62555460e-02  -5.72325773e-02
  -3.82096085e-02  -1.91866398e-02  -1.63671051e-04]
[ 583  654  975 1302 1516 1561 1557 1978 1854 2195] [ -1.90393358e-01  -1.71370390e-01  -1.52347421e-01  -1.33324452e-01
  -1.14301484e-01  -9.52785148e-02  -7.62555460e-02  -5.72325773e-02
  -3.82096085e-02  -1.91866398e-02  -1.63671051e-04]
-5.54834
6.18829
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148314 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.67182
Epoch 1, cost is  3.54511
Epoch 2, cost is  3.52236
Epoch 3, cost is  3.5117
Epoch 4, cost is  3.52409
Training took 0.088434 minutes
Weight histogram
[2067 2116 2049 2157 1766 1593 1871 1154  699  728] [ -1.83707401e-01  -1.65353028e-01  -1.46998655e-01  -1.28644282e-01
  -1.10289909e-01  -9.19355361e-02  -7.35811631e-02  -5.52267901e-02
  -3.68724171e-02  -1.85180441e-02  -1.63671051e-04]
[1233 1393 1748 1274 1460 1516 1575 2017 1908 2076] [ -1.83707401e-01  -1.65353028e-01  -1.46998655e-01  -1.28644282e-01
  -1.10289909e-01  -9.19355361e-02  -7.35811631e-02  -5.52267901e-02
  -3.68724171e-02  -1.85180441e-02  -1.63671051e-04]
-5.15402
5.77615
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.70092
Epoch 1, cost is  6.26821
Epoch 2, cost is  5.26075
Epoch 3, cost is  4.52692
Epoch 4, cost is  4.0925
Training took 0.091917 minutes
Weight histogram
[ 315 1707 1372 1511 1182 1130 1023 2656 3087  192] [-0.06149432 -0.05536101 -0.04922769 -0.04309438 -0.03696106 -0.03082775
 -0.02469443 -0.01856112 -0.0124278  -0.00629448 -0.00016117]
[3791 1392  859  971 1094 1100 1203 1235 1421 1109] [-0.06149432 -0.05536101 -0.04922769 -0.04309438 -0.03696106 -0.03082775
 -0.02469443 -0.01856112 -0.0124278  -0.00629448 -0.00016117]
-1.39844
1.63867
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041645 minutes
Epoch 0
Fine tuning took 0.044189 minutes
Epoch 0
Fine tuning took 0.044589 minutes
{'zero': {0: [0.19704433497536947, 0.17610837438423646, 0.17610837438423646, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66502463054187189, 0.59729064039408863, 0.61945812807881773, 0.68103448275862066], 5: [0.13793103448275862, 0.22660098522167488, 0.20443349753694581, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19704433497536947, 0.20320197044334976, 0.25123152709359609, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66502463054187189, 0.59113300492610843, 0.55418719211822665, 0.64532019704433496], 5: [0.13793103448275862, 0.20566502463054187, 0.19458128078817735, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19704433497536947, 0.20073891625615764, 0.22783251231527094, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66502463054187189, 0.59975369458128081, 0.58004926108374388, 0.64162561576354682], 5: [0.13793103448275862, 0.19950738916256158, 0.19211822660098521, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19704433497536947, 0.19704433497536947, 0.23275862068965517, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66502463054187189, 0.59729064039408863, 0.5788177339901478, 0.63423645320197042], 5: [0.13793103448275862, 0.20566502463054187, 0.18842364532019704, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149633 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.37852
Epoch 1, cost is  3.25867
Epoch 2, cost is  3.22258
Epoch 3, cost is  3.21542
Epoch 4, cost is  3.21861
Training took 0.086525 minutes
Weight histogram
[2017 2711 1705 2091 1994 1492  901  573  403  288] [ -1.90393358e-01  -1.71370390e-01  -1.52347421e-01  -1.33324452e-01
  -1.14301484e-01  -9.52785148e-02  -7.62555460e-02  -5.72325773e-02
  -3.82096085e-02  -1.91866398e-02  -1.63671051e-04]
[ 583  654  975 1302 1516 1561 1557 1978 1854 2195] [ -1.90393358e-01  -1.71370390e-01  -1.52347421e-01  -1.33324452e-01
  -1.14301484e-01  -9.52785148e-02  -7.62555460e-02  -5.72325773e-02
  -3.82096085e-02  -1.91866398e-02  -1.63671051e-04]
-5.54834
6.18829
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147504 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.67182
Epoch 1, cost is  3.54511
Epoch 2, cost is  3.52236
Epoch 3, cost is  3.5117
Epoch 4, cost is  3.52409
Training took 0.088697 minutes
Weight histogram
[2067 2116 2049 2157 1766 1593 1871 1154  699  728] [ -1.83707401e-01  -1.65353028e-01  -1.46998655e-01  -1.28644282e-01
  -1.10289909e-01  -9.19355361e-02  -7.35811631e-02  -5.52267901e-02
  -3.68724171e-02  -1.85180441e-02  -1.63671051e-04]
[1233 1393 1748 1274 1460 1516 1575 2017 1908 2076] [ -1.83707401e-01  -1.65353028e-01  -1.46998655e-01  -1.28644282e-01
  -1.10289909e-01  -9.19355361e-02  -7.35811631e-02  -5.52267901e-02
  -3.68724171e-02  -1.85180441e-02  -1.63671051e-04]
-5.15402
5.77615
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.60358
Epoch 1, cost is  6.20621
Epoch 2, cost is  5.17468
Epoch 3, cost is  4.29315
Epoch 4, cost is  3.646
Training took 0.117970 minutes
Weight histogram
[ 342 1632 1907 1745 1649 2256 3452  883  206  103] [-0.03940029 -0.03547917 -0.03155805 -0.02763694 -0.02371582 -0.0197947
 -0.01587358 -0.01195246 -0.00803134 -0.00411023 -0.00018911]
[3755 1643  892 1024 1046 1085 1166 1257 1387  920] [-0.03940029 -0.03547917 -0.03155805 -0.02763694 -0.02371582 -0.0197947
 -0.01587358 -0.01195246 -0.00803134 -0.00411023 -0.00018911]
-1.28853
1.49735
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045896 minutes
Epoch 0
Fine tuning took 0.044297 minutes
Epoch 0
Fine tuning took 0.044885 minutes
{'zero': {0: [0.23522167487684728, 0.12192118226600986, 0.18719211822660098, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58497536945812811, 0.6280788177339901, 0.58866995073891626, 0.68349753694581283], 5: [0.17980295566502463, 0.25, 0.22413793103448276, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23522167487684728, 0.17857142857142858, 0.1748768472906404, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58497536945812811, 0.66009852216748766, 0.63300492610837433, 0.73768472906403937], 5: [0.17980295566502463, 0.16133004926108374, 0.19211822660098521, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23522167487684728, 0.17241379310344829, 0.18596059113300492, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58497536945812811, 0.65270935960591137, 0.59605911330049266, 0.74014778325123154], 5: [0.17980295566502463, 0.1748768472906404, 0.21798029556650247, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23522167487684728, 0.16625615763546797, 0.14655172413793102, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58497536945812811, 0.68472906403940892, 0.6576354679802956, 0.74630541871921185], 5: [0.17980295566502463, 0.14901477832512317, 0.19581280788177341, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149678 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.58482
Epoch 1, cost is  3.52511
Epoch 2, cost is  3.48325
Epoch 3, cost is  3.45716
Epoch 4, cost is  3.41975
Training took 0.089931 minutes
Weight histogram
[1918 2416 1452 1168 1543 1315 1172 1101 1458  632] [ -6.71166554e-02  -6.04028520e-02  -5.36890486e-02  -4.69752452e-02
  -4.02614417e-02  -3.35476383e-02  -2.68338349e-02  -2.01200315e-02
  -1.34062281e-02  -6.69242465e-03   2.13787716e-05]
[1979 1087  966 1044 1092 1191 1485 1637 1584 2110] [ -6.71166554e-02  -6.04028520e-02  -5.36890486e-02  -4.69752452e-02
  -4.02614417e-02  -3.35476383e-02  -2.68338349e-02  -2.01200315e-02
  -1.34062281e-02  -6.69242465e-03   2.13787716e-05]
-1.24428
1.41465
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149305 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.80234
Epoch 1, cost is  3.75329
Epoch 2, cost is  3.71222
Epoch 3, cost is  3.68427
Epoch 4, cost is  3.6542
Training took 0.088198 minutes
Weight histogram
[1898 1721 1592 1180 1243 1171 1020 1277 4412  686] [ -6.08244836e-02  -5.47398974e-02  -4.86553112e-02  -4.25707249e-02
  -3.64861387e-02  -3.04015524e-02  -2.43169662e-02  -1.82323799e-02
  -1.21477937e-02  -6.06320747e-03   2.13787716e-05]
[4487 1030  929  997 1092 1185 1377 1521 1638 1944] [ -6.08244836e-02  -5.47398974e-02  -4.86553112e-02  -4.25707249e-02
  -3.64861387e-02  -3.04015524e-02  -2.43169662e-02  -1.82323799e-02
  -1.21477937e-02  -6.06320747e-03   2.13787716e-05]
-1.07359
1.31603
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.88516
Epoch 1, cost is  6.82554
Epoch 2, cost is  6.77881
Epoch 3, cost is  6.73562
Epoch 4, cost is  6.69278
Training took 0.083696 minutes
Weight histogram
[4467 4572 1638 1023  723  538  407  329  260  218] [ -1.41098574e-02  -1.26891519e-02  -1.12684464e-02  -9.84774087e-03
  -8.42703537e-03  -7.00632986e-03  -5.58562436e-03  -4.16491886e-03
  -2.74421335e-03  -1.32350785e-03   9.71976551e-05]
[8080 3659  876  466  406  241  131  116  100  100] [ -1.41098574e-02  -1.26891519e-02  -1.12684464e-02  -9.84774087e-03
  -8.42703537e-03  -7.00632986e-03  -5.58562436e-03  -4.16491886e-03
  -2.74421335e-03  -1.32350785e-03   9.71976551e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042703 minutes
Epoch 0
Fine tuning took 0.041870 minutes
Epoch 0
Fine tuning took 0.041713 minutes
{'zero': {0: [0.28201970443349755, 0.096059113300492605, 0.23152709359605911, 0.31034482758620691], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.61576354679802958, 0.5357142857142857, 0.47536945812807879], 5: [0.26231527093596058, 0.28817733990147781, 0.23275862068965517, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28201970443349755, 0.091133004926108374, 0.25492610837438423, 0.30788177339901479], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.61945812807881773, 0.52586206896551724, 0.47783251231527096], 5: [0.26231527093596058, 0.2894088669950739, 0.21921182266009853, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28201970443349755, 0.10344827586206896, 0.24753694581280788, 0.34482758620689657], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.5923645320197044, 0.5, 0.44581280788177341], 5: [0.26231527093596058, 0.30418719211822659, 0.25246305418719212, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28201970443349755, 0.087438423645320201, 0.22413793103448276, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.6071428571428571, 0.53694581280788178, 0.4963054187192118], 5: [0.26231527093596058, 0.30541871921182268, 0.23891625615763548, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149555 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.58482
Epoch 1, cost is  3.52511
Epoch 2, cost is  3.48325
Epoch 3, cost is  3.45716
Epoch 4, cost is  3.41975
Training took 0.086944 minutes
Weight histogram
[1918 2416 1452 1168 1543 1315 1172 1101 1458  632] [ -6.71166554e-02  -6.04028520e-02  -5.36890486e-02  -4.69752452e-02
  -4.02614417e-02  -3.35476383e-02  -2.68338349e-02  -2.01200315e-02
  -1.34062281e-02  -6.69242465e-03   2.13787716e-05]
[1979 1087  966 1044 1092 1191 1485 1637 1584 2110] [ -6.71166554e-02  -6.04028520e-02  -5.36890486e-02  -4.69752452e-02
  -4.02614417e-02  -3.35476383e-02  -2.68338349e-02  -2.01200315e-02
  -1.34062281e-02  -6.69242465e-03   2.13787716e-05]
-1.24428
1.41465
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147821 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.80234
Epoch 1, cost is  3.75329
Epoch 2, cost is  3.71222
Epoch 3, cost is  3.68427
Epoch 4, cost is  3.6542
Training took 0.089457 minutes
Weight histogram
[1898 1721 1592 1180 1243 1171 1020 1277 4412  686] [ -6.08244836e-02  -5.47398974e-02  -4.86553112e-02  -4.25707249e-02
  -3.64861387e-02  -3.04015524e-02  -2.43169662e-02  -1.82323799e-02
  -1.21477937e-02  -6.06320747e-03   2.13787716e-05]
[4487 1030  929  997 1092 1185 1377 1521 1638 1944] [ -6.08244836e-02  -5.47398974e-02  -4.86553112e-02  -4.25707249e-02
  -3.64861387e-02  -3.04015524e-02  -2.43169662e-02  -1.82323799e-02
  -1.21477937e-02  -6.06320747e-03   2.13787716e-05]
-1.07359
1.31603
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.86062
Epoch 1, cost is  6.78644
Epoch 2, cost is  6.73519
Epoch 3, cost is  6.69021
Epoch 4, cost is  6.64569
Training took 0.093473 minutes
Weight histogram
[2724 5636 1953 1171  805  590  445  347  275  229] [ -1.45929288e-02  -1.31339439e-02  -1.16749590e-02  -1.02159740e-02
  -8.75698912e-03  -7.29800420e-03  -5.83901928e-03  -4.38003435e-03
  -2.92104943e-03  -1.46206451e-03  -3.07958544e-06]
[7347 3980 1123  457  408  349  143  125  125  118] [ -1.45929288e-02  -1.31339439e-02  -1.16749590e-02  -1.02159740e-02
  -8.75698912e-03  -7.29800420e-03  -5.83901928e-03  -4.38003435e-03
  -2.92104943e-03  -1.46206451e-03  -3.07958544e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042184 minutes
Epoch 0
Fine tuning took 0.041667 minutes
Epoch 0
Fine tuning took 0.042654 minutes
{'zero': {0: [0.29433497536945813, 0.11083743842364532, 0.27955665024630544, 0.36576354679802958], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.5714285714285714, 0.47413793103448276, 0.44827586206896552], 5: [0.23275862068965517, 0.31773399014778325, 0.24630541871921183, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29433497536945813, 0.10344827586206896, 0.26108374384236455, 0.37315270935960593], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.56896551724137934, 0.50492610837438423, 0.46551724137931033], 5: [0.23275862068965517, 0.32758620689655171, 0.23399014778325122, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29433497536945813, 0.12315270935960591, 0.2536945812807882, 0.35960591133004927], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.55172413793103448, 0.52709359605911332, 0.47413793103448276], 5: [0.23275862068965517, 0.3251231527093596, 0.21921182266009853, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29433497536945813, 0.10591133004926108, 0.25615763546798032, 0.33004926108374383], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.57635467980295563, 0.5357142857142857, 0.48275862068965519], 5: [0.23275862068965517, 0.31773399014778325, 0.20812807881773399, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151685 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3030 2964  375] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 124  145  201  257  413  719  849 1500 2750 7217] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.58482
Epoch 1, cost is  3.52511
Epoch 2, cost is  3.48325
Epoch 3, cost is  3.45716
Epoch 4, cost is  3.41975
Training took 0.086811 minutes
Weight histogram
[1918 2416 1452 1168 1543 1315 1172 1101 1458  632] [ -6.71166554e-02  -6.04028520e-02  -5.36890486e-02  -4.69752452e-02
  -4.02614417e-02  -3.35476383e-02  -2.68338349e-02  -2.01200315e-02
  -1.34062281e-02  -6.69242465e-03   2.13787716e-05]
[1979 1087  966 1044 1092 1191 1485 1637 1584 2110] [ -6.71166554e-02  -6.04028520e-02  -5.36890486e-02  -4.69752452e-02
  -4.02614417e-02  -3.35476383e-02  -2.68338349e-02  -2.01200315e-02
  -1.34062281e-02  -6.69242465e-03   2.13787716e-05]
-1.24428
1.41465
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149311 minutes
Weight histogram
[  39  185  637  884 1637 2393 3012 3673 3155  585] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 7146] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.80234
Epoch 1, cost is  3.75329
Epoch 2, cost is  3.71222
Epoch 3, cost is  3.68427
Epoch 4, cost is  3.6542
Training took 0.086809 minutes
Weight histogram
[1898 1721 1592 1180 1243 1171 1020 1277 4412  686] [ -6.08244836e-02  -5.47398974e-02  -4.86553112e-02  -4.25707249e-02
  -3.64861387e-02  -3.04015524e-02  -2.43169662e-02  -1.82323799e-02
  -1.21477937e-02  -6.06320747e-03   2.13787716e-05]
[4487 1030  929  997 1092 1185 1377 1521 1638 1944] [ -6.08244836e-02  -5.47398974e-02  -4.86553112e-02  -4.25707249e-02
  -3.64861387e-02  -3.04015524e-02  -2.43169662e-02  -1.82323799e-02
  -1.21477937e-02  -6.06320747e-03   2.13787716e-05]
-1.07359
1.31603
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78976
Epoch 1, cost is  6.67513
Epoch 2, cost is  6.61326
Epoch 3, cost is  6.55989
Epoch 4, cost is  6.51475
Training took 0.121545 minutes
Weight histogram
[ 527  900 5200 3061 1609 1014  699  506  373  286] [ -1.79279987e-02  -1.61395036e-02  -1.43510085e-02  -1.25625134e-02
  -1.07740183e-02  -8.98552317e-03  -7.19702807e-03  -5.40853296e-03
  -3.62003786e-03  -1.83154276e-03  -4.30476575e-05]
[6471 3667 1854  623  420  391  274  160  152  163] [ -1.79279987e-02  -1.61395036e-02  -1.43510085e-02  -1.25625134e-02
  -1.07740183e-02  -8.98552317e-03  -7.19702807e-03  -5.40853296e-03
  -3.62003786e-03  -1.83154276e-03  -4.30476575e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045876 minutes
Epoch 0
Fine tuning took 0.045291 minutes
Epoch 0
Fine tuning took 0.046618 minutes
{'zero': {0: [0.26847290640394089, 0.092364532019704432, 0.24384236453201971, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50123152709359609, 0.54802955665024633, 0.59359605911330049, 0.68719211822660098], 5: [0.23029556650246305, 0.35960591133004927, 0.1625615763546798, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26847290640394089, 0.075123152709359611, 0.26477832512315269, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50123152709359609, 0.56157635467980294, 0.59359605911330049, 0.65517241379310343], 5: [0.23029556650246305, 0.36330049261083741, 0.14162561576354679, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26847290640394089, 0.075123152709359611, 0.28325123152709358, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50123152709359609, 0.58251231527093594, 0.53694581280788178, 0.6354679802955665], 5: [0.23029556650246305, 0.34236453201970446, 0.17980295566502463, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26847290640394089, 0.082512315270935957, 0.2894088669950739, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50123152709359609, 0.58004926108374388, 0.56650246305418717, 0.66995073891625612], 5: [0.23029556650246305, 0.33743842364532017, 0.14408866995073891, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149025 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.2531
Epoch 1, cost is  16.3145
Epoch 2, cost is  16.1792
Epoch 3, cost is  16.2507
Epoch 4, cost is  16.3982
Training took 0.085866 minutes
Weight histogram
[1683 1562 1685 1884 1602 1696 1834 1774 1844  636] [-1.34074974 -1.20687618 -1.07300263 -0.93912907 -0.80525551 -0.67138196
 -0.5375084  -0.40363484 -0.26976129 -0.13588773 -0.00201417]
[ 915 1574 1664 1666 1604 1722 1754 1760 1803 1738] [-1.34074974 -1.20687618 -1.07300263 -0.93912907 -0.80525551 -0.67138196
 -0.5375084  -0.40363484 -0.26976129 -0.13588773 -0.00201417]
-36.8684
45.5888
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148461 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.2538
Epoch 1, cost is  19.2968
Epoch 2, cost is  19.1317
Epoch 3, cost is  19.1927
Epoch 4, cost is  19.3785
Training took 0.085373 minutes
Weight histogram
[1682 1551 1559 1668 1899 1658 1789 1775 3107 1537] [-1.40066612 -1.26080092 -1.12093573 -0.98107053 -0.84120534 -0.70134015
 -0.56147495 -0.42160976 -0.28174456 -0.14187937 -0.00201417]
[2034 2526 1601 1711 1586 1767 1802 1716 1723 1759] [-1.40066612 -1.26080092 -1.12093573 -0.98107053 -0.84120534 -0.70134015
 -0.56147495 -0.42160976 -0.28174456 -0.14187937 -0.00201417]
-47.9194
48.6636
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.12305
Epoch 1, cost is  4.49029
Epoch 2, cost is  4.86265
Epoch 3, cost is  5.36446
Epoch 4, cost is  6.06616
Training took 0.083478 minutes
Weight histogram
[1245 2669 2538 2576 2275 1955 1116  728  466  632] [-0.29092243 -0.26199297 -0.23306351 -0.20413404 -0.17520458 -0.14627512
 -0.11734566 -0.08841619 -0.05948673 -0.03055727 -0.0016278 ]
[ 933  846 1171 1532 1747 1919 2016 2058 2074 1904] [-0.29092243 -0.26199297 -0.23306351 -0.20413404 -0.17520458 -0.14627512
 -0.11734566 -0.08841619 -0.05948673 -0.03055727 -0.0016278 ]
-11.6165
14.2751
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041662 minutes
Epoch 0
Fine tuning took 0.042313 minutes
Epoch 0
Fine tuning took 0.041870 minutes
{'zero': {0: [0.22167487684729065, 0.37807881773399016, 0.26600985221674878, 0.4039408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60837438423645318, 0.34729064039408869, 0.42241379310344829, 0.45443349753694579], 5: [0.16995073891625614, 0.27463054187192121, 0.31157635467980294, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22167487684729065, 0.21551724137931033, 0.16625615763546797, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60837438423645318, 0.59482758620689657, 0.67364532019704437, 0.63916256157635465], 5: [0.16995073891625614, 0.18965517241379309, 0.16009852216748768, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22167487684729065, 0.24384236453201971, 0.19334975369458129, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60837438423645318, 0.55788177339901479, 0.60467980295566504, 0.60960591133004927], 5: [0.16995073891625614, 0.19827586206896552, 0.2019704433497537, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22167487684729065, 0.2019704433497537, 0.15270935960591134, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60837438423645318, 0.63177339901477836, 0.66748768472906406, 0.65517241379310343], 5: [0.16995073891625614, 0.16625615763546797, 0.17980295566502463, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.166668 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.2531
Epoch 1, cost is  16.3145
Epoch 2, cost is  16.1792
Epoch 3, cost is  16.2507
Epoch 4, cost is  16.3982
Training took 0.117696 minutes
Weight histogram
[1683 1562 1685 1884 1602 1696 1834 1774 1844  636] [-1.34074974 -1.20687618 -1.07300263 -0.93912907 -0.80525551 -0.67138196
 -0.5375084  -0.40363484 -0.26976129 -0.13588773 -0.00201417]
[ 915 1574 1664 1666 1604 1722 1754 1760 1803 1738] [-1.34074974 -1.20687618 -1.07300263 -0.93912907 -0.80525551 -0.67138196
 -0.5375084  -0.40363484 -0.26976129 -0.13588773 -0.00201417]
-36.8684
45.5888
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.171146 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.2538
Epoch 1, cost is  19.2968
Epoch 2, cost is  19.1317
Epoch 3, cost is  19.1927
Epoch 4, cost is  19.3785
Training took 0.093978 minutes
Weight histogram
[1682 1551 1559 1668 1899 1658 1789 1775 3107 1537] [-1.40066612 -1.26080092 -1.12093573 -0.98107053 -0.84120534 -0.70134015
 -0.56147495 -0.42160976 -0.28174456 -0.14187937 -0.00201417]
[2034 2526 1601 1711 1586 1767 1802 1716 1723 1759] [-1.40066612 -1.26080092 -1.12093573 -0.98107053 -0.84120534 -0.70134015
 -0.56147495 -0.42160976 -0.28174456 -0.14187937 -0.00201417]
-47.9194
48.6636
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.79483
Epoch 1, cost is  3.7468
Epoch 2, cost is  3.85412
Epoch 3, cost is  4.25013
Epoch 4, cost is  4.68028
Training took 0.098889 minutes
Weight histogram
[1380 2438 2125 2447 2249 1926 1686  851  453  645] [-0.23938569 -0.21562639 -0.19186708 -0.16810777 -0.14434847 -0.12058916
 -0.09682985 -0.07307054 -0.04931124 -0.02555193 -0.00179262]
[1011  981 1311 1509 1703 1889 1939 2019 1978 1860] [-0.23938569 -0.21562639 -0.19186708 -0.16810777 -0.14434847 -0.12058916
 -0.09682985 -0.07307054 -0.04931124 -0.02555193 -0.00179262]
-8.56558
9.19243
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042801 minutes
Epoch 0
Fine tuning took 0.044184 minutes
Epoch 0
Fine tuning took 0.041195 minutes
{'zero': {0: [0.18472906403940886, 0.35344827586206895, 0.31527093596059114, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6711822660098522, 0.4963054187192118, 0.4211822660098522, 0.51354679802955661], 5: [0.14408866995073891, 0.15024630541871922, 0.26354679802955666, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18472906403940886, 0.15147783251231528, 0.12315270935960591, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6711822660098522, 0.71921182266009853, 0.73768472906403937, 0.72167487684729059], 5: [0.14408866995073891, 0.12931034482758622, 0.13916256157635468, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18472906403940886, 0.19950738916256158, 0.19704433497536947, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6711822660098522, 0.64655172413793105, 0.64901477832512311, 0.65024630541871919], 5: [0.14408866995073891, 0.1539408866995074, 0.1539408866995074, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18472906403940886, 0.13054187192118227, 0.12561576354679804, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6711822660098522, 0.79556650246305416, 0.82758620689655171, 0.76970443349753692], 5: [0.14408866995073891, 0.073891625615763554, 0.046798029556650245, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.168262 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  17.2531
Epoch 1, cost is  16.3145
Epoch 2, cost is  16.1792
Epoch 3, cost is  16.2507
Epoch 4, cost is  16.3982
Training took 0.093613 minutes
Weight histogram
[1683 1562 1685 1884 1602 1696 1834 1774 1844  636] [-1.34074974 -1.20687618 -1.07300263 -0.93912907 -0.80525551 -0.67138196
 -0.5375084  -0.40363484 -0.26976129 -0.13588773 -0.00201417]
[ 915 1574 1664 1666 1604 1722 1754 1760 1803 1738] [-1.34074974 -1.20687618 -1.07300263 -0.93912907 -0.80525551 -0.67138196
 -0.5375084  -0.40363484 -0.26976129 -0.13588773 -0.00201417]
-36.8684
45.5888
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.160888 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.2538
Epoch 1, cost is  19.2968
Epoch 2, cost is  19.1317
Epoch 3, cost is  19.1927
Epoch 4, cost is  19.3785
Training took 0.086090 minutes
Weight histogram
[1682 1551 1559 1668 1899 1658 1789 1775 3107 1537] [-1.40066612 -1.26080092 -1.12093573 -0.98107053 -0.84120534 -0.70134015
 -0.56147495 -0.42160976 -0.28174456 -0.14187937 -0.00201417]
[2034 2526 1601 1711 1586 1767 1802 1716 1723 1759] [-1.40066612 -1.26080092 -1.12093573 -0.98107053 -0.84120534 -0.70134015
 -0.56147495 -0.42160976 -0.28174456 -0.14187937 -0.00201417]
-47.9194
48.6636
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.55339
Epoch 1, cost is  2.8667
Epoch 2, cost is  2.60723
Epoch 3, cost is  2.61881
Epoch 4, cost is  2.70937
Training took 0.117728 minutes
Weight histogram
[1732 2594 2310 2260 2092 1806 1516  943  563  384] [-0.14670672 -0.13220875 -0.11771079 -0.10321282 -0.08871486 -0.0742169
 -0.05971893 -0.04522097 -0.030723   -0.01622504 -0.00172708]
[1124  920 1257 1503 1671 1885 1932 1986 2095 1827] [-0.14670672 -0.13220875 -0.11771079 -0.10321282 -0.08871486 -0.0742169
 -0.05971893 -0.04522097 -0.030723   -0.01622504 -0.00172708]
-6.51553
7.36821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042951 minWARNING (theano.gof.cmodule): Removing key file /homes/js3611/.theano/compiledir_Linux-3.13--generic-x86_64-with-Ubuntu-14.04-trusty-x86_64-2.7.6-64/tmpLTedoK/key.pkl because the corresponding module is gone from the file system.
utes
Epoch 0
Fine tuning took 0.045955 minutes
Epoch 0
Fine tuning took 0.045322 minutes
{'zero': {0: [0.13300492610837439, 0.19704433497536947, 0.2105911330049261, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.53940886699507384, 0.57389162561576357, 0.48152709359605911], 5: [0.14778325123152711, 0.26354679802955666, 0.21551724137931033, 0.22167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13300492610837439, 0.13054187192118227, 0.10960591133004927, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.73522167487684731, 0.74507389162561577, 0.75369458128078815], 5: [0.14778325123152711, 0.13423645320197045, 0.14532019704433496, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13300492610837439, 0.14039408866995073, 0.15270935960591134, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.71674876847290636, 0.7068965517241379, 0.70320197044334976], 5: [0.14778325123152711, 0.14285714285714285, 0.14039408866995073, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13300492610837439, 0.13054187192118227, 0.1268472906403941, 0.064039408866995079], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.7142857142857143, 0.73399014778325122, 0.81034482758620685], 5: [0.14778325123152711, 0.15517241379310345, 0.13916256157635468, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.158931 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.57637
Epoch 1, cost is  3.45398
Epoch 2, cost is  3.43094
Epoch 3, cost is  3.43243
Epoch 4, cost is  3.43749
Training took 0.094683 minutes
Weight histogram
[2755 1967 2965 1676 2420 1855 1130  669  449  314] [ -2.07469434e-01  -1.86738857e-01  -1.66008281e-01  -1.45277705e-01
  -1.24547129e-01  -1.03816552e-01  -8.30859760e-02  -6.23553998e-02
  -4.16248236e-02  -2.08942473e-02  -1.63671051e-04]
[ 640  767 1184 1490 1747 1685 2050 2057 2363 2217] [ -2.07469434e-01  -1.86738857e-01  -1.66008281e-01  -1.45277705e-01
  -1.24547129e-01  -1.03816552e-01  -8.30859760e-02  -6.23553998e-02
  -4.16248236e-02  -2.08942473e-02  -1.63671051e-04]
-6.27009
6.93081
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.160521 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.03001
Epoch 1, cost is  3.90924
Epoch 2, cost is  3.87945
Epoch 3, cost is  3.88475
Epoch 4, cost is  3.88985
Training took 0.095373 minutes
Weight histogram
[2465 2365 2492 1971 2106 1835 2015 1366  825  785] [ -2.00267777e-01  -1.80257366e-01  -1.60246956e-01  -1.40236545e-01
  -1.20226135e-01  -1.00215724e-01  -8.02053134e-02  -6.01949028e-02
  -4.01844922e-02  -2.01740816e-02  -1.63671051e-04]
[1347 1654 1753 1471 1655 1689 2086 2136 2261 2173] [ -2.00267777e-01  -1.80257366e-01  -1.60246956e-01  -1.40236545e-01
  -1.20226135e-01  -1.00215724e-01  -8.02053134e-02  -6.01949028e-02
  -4.01844922e-02  -2.01740816e-02  -1.63671051e-04]
-5.65452
6.53713
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.73974
Epoch 1, cost is  6.28564
Epoch 2, cost is  5.43335
Epoch 3, cost is  4.83909
Epoch 4, cost is  4.53162
Training took 0.088863 minutes
Weight histogram
[ 241 1508 1891 1147 1518 1226 1201 1423 5646  399] [ -8.42979699e-02  -7.58742939e-02  -6.74506178e-02  -5.90269417e-02
  -5.06032656e-02  -4.21795895e-02  -3.37559134e-02  -2.53322373e-02
  -1.69085612e-02  -8.48488516e-03  -6.12090735e-05]
[4421 1338 1046 1080 1189 1280 1305 1515 1646 1380] [ -8.42979699e-02  -7.58742939e-02  -6.74506178e-02  -5.90269417e-02
  -5.06032656e-02  -4.21795895e-02  -3.37559134e-02  -2.53322373e-02
  -1.69085612e-02  -8.48488516e-03  -6.12090735e-05]
-1.60314
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047213 minutes
Epoch 0
Fine tuning took 0.043657 minutes
Epoch 0
Fine tuning took 0.041771 minutes
{'zero': {0: [0.2376847290640394, 0.21674876847290642, 0.22536945812807882, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.59729064039408863, 0.58866995073891626, 0.66502463054187189], 5: [0.1206896551724138, 0.18596059113300492, 0.18596059113300492, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2376847290640394, 0.24876847290640394, 0.2376847290640394, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.53694581280788178, 0.55541871921182262, 0.5788177339901478], 5: [0.1206896551724138, 0.21428571428571427, 0.20689655172413793, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2376847290640394, 0.24384236453201971, 0.24507389162561577, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.54556650246305416, 0.51600985221674878, 0.55295566502463056], 5: [0.1206896551724138, 0.2105911330049261, 0.23891625615763548, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2376847290640394, 0.22044334975369459, 0.23522167487684728, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.57758620689655171, 0.55665024630541871, 0.55418719211822665], 5: [0.1206896551724138, 0.2019704433497537, 0.20812807881773399, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151286 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.57637
Epoch 1, cost is  3.45398
Epoch 2, cost is  3.43094
Epoch 3, cost is  3.43243
Epoch 4, cost is  3.43749
Training took 0.089157 minutes
Weight histogram
[2755 1967 2965 1676 2420 1855 1130  669  449  314] [ -2.07469434e-01  -1.86738857e-01  -1.66008281e-01  -1.45277705e-01
  -1.24547129e-01  -1.03816552e-01  -8.30859760e-02  -6.23553998e-02
  -4.16248236e-02  -2.08942473e-02  -1.63671051e-04]
[ 640  767 1184 1490 1747 1685 2050 2057 2363 2217] [ -2.07469434e-01  -1.86738857e-01  -1.66008281e-01  -1.45277705e-01
  -1.24547129e-01  -1.03816552e-01  -8.30859760e-02  -6.23553998e-02
  -4.16248236e-02  -2.08942473e-02  -1.63671051e-04]
-6.27009
6.93081
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147540 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.03001
Epoch 1, cost is  3.90924
Epoch 2, cost is  3.87945
Epoch 3, cost is  3.88475
Epoch 4, cost is  3.88985
Training took 0.089858 minutes
Weight histogram
[2465 2365 2492 1971 2106 1835 2015 1366  825  785] [ -2.00267777e-01  -1.80257366e-01  -1.60246956e-01  -1.40236545e-01
  -1.20226135e-01  -1.00215724e-01  -8.02053134e-02  -6.01949028e-02
  -4.01844922e-02  -2.01740816e-02  -1.63671051e-04]
[1347 1654 1753 1471 1655 1689 2086 2136 2261 2173] [ -2.00267777e-01  -1.80257366e-01  -1.60246956e-01  -1.40236545e-01
  -1.20226135e-01  -1.00215724e-01  -8.02053134e-02  -6.01949028e-02
  -4.01844922e-02  -2.01740816e-02  -1.63671051e-04]
-5.65452
6.53713
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.70105
Epoch 1, cost is  6.25895
Epoch 2, cost is  5.25365
Epoch 3, cost is  4.52891
Epoch 4, cost is  4.09943
Training took 0.091523 minutes
Weight histogram
[ 322 1936 1595 1725 1352 1312 1159 2984 3594  221] [-0.06149432 -0.05536101 -0.04922769 -0.04309438 -0.03696106 -0.03082775
 -0.02469443 -0.01856112 -0.0124278  -0.00629448 -0.00016117]
[4305 1603  984 1112 1256 1264 1384 1418 1635 1239] [-0.06149432 -0.05536101 -0.04922769 -0.04309438 -0.03696106 -0.03082775
 -0.02469443 -0.01856112 -0.0124278  -0.00629448 -0.00016117]
-1.39844
1.63867
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044509 minutes
Epoch 0
Fine tuning took 0.042150 minutes
Epoch 0
Fine tuning took 0.043934 minutes
{'zero': {0: [0.24630541871921183, 0.21182266009852216, 0.21798029556650247, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.55788177339901479, 0.56527093596059108, 0.62068965517241381], 5: [0.1206896551724138, 0.23029556650246305, 0.21674876847290642, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24630541871921183, 0.2105911330049261, 0.26108374384236455, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.53201970443349755, 0.49753694581280788, 0.55541871921182262], 5: [0.1206896551724138, 0.25738916256157635, 0.2413793103448276, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24630541871921183, 0.21798029556650247, 0.23891625615763548, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.58128078817733986, 0.50862068965517238, 0.59605911330049266], 5: [0.1206896551724138, 0.20073891625615764, 0.25246305418719212, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24630541871921183, 0.21182266009852216, 0.26847290640394089, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.53201970443349755, 0.48029556650246308, 0.5431034482758621], 5: [0.1206896551724138, 0.25615763546798032, 0.25123152709359609, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151024 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.57637
Epoch 1, cost is  3.45398
Epoch 2, cost is  3.43094
Epoch 3, cost is  3.43243
Epoch 4, cost is  3.43749
Training took 0.087051 minutes
Weight histogram
[2755 1967 2965 1676 2420 1855 1130  669  449  314] [ -2.07469434e-01  -1.86738857e-01  -1.66008281e-01  -1.45277705e-01
  -1.24547129e-01  -1.03816552e-01  -8.30859760e-02  -6.23553998e-02
  -4.16248236e-02  -2.08942473e-02  -1.63671051e-04]
[ 640  767 1184 1490 1747 1685 2050 2057 2363 2217] [ -2.07469434e-01  -1.86738857e-01  -1.66008281e-01  -1.45277705e-01
  -1.24547129e-01  -1.03816552e-01  -8.30859760e-02  -6.23553998e-02
  -4.16248236e-02  -2.08942473e-02  -1.63671051e-04]
-6.27009
6.93081
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149727 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.03001
Epoch 1, cost is  3.90924
Epoch 2, cost is  3.87945
Epoch 3, cost is  3.88475
Epoch 4, cost is  3.88985
Training took 0.088094 minutes
Weight histogram
[2465 2365 2492 1971 2106 1835 2015 1366  825  785] [ -2.00267777e-01  -1.80257366e-01  -1.60246956e-01  -1.40236545e-01
  -1.20226135e-01  -1.00215724e-01  -8.02053134e-02  -6.01949028e-02
  -4.01844922e-02  -2.01740816e-02  -1.63671051e-04]
[1347 1654 1753 1471 1655 1689 2086 2136 2261 2173] [ -2.00267777e-01  -1.80257366e-01  -1.60246956e-01  -1.40236545e-01
  -1.20226135e-01  -1.00215724e-01  -8.02053134e-02  -6.01949028e-02
  -4.01844922e-02  -2.01740816e-02  -1.63671051e-04]
-5.65452
6.53713
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.60392
Epoch 1, cost is  6.19976
Epoch 2, cost is  5.1727
Epoch 3, cost is  4.30846
Epoch 4, cost is  3.66501
Training took 0.118988 minutes
Weight histogram
[ 342 1834 2205 2037 1873 2582 3891 1079  238  119] [-0.03940029 -0.03547917 -0.03155805 -0.02763694 -0.02371582 -0.0197947
 -0.01587358 -0.01195246 -0.00803134 -0.00411023 -0.00018911]
[4261 1894 1025 1177 1203 1251 1344 1447 1591 1007] [-0.03940029 -0.03547917 -0.03155805 -0.02763694 -0.02371582 -0.0197947
 -0.01587358 -0.01195246 -0.00803134 -0.00411023 -0.00018911]
-1.28853
1.49735
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046324 minutes
Epoch 0
Fine tuning took 0.045103 minutes
Epoch 0
Fine tuning took 0.046478 minutes
{'zero': {0: [0.16995073891625614, 0.14408866995073891, 0.17364532019704434, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.61822660098522164, 0.6280788177339901, 0.61699507389162567], 5: [0.1539408866995074, 0.2376847290640394, 0.19827586206896552, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16995073891625614, 0.14901477832512317, 0.18226600985221675, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.60467980295566504, 0.61330049261083741, 0.6428571428571429], 5: [0.1539408866995074, 0.24630541871921183, 0.20443349753694581, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16995073891625614, 0.14162561576354679, 0.18842364532019704, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.62438423645320196, 0.5923645320197044, 0.66871921182266014], 5: [0.1539408866995074, 0.23399014778325122, 0.21921182266009853, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16995073891625614, 0.14778325123152711, 0.20443349753694581, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.60837438423645318, 0.58374384236453203, 0.66995073891625612], 5: [0.1539408866995074, 0.24384236453201971, 0.21182266009852216, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149394 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.52605
Epoch 1, cost is  3.48684
Epoch 2, cost is  3.45908
Epoch 3, cost is  3.43734
Epoch 4, cost is  3.4099
Training took 0.088504 minutes
Weight histogram
[3187 1724 2343 1192 1628 1450 1278 1230 1224  944] [ -7.13315308e-02  -6.41962399e-02  -5.70609489e-02  -4.99256579e-02
  -4.27903670e-02  -3.56550760e-02  -2.85197851e-02  -2.13844941e-02
  -1.42492031e-02  -7.11391219e-03   2.13787716e-05]
[2103 1161 1092 1161 1249 1442 1850 1726 2256 2160] [ -7.13315308e-02  -6.41962399e-02  -5.70609489e-02  -4.99256579e-02
  -4.27903670e-02  -3.56550760e-02  -2.85197851e-02  -2.13844941e-02
  -1.42492031e-02  -7.11391219e-03   2.13787716e-05]
-1.36791
1.57286
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149026 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.72433
Epoch 1, cost is  3.68082
Epoch 2, cost is  3.65052
Epoch 3, cost is  3.62342
Epoch 4, cost is  3.59469
Training took 0.089166 minutes
Weight histogram
[2750 1679 1965 1469 1282 1335 1171 1217 4525  832] [ -6.56677037e-02  -5.90987955e-02  -5.25298872e-02  -4.59609790e-02
  -3.93920707e-02  -3.28231625e-02  -2.62542542e-02  -1.96853460e-02
  -1.31164377e-02  -6.54752948e-03   2.13787716e-05]
[4646 1059 1038 1125 1250 1418 1618 1755 2109 2207] [ -6.56677037e-02  -5.90987955e-02  -5.25298872e-02  -4.59609790e-02
  -3.93920707e-02  -3.28231625e-02  -2.62542542e-02  -1.96853460e-02
  -1.31164377e-02  -6.54752948e-03   2.13787716e-05]
-1.13309
1.43076
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.88644
Epoch 1, cost is  6.82825
Epoch 2, cost is  6.78281
Epoch 3, cost is  6.74055
Epoch 4, cost is  6.69838
Training took 0.084170 minutes
Weight histogram
[4467 5675 1962 1209  847  629  474  382  302  253] [ -1.41098574e-02  -1.26891519e-02  -1.12684464e-02  -9.84774087e-03
  -8.42703537e-03  -7.00632986e-03  -5.58562436e-03  -4.16491886e-03
  -2.74421335e-03  -1.32350785e-03   9.71976551e-05]
[9342 4324  974  466  406  241  131  116  100  100] [ -1.41098574e-02  -1.26891519e-02  -1.12684464e-02  -9.84774087e-03
  -8.42703537e-03  -7.00632986e-03  -5.58562436e-03  -4.16491886e-03
  -2.74421335e-03  -1.32350785e-03   9.71976551e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042219 minutes
Epoch 0
Fine tuning took 0.041086 minutes
Epoch 0
Fine tuning took 0.042778 minutes
{'zero': {0: [0.30788177339901479, 0.15886699507389163, 0.31773399014778325, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.71551724137931039, 0.52832512315270941, 0.53940886699507384], 5: [0.16009852216748768, 0.12561576354679804, 0.1539408866995074, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30788177339901479, 0.15886699507389163, 0.34113300492610837, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.68226600985221675, 0.51600985221674878, 0.51108374384236455], 5: [0.16009852216748768, 0.15886699507389163, 0.14285714285714285, 0.31403940886699505], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30788177339901479, 0.16871921182266009, 0.31527093596059114, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.68965517241379315, 0.54802955665024633, 0.51477832512315269], 5: [0.16009852216748768, 0.14162561576354679, 0.13669950738916256, 0.32142857142857145], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30788177339901479, 0.15640394088669951, 0.31527093596059114, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53201970443349755, 0.70320197044334976, 0.51724137931034486, 0.52093596059113301], 5: [0.16009852216748768, 0.14039408866995073, 0.16748768472906403, 0.28201970443349755], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151046 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.52605
Epoch 1, cost is  3.48684
Epoch 2, cost is  3.45908
Epoch 3, cost is  3.43734
Epoch 4, cost is  3.4099
Training took 0.086991 minutes
Weight histogram
[3187 1724 2343 1192 1628 1450 1278 1230 1224  944] [ -7.13315308e-02  -6.41962399e-02  -5.70609489e-02  -4.99256579e-02
  -4.27903670e-02  -3.56550760e-02  -2.85197851e-02  -2.13844941e-02
  -1.42492031e-02  -7.11391219e-03   2.13787716e-05]
[2103 1161 1092 1161 1249 1442 1850 1726 2256 2160] [ -7.13315308e-02  -6.41962399e-02  -5.70609489e-02  -4.99256579e-02
  -4.27903670e-02  -3.56550760e-02  -2.85197851e-02  -2.13844941e-02
  -1.42492031e-02  -7.11391219e-03   2.13787716e-05]
-1.36791
1.57286
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147963 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.72433
Epoch 1, cost is  3.68082
Epoch 2, cost is  3.65052
Epoch 3, cost is  3.62342
Epoch 4, cost is  3.59469
Training took 0.088375 minutes
Weight histogram
[2750 1679 1965 1469 1282 1335 1171 1217 4525  832] [ -6.56677037e-02  -5.90987955e-02  -5.25298872e-02  -4.59609790e-02
  -3.93920707e-02  -3.28231625e-02  -2.62542542e-02  -1.96853460e-02
  -1.31164377e-02  -6.54752948e-03   2.13787716e-05]
[4646 1059 1038 1125 1250 1418 1618 1755 2109 2207] [ -6.56677037e-02  -5.90987955e-02  -5.25298872e-02  -4.59609790e-02
  -3.93920707e-02  -3.28231625e-02  -2.62542542e-02  -1.96853460e-02
  -1.31164377e-02  -6.54752948e-03   2.13787716e-05]
-1.13309
1.43076
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.86242
Epoch 1, cost is  6.78978
Epoch 2, cost is  6.73898
Epoch 3, cost is  6.69528
Epoch 4, cost is  6.65074
Training took 0.092660 minutes
Weight histogram
[2724 6585 2361 1388  944  690  519  404  320  265] [ -1.45929288e-02  -1.31339439e-02  -1.16749590e-02  -1.02159740e-02
  -8.75698912e-03  -7.29800420e-03  -5.83901928e-03  -4.38003435e-03
  -2.92104943e-03  -1.46206451e-03  -3.07958544e-06]
[8496 4619 1360  457  408  349  143  125  125  118] [ -1.45929288e-02  -1.31339439e-02  -1.16749590e-02  -1.02159740e-02
  -8.75698912e-03  -7.29800420e-03  -5.83901928e-03  -4.38003435e-03
  -2.92104943e-03  -1.46206451e-03  -3.07958544e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042893 minutes
Epoch 0
Fine tuning took 0.042331 minutes
Epoch 0
Fine tuning took 0.042873 minutes
{'zero': {0: [0.31034482758620691, 0.16748768472906403, 0.15886699507389163, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.65147783251231528, 0.73522167487684731, 0.52216748768472909], 5: [0.11206896551724138, 0.18103448275862069, 0.10591133004926108, 0.33990147783251229], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31034482758620691, 0.17241379310344829, 0.18103448275862069, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.6428571428571429, 0.71305418719211822, 0.48399014778325122], 5: [0.11206896551724138, 0.18472906403940886, 0.10591133004926108, 0.36206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31034482758620691, 0.17980295566502463, 0.17610837438423646, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.64901477832512311, 0.72044334975369462, 0.49507389162561577], 5: [0.11206896551724138, 0.17118226600985223, 0.10344827586206896, 0.36206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31034482758620691, 0.16625615763546797, 0.18349753694581281, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.64532019704433496, 0.7142857142857143, 0.49137931034482757], 5: [0.11206896551724138, 0.18842364532019704, 0.10221674876847291, 0.35344827586206895], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149971 minutes
Weight histogram
[  48  282  448  804  587 2443 3194 3188 4241  965] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
[ 127  150  206  295  425  737  958 1730 3581 7991] [ -3.91909212e-04  -9.99960175e-05   1.91917177e-04   4.83830372e-04
   7.75743567e-04   1.06765676e-03   1.35956996e-03   1.65148315e-03
   1.94339635e-03   2.23530954e-03   2.52722274e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.52605
Epoch 1, cost is  3.48684
Epoch 2, cost is  3.45908
Epoch 3, cost is  3.43734
Epoch 4, cost is  3.4099
Training took 0.090682 minutes
Weight histogram
[3187 1724 2343 1192 1628 1450 1278 1230 1224  944] [ -7.13315308e-02  -6.41962399e-02  -5.70609489e-02  -4.99256579e-02
  -4.27903670e-02  -3.56550760e-02  -2.85197851e-02  -2.13844941e-02
  -1.42492031e-02  -7.11391219e-03   2.13787716e-05]
[2103 1161 1092 1161 1249 1442 1850 1726 2256 2160] [ -7.13315308e-02  -6.41962399e-02  -5.70609489e-02  -4.99256579e-02
  -4.27903670e-02  -3.56550760e-02  -2.85197851e-02  -2.13844941e-02
  -1.42492031e-02  -7.11391219e-03   2.13787716e-05]
-1.36791
1.57286
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148779 minutes
Weight histogram
[  39  185  637  884 1637 2397 3256 4450 3999  741] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[ 257  299  410  549  860 1394  905 1675 2705 9171] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.72433
Epoch 1, cost is  3.68082
Epoch 2, cost is  3.65052
Epoch 3, cost is  3.62342
Epoch 4, cost is  3.59469
Training took 0.088791 minutes
Weight histogram
[2750 1679 1965 1469 1282 1335 1171 1217 4525  832] [ -6.56677037e-02  -5.90987955e-02  -5.25298872e-02  -4.59609790e-02
  -3.93920707e-02  -3.28231625e-02  -2.62542542e-02  -1.96853460e-02
  -1.31164377e-02  -6.54752948e-03   2.13787716e-05]
[4646 1059 1038 1125 1250 1418 1618 1755 2109 2207] [ -6.56677037e-02  -5.90987955e-02  -5.25298872e-02  -4.59609790e-02
  -3.93920707e-02  -3.28231625e-02  -2.62542542e-02  -1.96853460e-02
  -1.31164377e-02  -6.54752948e-03   2.13787716e-05]
-1.13309
1.43076
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79334
Epoch 1, cost is  6.68034
Epoch 2, cost is  6.61864
Epoch 3, cost is  6.56633
Epoch 4, cost is  6.52053
Training took 0.117788 minutes
Weight histogram
[ 527  900 5755 3744 1910 1191  819  589  433  332] [ -1.79279987e-02  -1.61395036e-02  -1.43510085e-02  -1.25625134e-02
  -1.07740183e-02  -8.98552317e-03  -7.19702807e-03  -5.40853296e-03
  -3.62003786e-03  -1.83154276e-03  -4.30476575e-05]
[7475 4244 2298  623  420  391  274  160  152  163] [ -1.79279987e-02  -1.61395036e-02  -1.43510085e-02  -1.25625134e-02
  -1.07740183e-02  -8.98552317e-03  -7.19702807e-03  -5.40853296e-03
  -3.62003786e-03  -1.83154276e-03  -4.30476575e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046641 minutes
Epoch 0
Fine tuning took 0.047001 minutes
Epoch 0
Fine tuning took 0.043754 minutes
{'zero': {0: [0.29802955665024633, 0.1206896551724138, 0.1748768472906404, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.71551724137931039, 0.71182266009852213, 0.64901477832512311], 5: [0.13793103448275862, 0.16379310344827586, 0.11330049261083744, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29802955665024633, 0.10344827586206896, 0.19211822660098521, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.73768472906403937, 0.68965517241379315, 0.6071428571428571], 5: [0.13793103448275862, 0.15886699507389163, 0.11822660098522167, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29802955665024633, 0.11206896551724138, 0.19088669950738915, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.73768472906403937, 0.69950738916256161, 0.61576354679802958], 5: [0.13793103448275862, 0.15024630541871922, 0.10960591133004927, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29802955665024633, 0.11330049261083744, 0.19334975369458129, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.72660098522167482, 0.72044334975369462, 0.63669950738916259], 5: [0.13793103448275862, 0.16009852216748768, 0.086206896551724144, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149311 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.8323
Epoch 1, cost is  19.9109
Epoch 2, cost is  19.7492
Epoch 3, cost is  19.7998
Epoch 4, cost is  19.957
Training took 0.086392 minutes
Weight histogram
[2024 1839 1984 1836 2080 1633 1908 2013 2090  818] [-1.48884153 -1.3401588  -1.19147606 -1.04279333 -0.89411059 -0.74542785
 -0.59674512 -0.44806238 -0.29937965 -0.15069691 -0.00201417]
[1082 1790 1862 1833 1796 1993 1920 2035 1947 1967] [-1.48884153 -1.3401588  -1.19147606 -1.04279333 -0.89411059 -0.74542785
 -0.59674512 -0.44806238 -0.29937965 -0.15069691 -0.00201417]
-43.3195
53.3991
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149388 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.3368
Epoch 1, cost is  21.3659
Epoch 2, cost is  21.2393
Epoch 3, cost is  21.3046
Epoch 4, cost is  21.4929
Training took 0.086681 minutes
Weight histogram
[2164 1834 1730 1819 2058 1779 1822 2024 3162 1858] [-1.53922546 -1.38550433 -1.2317832  -1.07806207 -0.92434094 -0.77061982
 -0.61689869 -0.46317756 -0.30945643 -0.1557353  -0.00201417]
[2360 2551 1829 1839 1767 2073 1905 1887 1969 2070] [-1.53922546 -1.38550433 -1.2317832  -1.07806207 -0.92434094 -0.77061982
 -0.61689869 -0.46317756 -0.30945643 -0.1557353  -0.00201417]
-51.4731
55.8793
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.12438
Epoch 1, cost is  4.48273
Epoch 2, cost is  4.8765
Epoch 3, cost is  5.43153
Epoch 4, cost is  6.09278
Training took 0.082835 minutes
Weight histogram
[1363 2978 2875 2894 2563 2234 1259  823  528  708] [-0.29092243 -0.26199118 -0.23305992 -0.20412867 -0.17519741 -0.14626615
 -0.1173349  -0.08840364 -0.05947239 -0.03054113 -0.00160987]
[1047  954 1319 1722 1971 2158 2267 2321 2338 2128] [-0.29092243 -0.26199118 -0.23305992 -0.20412867 -0.17519741 -0.14626615
 -0.1173349  -0.08840364 -0.05947239 -0.03054113 -0.00160987]
-11.6165
14.2751
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040216 minutes
Epoch 0
Fine tuning took 0.041646 minutes
Epoch 0
Fine tuning took 0.041761 minutes
{'zero': {0: [0.27709359605911332, 0.41625615763546797, 0.52339901477832518, 0.47044334975369456], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56280788177339902, 0.43103448275862066, 0.36576354679802958, 0.35098522167487683], 5: [0.16009852216748768, 0.15270935960591134, 0.11083743842364532, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27709359605911332, 0.15763546798029557, 0.13177339901477833, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56280788177339902, 0.63793103448275867, 0.6428571428571429, 0.6576354679802956], 5: [0.16009852216748768, 0.20443349753694581, 0.22536945812807882, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27709359605911332, 0.20443349753694581, 0.20320197044334976, 0.22783251231527094], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56280788177339902, 0.64408866995073888, 0.58497536945812811, 0.60344827586206895], 5: [0.16009852216748768, 0.15147783251231528, 0.21182266009852216, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27709359605911332, 0.12192118226600986, 0.1206896551724138, 0.070197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56280788177339902, 0.55049261083743839, 0.56157635467980294, 0.66871921182266014], 5: [0.16009852216748768, 0.32758620689655171, 0.31773399014778325, 0.26108374384236455], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148608 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.8323
Epoch 1, cost is  19.9109
Epoch 2, cost is  19.7492
Epoch 3, cost is  19.7998
Epoch 4, cost is  19.957
Training took 0.086715 minutes
Weight histogram
[2024 1839 1984 1836 2080 1633 1908 2013 2090  818] [-1.48884153 -1.3401588  -1.19147606 -1.04279333 -0.89411059 -0.74542785
 -0.59674512 -0.44806238 -0.29937965 -0.15069691 -0.00201417]
[1082 1790 1862 1833 1796 1993 1920 2035 1947 1967] [-1.48884153 -1.3401588  -1.19147606 -1.04279333 -0.89411059 -0.74542785
 -0.59674512 -0.44806238 -0.29937965 -0.15069691 -0.00201417]
-43.3195
53.3991
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150434 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.3368
Epoch 1, cost is  21.3659
Epoch 2, cost is  21.2393
Epoch 3, cost is  21.3046
Epoch 4, cost is  21.4929
Training took 0.087027 minutes
Weight histogram
[2164 1834 1730 1819 2058 1779 1822 2024 3162 1858] [-1.53922546 -1.38550433 -1.2317832  -1.07806207 -0.92434094 -0.77061982
 -0.61689869 -0.46317756 -0.30945643 -0.1557353  -0.00201417]
[2360 2551 1829 1839 1767 2073 1905 1887 1969 2070] [-1.53922546 -1.38550433 -1.2317832  -1.07806207 -0.92434094 -0.77061982
 -0.61689869 -0.46317756 -0.30945643 -0.1557353  -0.00201417]
-51.4731
55.8793
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.79564
Epoch 1, cost is  3.7311
Epoch 2, cost is  3.88089
Epoch 3, cost is  4.23342
Epoch 4, cost is  4.6593
Training took 0.102162 minutes
Weight histogram
[1537 2720 2396 2762 2543 2155 1913  953  521  725] [-0.23938569 -0.21562639 -0.19186708 -0.16810777 -0.14434847 -0.12058916
 -0.09682985 -0.07307054 -0.04931124 -0.02555193 -0.00179262]
[1135 1104 1480 1704 1917 2127 2182 2264 2219 2093] [-0.23938569 -0.21562639 -0.19186708 -0.16810777 -0.14434847 -0.12058916
 -0.09682985 -0.07307054 -0.04931124 -0.02555193 -0.00179262]
-8.56558
9.19243
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048656 minutes
Epoch 0
Fine tuning took 0.041380 minutes
Epoch 0
Fine tuning took 0.042955 minutes
{'zero': {0: [0.17118226600985223, 0.2019704433497537, 0.38423645320197042, 0.31650246305418717], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68719211822660098, 0.60837438423645318, 0.51847290640394084, 0.57019704433497542], 5: [0.14162561576354679, 0.18965517241379309, 0.097290640394088676, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17118226600985223, 0.13916256157635468, 0.16133004926108374, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68719211822660098, 0.71182266009852213, 0.6711822660098522, 0.7142857142857143], 5: [0.14162561576354679, 0.14901477832512317, 0.16748768472906403, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17118226600985223, 0.17364532019704434, 0.15517241379310345, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68719211822660098, 0.66995073891625612, 0.67980295566502458, 0.69211822660098521], 5: [0.14162561576354679, 0.15640394088669951, 0.16502463054187191, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17118226600985223, 0.070197044334975367, 0.048029556650246302, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68719211822660098, 0.81527093596059108, 0.81650246305418717, 0.77832512315270941], 5: [0.14162561576354679, 0.1145320197044335, 0.1354679802955665, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150560 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  20.8323
Epoch 1, cost is  19.9109
Epoch 2, cost is  19.7492
Epoch 3, cost is  19.7998
Epoch 4, cost is  19.957
Training took 0.088232 minutes
Weight histogram
[2024 1839 1984 1836 2080 1633 1908 2013 2090  818] [-1.48884153 -1.3401588  -1.19147606 -1.04279333 -0.89411059 -0.74542785
 -0.59674512 -0.44806238 -0.29937965 -0.15069691 -0.00201417]
[1082 1790 1862 1833 1796 1993 1920 2035 1947 1967] [-1.48884153 -1.3401588  -1.19147606 -1.04279333 -0.89411059 -0.74542785
 -0.59674512 -0.44806238 -0.29937965 -0.15069691 -0.00201417]
-43.3195
53.3991
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149154 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  22.3368
Epoch 1, cost is  21.3659
Epoch 2, cost is  21.2393
Epoch 3, cost is  21.3046
Epoch 4, cost is  21.4929
Training took 0.086129 minutes
Weight histogram
[2164 1834 1730 1819 2058 1779 1822 2024 3162 1858] [-1.53922546 -1.38550433 -1.2317832  -1.07806207 -0.92434094 -0.77061982
 -0.61689869 -0.46317756 -0.30945643 -0.1557353  -0.00201417]
[2360 2551 1829 1839 1767 2073 1905 1887 1969 2070] [-1.53922546 -1.38550433 -1.2317832  -1.07806207 -0.92434094 -0.77061982
 -0.61689869 -0.46317756 -0.30945643 -0.1557353  -0.00201417]
-51.4731
55.8793
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.55765
Epoch 1, cost is  2.87238
Epoch 2, cost is  2.60329
Epoch 3, cost is  2.59401
Epoch 4, cost is  2.71429
Training took 0.121260 minutes
Weight histogram
[1917 2938 2608 2518 2348 2037 1730 1063  634  432] [-0.14670672 -0.13220875 -0.11771079 -0.10321282 -0.08871486 -0.0742169
 -0.05971893 -0.04522097 -0.030723   -0.01622504 -0.00172708]
[1264 1040 1422 1692 1887 2116 2170 2236 2349 2049] [-0.14670672 -0.13220875 -0.11771079 -0.10321282 -0.08871486 -0.0742169
 -0.05971893 -0.04522097 -0.030723   -0.01622504 -0.00172708]
-6.51553
7.36821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.052029 minutes
Epoch 0
Fine tuning took 0.044805 minutes
Epoch 0
Fine tuning took 0.047030 minutes
{'zero': {0: [0.14532019704433496, 0.35344827586206895, 0.24014778325123154, 0.3288177339901478], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.54556650246305416, 0.64655172413793105, 0.59605911330049266], 5: [0.13916256157635468, 0.10098522167487685, 0.11330049261083744, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14532019704433496, 0.18103448275862069, 0.18842364532019704, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.66009852216748766, 0.66133004926108374, 0.59605911330049266], 5: [0.13916256157635468, 0.15886699507389163, 0.15024630541871922, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14532019704433496, 0.17364532019704434, 0.18226600985221675, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.70197044334975367, 0.65640394088669951, 0.63916256157635465], 5: [0.13916256157635468, 0.12438423645320197, 0.16133004926108374, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14532019704433496, 0.1539408866995074, 0.16133004926108374, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.64778325123152714, 0.64901477832512311, 0.50369458128078815], 5: [0.13916256157635468, 0.19827586206896552, 0.18965517241379309, 0.29679802955665024], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149359 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.76612
Epoch 1, cost is  3.65732
Epoch 2, cost is  3.63216
Epoch 3, cost is  3.626
Epoch 4, cost is  3.63382
Training took 0.088997 minutes
Weight histogram
[3264 2730 2536 2048 2532 2145 1328  812  486  344] [ -2.19445229e-01  -1.97517073e-01  -1.75588917e-01  -1.53660761e-01
  -1.31732606e-01  -1.09804450e-01  -8.78762941e-02  -6.59481383e-02
  -4.40199826e-02  -2.20918268e-02  -1.63671051e-04]
[ 686  873 1373 1721 1866 1936 2287 2422 2459 2602] [ -2.19445229e-01  -1.97517073e-01  -1.75588917e-01  -1.53660761e-01
  -1.31732606e-01  -1.09804450e-01  -8.78762941e-02  -6.59481383e-02
  -4.40199826e-02  -2.20918268e-02  -1.63671051e-04]
-6.97861
7.55592
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.156617 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.97501
Epoch 1, cost is  3.82585
Epoch 2, cost is  3.7784
Epoch 3, cost is  3.76856
Epoch 4, cost is  3.7635
Training took 0.095606 minutes
Weight histogram
[3255 2767 2050 2451 2380 1885 2059 1703  876  824] [ -2.12138951e-01  -1.90941423e-01  -1.69743895e-01  -1.48546367e-01
  -1.27348839e-01  -1.06151311e-01  -8.49537830e-02  -6.37562550e-02
  -4.25587270e-02  -2.13611990e-02  -1.63671051e-04]
[1447 1860 1780 1665 1801 1953 2355 2375 2381 2633] [ -2.12138951e-01  -1.90941423e-01  -1.69743895e-01  -1.48546367e-01
  -1.27348839e-01  -1.06151311e-01  -8.49537830e-02  -6.37562550e-02
  -4.25587270e-02  -2.13611990e-02  -1.63671051e-04]
-6.08722
6.77204
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74004
Epoch 1, cost is  6.27192
Epoch 2, cost is  5.44355
Epoch 3, cost is  4.86729
Epoch 4, cost is  4.55285
Training took 0.089411 minutes
Weight histogram
[ 241 1633 2153 1334 1731 1392 1366 1598 6324  453] [ -8.42979699e-02  -7.58742939e-02  -6.74506178e-02  -5.90269417e-02
  -5.06032656e-02  -4.21795895e-02  -3.37559134e-02  -2.53322373e-02
  -1.69085612e-02  -8.48488516e-03  -6.12090735e-05]
[4930 1514 1185 1218 1351 1452 1477 1714 1866 1518] [ -8.42979699e-02  -7.58742939e-02  -6.74506178e-02  -5.90269417e-02
  -5.06032656e-02  -4.21795895e-02  -3.37559134e-02  -2.53322373e-02
  -1.69085612e-02  -8.48488516e-03  -6.12090735e-05]
-1.60314
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045612 minutes
Epoch 0
Fine tuning took 0.043863 minutes
Epoch 0
Fine tuning took 0.042716 minutes
{'zero': {0: [0.16379310344827586, 0.15763546798029557, 0.13423645320197045, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.65394088669950734, 0.67610837438423643, 0.69827586206896552], 5: [0.15147783251231528, 0.18842364532019704, 0.18965517241379309, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16379310344827586, 0.13177339901477833, 0.11822660098522167, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.6785714285714286, 0.64655172413793105, 0.64655172413793105], 5: [0.15147783251231528, 0.18965517241379309, 0.23522167487684728, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16379310344827586, 0.14162561576354679, 0.11822660098522167, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.69458128078817738, 0.66379310344827591, 0.68842364532019706], 5: [0.15147783251231528, 0.16379310344827586, 0.21798029556650247, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16379310344827586, 0.13793103448275862, 0.12931034482758622, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.65394088669950734, 0.66502463054187189, 0.67487684729064035], 5: [0.15147783251231528, 0.20812807881773399, 0.20566502463054187, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151678 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.76612
Epoch 1, cost is  3.65732
Epoch 2, cost is  3.63216
Epoch 3, cost is  3.626
Epoch 4, cost is  3.63382
Training took 0.090276 minutes
Weight histogram
[3264 2730 2536 2048 2532 2145 1328  812  486  344] [ -2.19445229e-01  -1.97517073e-01  -1.75588917e-01  -1.53660761e-01
  -1.31732606e-01  -1.09804450e-01  -8.78762941e-02  -6.59481383e-02
  -4.40199826e-02  -2.20918268e-02  -1.63671051e-04]
[ 686  873 1373 1721 1866 1936 2287 2422 2459 2602] [ -2.19445229e-01  -1.97517073e-01  -1.75588917e-01  -1.53660761e-01
  -1.31732606e-01  -1.09804450e-01  -8.78762941e-02  -6.59481383e-02
  -4.40199826e-02  -2.20918268e-02  -1.63671051e-04]
-6.97861
7.55592
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149956 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.97501
Epoch 1, cost is  3.82585
Epoch 2, cost is  3.7784
Epoch 3, cost is  3.76856
Epoch 4, cost is  3.7635
Training took 0.087731 minutes
Weight histogram
[3255 2767 2050 2451 2380 1885 2059 1703  876  824] [ -2.12138951e-01  -1.90941423e-01  -1.69743895e-01  -1.48546367e-01
  -1.27348839e-01  -1.06151311e-01  -8.49537830e-02  -6.37562550e-02
  -4.25587270e-02  -2.13611990e-02  -1.63671051e-04]
[1447 1860 1780 1665 1801 1953 2355 2375 2381 2633] [ -2.12138951e-01  -1.90941423e-01  -1.69743895e-01  -1.48546367e-01
  -1.27348839e-01  -1.06151311e-01  -8.49537830e-02  -6.37562550e-02
  -4.25587270e-02  -2.13611990e-02  -1.63671051e-04]
-6.08722
6.77204
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.70125
Epoch 1, cost is  6.24973
Epoch 2, cost is  5.26945
Epoch 3, cost is  4.55114
Epoch 4, cost is  4.11713
Training took 0.093682 minutes
Weight histogram
[ 322 2121 1853 1947 1548 1483 1312 3280 4109  250] [-0.06149432 -0.05536101 -0.04922769 -0.04309438 -0.03696106 -0.03082775
 -0.02469443 -0.01856112 -0.0124278  -0.00629448 -0.00016117]
[4803 1821 1113 1257 1421 1431 1562 1603 1852 1362] [-0.06149432 -0.05536101 -0.04922769 -0.04309438 -0.03696106 -0.03082775
 -0.02469443 -0.01856112 -0.0124278  -0.00629448 -0.00016117]
-1.39844
1.63867
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044270 minutes
Epoch 0
Fine tuning took 0.043625 minutes
Epoch 0
Fine tuning took 0.042743 minutes
{'zero': {0: [0.18596059113300492, 0.24384236453201971, 0.21305418719211822, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.53694581280788178, 0.6219211822660099, 0.6145320197044335], 5: [0.14655172413793102, 0.21921182266009853, 0.16502463054187191, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18596059113300492, 0.16133004926108374, 0.19211822660098521, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.63423645320197042, 0.62438423645320196, 0.61206896551724133], 5: [0.14655172413793102, 0.20443349753694581, 0.18349753694581281, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18596059113300492, 0.18103448275862069, 0.18103448275862069, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.59975369458128081, 0.64039408866995073, 0.65147783251231528], 5: [0.14655172413793102, 0.21921182266009853, 0.17857142857142858, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18596059113300492, 0.1625615763546798, 0.16748768472906403, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66748768472906406, 0.55418719211822665, 0.64655172413793105, 0.65270935960591137], 5: [0.14655172413793102, 0.28325123152709358, 0.18596059113300492, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150259 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.76612
Epoch 1, cost is  3.65732
Epoch 2, cost is  3.63216
Epoch 3, cost is  3.626
Epoch 4, cost is  3.63382
Training took 0.086628 minutes
Weight histogram
[3264 2730 2536 2048 2532 2145 1328  812  486  344] [ -2.19445229e-01  -1.97517073e-01  -1.75588917e-01  -1.53660761e-01
  -1.31732606e-01  -1.09804450e-01  -8.78762941e-02  -6.59481383e-02
  -4.40199826e-02  -2.20918268e-02  -1.63671051e-04]
[ 686  873 1373 1721 1866 1936 2287 2422 2459 2602] [ -2.19445229e-01  -1.97517073e-01  -1.75588917e-01  -1.53660761e-01
  -1.31732606e-01  -1.09804450e-01  -8.78762941e-02  -6.59481383e-02
  -4.40199826e-02  -2.20918268e-02  -1.63671051e-04]
-6.97861
7.55592
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147969 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.97501
Epoch 1, cost is  3.82585
Epoch 2, cost is  3.7784
Epoch 3, cost is  3.76856
Epoch 4, cost is  3.7635
Training took 0.089115 minutes
Weight histogram
[3255 2767 2050 2451 2380 1885 2059 1703  876  824] [ -2.12138951e-01  -1.90941423e-01  -1.69743895e-01  -1.48546367e-01
  -1.27348839e-01  -1.06151311e-01  -8.49537830e-02  -6.37562550e-02
  -4.25587270e-02  -2.13611990e-02  -1.63671051e-04]
[1447 1860 1780 1665 1801 1953 2355 2375 2381 2633] [ -2.12138951e-01  -1.90941423e-01  -1.69743895e-01  -1.48546367e-01
  -1.27348839e-01  -1.06151311e-01  -8.49537830e-02  -6.37562550e-02
  -4.25587270e-02  -2.13611990e-02  -1.63671051e-04]
-6.08722
6.77204
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.60442
Epoch 1, cost is  6.19881
Epoch 2, cost is  5.19043
Epoch 3, cost is  4.32711
Epoch 4, cost is  3.68401
Training took 0.118233 minutes
Weight histogram
[ 342 1998 2532 2310 2125 2909 4313 1291  270  135] [-0.03940029 -0.03547894 -0.03155759 -0.02763623 -0.02371488 -0.01979353
 -0.01587218 -0.01195083 -0.00802947 -0.00410812 -0.00018677]
[4754 2153 1162 1332 1361 1419 1522 1638 1795 1089] [-0.03940029 -0.03547894 -0.03155759 -0.02763623 -0.02371488 -0.01979353
 -0.01587218 -0.01195083 -0.00802947 -0.00410812 -0.00018677]
-1.29229
1.49735
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044233 minutes
Epoch 0
Fine tuning took 0.044180 minutes
Epoch 0
Fine tuning took 0.046836 minutes
{'zero': {0: [0.18719211822660098, 0.22536945812807882, 0.18349753694581281, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.48029556650246308, 0.55172413793103448, 0.51600985221674878], 5: [0.20812807881773399, 0.29433497536945813, 0.26477832512315269, 0.26231527093596058], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18719211822660098, 0.20812807881773399, 0.15024630541871922, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.6071428571428571, 0.67733990147783252, 0.68719211822660098], 5: [0.20812807881773399, 0.18472906403940886, 0.17241379310344829, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18719211822660098, 0.18103448275862069, 0.18103448275862069, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.60221674876847286, 0.60467980295566504, 0.63054187192118227], 5: [0.20812807881773399, 0.21674876847290642, 0.21428571428571427, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18719211822660098, 0.19950738916256158, 0.16379310344827586, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.64039408866995073, 0.68226600985221675, 0.69088669950738912], 5: [0.20812807881773399, 0.16009852216748768, 0.1539408866995074, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149493 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4342
Epoch 1, cost is  3.39993
Epoch 2, cost is  3.37296
Epoch 3, cost is  3.35419
Epoch 4, cost is  3.33355
Training took 0.088315 minutes
Weight histogram
[2861 2936 2514 1643 1423 1780 1328 1394  793 1553] [ -7.70533755e-02  -6.93459001e-02  -6.16384247e-02  -5.39309492e-02
  -4.62234738e-02  -3.85159984e-02  -3.08085230e-02  -2.31010475e-02
  -1.53935721e-02  -7.68609666e-03   2.13787716e-05]
[2206 1228 1192 1278 1389 1786 1905 2215 2353 2673] [ -7.70533755e-02  -6.93459001e-02  -6.16384247e-02  -5.39309492e-02
  -4.62234738e-02  -3.85159984e-02  -3.08085230e-02  -2.31010475e-02
  -1.53935721e-02  -7.68609666e-03   2.13787716e-05]
-1.52549
1.72015
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148511 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.5453
Epoch 1, cost is  3.50349
Epoch 2, cost is  3.47399
Epoch 3, cost is  3.45001
Epoch 4, cost is  3.42806
Training took 0.086288 minutes
Weight histogram
[2642 2731 1881 1939 1372 1496 1282 1298 3651 1958] [ -7.14396387e-02  -6.42935370e-02  -5.71474352e-02  -5.00013335e-02
  -4.28552317e-02  -3.57091300e-02  -2.85630282e-02  -2.14169265e-02
  -1.42708247e-02  -7.12472298e-03   2.13787716e-05]
[4736 1124 1141 1257 1389 1645 1824 2122 2358 2654] [ -7.14396387e-02  -6.42935370e-02  -5.71474352e-02  -5.00013335e-02
  -4.28552317e-02  -3.57091300e-02  -2.85630282e-02  -2.14169265e-02
  -1.42708247e-02  -7.12472298e-03   2.13787716e-05]
-1.18025
1.5975
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8861
Epoch 1, cost is  6.8275
Epoch 2, cost is  6.78151
Epoch 3, cost is  6.73879
Epoch 4, cost is  6.6968
Training took 0.081117 minutes
Weight histogram
[4467 6785 2282 1395  970  718  541  435  344  288] [ -1.41098574e-02  -1.26891519e-02  -1.12684464e-02  -9.84774087e-03
  -8.42703537e-03  -7.00632986e-03  -5.58562436e-03  -4.16491886e-03
  -2.74421335e-03  -1.32350785e-03   9.71976551e-05]
[10610  4997  1058   466   406   241   131   116   100   100] [ -1.41098574e-02  -1.26891519e-02  -1.12684464e-02  -9.84774087e-03
  -8.42703537e-03  -7.00632986e-03  -5.58562436e-03  -4.16491886e-03
  -2.74421335e-03  -1.32350785e-03   9.71976551e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042366 minutes
Epoch 0
Fine tuning took 0.043002 minutes
Epoch 0
Fine tuning took 0.041277 minutes
{'zero': {0: [0.046798029556650245, 0.20566502463054187, 0.33743842364532017, 0.36330049261083741], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83620689655172409, 0.48891625615763545, 0.33990147783251229, 0.50615763546798032], 5: [0.11699507389162561, 0.30541871921182268, 0.32266009852216748, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.046798029556650245, 0.2105911330049261, 0.34729064039408869, 0.32389162561576357], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83620689655172409, 0.48029556650246308, 0.32758620689655171, 0.53940886699507384], 5: [0.11699507389162561, 0.30911330049261082, 0.3251231527093596, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.046798029556650245, 0.26477832512315269, 0.34113300492610837, 0.33374384236453203], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83620689655172409, 0.42980295566502463, 0.3460591133004926, 0.53448275862068961], 5: [0.11699507389162561, 0.30541871921182268, 0.31280788177339902, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.046798029556650245, 0.24630541871921183, 0.3460591133004926, 0.31896551724137934], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83620689655172409, 0.46182266009852219, 0.31280788177339902, 0.54064039408866993], 5: [0.11699507389162561, 0.29187192118226601, 0.34113300492610837, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151095 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4342
Epoch 1, cost is  3.39993
Epoch 2, cost is  3.37296
Epoch 3, cost is  3.35419
Epoch 4, cost is  3.33355
Training took 0.086984 minutes
Weight histogram
[2861 2936 2514 1643 1423 1780 1328 1394  793 1553] [ -7.70533755e-02  -6.93459001e-02  -6.16384247e-02  -5.39309492e-02
  -4.62234738e-02  -3.85159984e-02  -3.08085230e-02  -2.31010475e-02
  -1.53935721e-02  -7.68609666e-03   2.13787716e-05]
[2206 1228 1192 1278 1389 1786 1905 2215 2353 2673] [ -7.70533755e-02  -6.93459001e-02  -6.16384247e-02  -5.39309492e-02
  -4.62234738e-02  -3.85159984e-02  -3.08085230e-02  -2.31010475e-02
  -1.53935721e-02  -7.68609666e-03   2.13787716e-05]
-1.52549
1.72015
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147542 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.5453
Epoch 1, cost is  3.50349
Epoch 2, cost is  3.47399
Epoch 3, cost is  3.45001
Epoch 4, cost is  3.42806
Training took 0.089992 minutes
Weight histogram
[2642 2731 1881 1939 1372 1496 1282 1298 3651 1958] [ -7.14396387e-02  -6.42935370e-02  -5.71474352e-02  -5.00013335e-02
  -4.28552317e-02  -3.57091300e-02  -2.85630282e-02  -2.14169265e-02
  -1.42708247e-02  -7.12472298e-03   2.13787716e-05]
[4736 1124 1141 1257 1389 1645 1824 2122 2358 2654] [ -7.14396387e-02  -6.42935370e-02  -5.71474352e-02  -5.00013335e-02
  -4.28552317e-02  -3.57091300e-02  -2.85630282e-02  -2.14169265e-02
  -1.42708247e-02  -7.12472298e-03   2.13787716e-05]
-1.18025
1.5975
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.86201
Epoch 1, cost is  6.78889
Epoch 2, cost is  6.73771
Epoch 3, cost is  6.69317
Epoch 4, cost is  6.64922
Training took 0.092738 minutes
Weight histogram
[2724 7542 2765 1604 1081  791  592  460  365  301] [ -1.45929288e-02  -1.31339147e-02  -1.16749005e-02  -1.02158863e-02
  -8.75687217e-03  -7.29785801e-03  -5.83884386e-03  -4.37982970e-03
  -2.92081554e-03  -1.46180138e-03  -2.78721745e-06]
[9652 5262 1586  457  408  349  143  125  125  118] [ -1.45929288e-02  -1.31339147e-02  -1.16749005e-02  -1.02158863e-02
  -8.75687217e-03  -7.29785801e-03  -5.83884386e-03  -4.37982970e-03
  -2.92081554e-03  -1.46180138e-03  -2.78721745e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042446 minutes
Epoch 0
Fine tuning took 0.044305 minutes
Epoch 0
Fine tuning took 0.044214 minutes
{'zero': {0: [0.048029556650246302, 0.25738916256157635, 0.34236453201970446, 0.32266009852216748], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84729064039408863, 0.42241379310344829, 0.29556650246305421, 0.52832512315270941], 5: [0.10467980295566502, 0.32019704433497537, 0.36206896551724138, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.048029556650246302, 0.26970443349753692, 0.39162561576354682, 0.33004926108374383], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84729064039408863, 0.42733990147783252, 0.27339901477832512, 0.52586206896551724], 5: [0.10467980295566502, 0.30295566502463056, 0.33497536945812806, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.048029556650246302, 0.24876847290640394, 0.35344827586206895, 0.30911330049261082], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84729064039408863, 0.4248768472906404, 0.31034482758620691, 0.55049261083743839], 5: [0.10467980295566502, 0.32635467980295568, 0.33620689655172414, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.048029556650246302, 0.28448275862068967, 0.38793103448275862, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84729064039408863, 0.38669950738916259, 0.24384236453201971, 0.52093596059113301], 5: [0.10467980295566502, 0.3288177339901478, 0.3682266009852217, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149076 minutes
Weight histogram
[  50  294  483  814  684 2819 2972 4141 5128  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 10016] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4342
Epoch 1, cost is  3.39993
Epoch 2, cost is  3.37296
Epoch 3, cost is  3.35419
Epoch 4, cost is  3.33355
Training took 0.088528 minutes
Weight histogram
[2861 2936 2514 1643 1423 1780 1328 1394  793 1553] [ -7.70533755e-02  -6.93459001e-02  -6.16384247e-02  -5.39309492e-02
  -4.62234738e-02  -3.85159984e-02  -3.08085230e-02  -2.31010475e-02
  -1.53935721e-02  -7.68609666e-03   2.13787716e-05]
[2206 1228 1192 1278 1389 1786 1905 2215 2353 2673] [ -7.70533755e-02  -6.93459001e-02  -6.16384247e-02  -5.39309492e-02
  -4.62234738e-02  -3.85159984e-02  -3.08085230e-02  -2.31010475e-02
  -1.53935721e-02  -7.68609666e-03   2.13787716e-05]
-1.52549
1.72015
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148087 minutes
Weight histogram
[  39  185  637  884 1637 2397 3264 4786 5128 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 11196] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.5453
Epoch 1, cost is  3.50349
Epoch 2, cost is  3.47399
Epoch 3, cost is  3.45001
Epoch 4, cost is  3.42806
Training took 0.086590 minutes
Weight histogram
[2642 2731 1881 1939 1372 1496 1282 1298 3651 1958] [ -7.14396387e-02  -6.42935370e-02  -5.71474352e-02  -5.00013335e-02
  -4.28552317e-02  -3.57091300e-02  -2.85630282e-02  -2.14169265e-02
  -1.42708247e-02  -7.12472298e-03   2.13787716e-05]
[4736 1124 1141 1257 1389 1645 1824 2122 2358 2654] [ -7.14396387e-02  -6.42935370e-02  -5.71474352e-02  -5.00013335e-02
  -4.28552317e-02  -3.57091300e-02  -2.85630282e-02  -2.14169265e-02
  -1.42708247e-02  -7.12472298e-03   2.13787716e-05]
-1.18025
1.5975
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79218
Epoch 1, cost is  6.67865
Epoch 2, cost is  6.61684
Epoch 3, cost is  6.56426
Epoch 4, cost is  6.51839
Training took 0.120044 minutes
Weight histogram
[ 527  900 6319 4423 2209 1367  937  672  494  377] [ -1.79279987e-02  -1.61394760e-02  -1.43509533e-02  -1.25624306e-02
  -1.07739079e-02  -8.98538516e-03  -7.19686245e-03  -5.40833975e-03
  -3.61981704e-03  -1.83129434e-03  -4.27716368e-05]
[8475 4827 2740  623  420  391  274  160  152  163] [ -1.79279987e-02  -1.61394760e-02  -1.43509533e-02  -1.25624306e-02
  -1.07739079e-02  -8.98538516e-03  -7.19686245e-03  -5.40833975e-03
  -3.61981704e-03  -1.83129434e-03  -4.27716368e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043892 minutes
Epoch 0
Fine tuning took 0.046400 minutes
Epoch 0
Fine tuning took 0.046330 minutes
{'zero': {0: [0.024630541871921183, 0.30788177339901479, 0.21674876847290642, 0.30788177339901479], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86206896551724133, 0.40763546798029554, 0.43226600985221675, 0.54187192118226601], 5: [0.11330049261083744, 0.28448275862068967, 0.35098522167487683, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.024630541871921183, 0.2894088669950739, 0.23522167487684728, 0.33743842364532017], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86206896551724133, 0.38423645320197042, 0.41502463054187194, 0.52832512315270941], 5: [0.11330049261083744, 0.32635467980295568, 0.34975369458128081, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.024630541871921183, 0.26477832512315269, 0.20812807881773399, 0.32142857142857145], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86206896551724133, 0.41379310344827586, 0.42857142857142855, 0.5357142857142857], 5: [0.11330049261083744, 0.32142857142857145, 0.36330049261083741, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.024630541871921183, 0.26108374384236455, 0.23275862068965517, 0.34482758620689657], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86206896551724133, 0.39655172413793105, 0.41256157635467983, 0.50246305418719217], 5: [0.11330049261083744, 0.34236453201970446, 0.35467980295566504, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.151648 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  21.2335
Epoch 1, cost is  20.3186
Epoch 2, cost is  20.1427
Epoch 3, cost is  20.1662
Epoch 4, cost is  20.2939
Training took 0.088874 minutes
Weight histogram
[1995 2139 1907 2051 2347 2053 2123 2141 2488 1006] [-1.66615176 -1.499738   -1.33332424 -1.16691049 -1.00049673 -0.83408297
 -0.66766921 -0.50125545 -0.33484169 -0.16842793 -0.00201417]
[1245 2011 2051 1983 2116 2127 2215 2164 2176 2162] [-1.66615176 -1.499738   -1.33332424 -1.16691049 -1.00049673 -0.83408297
 -0.66766921 -0.50125545 -0.33484169 -0.16842793 -0.00201417]
-51.7245
59.1296
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149430 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  24.621
Epoch 1, cost is  23.5027
Epoch 2, cost is  23.339
Epoch 3, cost is  23.3995
Epoch 4, cost is  23.5462
Training took 0.088142 minutes
Weight histogram
[2016 2410 1762 1952 2036 2414 1954 2241 3219 2271] [-1.71186638 -1.54088116 -1.36989594 -1.19891072 -1.0279255  -0.85694028
 -0.68595506 -0.51496984 -0.34398461 -0.17299939 -0.00201417]
[2716 2562 2052 1965 2154 2162 2094 2162 2263 2145] [-1.71186638 -1.54088116 -1.36989594 -1.19891072 -1.0279255  -0.85694028
 -0.68595506 -0.51496984 -0.34398461 -0.17299939 -0.00201417]
-52.5109
55.8793
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.12928
Epoch 1, cost is  4.50448
Epoch 2, cost is  4.91932
Epoch 3, cost is  5.49655
Epoch 4, cost is  6.1982
Training took 0.082241 minutes
Weight histogram
[1418 3364 3179 3256 2826 2502 1414  920  585  786] [-0.29092243 -0.26198998 -0.23305753 -0.20412508 -0.17519264 -0.14626019
 -0.11732774 -0.08839529 -0.05946284 -0.03053039 -0.00159794]
[1161 1061 1469 1915 2195 2397 2517 2568 2595 2372] [-0.29092243 -0.26198998 -0.23305753 -0.20412508 -0.17519264 -0.14626019
 -0.11732774 -0.08839529 -0.05946284 -0.03053039 -0.00159794]
-11.6165
14.2751
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041138 minutes
Epoch 0
Fine tuning took 0.039942 minutes
Epoch 0
Fine tuning took 0.041828 minutes
{'zero': {0: [0.22413793103448276, 0.092364532019704432, 0.10467980295566502, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.72044334975369462, 0.57389162561576357, 0.68965517241379315], 5: [0.20443349753694581, 0.18719211822660098, 0.32142857142857145, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22413793103448276, 0.23029556650246305, 0.20812807881773399, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.57758620689655171, 0.64655172413793105, 0.61083743842364535], 5: [0.20443349753694581, 0.19211822660098521, 0.14532019704433496, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22413793103448276, 0.20443349753694581, 0.22413793103448276, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.62684729064039413, 0.59359605911330049, 0.60591133004926112], 5: [0.20443349753694581, 0.16871921182266009, 0.18226600985221675, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22413793103448276, 0.21674876847290642, 0.22660098522167488, 0.2857142857142857], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.52709359605911332, 0.56157635467980294, 0.46305418719211822], 5: [0.20443349753694581, 0.25615763546798032, 0.21182266009852216, 0.25123152709359609], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149034 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  21.2335
Epoch 1, cost is  20.3186
Epoch 2, cost is  20.1427
Epoch 3, cost is  20.1662
Epoch 4, cost is  20.2939
Training took 0.087275 minutes
Weight histogram
[1995 2139 1907 2051 2347 2053 2123 2141 2488 1006] [-1.66615176 -1.499738   -1.33332424 -1.16691049 -1.00049673 -0.83408297
 -0.66766921 -0.50125545 -0.33484169 -0.16842793 -0.00201417]
[1245 2011 2051 1983 2116 2127 2215 2164 2176 2162] [-1.66615176 -1.499738   -1.33332424 -1.16691049 -1.00049673 -0.83408297
 -0.66766921 -0.50125545 -0.33484169 -0.16842793 -0.00201417]
-51.7245
59.1296
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147906 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  24.621
Epoch 1, cost is  23.5027
Epoch 2, cost is  23.339
Epoch 3, cost is  23.3995
Epoch 4, cost is  23.5462
Training took 0.088837 minutes
Weight histogram
[2016 2410 1762 1952 2036 2414 1954 2241 3219 2271] [-1.71186638 -1.54088116 -1.36989594 -1.19891072 -1.0279255  -0.85694028
 -0.68595506 -0.51496984 -0.34398461 -0.17299939 -0.00201417]
[2716 2562 2052 1965 2154 2162 2094 2162 2263 2145] [-1.71186638 -1.54088116 -1.36989594 -1.19891072 -1.0279255  -0.85694028
 -0.68595506 -0.51496984 -0.34398461 -0.17299939 -0.00201417]
-52.5109
55.8793
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.79641
Epoch 1, cost is  3.74998
Epoch 2, cost is  3.91862
Epoch 3, cost is  4.29052
Epoch 4, cost is  4.70341
Training took 0.093802 minutes
Weight histogram
[1619 3039 2689 3084 2837 2389 2123 1086  578  806] [-0.23938569 -0.21562555 -0.1918654  -0.16810526 -0.14434511 -0.12058497
 -0.09682482 -0.07306468 -0.04930453 -0.02554438 -0.00178424]
[1260 1228 1643 1894 2131 2363 2425 2513 2465 2328] [-0.23938569 -0.21562555 -0.1918654  -0.16810526 -0.14434511 -0.12058497
 -0.09682482 -0.07306468 -0.04930453 -0.02554438 -0.00178424]
-8.56558
9.19243
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040249 minutes
Epoch 0
Fine tuning took 0.042339 minutes
Epoch 0
Fine tuning took 0.041565 minutes
{'zero': {0: [0.23029556650246305, 0.094827586206896547, 0.16133004926108374, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.69827586206896552, 0.58251231527093594, 0.64778325123152714], 5: [0.13423645320197045, 0.20689655172413793, 0.25615763546798032, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23029556650246305, 0.22783251231527094, 0.2376847290640394, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.66995073891625612, 0.63177339901477836, 0.70073891625615758], 5: [0.13423645320197045, 0.10221674876847291, 0.13054187192118227, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23029556650246305, 0.21305418719211822, 0.20566502463054187, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.63669950738916259, 0.62684729064039413, 0.66502463054187189], 5: [0.13423645320197045, 0.15024630541871922, 0.16748768472906403, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23029556650246305, 0.27339901477832512, 0.29187192118226601, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.67610837438423643, 0.59113300492610843, 0.69704433497536944], 5: [0.13423645320197045, 0.050492610837438424, 0.11699507389162561, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.166228 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  21.2335
Epoch 1, cost is  20.3186
Epoch 2, cost is  20.1427
Epoch 3, cost is  20.1662
Epoch 4, cost is  20.2939
Training took 0.096784 minutes
Weight histogram
[1995 2139 1907 2051 2347 2053 2123 2141 2488 1006] [-1.66615176 -1.499738   -1.33332424 -1.16691049 -1.00049673 -0.83408297
 -0.66766921 -0.50125545 -0.33484169 -0.16842793 -0.00201417]
[1245 2011 2051 1983 2116 2127 2215 2164 2176 2162] [-1.66615176 -1.499738   -1.33332424 -1.16691049 -1.00049673 -0.83408297
 -0.66766921 -0.50125545 -0.33484169 -0.16842793 -0.00201417]
-51.7245
59.1296
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.165414 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  24.621
Epoch 1, cost is  23.5027
Epoch 2, cost is  23.339
Epoch 3, cost is  23.3995
Epoch 4, cost is  23.5462
Training took 0.095633 minutes
Weight histogram
[2016 2410 1762 1952 2036 2414 1954 2241 3219 2271] [-1.71186638 -1.54088116 -1.36989594 -1.19891072 -1.0279255  -0.85694028
 -0.68595506 -0.51496984 -0.34398461 -0.17299939 -0.00201417]
[2716 2562 2052 1965 2154 2162 2094 2162 2263 2145] [-1.71186638 -1.54088116 -1.36989594 -1.19891072 -1.0279255  -0.85694028
 -0.68595506 -0.51496984 -0.34398461 -0.17299939 -0.00201417]
-52.5109
55.8793
... retrieved True_rbm_200-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.55592
Epoch 1, cost is  2.87546
Epoch 2, cost is  2.59919
Epoch 3, cost is  2.56692
Epoch 4, cost is  2.66113
Training took 0.131227 minutes
Weight histogram
[2105 3265 2883 2817 2605 2258 1947 1184  707  479] [-0.14670672 -0.13220875 -0.11771079 -0.10321282 -0.08871486 -0.0742169
 -0.05971893 -0.04522097 -0.030723   -0.01622504 -0.00172708]
[1402 1158 1583 1881 2093 2348 2404 2476 2605 2300] [-0.14670672 -0.13220875 -0.11771079 -0.10321282 -0.08871486 -0.0742169
 -0.05971893 -0.04522097 -0.030723   -0.01622504 -0.00172708]
-6.51553
7.36821
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043194 minutes
Epoch 0
Fine tuning took 0.045461 minutes
Epoch 0
Fine tuning took 0.044761 minutes
{'zero': {0: [0.16009852216748768, 0.10467980295566502, 0.2105911330049261, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.83128078817733986, 0.63177339901477836, 0.69458128078817738], 5: [0.080049261083743842, 0.064039408866995079, 0.15763546798029557, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16009852216748768, 0.19088669950738915, 0.21305418719211822, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.70812807881773399, 0.6711822660098522, 0.62931034482758619], 5: [0.080049261083743842, 0.10098522167487685, 0.11576354679802955, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16009852216748768, 0.16748768472906403, 0.19704433497536947, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.69827586206896552, 0.68103448275862066, 0.67364532019704437], 5: [0.080049261083743842, 0.13423645320197045, 0.12192118226600986, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16009852216748768, 0.16009852216748768, 0.27093596059113301, 0.33374384236453203], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.76847290640394084, 0.60467980295566504, 0.47044334975369456], 5: [0.080049261083743842, 0.071428571428571425, 0.12438423645320197, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.166104 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.84377
Epoch 1, cost is  3.71375
Epoch 2, cost is  3.69227
Epoch 3, cost is  3.68205
Epoch 4, cost is  3.67995
Training took 0.097045 minutes
Weight histogram
[3399 2961 2429 3254 2128 2458 1778  950  523  370] [ -2.36768559e-01  -2.13108070e-01  -1.89447581e-01  -1.65787092e-01
  -1.42126604e-01  -1.18466115e-01  -9.48056261e-02  -7.11451373e-02
  -4.74846486e-02  -2.38241598e-02  -1.63671051e-04]
[ 738  992 1566 1925 1983 2326 2395 2771 2717 2837] [ -2.36768559e-01  -2.13108070e-01  -1.89447581e-01  -1.65787092e-01
  -1.42126604e-01  -1.18466115e-01  -9.48056261e-02  -7.11451373e-02
  -4.74846486e-02  -2.38241598e-02  -1.63671051e-04]
-7.52581
7.87328
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.152962 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.36822
Epoch 1, cost is  4.22011
Epoch 2, cost is  4.18655
Epoch 3, cost is  4.17838
Epoch 4, cost is  4.18287
Training took 0.089506 minutes
Weight histogram
[2192 3461 2681 2990 2409 2273 2213 2070 1095  891] [ -2.34628633e-01  -2.11182137e-01  -1.87735640e-01  -1.64289144e-01
  -1.40842648e-01  -1.17396152e-01  -9.39496557e-02  -7.05031595e-02
  -4.70566634e-02  -2.36101672e-02  -1.63671051e-04]
[1575 2149 1757 1874 1964 2390 2507 2650 2750 2659] [ -2.34628633e-01  -2.11182137e-01  -1.87735640e-01  -1.64289144e-01
  -1.40842648e-01  -1.17396152e-01  -9.39496557e-02  -7.05031595e-02
  -4.70566634e-02  -2.36101672e-02  -1.63671051e-04]
-6.73587
7.63732
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74064
Epoch 1, cost is  6.26768
Epoch 2, cost is  5.41774
Epoch 3, cost is  4.85208
Epoch 4, cost is  4.55562
Training took 0.084150 minutes
Weight histogram
[ 241 1781 2426 1494 1944 1557 1526 1761 7012  508] [ -8.42979699e-02  -7.58742939e-02  -6.74506178e-02  -5.90269417e-02
  -5.06032656e-02  -4.21795895e-02  -3.37559134e-02  -2.53322373e-02
  -1.69085612e-02  -8.48488516e-03  -6.12090735e-05]
[5439 1688 1315 1356 1510 1623 1649 1914 2090 1666] [ -8.42979699e-02  -7.58742939e-02  -6.74506178e-02  -5.90269417e-02
  -5.06032656e-02  -4.21795895e-02  -3.37559134e-02  -2.53322373e-02
  -1.69085612e-02  -8.48488516e-03  -6.12090735e-05]
-1.60314
2.15588
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042845 minutes
Epoch 0
Fine tuning took 0.042768 minutes
Epoch 0
Fine tuning took 0.040714 minutes
{'zero': {0: [0.15640394088669951, 0.26600985221674878, 0.29679802955665024, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.57512315270935965, 0.50862068965517238, 0.58620689655172409], 5: [0.16748768472906403, 0.15886699507389163, 0.19458128078817735, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15640394088669951, 0.28078817733990147, 0.31773399014778325, 0.30418719211822659], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.54556650246305416, 0.46182266009852219, 0.51970443349753692], 5: [0.16748768472906403, 0.17364532019704434, 0.22044334975369459, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15640394088669951, 0.26108374384236455, 0.34236453201970446, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.56403940886699511, 0.46921182266009853, 0.52832512315270941], 5: [0.16748768472906403, 0.1748768472906404, 0.18842364532019704, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15640394088669951, 0.26108374384236455, 0.33128078817733991, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67610837438423643, 0.55541871921182262, 0.46182266009852219, 0.54926108374384242], 5: [0.16748768472906403, 0.18349753694581281, 0.20689655172413793, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149732 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.84377
Epoch 1, cost is  3.71375
Epoch 2, cost is  3.69227
Epoch 3, cost is  3.68205
Epoch 4, cost is  3.67995
Training took 0.086682 minutes
Weight histogram
[3399 2961 2429 3254 2128 2458 1778  950  523  370] [ -2.36768559e-01  -2.13108070e-01  -1.89447581e-01  -1.65787092e-01
  -1.42126604e-01  -1.18466115e-01  -9.48056261e-02  -7.11451373e-02
  -4.74846486e-02  -2.38241598e-02  -1.63671051e-04]
[ 738  992 1566 1925 1983 2326 2395 2771 2717 2837] [ -2.36768559e-01  -2.13108070e-01  -1.89447581e-01  -1.65787092e-01
  -1.42126604e-01  -1.18466115e-01  -9.48056261e-02  -7.11451373e-02
  -4.74846486e-02  -2.38241598e-02  -1.63671051e-04]
-7.52581
7.87328
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147823 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.36822
Epoch 1, cost is  4.22011
Epoch 2, cost is  4.18655
Epoch 3, cost is  4.17838
Epoch 4, cost is  4.18287
Training took 0.089280 minutes
Weight histogram
[2192 3461 2681 2990 2409 2273 2213 2070 1095  891] [ -2.34628633e-01  -2.11182137e-01  -1.87735640e-01  -1.64289144e-01
  -1.40842648e-01  -1.17396152e-01  -9.39496557e-02  -7.05031595e-02
  -4.70566634e-02  -2.36101672e-02  -1.63671051e-04]
[1575 2149 1757 1874 1964 2390 2507 2650 2750 2659] [ -2.34628633e-01  -2.11182137e-01  -1.87735640e-01  -1.64289144e-01
  -1.40842648e-01  -1.17396152e-01  -9.39496557e-02  -7.05031595e-02
  -4.70566634e-02  -2.36101672e-02  -1.63671051e-04]
-6.73587
7.63732
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.70179
Epoch 1, cost is  6.24158
Epoch 2, cost is  5.23681
Epoch 3, cost is  4.53673
Epoch 4, cost is  4.11845
Training took 0.093750 minutes
Weight histogram
[ 322 2361 2071 2163 1737 1670 1447 3572 4628  279] [-0.06149432 -0.05536101 -0.04922769 -0.04309438 -0.03696106 -0.03082775
 -0.02469443 -0.01856112 -0.0124278  -0.00629448 -0.00016117]
[5298 2037 1236 1403 1585 1596 1744 1791 2068 1492] [-0.06149432 -0.05536101 -0.04922769 -0.04309438 -0.03696106 -0.03082775
 -0.02469443 -0.01856112 -0.0124278  -0.00629448 -0.00016117]
-1.39844
1.63867
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041676 minutes
Epoch 0
Fine tuning took 0.041820 minutes
Epoch 0
Fine tuning took 0.043776 minutes
{'zero': {0: [0.20443349753694581, 0.21674876847290642, 0.21182266009852216, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.52709359605911332, 0.53078817733990147, 0.60098522167487689], 5: [0.19088669950738915, 0.25615763546798032, 0.25738916256157635, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20443349753694581, 0.29926108374384236, 0.31280788177339902, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.5, 0.49261083743842365, 0.52955665024630538], 5: [0.19088669950738915, 0.20073891625615764, 0.19458128078817735, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20443349753694581, 0.25246305418719212, 0.25123152709359609, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.50862068965517238, 0.51847290640394084, 0.60221674876847286], 5: [0.19088669950738915, 0.23891625615763548, 0.23029556650246305, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20443349753694581, 0.30911330049261082, 0.27586206896551724, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60467980295566504, 0.48399014778325122, 0.49753694581280788, 0.56650246305418717], 5: [0.19088669950738915, 0.20689655172413793, 0.22660098522167488, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149428 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.84377
Epoch 1, cost is  3.71375
Epoch 2, cost is  3.69227
Epoch 3, cost is  3.68205
Epoch 4, cost is  3.67995
Training took 0.090326 minutes
Weight histogram
[3399 2961 2429 3254 2128 2458 1778  950  523  370] [ -2.36768559e-01  -2.13108070e-01  -1.89447581e-01  -1.65787092e-01
  -1.42126604e-01  -1.18466115e-01  -9.48056261e-02  -7.11451373e-02
  -4.74846486e-02  -2.38241598e-02  -1.63671051e-04]
[ 738  992 1566 1925 1983 2326 2395 2771 2717 2837] [ -2.36768559e-01  -2.13108070e-01  -1.89447581e-01  -1.65787092e-01
  -1.42126604e-01  -1.18466115e-01  -9.48056261e-02  -7.11451373e-02
  -4.74846486e-02  -2.38241598e-02  -1.63671051e-04]
-7.52581
7.87328
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148944 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.36822
Epoch 1, cost is  4.22011
Epoch 2, cost is  4.18655
Epoch 3, cost is  4.17838
Epoch 4, cost is  4.18287
Training took 0.088053 minutes
Weight histogram
[2192 3461 2681 2990 2409 2273 2213 2070 1095  891] [ -2.34628633e-01  -2.11182137e-01  -1.87735640e-01  -1.64289144e-01
  -1.40842648e-01  -1.17396152e-01  -9.39496557e-02  -7.05031595e-02
  -4.70566634e-02  -2.36101672e-02  -1.63671051e-04]
[1575 2149 1757 1874 1964 2390 2507 2650 2750 2659] [ -2.34628633e-01  -2.11182137e-01  -1.87735640e-01  -1.64289144e-01
  -1.40842648e-01  -1.17396152e-01  -9.39496557e-02  -7.05031595e-02
  -4.70566634e-02  -2.36101672e-02  -1.63671051e-04]
-6.73587
7.63732
... retrieved True_rbm_200-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.60469
Epoch 1, cost is  6.18933
Epoch 2, cost is  5.1666
Epoch 3, cost is  4.32578
Epoch 4, cost is  3.69015
Training took 0.120511 minutes
Weight histogram
[ 342 2176 2861 2604 2363 3215 4722 1513  303  151] [-0.03940029 -0.03547894 -0.03155759 -0.02763623 -0.02371488 -0.01979353
 -0.01587218 -0.01195083 -0.00802947 -0.00410812 -0.00018677]
[5240 2410 1300 1483 1523 1585 1706 1829 2004 1170] [-0.03940029 -0.03547894 -0.03155759 -0.02763623 -0.02371488 -0.01979353
 -0.01587218 -0.01195083 -0.00802947 -0.00410812 -0.00018677]
-1.29229
1.49735
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046059 minutes
Epoch 0
Fine tuning took 0.047139 minutes
Epoch 0
Fine tuning took 0.045355 minutes
{'zero': {0: [0.20935960591133004, 0.20689655172413793, 0.27955665024630544, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.51724137931034486, 0.45566502463054187, 0.53694581280788178], 5: [0.22660098522167488, 0.27586206896551724, 0.26477832512315269, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20935960591133004, 0.19704433497536947, 0.20812807881773399, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.60591133004926112, 0.52709359605911332, 0.62931034482758619], 5: [0.22660098522167488, 0.19704433497536947, 0.26477832512315269, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20935960591133004, 0.20812807881773399, 0.21674876847290642, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.5788177339901478, 0.51108374384236455, 0.61699507389162567], 5: [0.22660098522167488, 0.21305418719211822, 0.27216748768472904, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20935960591133004, 0.2105911330049261, 0.20689655172413793, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56403940886699511, 0.61083743842364535, 0.55665024630541871, 0.65147783251231528], 5: [0.22660098522167488, 0.17857142857142858, 0.23645320197044334, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150042 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.30821
Epoch 1, cost is  3.27558
Epoch 2, cost is  3.25319
Epoch 3, cost is  3.23393
Epoch 4, cost is  3.21404
Training took 0.090323 minutes
Weight histogram
[3669 3283 2133 2487 1378 1895 1437 1494  873 1601] [ -8.12435374e-02  -7.31170458e-02  -6.49905541e-02  -5.68640625e-02
  -4.87375709e-02  -4.06110793e-02  -3.24845877e-02  -2.43580961e-02
  -1.62316045e-02  -8.10511284e-03   2.13787716e-05]
[2310 1297 1294 1404 1566 2116 2018 2623 2724 2898] [ -8.12435374e-02  -7.31170458e-02  -6.49905541e-02  -5.68640625e-02
  -4.87375709e-02  -4.06110793e-02  -3.24845877e-02  -2.43580961e-02
  -1.62316045e-02  -8.10511284e-03   2.13787716e-05]
-1.65515
1.88686
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.150154 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.48355
Epoch 1, cost is  3.45126
Epoch 2, cost is  3.42728
Epoch 3, cost is  3.40487
Epoch 4, cost is  3.38746
Training took 0.088767 minutes
Weight histogram
[3596 2537 2174 2257 1553 1612 1434 1273 2969 2870] [ -7.57407844e-02  -6.81645681e-02  -6.05883518e-02  -5.30121355e-02
  -4.54359191e-02  -3.78597028e-02  -3.02834865e-02  -2.27072702e-02
  -1.51310539e-02  -7.55483755e-03   2.13787716e-05]
[4829 1197 1245 1402 1568 1874 2045 2516 2754 2845] [ -7.57407844e-02  -6.81645681e-02  -6.05883518e-02  -5.30121355e-02
  -4.54359191e-02  -3.78597028e-02  -3.02834865e-02  -2.27072702e-02
  -1.51310539e-02  -7.55483755e-03   2.13787716e-05]
-1.26212
1.66045
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.88703
Epoch 1, cost is  6.8294
Epoch 2, cost is  6.7846
Epoch 3, cost is  6.74273
Epoch 4, cost is  6.70154
Training took 0.085736 minutes
Weight histogram
[4467 7874 2613 1584 1096  809  609  489  386  323] [ -1.41098574e-02  -1.26890806e-02  -1.12683039e-02  -9.84752712e-03
  -8.42675037e-03  -7.00597362e-03  -5.58519687e-03  -4.16442012e-03
  -2.74364337e-03  -1.32286661e-03   9.79101387e-05]
[11892  5673  1125   466   406   241   131   116   100   100] [ -1.41098574e-02  -1.26890806e-02  -1.12683039e-02  -9.84752712e-03
  -8.42675037e-03  -7.00597362e-03  -5.58519687e-03  -4.16442012e-03
  -2.74364337e-03  -1.32286661e-03   9.79101387e-05]
-0.0901514
0.163112
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044082 minutes
Epoch 0
Fine tuning took 0.043401 minutes
Epoch 0
Fine tuning took 0.040756 minutes
{'zero': {0: [0.20073891625615764, 0.23399014778325122, 0.056650246305418719, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.68965517241379315, 0.65024630541871919, 0.70935960591133007], 5: [0.3891625615763547, 0.076354679802955669, 0.29310344827586204, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20073891625615764, 0.24384236453201971, 0.082512315270935957, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.6785714285714286, 0.63300492610837433, 0.69827586206896552], 5: [0.3891625615763547, 0.077586206896551727, 0.28448275862068967, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20073891625615764, 0.26231527093596058, 0.093596059113300489, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.66502463054187189, 0.61699507389162567, 0.66995073891625612], 5: [0.3891625615763547, 0.072660098522167482, 0.2894088669950739, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20073891625615764, 0.25862068965517243, 0.075123152709359611, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.68226600985221675, 0.64778325123152714, 0.67733990147783252], 5: [0.3891625615763547, 0.059113300492610835, 0.27709359605911332, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149469 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.30821
Epoch 1, cost is  3.27558
Epoch 2, cost is  3.25319
Epoch 3, cost is  3.23393
Epoch 4, cost is  3.21404
Training took 0.087079 minutes
Weight histogram
[3669 3283 2133 2487 1378 1895 1437 1494  873 1601] [ -8.12435374e-02  -7.31170458e-02  -6.49905541e-02  -5.68640625e-02
  -4.87375709e-02  -4.06110793e-02  -3.24845877e-02  -2.43580961e-02
  -1.62316045e-02  -8.10511284e-03   2.13787716e-05]
[2310 1297 1294 1404 1566 2116 2018 2623 2724 2898] [ -8.12435374e-02  -7.31170458e-02  -6.49905541e-02  -5.68640625e-02
  -4.87375709e-02  -4.06110793e-02  -3.24845877e-02  -2.43580961e-02
  -1.62316045e-02  -8.10511284e-03   2.13787716e-05]
-1.65515
1.88686
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148041 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.48355
Epoch 1, cost is  3.45126
Epoch 2, cost is  3.42728
Epoch 3, cost is  3.40487
Epoch 4, cost is  3.38746
Training took 0.088846 minutes
Weight histogram
[3596 2537 2174 2257 1553 1612 1434 1273 2969 2870] [ -7.57407844e-02  -6.81645681e-02  -6.05883518e-02  -5.30121355e-02
  -4.54359191e-02  -3.78597028e-02  -3.02834865e-02  -2.27072702e-02
  -1.51310539e-02  -7.55483755e-03   2.13787716e-05]
[4829 1197 1245 1402 1568 1874 2045 2516 2754 2845] [ -7.57407844e-02  -6.81645681e-02  -6.05883518e-02  -5.30121355e-02
  -4.54359191e-02  -3.78597028e-02  -3.02834865e-02  -2.27072702e-02
  -1.51310539e-02  -7.55483755e-03   2.13787716e-05]
-1.26212
1.66045
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.86331
Epoch 1, cost is  6.79157
Epoch 2, cost is  6.74164
Epoch 3, cost is  6.69822
Epoch 4, cost is  6.65487
Training took 0.093326 minutes
Weight histogram
[2724 8471 3184 1826 1222  892  667  517  410  337] [ -1.45929288e-02  -1.31338863e-02  -1.16748437e-02  -1.02158012e-02
  -8.75675866e-03  -7.29771613e-03  -5.83867359e-03  -4.37963105e-03
  -2.92058851e-03  -1.46154598e-03  -2.50343851e-06]
[10815  5910  1800   457   408   349   143   125   125   118] [ -1.45929288e-02  -1.31338863e-02  -1.16748437e-02  -1.02158012e-02
  -8.75675866e-03  -7.29771613e-03  -5.83867359e-03  -4.37963105e-03
  -2.92058851e-03  -1.46154598e-03  -2.50343851e-06]
-0.0877546
0.134522
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042658 minutes
Epoch 0
Fine tuning took 0.041970 minutes
Epoch 0
Fine tuning took 0.042813 minutes
{'zero': {0: [0.19704433497536947, 0.21305418719211822, 0.068965517241379309, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39162561576354682, 0.72536945812807885, 0.54926108374384242, 0.64162561576354682], 5: [0.41133004926108374, 0.061576354679802957, 0.3817733990147783, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19704433497536947, 0.20812807881773399, 0.091133004926108374, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39162561576354682, 0.72660098522167482, 0.54679802955665024, 0.65886699507389157], 5: [0.41133004926108374, 0.065270935960591137, 0.36206896551724138, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19704433497536947, 0.21305418719211822, 0.071428571428571425, 0.2105911330049261], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39162561576354682, 0.69950738916256161, 0.58497536945812811, 0.63669950738916259], 5: [0.41133004926108374, 0.087438423645320201, 0.34359605911330049, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19704433497536947, 0.19704433497536947, 0.067733990147783252, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39162561576354682, 0.72413793103448276, 0.59359605911330049, 0.66625615763546797], 5: [0.41133004926108374, 0.078817733990147784, 0.33866995073891626, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149865 minutes
Weight histogram
[  50  294  483  814  684 2819 3279 5635 5352  840] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
[  127   150   206   295   425   737   958  1730  3581 12041] [ -3.91909212e-04  -9.32350580e-05   2.05439096e-04   5.04113251e-04
   8.02787405e-04   1.10146156e-03   1.40013571e-03   1.69880987e-03
   1.99748402e-03   2.29615818e-03   2.59483233e-03]
-1.4104
0.983319
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.30821
Epoch 1, cost is  3.27558
Epoch 2, cost is  3.25319
Epoch 3, cost is  3.23393
Epoch 4, cost is  3.21404
Training took 0.086815 minutes
Weight histogram
[3669 3283 2133 2487 1378 1895 1437 1494  873 1601] [ -8.12435374e-02  -7.31170458e-02  -6.49905541e-02  -5.68640625e-02
  -4.87375709e-02  -4.06110793e-02  -3.24845877e-02  -2.43580961e-02
  -1.62316045e-02  -8.10511284e-03   2.13787716e-05]
[2310 1297 1294 1404 1566 2116 2018 2623 2724 2898] [ -8.12435374e-02  -7.31170458e-02  -6.49905541e-02  -5.68640625e-02
  -4.87375709e-02  -4.06110793e-02  -3.24845877e-02  -2.43580961e-02
  -1.62316045e-02  -8.10511284e-03   2.13787716e-05]
-1.65515
1.88686
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.149412 minutes
Weight histogram
[  39  185  637  884 1689 3077 4252 5069 5150 1293] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
[  257   299   410   549   860  1394   905  1675  2705 13221] [ -3.91909212e-04  -1.69862394e-04   5.21844253e-05   2.74231244e-04
   4.96278063e-04   7.18324882e-04   9.40371701e-04   1.16241852e-03
   1.38446534e-03   1.60651216e-03   1.82855898e-03]
-1.40124
1.10626
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.48355
Epoch 1, cost is  3.45126
Epoch 2, cost is  3.42728
Epoch 3, cost is  3.40487
Epoch 4, cost is  3.38746
Training took 0.088204 minutes
Weight histogram
[3596 2537 2174 2257 1553 1612 1434 1273 2969 2870] [ -7.57407844e-02  -6.81645681e-02  -6.05883518e-02  -5.30121355e-02
  -4.54359191e-02  -3.78597028e-02  -3.02834865e-02  -2.27072702e-02
  -1.51310539e-02  -7.55483755e-03   2.13787716e-05]
[4829 1197 1245 1402 1568 1874 2045 2516 2754 2845] [ -7.57407844e-02  -6.81645681e-02  -6.05883518e-02  -5.30121355e-02
  -4.54359191e-02  -3.78597028e-02  -3.02834865e-02  -2.27072702e-02
  -1.51310539e-02  -7.55483755e-03   2.13787716e-05]
-1.26212
1.66045
... retrieved True_rbm_200-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN4/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.7951
Epoch 1, cost is  6.68387
Epoch 2, cost is  6.6233
Epoch 3, cost is  6.57175
Epoch 4, cost is  6.52721
Training took 0.119430 minutes
Weight histogram
[ 527  900 6831 5140 2515 1546 1058  755  555  423] [ -1.79279987e-02  -1.61393625e-02  -1.43507263e-02  -1.25620901e-02
  -1.07734539e-02  -8.98481772e-03  -7.19618153e-03  -5.40754534e-03
  -3.61890915e-03  -1.83027295e-03  -4.16367620e-05]
[9492 5412 3163  623  420  391  274  160  152  163] [ -1.79279987e-02  -1.61393625e-02  -1.43507263e-02  -1.25620901e-02
  -1.07734539e-02  -8.98481772e-03  -7.19618153e-03  -5.40754534e-03
  -3.61890915e-03  -1.83027295e-03  -4.16367620e-05]
-0.0854489
0.104608
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046015 minutes
Epoch 0
Fine tuning took 0.046808 minutes
Epoch 0
Fine tuning took 0.046703 minutes
{'zero': {0: [0.15640394088669951, 0.21182266009852216, 0.16133004926108374, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.73275862068965514, 0.4605911330049261, 0.60960591133004927], 5: [0.4248768472906404, 0.055418719211822662, 0.37807881773399016, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15640394088669951, 0.22044334975369459, 0.17241379310344829, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.7426108374384236, 0.43596059113300495, 0.56527093596059108], 5: [0.4248768472906404, 0.036945812807881777, 0.39162561576354682, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15640394088669951, 0.22660098522167488, 0.13916256157635468, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.72044334975369462, 0.46182266009852219, 0.56896551724137934], 5: [0.4248768472906404, 0.05295566502463054, 0.39901477832512317, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15640394088669951, 0.21921182266009853, 0.15886699507389163, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.74630541871921185, 0.47044334975369456, 0.5788177339901478], 5: [0.4248768472906404, 0.034482758620689655, 0.37068965517241381, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}}
