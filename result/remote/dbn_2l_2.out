Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentDBN2
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentDBN2
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.403760 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.00005
Epoch 1, cost is  5.18873
Epoch 2, cost is  4.95792
Epoch 3, cost is  4.88381
Epoch 4, cost is  4.85719
Training took 0.109885 minutes
Weight histogram
[408 692 397 443  41  12  15   9   4   4] [-0.01216623 -0.01097119 -0.00977614 -0.00858109 -0.00738604 -0.006191
 -0.00499595 -0.0038009  -0.00260586 -0.00141081 -0.00021576]
[154 111 130 144 169 199 242 267 280 329] [-0.01216623 -0.01097119 -0.00977614 -0.00858109 -0.00738604 -0.006191
 -0.00499595 -0.0038009  -0.00260586 -0.00141081 -0.00021576]
-1.97522
1.70234
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.92411
Epoch 1, cost is  4.61263
Epoch 2, cost is  4.06656
Epoch 3, cost is  3.81978
Epoch 4, cost is  3.68936
Training took 0.073030 minutes
Weight histogram
[ 516  375  399  164  147  226 1295  869   42   17] [-0.03159258 -0.028452   -0.02531142 -0.02217084 -0.01903025 -0.01588967
 -0.01274909 -0.00960851 -0.00646793 -0.00332735 -0.00018676]
[364 225 252 332 397 467 519 573 591 330] [-0.03159258 -0.028452   -0.02531142 -0.02217084 -0.01903025 -0.01588967
 -0.01274909 -0.00960851 -0.00646793 -0.00332735 -0.00018676]
-1.97522
1.70234
fine tuning ...
Epoch 0
Fine tuning took 0.051722 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052734 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052948 minutes
{0: [0.029556650246305417, 0.027093596059113302, 0.029556650246305417, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94827586206896552, 0.95566502463054193, 0.95197044334975367, 0.96182266009852213], 5: [0.022167487684729065, 0.017241379310344827, 0.018472906403940888, 0.011083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.403679 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.00005
Epoch 1, cost is  5.18873
Epoch 2, cost is  4.95792
Epoch 3, cost is  4.88381
Epoch 4, cost is  4.85719
Training took 0.108618 minutes
Weight histogram
[408 692 397 443  41  12  15   9   4   4] [-0.01216623 -0.01097119 -0.00977614 -0.00858109 -0.00738604 -0.006191
 -0.00499595 -0.0038009  -0.00260586 -0.00141081 -0.00021576]
[154 111 130 144 169 199 242 267 280 329] [-0.01216623 -0.01097119 -0.00977614 -0.00858109 -0.00738604 -0.006191
 -0.00499595 -0.0038009  -0.00260586 -0.00141081 -0.00021576]
-1.97522
1.70234
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.51926
Epoch 1, cost is  3.91347
Epoch 2, cost is  3.3168
Epoch 3, cost is  3.04826
Epoch 4, cost is  2.86382
Training took 0.087922 minutes
Weight histogram
[ 339  495  570 1232  714  422  219   41   10    8] [-0.01617519 -0.01456914 -0.01296308 -0.01135703 -0.00975098 -0.00814493
 -0.00653887 -0.00493282 -0.00332677 -0.00172072 -0.00011466]
[360 258 342 383 496 607 727 268 280 329] [-0.01617519 -0.01456914 -0.01296308 -0.01135703 -0.00975098 -0.00814493
 -0.00653887 -0.00493282 -0.00332677 -0.00172072 -0.00011466]
-1.97522
1.70234
fine tuning ...
Epoch 0
Fine tuning took 0.055421 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053781 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053894 minutes
{0: [0.020935960591133004, 0.02832512315270936, 0.034482758620689655, 0.029556650246305417], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96551724137931039, 0.94827586206896552, 0.93596059113300489, 0.94458128078817738], 5: [0.013546798029556651, 0.023399014778325122, 0.029556650246305417, 0.025862068965517241], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.404005 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.00005
Epoch 1, cost is  5.18873
Epoch 2, cost is  4.95792
Epoch 3, cost is  4.88381
Epoch 4, cost is  4.85719
Training took 0.109862 minutes
Weight histogram
[408 692 397 443  41  12  15   9   4   4] [-0.01216623 -0.01097119 -0.00977614 -0.00858109 -0.00738604 -0.006191
 -0.00499595 -0.0038009  -0.00260586 -0.00141081 -0.00021576]
[154 111 130 144 169 199 242 267 280 329] [-0.01216623 -0.01097119 -0.00977614 -0.00858109 -0.00738604 -0.006191
 -0.00499595 -0.0038009  -0.00260586 -0.00141081 -0.00021576]
-1.97522
1.70234
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.23784
Epoch 1, cost is  3.43849
Epoch 2, cost is  2.86057
Epoch 3, cost is  2.5592
Epoch 4, cost is  2.40873
Training took 0.112797 minutes
Weight histogram
[ 408  692  673 1109  446  285  218  192   20    7] [-0.01216623 -0.01097119 -0.00977614 -0.00858109 -0.00738604 -0.006191
 -0.00499595 -0.0038009  -0.00260586 -0.00141081 -0.00021576]
[411 377 509 699 737 199 242 267 280 329] [-0.01216623 -0.01097119 -0.00977614 -0.00858109 -0.00738604 -0.006191
 -0.00499595 -0.0038009  -0.00260586 -0.00141081 -0.00021576]
-1.97522
1.70234
fine tuning ...
Epoch 0
Fine tuning took 0.056664 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057744 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057218 minutes
{0: [0.032019704433497539, 0.022167487684729065, 0.023399014778325122, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94458128078817738, 0.95197044334975367, 0.96059113300492616, 0.94334975369458129], 5: [0.023399014778325122, 0.025862068965517241, 0.01600985221674877, 0.017241379310344827], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.404325 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.55727
Epoch 1, cost is  4.5235
Epoch 2, cost is  4.24716
Epoch 3, cost is  4.07765
Epoch 4, cost is  3.97532
Training took 0.153093 minutes
Weight histogram
[403 696 406 398 110   3   3   2   2   2] [-0.00742831 -0.00671611 -0.00600392 -0.00529172 -0.00457953 -0.00386733
 -0.00315514 -0.00244294 -0.00173075 -0.00101855 -0.00030636]
[146 111 126 152 176 212 248 260 289 305] [-0.00742831 -0.00671611 -0.00600392 -0.00529172 -0.00457953 -0.00386733
 -0.00315514 -0.00244294 -0.00173075 -0.00101855 -0.00030636]
-1.4864
1.65958
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.62021
Epoch 1, cost is  4.4944
Epoch 2, cost is  4.15198
Epoch 3, cost is  4.01208
Epoch 4, cost is  3.94066
Training took 0.088431 minutes
Weight histogram
[ 564  467  313  241  130  106  148 1122  945   14] [-0.02898507 -0.0261172  -0.02324933 -0.02038146 -0.01751359 -0.01464572
 -0.01177785 -0.00890997 -0.0060421  -0.00317423 -0.00030636]
[375 327 449 560 654 533 244 271 293 344] [-0.02898507 -0.0261172  -0.02324933 -0.02038146 -0.01751359 -0.01464572
 -0.01177785 -0.00890997 -0.0060421  -0.00317423 -0.00030636]
-1.92349
1.74953
fine tuning ...
Epoch 0
Fine tuning took 0.060887 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060475 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060291 minutes
{0: [0.043103448275862072, 0.046798029556650245, 0.062807881773399021, 0.039408866995073892], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93226600985221675, 0.9076354679802956, 0.89778325123152714, 0.9076354679802956], 5: [0.024630541871921183, 0.045566502463054187, 0.039408866995073892, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.413874 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.55727
Epoch 1, cost is  4.5235
Epoch 2, cost is  4.24716
Epoch 3, cost is  4.07765
Epoch 4, cost is  3.97532
Training took 0.153738 minutes
Weight histogram
[403 696 406 398 110   3   3   2   2   2] [-0.00742831 -0.00671611 -0.00600392 -0.00529172 -0.00457953 -0.00386733
 -0.00315514 -0.00244294 -0.00173075 -0.00101855 -0.00030636]
[146 111 126 152 176 212 248 260 289 305] [-0.00742831 -0.00671611 -0.00600392 -0.00529172 -0.00457953 -0.00386733
 -0.00315514 -0.00244294 -0.00173075 -0.00101855 -0.00030636]
-1.4864
1.65958
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.10049
Epoch 1, cost is  3.71695
Epoch 2, cost is  3.31896
Epoch 3, cost is  3.11633
Epoch 4, cost is  3.01662
Training took 0.113451 minutes
Weight histogram
[ 403  530  415  325  132  645 1150  433   10    7] [-0.01620669 -0.01461666 -0.01302663 -0.01143659 -0.00984656 -0.00825653
 -0.00666649 -0.00507646 -0.00348643 -0.00189639 -0.00030636]
[292 222 260 313 380 462 516 572 643 390] [-0.01620669 -0.01461666 -0.01302663 -0.01143659 -0.00984656 -0.00825653
 -0.00666649 -0.00507646 -0.00348643 -0.00189639 -0.00030636]
-1.4864
1.65958
fine tuning ...
Epoch 0
Fine tuning took 0.063055 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063683 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062788 minutes
{0: [0.05295566502463054, 0.054187192118226604, 0.045566502463054187, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90886699507389157, 0.90886699507389157, 0.90024630541871919, 0.8780788177339901], 5: [0.038177339901477834, 0.036945812807881777, 0.054187192118226604, 0.060344827586206899], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.404545 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.55727
Epoch 1, cost is  4.5235
Epoch 2, cost is  4.24716
Epoch 3, cost is  4.07765
Epoch 4, cost is  3.97532
Training took 0.155755 minutes
Weight histogram
[403 696 406 398 110   3   3   2   2   2] [-0.00742831 -0.00671611 -0.00600392 -0.00529172 -0.00457953 -0.00386733
 -0.00315514 -0.00244294 -0.00173075 -0.00101855 -0.00030636]
[146 111 126 152 176 212 248 260 289 305] [-0.00742831 -0.00671611 -0.00600392 -0.00529172 -0.00457953 -0.00386733
 -0.00315514 -0.00244294 -0.00173075 -0.00101855 -0.00030636]
-1.4864
1.65958
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.67185
Epoch 1, cost is  3.17131
Epoch 2, cost is  2.77745
Epoch 3, cost is  2.59789
Epoch 4, cost is  2.48394
Training took 0.158547 minutes
Weight histogram
[ 519  475  538 1157  681  569   93    9    4    5] [-0.00993609 -0.00897312 -0.00801015 -0.00704717 -0.0060842  -0.00512123
 -0.00415825 -0.00319528 -0.00223231 -0.00126933 -0.00030636]
[305 252 306 392 494 610 736 361 288 306] [-0.00993609 -0.00897312 -0.00801015 -0.00704717 -0.0060842  -0.00512123
 -0.00415825 -0.00319528 -0.00223231 -0.00126933 -0.00030636]
-1.4864
1.65958
fine tuning ...
Epoch 0
Fine tuning took 0.069931 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068801 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069431 minutes
{0: [0.051724137931034482, 0.046798029556650245, 0.056650246305418719, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90640394088669951, 0.91502463054187189, 0.9076354679802956, 0.87438423645320196], 5: [0.041871921182266007, 0.038177339901477834, 0.035714285714285712, 0.067733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.405927 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.1909
Epoch 1, cost is  4.04005
Epoch 2, cost is  3.67876
Epoch 3, cost is  3.47797
Epoch 4, cost is  3.33116
Training took 0.210275 minutes
Weight histogram
[331 645 350 403 283   8   1   1   1   2] [-0.00514995 -0.00466341 -0.00417687 -0.00369033 -0.00320379 -0.00271725
 -0.00223071 -0.00174417 -0.00125763 -0.00077109 -0.00028455]
[142 115 136 166 188 217 236 254 273 298] [-0.00514995 -0.00466341 -0.00417687 -0.00369033 -0.00320379 -0.00271725
 -0.00223071 -0.00174417 -0.00125763 -0.00077109 -0.00028455]
-1.11105
1.08389
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.57737
Epoch 1, cost is  4.69802
Epoch 2, cost is  4.43716
Epoch 3, cost is  4.3214
Epoch 4, cost is  4.28451
Training took 0.110066 minutes
Weight histogram
[ 656  474  349  231   90   77   56   62 1989   66] [-0.02629331 -0.02369243 -0.02109155 -0.01849068 -0.0158898  -0.01328893
 -0.01068805 -0.00808717 -0.0054863  -0.00288542 -0.00028455]
[432 509 691 816 261 206 250 263 291 331] [-0.02629331 -0.02369243 -0.02109155 -0.01849068 -0.0158898  -0.01328893
 -0.01068805 -0.00808717 -0.0054863  -0.00288542 -0.00028455]
-2.04504
1.92956
fine tuning ...
Epoch 0
Fine tuning took 0.069941 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068744 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067465 minutes
{0: [0.057881773399014777, 0.065270935960591137, 0.059113300492610835, 0.043103448275862072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90517241379310343, 0.88793103448275867, 0.90270935960591137, 0.90024630541871919], 5: [0.036945812807881777, 0.046798029556650245, 0.038177339901477834, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.404528 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.1909
Epoch 1, cost is  4.04005
Epoch 2, cost is  3.67876
Epoch 3, cost is  3.47797
Epoch 4, cost is  3.33116
Training took 0.211135 minutes
Weight histogram
[331 645 350 403 283   8   1   1   1   2] [-0.00514995 -0.00466341 -0.00417687 -0.00369033 -0.00320379 -0.00271725
 -0.00223071 -0.00174417 -0.00125763 -0.00077109 -0.00028455]
[142 115 136 166 188 217 236 254 273 298] [-0.00514995 -0.00466341 -0.00417687 -0.00369033 -0.00320379 -0.00271725
 -0.00223071 -0.00174417 -0.00125763 -0.00077109 -0.00028455]
-1.11105
1.08389
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.00722
Epoch 1, cost is  3.89816
Epoch 2, cost is  3.59221
Epoch 3, cost is  3.44776
Epoch 4, cost is  3.3676
Training took 0.159983 minutes
Weight histogram
[ 542  563  304  297  142   94  138 1551  412    7] [-0.01564043 -0.01410484 -0.01256925 -0.01103366 -0.00949807 -0.00796249
 -0.0064269  -0.00489131 -0.00335572 -0.00182014 -0.00028455]
[318 302 376 469 546 616 505 269 306 343] [-0.01564043 -0.01410484 -0.01256925 -0.01103366 -0.00949807 -0.00796249
 -0.0064269  -0.00489131 -0.00335572 -0.00182014 -0.00028455]
-1.24186
1.48766
fine tuning ...
Epoch 0
Fine tuning took 0.073751 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073574 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074007 minutes
{0: [0.064039408866995079, 0.051724137931034482, 0.046798029556650245, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87561576354679804, 0.8854679802955665, 0.89039408866995073, 0.86945812807881773], 5: [0.060344827586206899, 0.062807881773399021, 0.062807881773399021, 0.068965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.404514 minutes
Weight histogram
[ 15  47 188 338 489 447 299 161  33   8] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 72  75 101 126 153 186 252 281 345 434] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.501207
0.542263
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.1909
Epoch 1, cost is  4.04005
Epoch 2, cost is  3.67876
Epoch 3, cost is  3.47797
Epoch 4, cost is  3.33116
Training took 0.208955 minutes
Weight histogram
[331 645 350 403 283   8   1   1   1   2] [-0.00514995 -0.00466341 -0.00417687 -0.00369033 -0.00320379 -0.00271725
 -0.00223071 -0.00174417 -0.00125763 -0.00077109 -0.00028455]
[142 115 136 166 188 217 236 254 273 298] [-0.00514995 -0.00466341 -0.00417687 -0.00369033 -0.00320379 -0.00271725
 -0.00223071 -0.00174417 -0.00125763 -0.00077109 -0.00028455]
-1.11105
1.08389
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.58425
Epoch 1, cost is  3.36167
Epoch 2, cost is  3.0454
Epoch 3, cost is  2.90261
Epoch 4, cost is  2.80947
Training took 0.217754 minutes
Weight histogram
[ 580  489  328  304  237 1115  742  247    3    5] [-0.00982273 -0.00886891 -0.0079151  -0.00696128 -0.00600746 -0.00505364
 -0.00409982 -0.003146   -0.00219218 -0.00123837 -0.00028455]
[261 214 254 306 359 422 481 536 582 635] [-0.00982273 -0.00886891 -0.0079151  -0.00696128 -0.00600746 -0.00505364
 -0.00409982 -0.003146   -0.00219218 -0.00123837 -0.00028455]
-1.11105
1.08389
fine tuning ...
Epoch 0
Fine tuning took 0.079902 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079460 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079855 minutes
{0: [0.064039408866995079, 0.064039408866995079, 0.04064039408866995, 0.075123152709359611], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88177339901477836, 0.8719211822660099, 0.89655172413793105, 0.86576354679802958], 5: [0.054187192118226604, 0.064039408866995079, 0.062807881773399021, 0.059113300492610835], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382269 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.89669
Epoch 1, cost is  4.81686
Epoch 2, cost is  4.80872
Epoch 3, cost is  4.82247
Epoch 4, cost is  4.84003
Training took 0.110193 minutes
Weight histogram
[1692  310  101  916  561  423   14   22    6    5] [-0.01651809 -0.01488785 -0.01325762 -0.01162739 -0.00999716 -0.00836692
 -0.00673669 -0.00510646 -0.00347623 -0.00184599 -0.00021576]
[204 191 224 286 365 421 541 598 600 620] [-0.01651809 -0.01488785 -0.01325762 -0.01162739 -0.00999716 -0.00836692
 -0.00673669 -0.00510646 -0.00347623 -0.00184599 -0.00021576]
-3.41071
2.65454
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.80867
Epoch 1, cost is  3.68225
Epoch 2, cost is  3.63533
Epoch 3, cost is  3.58907
Epoch 4, cost is  3.58165
Training took 0.071275 minutes
Weight histogram
[ 968 1402  483  462  219  106  637 1669  103   26] [-0.03677205 -0.03311352 -0.029455   -0.02579647 -0.02213794 -0.01847941
 -0.01482088 -0.01116235 -0.00750382 -0.00384529 -0.00018676]
[456 350 481 619 742 871 561 658 656 681] [-0.03677205 -0.03311352 -0.029455   -0.02579647 -0.02213794 -0.01847941
 -0.01482088 -0.01116235 -0.00750382 -0.00384529 -0.00018676]
-2.61552
2.87933
fine tuning ...
Epoch 0
Fine tuning took 0.051638 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053073 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053044 minutes
{0: [0.032019704433497539, 0.030788177339901478, 0.025862068965517241, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.92241379310344829, 0.93719211822660098, 0.95443349753694584, 0.91625615763546797], 5: [0.045566502463054187, 0.032019704433497539, 0.019704433497536946, 0.035714285714285712], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379856 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.89669
Epoch 1, cost is  4.81686
Epoch 2, cost is  4.80872
Epoch 3, cost is  4.82247
Epoch 4, cost is  4.84003
Training took 0.110056 minutes
Weight histogram
[1692  310  101  916  561  423   14   22    6    5] [-0.01651809 -0.01488785 -0.01325762 -0.01162739 -0.00999716 -0.00836692
 -0.00673669 -0.00510646 -0.00347623 -0.00184599 -0.00021576]
[204 191 224 286 365 421 541 598 600 620] [-0.01651809 -0.01488785 -0.01325762 -0.01162739 -0.00999716 -0.00836692
 -0.00673669 -0.00510646 -0.00347623 -0.00184599 -0.00021576]
-3.41071
2.65454
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.96918
Epoch 1, cost is  2.83702
Epoch 2, cost is  2.75813
Epoch 3, cost is  2.69087
Epoch 4, cost is  2.63919
Training took 0.087003 minutes
Weight histogram
[ 893  884  828  599 1349  944  330  210   29    9] [-0.01966657 -0.01771138 -0.01575619 -0.01380099 -0.0118458  -0.00989061
 -0.00793542 -0.00598023 -0.00402504 -0.00206985 -0.00011466]
[ 360  258  342  383  496  607  727 1127 1107  668] [-0.01966657 -0.01771138 -0.01575619 -0.01380099 -0.0118458  -0.00989061
 -0.00793542 -0.00598023 -0.00402504 -0.00206985 -0.00011466]
-1.97522
1.70234
fine tuning ...
Epoch 0
Fine tuning took 0.053449 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054733 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054199 minutes
{0: [0.029556650246305417, 0.036945812807881777, 0.041871921182266007, 0.032019704433497539], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93719211822660098, 0.92241379310344829, 0.92610837438423643, 0.94088669950738912], 5: [0.033251231527093597, 0.04064039408866995, 0.032019704433497539, 0.027093596059113302], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380020 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.89669
Epoch 1, cost is  4.81686
Epoch 2, cost is  4.80872
Epoch 3, cost is  4.82247
Epoch 4, cost is  4.84003
Training took 0.109270 minutes
Weight histogram
[1692  310  101  916  561  423   14   22    6    5] [-0.01651809 -0.01488785 -0.01325762 -0.01162739 -0.00999716 -0.00836692
 -0.00673669 -0.00510646 -0.00347623 -0.00184599 -0.00021576]
[204 191 224 286 365 421 541 598 600 620] [-0.01651809 -0.01488785 -0.01325762 -0.01162739 -0.00999716 -0.00836692
 -0.00673669 -0.00510646 -0.00347623 -0.00184599 -0.00021576]
-3.41071
2.65454
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.63167
Epoch 1, cost is  2.49422
Epoch 2, cost is  2.39984
Epoch 3, cost is  2.33039
Epoch 4, cost is  2.29993
Training took 0.109189 minutes
Weight histogram
[ 930 1486 1205 1235  480  297  219  194   22    7] [-0.01228935 -0.01108199 -0.00987463 -0.00866727 -0.00745991 -0.00625255
 -0.0050452  -0.00383784 -0.00263048 -0.00142312 -0.00021576]
[ 411  377  509  699 1170 1527  506  267  280  329] [-0.01228935 -0.01108199 -0.00987463 -0.00866727 -0.00745991 -0.00625255
 -0.0050452  -0.00383784 -0.00263048 -0.00142312 -0.00021576]
-1.97522
1.70234
fine tuning ...
Epoch 0
Fine tuning took 0.057067 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055844 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057218 minutes
{0: [0.027093596059113302, 0.034482758620689655, 0.041871921182266007, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93103448275862066, 0.94211822660098521, 0.92364532019704437, 0.91748768472906406], 5: [0.041871921182266007, 0.023399014778325122, 0.034482758620689655, 0.038177339901477834], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380059 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.9464
Epoch 1, cost is  3.79752
Epoch 2, cost is  3.73467
Epoch 3, cost is  3.69452
Epoch 4, cost is  3.67637
Training took 0.153765 minutes
Weight histogram
[ 894 1066   59  650  773  553   45    5    2    3] [-0.01071552 -0.00967461 -0.00863369 -0.00759278 -0.00655186 -0.00551094
 -0.00447003 -0.00342911 -0.00238819 -0.00134728 -0.00030636]
[206 187 243 329 383 443 558 566 558 577] [-0.01071552 -0.00967461 -0.00863369 -0.00759278 -0.00655186 -0.00551094
 -0.00447003 -0.00342911 -0.00238819 -0.00134728 -0.00030636]
-2.41975
2.70149
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.13414
Epoch 1, cost is  4.03906
Epoch 2, cost is  4.00189
Epoch 3, cost is  4.00461
Epoch 4, cost is  4.01323
Training took 0.086497 minutes
Weight histogram
[1065 1148  744  373  283  147  209  139 1948   19] [-0.03448059 -0.03106317 -0.02764575 -0.02422832 -0.0208109  -0.01739348
 -0.01397605 -0.01055863 -0.00714121 -0.00372378 -0.00030636]
[512 593 829 925 353 413 511 634 629 676] [-0.03448059 -0.03106317 -0.02764575 -0.02422832 -0.0208109  -0.01739348
 -0.01397605 -0.01055863 -0.00714121 -0.00372378 -0.00030636]
-3.23728
2.93164
fine tuning ...
Epoch 0
Fine tuning took 0.059689 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059107 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060549 minutes
{0: [0.029556650246305417, 0.067733990147783252, 0.038177339901477834, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91133004926108374, 0.89778325123152714, 0.91502463054187189, 0.89655172413793105], 5: [0.059113300492610835, 0.034482758620689655, 0.046798029556650245, 0.045566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380364 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.9464
Epoch 1, cost is  3.79752
Epoch 2, cost is  3.73467
Epoch 3, cost is  3.69452
Epoch 4, cost is  3.67637
Training took 0.153509 minutes
Weight histogram
[ 894 1066   59  650  773  553   45    5    2    3] [-0.01071552 -0.00967461 -0.00863369 -0.00759278 -0.00655186 -0.00551094
 -0.00447003 -0.00342911 -0.00238819 -0.00134728 -0.00030636]
[206 187 243 329 383 443 558 566 558 577] [-0.01071552 -0.00967461 -0.00863369 -0.00759278 -0.00655186 -0.00551094
 -0.00447003 -0.00342911 -0.00238819 -0.00134728 -0.00030636]
-2.41975
2.70149
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.11887
Epoch 1, cost is  2.97245
Epoch 2, cost is  2.90206
Epoch 3, cost is  2.86389
Epoch 4, cost is  2.83026
Training took 0.111713 minutes
Weight histogram
[ 967 1094  727  556  358  148 1280  923   13    9] [-0.01928025 -0.01738286 -0.01548547 -0.01358808 -0.01169069 -0.00979331
 -0.00789592 -0.00599853 -0.00410114 -0.00220375 -0.00030636]
[374 346 432 584 701 816 716 719 682 705] [-0.01928025 -0.01738286 -0.01548547 -0.01358808 -0.01169069 -0.00979331
 -0.00789592 -0.00599853 -0.00410114 -0.00220375 -0.00030636]
-2.1212
2.06104
fine tuning ...
Epoch 0
Fine tuning took 0.063875 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063365 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062998 minutes
{0: [0.044334975369458129, 0.065270935960591137, 0.072660098522167482, 0.075123152709359611], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90517241379310343, 0.87315270935960587, 0.87561576354679804, 0.85837438423645318], 5: [0.050492610837438424, 0.061576354679802957, 0.051724137931034482, 0.066502463054187194], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380126 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.9464
Epoch 1, cost is  3.79752
Epoch 2, cost is  3.73467
Epoch 3, cost is  3.69452
Epoch 4, cost is  3.67637
Training took 0.152796 minutes
Weight histogram
[ 894 1066   59  650  773  553   45    5    2    3] [-0.01071552 -0.00967461 -0.00863369 -0.00759278 -0.00655186 -0.00551094
 -0.00447003 -0.00342911 -0.00238819 -0.00134728 -0.00030636]
[206 187 243 329 383 443 558 566 558 577] [-0.01071552 -0.00967461 -0.00863369 -0.00759278 -0.00655186 -0.00551094
 -0.00447003 -0.00342911 -0.00238819 -0.00134728 -0.00030636]
-2.41975
2.70149
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.67682
Epoch 1, cost is  2.50467
Epoch 2, cost is  2.41357
Epoch 3, cost is  2.34813
Epoch 4, cost is  2.30501
Training took 0.152641 minutes
Weight histogram
[ 777 1026  622  618  881 1362  694   81    8    6] [-0.01299989 -0.01173054 -0.01046119 -0.00919183 -0.00792248 -0.00665313
 -0.00538377 -0.00411442 -0.00284507 -0.00157571 -0.00030636]
[ 307  255  309  399  500  622  747 1008  960  968] [-0.01299989 -0.01173054 -0.01046119 -0.00919183 -0.00792248 -0.00665313
 -0.00538377 -0.00411442 -0.00284507 -0.00157571 -0.00030636]
-1.4864
1.65958
fine tuning ...
Epoch 0
Fine tuning took 0.068137 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068061 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067460 minutes
{0: [0.036945812807881777, 0.055418719211822662, 0.044334975369458129, 0.070197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91133004926108374, 0.91502463054187189, 0.87931034482758619, 0.8780788177339901], 5: [0.051724137931034482, 0.029556650246305417, 0.076354679802955669, 0.051724137931034482], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379810 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.33758
Epoch 1, cost is  3.13608
Epoch 2, cost is  3.03239
Epoch 3, cost is  2.95689
Epoch 4, cost is  2.90398
Training took 0.208945 minutes
Weight histogram
[ 631 1308   80  347  821  570  286    3    2    2] [-0.00757502 -0.00684597 -0.00611692 -0.00538787 -0.00465883 -0.00392978
 -0.00320073 -0.00247169 -0.00174264 -0.00101359 -0.00028455]
[203 207 270 334 386 428 554 548 554 566] [-0.00757502 -0.00684597 -0.00611692 -0.00538787 -0.00465883 -0.00392978
 -0.00320073 -0.00247169 -0.00174264 -0.00101359 -0.00028455]
-1.64872
1.67088
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.39706
Epoch 1, cost is  4.32298
Epoch 2, cost is  4.32813
Epoch 3, cost is  4.3385
Epoch 4, cost is  4.36867
Training took 0.106744 minutes
Weight histogram
[ 887 1162  768  512  376  122   94   80 1508  566] [-0.03249919 -0.02927772 -0.02605626 -0.02283479 -0.01961333 -0.01639187
 -0.0131704  -0.00994894 -0.00672748 -0.00350601 -0.00028455]
[670 979 976 301 378 436 539 572 594 630] [-0.03249919 -0.02927772 -0.02605626 -0.02283479 -0.01961333 -0.01639187
 -0.0131704  -0.00994894 -0.00672748 -0.00350601 -0.00028455]
-3.67063
3.32676
fine tuning ...
Epoch 0
Fine tuning took 0.068203 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069552 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068416 minutes
{0: [0.035714285714285712, 0.050492610837438424, 0.060344827586206899, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90394088669950734, 0.90394088669950734, 0.89039408866995073, 0.8719211822660099], 5: [0.060344827586206899, 0.045566502463054187, 0.049261083743842367, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380620 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.33758
Epoch 1, cost is  3.13608
Epoch 2, cost is  3.03239
Epoch 3, cost is  2.95689
Epoch 4, cost is  2.90398
Training took 0.210145 minutes
Weight histogram
[ 631 1308   80  347  821  570  286    3    2    2] [-0.00757502 -0.00684597 -0.00611692 -0.00538787 -0.00465883 -0.00392978
 -0.00320073 -0.00247169 -0.00174264 -0.00101359 -0.00028455]
[203 207 270 334 386 428 554 548 554 566] [-0.00757502 -0.00684597 -0.00611692 -0.00538787 -0.00465883 -0.00392978
 -0.00320073 -0.00247169 -0.00174264 -0.00101359 -0.00028455]
-1.64872
1.67088
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.42614
Epoch 1, cost is  3.27965
Epoch 2, cost is  3.23172
Epoch 3, cost is  3.2077
Epoch 4, cost is  3.19404
Training took 0.154184 minutes
Weight histogram
[1040  983  753  575  339  188  135 1062  992    8] [-0.01938817 -0.01747781 -0.01556745 -0.01365709 -0.01174672 -0.00983636
 -0.007926   -0.00601563 -0.00410527 -0.00219491 -0.00028455]
[456 520 713 871 666 432 555 614 610 638] [-0.01938817 -0.01747781 -0.01556745 -0.01365709 -0.01174672 -0.00983636
 -0.007926   -0.00601563 -0.00410527 -0.00219491 -0.00028455]
-2.19561
2.61835
fine tuning ...
Epoch 0
Fine tuning took 0.073743 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074256 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.072989 minutes
{0: [0.11699507389162561, 0.071428571428571425, 0.059113300492610835, 0.080049261083743842], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83004926108374388, 0.85837438423645318, 0.85960591133004927, 0.84113300492610843], 5: [0.05295566502463054, 0.070197044334975367, 0.081280788177339899, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380239 minutes
Weight histogram
[  15   47  188  353  621  967 1086  619  145    9] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[110 162 222 323 420 563 430 403 601 816] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.78114
0.731389
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.33758
Epoch 1, cost is  3.13608
Epoch 2, cost is  3.03239
Epoch 3, cost is  2.95689
Epoch 4, cost is  2.90398
Training took 0.206969 minutes
Weight histogram
[ 631 1308   80  347  821  570  286    3    2    2] [-0.00757502 -0.00684597 -0.00611692 -0.00538787 -0.00465883 -0.00392978
 -0.00320073 -0.00247169 -0.00174264 -0.00101359 -0.00028455]
[203 207 270 334 386 428 554 548 554 566] [-0.00757502 -0.00684597 -0.00611692 -0.00538787 -0.00465883 -0.00392978
 -0.00320073 -0.00247169 -0.00174264 -0.00101359 -0.00028455]
-1.64872
1.67088
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.84518
Epoch 1, cost is  2.67871
Epoch 2, cost is  2.60555
Epoch 3, cost is  2.5525
Epoch 4, cost is  2.52072
Training took 0.208324 minutes
Weight histogram
[ 943  958  631  637  437  301 1250  897   16    5] [-0.01264398 -0.01140804 -0.01017209 -0.00893615 -0.00770021 -0.00646426
 -0.00522832 -0.00399238 -0.00275643 -0.00152049 -0.00028455]
[358 360 469 594 725 835 912 593 600 629] [-0.01264398 -0.01140804 -0.01017209 -0.00893615 -0.00770021 -0.00646426
 -0.00522832 -0.00399238 -0.00275643 -0.00152049 -0.00028455]
-1.49116
1.44895
fine tuning ...
Epoch 0
Fine tuning took 0.078389 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079208 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078813 minutes
{0: [0.094827586206896547, 0.067733990147783252, 0.071428571428571425, 0.083743842364532015], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83990147783251234, 0.86822660098522164, 0.83497536945812811, 0.85591133004926112], 5: [0.065270935960591137, 0.064039408866995079, 0.093596059113300489, 0.060344827586206899], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381053 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.81139
Epoch 1, cost is  4.71009
Epoch 2, cost is  4.71507
Epoch 3, cost is  4.73585
Epoch 4, cost is  4.77919
Training took 0.108725 minutes
Weight histogram
[1900  103  546 1475  248 1113  639   20   24    7] [-0.02242196 -0.02020134 -0.01798072 -0.0157601  -0.01353948 -0.01131886
 -0.00909824 -0.00687762 -0.004657   -0.00243638 -0.00021576]
[257 262 345 478 574 764 775 854 887 879] [-0.02242196 -0.02020134 -0.01798072 -0.0157601  -0.01353948 -0.01131886
 -0.00909824 -0.00687762 -0.004657   -0.00243638 -0.00021576]
-4.20581
3.42666
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.78297
Epoch 1, cost is  3.68859
Epoch 2, cost is  3.67587
Epoch 3, cost is  3.67489
Epoch 4, cost is  3.67019
Training took 0.071239 minutes
Weight histogram
[2583 1372  781  525  241  158  436 1796  175   33] [-0.03818929 -0.03438904 -0.03058879 -0.02678853 -0.02298828 -0.01918803
 -0.01538777 -0.01158752 -0.00778727 -0.00398702 -0.00018676]
[ 532  480  690  890 1068  689  783  837 1085 1046] [-0.03818929 -0.03438904 -0.03058879 -0.02678853 -0.02298828 -0.01918803
 -0.01538777 -0.01158752 -0.00778727 -0.00398702 -0.00018676]
-3.72274
3.97926
fine tuning ...
Epoch 0
Fine tuning took 0.050940 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051042 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052008 minutes
{0: [0.017241379310344827, 0.041871921182266007, 0.02832512315270936, 0.041871921182266007], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.95073891625615758, 0.9285714285714286, 0.93842364532019706, 0.92241379310344829], 5: [0.032019704433497539, 0.029556650246305417, 0.033251231527093597, 0.035714285714285712], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380532 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.81139
Epoch 1, cost is  4.71009
Epoch 2, cost is  4.71507
Epoch 3, cost is  4.73585
Epoch 4, cost is  4.77919
Training took 0.106627 minutes
Weight histogram
[1900  103  546 1475  248 1113  639   20   24    7] [-0.02242196 -0.02020134 -0.01798072 -0.0157601  -0.01353948 -0.01131886
 -0.00909824 -0.00687762 -0.004657   -0.00243638 -0.00021576]
[257 262 345 478 574 764 775 854 887 879] [-0.02242196 -0.02020134 -0.01798072 -0.0157601  -0.01353948 -0.01131886
 -0.00909824 -0.00687762 -0.004657   -0.00243638 -0.00021576]
-4.20581
3.42666
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.85072
Epoch 1, cost is  2.74078
Epoch 2, cost is  2.68494
Epoch 3, cost is  2.64161
Epoch 4, cost is  2.61105
Training took 0.086592 minutes
Weight histogram
[1126 1856 1044  851 1034 1224  685  235   34   11] [-0.02158405 -0.01943711 -0.01729017 -0.01514323 -0.01299629 -0.01084936
 -0.00870242 -0.00655548 -0.00440854 -0.0022616  -0.00011466]
[ 388  330  394  503  645  812 1234 1270 1425 1099] [-0.02158405 -0.01943711 -0.01729017 -0.01514323 -0.01299629 -0.01084936
 -0.00870242 -0.00655548 -0.00440854 -0.0022616  -0.00011466]
-2.41314
2.16111
fine tuning ...
Epoch 0
Fine tuning took 0.053412 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054325 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053775 minutes
{0: [0.033251231527093597, 0.048029556650246302, 0.030788177339901478, 0.036945812807881777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91502463054187189, 0.90517241379310343, 0.92980295566502458, 0.91009852216748766], 5: [0.051724137931034482, 0.046798029556650245, 0.039408866995073892, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380512 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.81139
Epoch 1, cost is  4.71009
Epoch 2, cost is  4.71507
Epoch 3, cost is  4.73585
Epoch 4, cost is  4.77919
Training took 0.109454 minutes
Weight histogram
[1900  103  546 1475  248 1113  639   20   24    7] [-0.02242196 -0.02020134 -0.01798072 -0.0157601  -0.01353948 -0.01131886
 -0.00909824 -0.00687762 -0.004657   -0.00243638 -0.00021576]
[257 262 345 478 574 764 775 854 887 879] [-0.02242196 -0.02020134 -0.01798072 -0.0157601  -0.01353948 -0.01131886
 -0.00909824 -0.00687762 -0.004657   -0.00243638 -0.00021576]
-4.20581
3.42666
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.43746
Epoch 1, cost is  2.312
Epoch 2, cost is  2.24823
Epoch 3, cost is  2.19495
Epoch 4, cost is  2.14678
Training took 0.108479 minutes
Weight histogram
[1098  732 1124 1851 1638  867  383  248  150    9] [-0.01574206 -0.01418943 -0.0126368  -0.01108417 -0.00953154 -0.00797891
 -0.00642628 -0.00487365 -0.00332102 -0.00176839 -0.00021576]
[ 411  377  509  699 1170 1527 2051  747  280  329] [-0.01574206 -0.01418943 -0.0126368  -0.01108417 -0.00953154 -0.00797891
 -0.00642628 -0.00487365 -0.00332102 -0.00176839 -0.00021576]
-1.97522
1.70234
fine tuning ...
Epoch 0
Fine tuning took 0.057707 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055881 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057278 minutes
{0: [0.049261083743842367, 0.044334975369458129, 0.041871921182266007, 0.036945812807881777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89039408866995073, 0.89901477832512311, 0.90024630541871919, 0.89778325123152714], 5: [0.060344827586206899, 0.056650246305418719, 0.057881773399014777, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381808 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8675
Epoch 1, cost is  3.71788
Epoch 2, cost is  3.678
Epoch 3, cost is  3.66999
Epoch 4, cost is  3.66773
Training took 0.153378 minutes
Weight histogram
[1227  772   15 1281  739  898  797  337    5    4] [-0.0154582  -0.01394301 -0.01242783 -0.01091265 -0.00939746 -0.00788228
 -0.00636709 -0.00485191 -0.00333673 -0.00182154 -0.00030636]
[263 288 411 522 649 772 747 818 804 801] [-0.0154582  -0.01394301 -0.01242783 -0.01091265 -0.00939746 -0.00788228
 -0.00636709 -0.00485191 -0.00333673 -0.00182154 -0.00030636]
-3.55639
3.39544
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.23062
Epoch 1, cost is  4.13139
Epoch 2, cost is  4.12218
Epoch 3, cost is  4.1217
Epoch 4, cost is  4.14301
Training took 0.084673 minutes
Weight histogram
[1472 1524 1237  790  401  290  152  166 2045   23] [-0.03938537 -0.03547747 -0.03156957 -0.02766166 -0.02375376 -0.01984586
 -0.01593796 -0.01203006 -0.00812216 -0.00421426 -0.00030636]
[ 637  871 1214  500  535  728  789  859  983  984] [-0.03938537 -0.03547747 -0.03156957 -0.02766166 -0.02375376 -0.01984586
 -0.01593796 -0.01203006 -0.00812216 -0.00421426 -0.00030636]
-4.11043
3.68643
fine tuning ...
Epoch 0
Fine tuning took 0.059371 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060026 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059875 minutes
{0: [0.057881773399014777, 0.072660098522167482, 0.075123152709359611, 0.073891625615763554], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87931034482758619, 0.85344827586206895, 0.8645320197044335, 0.85098522167487689], 5: [0.062807881773399021, 0.073891625615763554, 0.060344827586206899, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381978 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8675
Epoch 1, cost is  3.71788
Epoch 2, cost is  3.678
Epoch 3, cost is  3.66999
Epoch 4, cost is  3.66773
Training took 0.153292 minutes
Weight histogram
[1227  772   15 1281  739  898  797  337    5    4] [-0.0154582  -0.01394301 -0.01242783 -0.01091265 -0.00939746 -0.00788228
 -0.00636709 -0.00485191 -0.00333673 -0.00182154 -0.00030636]
[263 288 411 522 649 772 747 818 804 801] [-0.0154582  -0.01394301 -0.01242783 -0.01091265 -0.00939746 -0.00788228
 -0.00636709 -0.00485191 -0.00333673 -0.00182154 -0.00030636]
-3.55639
3.39544
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.05725
Epoch 1, cost is  2.95577
Epoch 2, cost is  2.91553
Epoch 3, cost is  2.90608
Epoch 4, cost is  2.89446
Training took 0.110035 minutes
Weight histogram
[1251  923 1515 1039  641  391  323 1651  356   10] [-0.0230676  -0.02079148 -0.01851535 -0.01623923 -0.0139631  -0.01168698
 -0.00941086 -0.00713473 -0.00485861 -0.00258248 -0.00030636]
[ 464  484  674  903 1092  841  880  931  913  918] [-0.0230676  -0.02079148 -0.01851535 -0.01623923 -0.0139631  -0.01168698
 -0.00941086 -0.00713473 -0.00485861 -0.00258248 -0.00030636]
-2.78157
2.73521
fine tuning ...
Epoch 0
Fine tuning took 0.062796 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062951 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063544 minutes
{0: [0.05295566502463054, 0.11576354679802955, 0.094827586206896547, 0.094827586206896547], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8780788177339901, 0.81034482758620685, 0.82758620689655171, 0.82266009852216748], 5: [0.068965517241379309, 0.073891625615763554, 0.077586206896551727, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381728 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8675
Epoch 1, cost is  3.71788
Epoch 2, cost is  3.678
Epoch 3, cost is  3.66999
Epoch 4, cost is  3.66773
Training took 0.154086 minutes
Weight histogram
[1227  772   15 1281  739  898  797  337    5    4] [-0.0154582  -0.01394301 -0.01242783 -0.01091265 -0.00939746 -0.00788228
 -0.00636709 -0.00485191 -0.00333673 -0.00182154 -0.00030636]
[263 288 411 522 649 772 747 818 804 801] [-0.0154582  -0.01394301 -0.01242783 -0.01091265 -0.00939746 -0.00788228
 -0.00636709 -0.00485191 -0.00333673 -0.00182154 -0.00030636]
-3.55639
3.39544
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.42142
Epoch 1, cost is  2.29127
Epoch 2, cost is  2.22415
Epoch 3, cost is  2.18856
Epoch 4, cost is  2.15921
Training took 0.152338 minutes
Weight histogram
[1655  750 1328  726  731 1470 1022  400   11    7] [-0.01506469 -0.01358885 -0.01211302 -0.01063719 -0.00916136 -0.00768552
 -0.00620969 -0.00473386 -0.00325802 -0.00178219 -0.00030636]
[ 368  335  449  609  790 1083 1202 1217 1073  974] [-0.01506469 -0.01358885 -0.01211302 -0.01063719 -0.00916136 -0.00768552
 -0.00620969 -0.00473386 -0.00325802 -0.00178219 -0.00030636]
-1.69055
1.73697
fine tuning ...
Epoch 0
Fine tuning took 0.067456 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067605 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068850 minutes
{0: [0.077586206896551727, 0.099753694581280791, 0.067733990147783252, 0.093596059113300489], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.85221674876847286, 0.82019704433497542, 0.84113300492610843, 0.80541871921182262], 5: [0.070197044334975367, 0.080049261083743842, 0.091133004926108374, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382038 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.00485
Epoch 1, cost is  2.82187
Epoch 2, cost is  2.74493
Epoch 3, cost is  2.69688
Epoch 4, cost is  2.66555
Training took 0.208571 minutes
Weight histogram
[1264  745   63 1625  348  862  809  353    3    3] [-0.01027919 -0.00927972 -0.00828026 -0.00728079 -0.00628133 -0.00528187
 -0.0042824  -0.00328294 -0.00228348 -0.00128401 -0.00028455]
[258 304 406 495 579 747 711 816 894 865] [-0.01027919 -0.00927972 -0.00828026 -0.00728079 -0.00628133 -0.00528187
 -0.0042824  -0.00328294 -0.00228348 -0.00128401 -0.00028455]
-2.17588
2.20072
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.42296
Epoch 1, cost is  4.34579
Epoch 2, cost is  4.33794
Epoch 3, cost is  4.3636
Epoch 4, cost is  4.3953
Training took 0.109939 minutes
Weight histogram
[1544  680 1473 1062  594  408  116  110 1058 1055] [-0.03900689 -0.03513465 -0.03126242 -0.02739018 -0.02351795 -0.01964572
 -0.01577348 -0.01190125 -0.00802901 -0.00415678 -0.00028455]
[ 885 1402  569  466  567  721  739  822  973  956] [-0.03900689 -0.03513465 -0.03126242 -0.02739018 -0.02351795 -0.01964572
 -0.01577348 -0.01190125 -0.00802901 -0.00415678 -0.00028455]
-4.6088
4.10639
fine tuning ...
Epoch 0
Fine tuning took 0.068426 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069024 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067676 minutes
{0: [0.059113300492610835, 0.066502463054187194, 0.065270935960591137, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8719211822660099, 0.85837438423645318, 0.86206896551724133, 0.8288177339901478], 5: [0.068965517241379309, 0.075123152709359611, 0.072660098522167482, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380197 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.00485
Epoch 1, cost is  2.82187
Epoch 2, cost is  2.74493
Epoch 3, cost is  2.69688
Epoch 4, cost is  2.66555
Training took 0.207993 minutes
Weight histogram
[1264  745   63 1625  348  862  809  353    3    3] [-0.01027919 -0.00927972 -0.00828026 -0.00728079 -0.00628133 -0.00528187
 -0.0042824  -0.00328294 -0.00228348 -0.00128401 -0.00028455]
[258 304 406 495 579 747 711 816 894 865] [-0.01027919 -0.00927972 -0.00828026 -0.00728079 -0.00628133 -0.00528187
 -0.0042824  -0.00328294 -0.00228348 -0.00128401 -0.00028455]
-2.17588
2.20072
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.37307
Epoch 1, cost is  3.2665
Epoch 2, cost is  3.233
Epoch 3, cost is  3.21936
Epoch 4, cost is  3.22794
Training took 0.153570 minutes
Weight histogram
[1115  961 1436  948  840  436  198  155 1999   12] [-0.02377557 -0.02142647 -0.01907737 -0.01672826 -0.01437916 -0.01203006
 -0.00968096 -0.00733185 -0.00498275 -0.00263365 -0.00028455]
[ 592  792 1080  843  600  799  795  865  864  870] [-0.02377557 -0.02142647 -0.01907737 -0.01672826 -0.01437916 -0.01203006
 -0.00968096 -0.00733185 -0.00498275 -0.00263365 -0.00028455]
-2.97092
3.49648
fine tuning ...
Epoch 0
Fine tuning took 0.073453 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073708 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073743 minutes
{0: [0.10467980295566502, 0.12931034482758622, 0.087438423645320201, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79679802955665024, 0.79187192118226601, 0.78448275862068961, 0.77832512315270941], 5: [0.098522167487684734, 0.078817733990147784, 0.12807881773399016, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380631 minutes
Weight histogram
[  15   47  188  442  991 1588 1656  910  219   19] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
[ 119  185  257  358  511  649  353  518 1168 1957] [ -3.26082401e-04  -2.51141621e-04  -1.76200841e-04  -1.01260061e-04
  -2.63192807e-05   4.86214994e-05   1.23562280e-04   1.98503060e-04
   2.73443840e-04   3.48384620e-04   4.23325400e-04]
-0.782238
0.817557
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.00485
Epoch 1, cost is  2.82187
Epoch 2, cost is  2.74493
Epoch 3, cost is  2.69688
Epoch 4, cost is  2.66555
Training took 0.207955 minutes
Weight histogram
[1264  745   63 1625  348  862  809  353    3    3] [-0.01027919 -0.00927972 -0.00828026 -0.00728079 -0.00628133 -0.00528187
 -0.0042824  -0.00328294 -0.00228348 -0.00128401 -0.00028455]
[258 304 406 495 579 747 711 816 894 865] [-0.01027919 -0.00927972 -0.00828026 -0.00728079 -0.00628133 -0.00528187
 -0.0042824  -0.00328294 -0.00228348 -0.00128401 -0.00028455]
-2.17588
2.20072
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.60582
Epoch 1, cost is  2.46966
Epoch 2, cost is  2.411
Epoch 3, cost is  2.37739
Epoch 4, cost is  2.35439
Training took 0.206263 minutes
Weight histogram
[1403  952 1351  725  792  454  660 1437  319    7] [-0.01500056 -0.01352896 -0.01205736 -0.01058576 -0.00911415 -0.00764255
 -0.00617095 -0.00469935 -0.00322775 -0.00175615 -0.00028455]
[ 447  507  696  913 1105  950  753  847  956  926] [-0.01500056 -0.01352896 -0.01205736 -0.01058576 -0.00911415 -0.00764255
 -0.00617095 -0.00469935 -0.00322775 -0.00175615 -0.00028455]
-1.93741
1.7634
fine tuning ...
Epoch 0
Fine tuning took 0.078110 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078814 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079115 minutes
{0: [0.13177339901477833, 0.1206896551724138, 0.14039408866995073, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7857142857142857, 0.77832512315270941, 0.75862068965517238, 0.76970443349753692], 5: [0.082512315270935957, 0.10098522167487685, 0.10098522167487685, 0.081280788177339899], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380821 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.06232
Epoch 1, cost is  4.94479
Epoch 2, cost is  4.94715
Epoch 3, cost is  4.97406
Epoch 4, cost is  5.02784
Training took 0.109890 minutes
Weight histogram
[1920   78 1640  392 1760  298 1374  595   33   10] [-0.02945551 -0.02653153 -0.02360756 -0.02068358 -0.01775961 -0.01483563
 -0.01191166 -0.00898769 -0.00606371 -0.00313974 -0.00021576]
[ 312  345  515  666  929  950 1086 1070 1148 1079] [-0.02945551 -0.02653153 -0.02360756 -0.02068358 -0.01775961 -0.01483563
 -0.01191166 -0.00898769 -0.00606371 -0.00313974 -0.00021576]
-5.04065
4.24199
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.97803
Epoch 1, cost is  3.90151
Epoch 2, cost is  3.90966
Epoch 3, cost is  3.93298
Epoch 4, cost is  3.95696
Training took 0.070892 minutes
Weight histogram
[1605 3362 1504  574  408  204  277 1674  479   38] [-0.0414868  -0.0373568  -0.03322679 -0.02909679 -0.02496679 -0.02083678
 -0.01670678 -0.01257677 -0.00844677 -0.00431677 -0.00018676]
[ 609  636  938 1191  919  943 1088 1263 1264 1274] [-0.0414868  -0.0373568  -0.03322679 -0.02909679 -0.02496679 -0.02083678
 -0.01670678 -0.01257677 -0.00844677 -0.00431677 -0.00018676]
-4.14574
4.59925
fine tuning ...
Epoch 0
Fine tuning took 0.053556 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053368 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053449 minutes
{0: [0.048029556650246302, 0.046798029556650245, 0.035714285714285712, 0.036945812807881777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91871921182266014, 0.89408866995073888, 0.91379310344827591, 0.91871921182266014], 5: [0.033251231527093597, 0.059113300492610835, 0.050492610837438424, 0.044334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380249 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.06232
Epoch 1, cost is  4.94479
Epoch 2, cost is  4.94715
Epoch 3, cost is  4.97406
Epoch 4, cost is  5.02784
Training took 0.109210 minutes
Weight histogram
[1920   78 1640  392 1760  298 1374  595   33   10] [-0.02945551 -0.02653153 -0.02360756 -0.02068358 -0.01775961 -0.01483563
 -0.01191166 -0.00898769 -0.00606371 -0.00313974 -0.00021576]
[ 312  345  515  666  929  950 1086 1070 1148 1079] [-0.02945551 -0.02653153 -0.02360756 -0.02068358 -0.01775961 -0.01483563
 -0.01191166 -0.00898769 -0.00606371 -0.00313974 -0.00021576]
-5.04065
4.24199
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.80111
Epoch 1, cost is  2.68077
Epoch 2, cost is  2.65595
Epoch 3, cost is  2.62094
Epoch 4, cost is  2.6101
Training took 0.085284 minutes
Weight histogram
[1143 2586 1768 1081  706 1602  916  274   37   12] [-0.02343562 -0.02110353 -0.01877143 -0.01643933 -0.01410724 -0.01177514
 -0.00944305 -0.00711095 -0.00477886 -0.00244676 -0.00011466]
[ 436  423  507  706  912 1424 1546 1536 1338 1297] [-0.02343562 -0.02110353 -0.01877143 -0.01643933 -0.01410724 -0.01177514
 -0.00944305 -0.00711095 -0.00477886 -0.00244676 -0.00011466]
-2.90712
2.73526
fine tuning ...
Epoch 0
Fine tuning took 0.054368 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053125 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055021 minutes
{0: [0.049261083743842367, 0.057881773399014777, 0.057881773399014777, 0.059113300492610835], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89901477832512311, 0.88054187192118227, 0.89162561576354682, 0.88793103448275867], 5: [0.051724137931034482, 0.061576354679802957, 0.050492610837438424, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382114 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.06232
Epoch 1, cost is  4.94479
Epoch 2, cost is  4.94715
Epoch 3, cost is  4.97406
Epoch 4, cost is  5.02784
Training took 0.109588 minutes
Weight histogram
[1920   78 1640  392 1760  298 1374  595   33   10] [-0.02945551 -0.02653153 -0.02360756 -0.02068358 -0.01775961 -0.01483563
 -0.01191166 -0.00898769 -0.00606371 -0.00313974 -0.00021576]
[ 312  345  515  666  929  950 1086 1070 1148 1079] [-0.02945551 -0.02653153 -0.02360756 -0.02068358 -0.01775961 -0.01483563
 -0.01191166 -0.00898769 -0.00606371 -0.00313974 -0.00021576]
-5.04065
4.24199
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.27552
Epoch 1, cost is  2.19001
Epoch 2, cost is  2.13993
Epoch 3, cost is  2.09782
Epoch 4, cost is  2.05712
Training took 0.107999 minutes
Weight histogram
[1075 2123  708 1801 1779 1632  523  276  199    9] [-0.01745503 -0.0157311  -0.01400718 -0.01228325 -0.01055932 -0.00883539
 -0.00711147 -0.00538754 -0.00366361 -0.00193969 -0.00021576]
[ 411  377  509  699 1170 1527 2051 1888 1164  329] [-0.01745503 -0.0157311  -0.01400718 -0.01228325 -0.01055932 -0.00883539
 -0.00711147 -0.00538754 -0.00366361 -0.00193969 -0.00021576]
-1.97522
1.70234
fine tuning ...
Epoch 0
Fine tuning took 0.058167 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057861 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056971 minutes
{0: [0.067733990147783252, 0.054187192118226604, 0.065270935960591137, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88054187192118227, 0.86945812807881773, 0.88916256157635465, 0.88177339901477836], 5: [0.051724137931034482, 0.076354679802955669, 0.045566502463054187, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.390936 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8906
Epoch 1, cost is  3.74021
Epoch 2, cost is  3.70031
Epoch 3, cost is  3.69435
Epoch 4, cost is  3.70085
Training took 0.152914 minutes
Weight histogram
[1764  245  957 1060  231 1785 1031 1013    9    5] [-0.02010274 -0.0181231  -0.01614346 -0.01416382 -0.01218419 -0.01020455
 -0.00822491 -0.00624527 -0.00426563 -0.002286   -0.00030636]
[ 320  400  591  757  955  935 1019  989 1090 1044] [-0.02010274 -0.0181231  -0.01614346 -0.01416382 -0.01218419 -0.01020455
 -0.00822491 -0.00624527 -0.00426563 -0.002286   -0.00030636]
-4.20317
3.99658
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.54055
Epoch 1, cost is  4.43962
Epoch 2, cost is  4.4441
Epoch 3, cost is  4.47636
Epoch 4, cost is  4.50926
Training took 0.084964 minutes
Weight histogram
[1760 2155 1507 1229  650  370  193  184 1959  118] [-0.04286131 -0.03860581 -0.03435032 -0.03009482 -0.02583933 -0.02158383
 -0.01732834 -0.01307284 -0.00881735 -0.00456185 -0.00030636]
[ 780 1169 1086  602  844  947 1107 1160 1239 1191] [-0.04286131 -0.03860581 -0.03435032 -0.03009482 -0.02583933 -0.02158383
 -0.01732834 -0.01307284 -0.00881735 -0.00456185 -0.00030636]
-5.25039
4.53156
fine tuning ...
Epoch 0
Fine tuning took 0.060955 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060082 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061090 minutes
{0: [0.057881773399014777, 0.078817733990147784, 0.081280788177339899, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86206896551724133, 0.81527093596059108, 0.79064039408866993, 0.7931034482758621], 5: [0.080049261083743842, 0.10591133004926108, 0.12807881773399016, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380767 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8906
Epoch 1, cost is  3.74021
Epoch 2, cost is  3.70031
Epoch 3, cost is  3.69435
Epoch 4, cost is  3.70085
Training took 0.152708 minutes
Weight histogram
[1764  245  957 1060  231 1785 1031 1013    9    5] [-0.02010274 -0.0181231  -0.01614346 -0.01416382 -0.01218419 -0.01020455
 -0.00822491 -0.00624527 -0.00426563 -0.002286   -0.00030636]
[ 320  400  591  757  955  935 1019  989 1090 1044] [-0.02010274 -0.0181231  -0.01614346 -0.01416382 -0.01218419 -0.01020455
 -0.00822491 -0.00624527 -0.00426563 -0.002286   -0.00030636]
-4.20317
3.99658
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.15367
Epoch 1, cost is  3.06849
Epoch 2, cost is  3.03735
Epoch 3, cost is  3.03763
Epoch 4, cost is  3.04598
Training took 0.109736 minutes
Weight histogram
[1765 2090 1146 1385  748  570  246 1670  493   12] [-0.0247711  -0.02232463 -0.01987815 -0.01743168 -0.0149852  -0.01253873
 -0.01009226 -0.00764578 -0.00519931 -0.00275283 -0.00030636]
[ 557  648  976 1263 1072 1068 1136 1108 1172 1125] [-0.0247711  -0.02232463 -0.01987815 -0.01743168 -0.0149852  -0.01253873
 -0.01009226 -0.00764578 -0.00519931 -0.00275283 -0.00030636]
-3.26287
3.08894
fine tuning ...
Epoch 0
Fine tuning took 0.063231 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063684 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063797 minutes
{0: [0.076354679802955669, 0.11330049261083744, 0.10344827586206896, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.77832512315270941, 0.76970443349753692, 0.78448275862068961], 5: [0.13054187192118227, 0.10837438423645321, 0.1268472906403941, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381150 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8906
Epoch 1, cost is  3.74021
Epoch 2, cost is  3.70031
Epoch 3, cost is  3.69435
Epoch 4, cost is  3.70085
Training took 0.151900 minutes
Weight histogram
[1764  245  957 1060  231 1785 1031 1013    9    5] [-0.02010274 -0.0181231  -0.01614346 -0.01416382 -0.01218419 -0.01020455
 -0.00822491 -0.00624527 -0.00426563 -0.002286   -0.00030636]
[ 320  400  591  757  955  935 1019  989 1090 1044] [-0.02010274 -0.0181231  -0.01614346 -0.01416382 -0.01218419 -0.01020455
 -0.00822491 -0.00624527 -0.00426563 -0.002286   -0.00030636]
-4.20317
3.99658
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.34657
Epoch 1, cost is  2.23808
Epoch 2, cost is  2.20499
Epoch 3, cost is  2.16541
Epoch 4, cost is  2.14833
Training took 0.151792 minutes
Weight histogram
[1691 2210  973 1149  828  952 1615  686   14    7] [-0.01655625 -0.01493126 -0.01330627 -0.01168128 -0.01005629 -0.0084313
 -0.00680631 -0.00518133 -0.00355634 -0.00193135 -0.00030636]
[ 431  443  635  892 1271 1453 1441 1198 1203 1158] [-0.01655625 -0.01493126 -0.01330627 -0.01168128 -0.01005629 -0.0084313
 -0.00680631 -0.00518133 -0.00355634 -0.00193135 -0.00030636]
-2.01341
2.32993
fine tuning ...
Epoch 0
Fine tuning took 0.068075 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068235 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068594 minutes
{0: [0.10344827586206896, 0.11206896551724138, 0.12438423645320197, 0.082512315270935957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79802955665024633, 0.74630541871921185, 0.73768472906403937, 0.82512315270935965], 5: [0.098522167487684734, 0.14162561576354679, 0.13793103448275862, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379536 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.93365
Epoch 1, cost is  2.76088
Epoch 2, cost is  2.69888
Epoch 3, cost is  2.65912
Epoch 4, cost is  2.63426
Training took 0.206617 minutes
Weight histogram
[1743  277 1746  277 1097  930 1081  926   20    3] [-0.01283854 -0.01158314 -0.01032774 -0.00907234 -0.00781694 -0.00656155
 -0.00530615 -0.00405075 -0.00279535 -0.00153995 -0.00028455]
[ 316  412  562  685  894  878 1084 1061 1125 1083] [-0.01283854 -0.01158314 -0.01032774 -0.00907234 -0.00781694 -0.00656155
 -0.00530615 -0.00405075 -0.00279535 -0.00153995 -0.00028455]
-2.63348
2.5794
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.80798
Epoch 1, cost is  4.70581
Epoch 2, cost is  4.71365
Epoch 3, cost is  4.73906
Epoch 4, cost is  4.78102
Training took 0.106952 minutes
Weight histogram
[1803 2149  884 1374  970  522  177  112  704 1430] [-0.04228541 -0.03808532 -0.03388523 -0.02968515 -0.02548506 -0.02128498
 -0.01708489 -0.0128848  -0.00868472 -0.00448463 -0.00028455]
[1153 1492  514  659  865  913 1110 1161 1147 1111] [-0.04228541 -0.03808532 -0.03388523 -0.02968515 -0.02548506 -0.02128498
 -0.01708489 -0.0128848  -0.00868472 -0.00448463 -0.00028455]
-5.73381
4.73266
fine tuning ...
Epoch 0
Fine tuning took 0.068061 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069659 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068157 minutes
{0: [0.099753694581280791, 0.11945812807881774, 0.073891625615763554, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79802955665024633, 0.7931034482758621, 0.84482758620689657, 0.79433497536945807], 5: [0.10221674876847291, 0.087438423645320201, 0.081280788177339899, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381119 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.93365
Epoch 1, cost is  2.76088
Epoch 2, cost is  2.69888
Epoch 3, cost is  2.65912
Epoch 4, cost is  2.63426
Training took 0.206590 minutes
Weight histogram
[1743  277 1746  277 1097  930 1081  926   20    3] [-0.01283854 -0.01158314 -0.01032774 -0.00907234 -0.00781694 -0.00656155
 -0.00530615 -0.00405075 -0.00279535 -0.00153995 -0.00028455]
[ 316  412  562  685  894  878 1084 1061 1125 1083] [-0.01283854 -0.01158314 -0.01032774 -0.00907234 -0.00781694 -0.00656155
 -0.00530615 -0.00405075 -0.00279535 -0.00153995 -0.00028455]
-2.63348
2.5794
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.4
Epoch 1, cost is  3.30808
Epoch 2, cost is  3.30025
Epoch 3, cost is  3.30161
Epoch 4, cost is  3.31128
Training took 0.151000 minutes
Weight histogram
[1482 2044 1008 1527  858  671  324  161 1998   52] [-0.02609928 -0.02351781 -0.02093633 -0.01835486 -0.01577339 -0.01319191
 -0.01061044 -0.00802897 -0.00544749 -0.00286602 -0.00028455]
[ 741 1089 1308  692  960  976 1072 1056 1130 1101] [-0.02609928 -0.02351781 -0.02093633 -0.01835486 -0.01577339 -0.01319191
 -0.01061044 -0.00802897 -0.00544749 -0.00286602 -0.00028455]
-4.01109
3.93744
fine tuning ...
Epoch 0
Fine tuning took 0.074340 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074241 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073988 minutes
{0: [0.10837438423645321, 0.12561576354679804, 0.13669950738916256, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78201970443349755, 0.74507389162561577, 0.74137931034482762, 0.74507389162561577], 5: [0.10960591133004927, 0.12931034482758622, 0.12192118226600986, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382653 minutes
Weight histogram
[  28  208  811 2065 2287  798  655  891  333   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 134  200  309  458  617  531  496  802 2290 2263] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.93365
Epoch 1, cost is  2.76088
Epoch 2, cost is  2.69888
Epoch 3, cost is  2.65912
Epoch 4, cost is  2.63426
Training took 0.207560 minutes
Weight histogram
[1743  277 1746  277 1097  930 1081  926   20    3] [-0.01283854 -0.01158314 -0.01032774 -0.00907234 -0.00781694 -0.00656155
 -0.00530615 -0.00405075 -0.00279535 -0.00153995 -0.00028455]
[ 316  412  562  685  894  878 1084 1061 1125 1083] [-0.01283854 -0.01158314 -0.01032774 -0.00907234 -0.00781694 -0.00656155
 -0.00530615 -0.00405075 -0.00279535 -0.00153995 -0.00028455]
-2.63348
2.5794
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.45455
Epoch 1, cost is  2.33796
Epoch 2, cost is  2.29242
Epoch 3, cost is  2.27723
Epoch 4, cost is  2.25961
Training took 0.206371 minutes
Weight histogram
[1702 2281  922 1155  894  644  401 1576  543    7] [-0.01631281 -0.01470999 -0.01310716 -0.01150433 -0.00990151 -0.00829868
 -0.00669585 -0.00509303 -0.0034902  -0.00188737 -0.00028455]
[ 542  684  983 1278 1177  922 1111 1126 1172 1130] [-0.01631281 -0.01470999 -0.01310716 -0.01150433 -0.00990151 -0.00829868
 -0.00669585 -0.00509303 -0.0034902  -0.00188737 -0.00028455]
-2.28074
2.16873
fine tuning ...
Epoch 0
Fine tuning took 0.078104 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078500 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078150 minutes
{0: [0.10960591133004927, 0.13669950738916256, 0.15147783251231528, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.71059113300492616, 0.74384236453201968, 0.75369458128078815], 5: [0.11822660098522167, 0.15270935960591134, 0.10467980295566502, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381921 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.15265
Epoch 1, cost is  4.97463
Epoch 2, cost is  4.93736
Epoch 3, cost is  4.94226
Epoch 4, cost is  4.96407
Training took 0.107611 minutes
Weight histogram
[1996 1756  265 1748  295 2002  841 1167   41   14] [-0.03416239 -0.03076772 -0.02737306 -0.0239784  -0.02058374 -0.01718907
 -0.01379441 -0.01039975 -0.00700509 -0.00361042 -0.00021576]
[ 358  432  658  935 1076 1215 1228 1311 1396 1516] [-0.03416239 -0.03076772 -0.02737306 -0.0239784  -0.02058374 -0.01718907
 -0.01379441 -0.01039975 -0.00700509 -0.00361042 -0.00021576]
-6.19364
5.19359
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.00833
Epoch 1, cost is  3.88451
Epoch 2, cost is  3.86608
Epoch 3, cost is  3.88123
Epoch 4, cost is  3.88664
Training took 0.071071 minutes
Weight histogram
[2164 4106 1750  882  516  255  176 1672  589   40] [-0.04358585 -0.03924594 -0.03490603 -0.03056612 -0.02622621 -0.02188631
 -0.0175464  -0.01320649 -0.00886658 -0.00452667 -0.00018676]
[ 665  762 1123 1269  993 1074 1405 1401 1415 2043] [-0.04358585 -0.03924594 -0.03490603 -0.03056612 -0.02622621 -0.02188631
 -0.0175464  -0.01320649 -0.00886658 -0.00452667 -0.00018676]
-5.00621
5.44272
fine tuning ...
Epoch 0
Fine tuning took 0.053523 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051855 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053432 minutes
{0: [0.045566502463054187, 0.049261083743842367, 0.050492610837438424, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91009852216748766, 0.88793103448275867, 0.89778325123152714, 0.88669950738916259], 5: [0.044334975369458129, 0.062807881773399021, 0.051724137931034482, 0.055418719211822662], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380492 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.15265
Epoch 1, cost is  4.97463
Epoch 2, cost is  4.93736
Epoch 3, cost is  4.94226
Epoch 4, cost is  4.96407
Training took 0.108970 minutes
Weight histogram
[1996 1756  265 1748  295 2002  841 1167   41   14] [-0.03416239 -0.03076772 -0.02737306 -0.0239784  -0.02058374 -0.01718907
 -0.01379441 -0.01039975 -0.00700509 -0.00361042 -0.00021576]
[ 358  432  658  935 1076 1215 1228 1311 1396 1516] [-0.03416239 -0.03076772 -0.02737306 -0.0239784  -0.02058374 -0.01718907
 -0.01379441 -0.01039975 -0.00700509 -0.00361042 -0.00021576]
-6.19364
5.19359
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.79108
Epoch 1, cost is  2.72012
Epoch 2, cost is  2.70065
Epoch 3, cost is  2.69082
Epoch 4, cost is  2.67001
Training took 0.086686 minutes
Weight histogram
[1257 1948 2877 1752 1022 1356 1336  487  101   14] [-0.02630998 -0.02369045 -0.02107092 -0.01845139 -0.01583186 -0.01321232
 -0.01059279 -0.00797326 -0.00535373 -0.0027342  -0.00011466]
[ 479  500  637  903 1372 1694 1789 1513 1560 1703] [-0.02630998 -0.02369045 -0.02107092 -0.01845139 -0.01583186 -0.01321232
 -0.01059279 -0.00797326 -0.00535373 -0.0027342  -0.00011466]
-3.22439
3.12085
fine tuning ...
Epoch 0
Fine tuning took 0.055166 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053757 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053481 minutes
{0: [0.057881773399014777, 0.057881773399014777, 0.062807881773399021, 0.056650246305418719], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8780788177339901, 0.86945812807881773, 0.85591133004926112, 0.84729064039408863], 5: [0.064039408866995079, 0.072660098522167482, 0.081280788177339899, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380843 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.15265
Epoch 1, cost is  4.97463
Epoch 2, cost is  4.93736
Epoch 3, cost is  4.94226
Epoch 4, cost is  4.96407
Training took 0.108785 minutes
Weight histogram
[1996 1756  265 1748  295 2002  841 1167   41   14] [-0.03416239 -0.03076772 -0.02737306 -0.0239784  -0.02058374 -0.01718907
 -0.01379441 -0.01039975 -0.00700509 -0.00361042 -0.00021576]
[ 358  432  658  935 1076 1215 1228 1311 1396 1516] [-0.03416239 -0.03076772 -0.02737306 -0.0239784  -0.02058374 -0.01718907
 -0.01379441 -0.01039975 -0.00700509 -0.00361042 -0.00021576]
-6.19364
5.19359
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.17608
Epoch 1, cost is  2.11155
Epoch 2, cost is  2.07047
Epoch 3, cost is  2.04313
Epoch 4, cost is  2.01182
Training took 0.108009 minutes
Weight histogram
[1515 2318 1787  871 2258 2017  746  395  233   10] [-0.01901957 -0.01713919 -0.01525881 -0.01337843 -0.01149805 -0.00961767
 -0.00773729 -0.0058569  -0.00397652 -0.00209614 -0.00021576]
[ 411  377  509  699 1170 1527 2051 1888 2014 1504] [-0.01901957 -0.01713919 -0.01525881 -0.01337843 -0.01149805 -0.00961767
 -0.00773729 -0.0058569  -0.00397652 -0.00209614 -0.00021576]
-1.97522
1.70234
fine tuning ...
Epoch 0
Fine tuning took 0.056933 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057421 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056896 minutes
{0: [0.070197044334975367, 0.065270935960591137, 0.072660098522167482, 0.067733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83620689655172409, 0.84975369458128081, 0.86330049261083741, 0.83743842364532017], 5: [0.093596059113300489, 0.084975369458128072, 0.064039408866995079, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380016 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.85539
Epoch 1, cost is  3.70116
Epoch 2, cost is  3.65309
Epoch 3, cost is  3.64228
Epoch 4, cost is  3.64665
Training took 0.152607 minutes
Weight histogram
[1972 1422  639 1259  763 1305  843 1586  330    6] [-0.02300169 -0.02073216 -0.01846263 -0.01619309 -0.01392356 -0.01165403
 -0.00938449 -0.00711496 -0.00484543 -0.00257589 -0.00030636]
[ 377  525  779 1057 1081 1188 1177 1262 1328 1351] [-0.02300169 -0.02073216 -0.01846263 -0.01619309 -0.01392356 -0.01165403
 -0.00938449 -0.00711496 -0.00484543 -0.00257589 -0.00030636]
-4.7612
4.78563
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.65033
Epoch 1, cost is  4.55279
Epoch 2, cost is  4.53844
Epoch 3, cost is  4.56719
Epoch 4, cost is  4.59283
Training took 0.084838 minutes
Weight histogram
[3754 2186 1507 1229  650  370  193  184 1959  118] [-0.04286131 -0.03860581 -0.03435032 -0.03009482 -0.02583933 -0.02158383
 -0.01732834 -0.01307284 -0.00881735 -0.00456185 -0.00030636]
[ 913 1453  907  780 1093 1199 1332 1410 1471 1592] [-0.04286131 -0.03860581 -0.03435032 -0.03009482 -0.02583933 -0.02158383
 -0.01732834 -0.01307284 -0.00881735 -0.00456185 -0.00030636]
-5.99288
5.48551
fine tuning ...
Epoch 0
Fine tuning took 0.059372 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060694 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059416 minutes
{0: [0.1268472906403941, 0.092364532019704432, 0.068965517241379309, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79433497536945807, 0.80172413793103448, 0.83374384236453203, 0.76847290640394084], 5: [0.078817733990147784, 0.10591133004926108, 0.097290640394088676, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380339 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.85539
Epoch 1, cost is  3.70116
Epoch 2, cost is  3.65309
Epoch 3, cost is  3.64228
Epoch 4, cost is  3.64665
Training took 0.153131 minutes
Weight histogram
[1972 1422  639 1259  763 1305  843 1586  330    6] [-0.02300169 -0.02073216 -0.01846263 -0.01619309 -0.01392356 -0.01165403
 -0.00938449 -0.00711496 -0.00484543 -0.00257589 -0.00030636]
[ 377  525  779 1057 1081 1188 1177 1262 1328 1351] [-0.02300169 -0.02073216 -0.01846263 -0.01619309 -0.01392356 -0.01165403
 -0.00938449 -0.00711496 -0.00484543 -0.00257589 -0.00030636]
-4.7612
4.78563
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.09967
Epoch 1, cost is  3.00187
Epoch 2, cost is  2.97252
Epoch 3, cost is  2.98316
Epoch 4, cost is  2.97354
Training took 0.110403 minutes
Weight histogram
[2864 2495 1203 1487  970  616  282 1661  560   12] [-0.02582837 -0.02327617 -0.02072397 -0.01817177 -0.01561957 -0.01306736
 -0.01051516 -0.00796296 -0.00541076 -0.00285856 -0.00030636]
[ 640  831 1259 1334 1278 1306 1272 1356 1406 1468] [-0.02582837 -0.02327617 -0.02072397 -0.01817177 -0.01561957 -0.01306736
 -0.01051516 -0.00796296 -0.00541076 -0.00285856 -0.00030636]
-3.70819
3.51789
fine tuning ...
Epoch 0
Fine tuning took 0.062463 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062673 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.064120 minutes
{0: [0.14532019704433496, 0.1145320197044335, 0.12561576354679804, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.73029556650246308, 0.72167487684729059, 0.74014778325123154], 5: [0.14655172413793102, 0.15517241379310345, 0.15270935960591134, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380375 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.85539
Epoch 1, cost is  3.70116
Epoch 2, cost is  3.65309
Epoch 3, cost is  3.64228
Epoch 4, cost is  3.64665
Training took 0.153654 minutes
Weight histogram
[1972 1422  639 1259  763 1305  843 1586  330    6] [-0.02300169 -0.02073216 -0.01846263 -0.01619309 -0.01392356 -0.01165403
 -0.00938449 -0.00711496 -0.00484543 -0.00257589 -0.00030636]
[ 377  525  779 1057 1081 1188 1177 1262 1328 1351] [-0.02300169 -0.02073216 -0.01846263 -0.01619309 -0.01392356 -0.01165403
 -0.00938449 -0.00711496 -0.00484543 -0.00257589 -0.00030636]
-4.7612
4.78563
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.30107
Epoch 1, cost is  2.19411
Epoch 2, cost is  2.15293
Epoch 3, cost is  2.12837
Epoch 4, cost is  2.11297
Training took 0.152626 minutes
Weight histogram
[2539 2509 1166 1532  865  818 1838  848   27    8] [-0.01766708 -0.01593101 -0.01419494 -0.01245887 -0.01072279 -0.00898672
 -0.00725065 -0.00551458 -0.0037785  -0.00204243 -0.00030636]
[ 494  565  875 1223 1735 1685 1391 1403 1391 1388] [-0.01766708 -0.01593101 -0.01419494 -0.01245887 -0.01072279 -0.00898672
 -0.00725065 -0.00551458 -0.0037785  -0.00204243 -0.00030636]
-2.49636
2.72685
fine tuning ...
Epoch 0
Fine tuning took 0.067048 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068791 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068598 minutes
{0: [0.1625615763546798, 0.1354679802955665, 0.14039408866995073, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.74384236453201968, 0.72906403940886699, 0.72044334975369462], 5: [0.13916256157635468, 0.1206896551724138, 0.13054187192118227, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380550 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.85327
Epoch 1, cost is  2.67808
Epoch 2, cost is  2.61558
Epoch 3, cost is  2.58523
Epoch 4, cost is  2.55868
Training took 0.207763 minutes
Weight histogram
[1934  157 1927  702 1343 1283  810 1545  420    4] [-0.01572108 -0.01417743 -0.01263378 -0.01109012 -0.00954647 -0.00800282
 -0.00645916 -0.00491551 -0.00337185 -0.0018282  -0.00028455]
[ 376  536  715  982 1023 1249 1259 1305 1337 1343] [-0.01572108 -0.01417743 -0.01263378 -0.01109012 -0.00954647 -0.00800282
 -0.00645916 -0.00491551 -0.00337185 -0.0018282  -0.00028455]
-3.05789
2.91099
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.95802
Epoch 1, cost is  4.85651
Epoch 2, cost is  4.84557
Epoch 3, cost is  4.86811
Epoch 4, cost is  4.90357
Training took 0.106538 minutes
Weight histogram
[3391 2357  921 1474 1020  537  202  110  580 1558] [-0.04311646 -0.03883327 -0.03455008 -0.03026689 -0.0259837  -0.0217005
 -0.01741731 -0.01313412 -0.00885093 -0.00456774 -0.00028455]
[1389 1390  651  889 1026 1221 1333 1307 1409 1535] [-0.04311646 -0.03883327 -0.03455008 -0.03026689 -0.0259837  -0.0217005
 -0.01741731 -0.01313412 -0.00885093 -0.00456774 -0.00028455]
-6.48983
5.6681
fine tuning ...
Epoch 0
Fine tuning took 0.067773 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068987 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069225 minutes
{0: [0.10467980295566502, 0.10098522167487685, 0.12807881773399016, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8214285714285714, 0.77586206896551724, 0.76108374384236455, 0.76231527093596063], 5: [0.073891625615763554, 0.12315270935960591, 0.11083743842364532, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380554 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.85327
Epoch 1, cost is  2.67808
Epoch 2, cost is  2.61558
Epoch 3, cost is  2.58523
Epoch 4, cost is  2.55868
Training took 0.207328 minutes
Weight histogram
[1934  157 1927  702 1343 1283  810 1545  420    4] [-0.01572108 -0.01417743 -0.01263378 -0.01109012 -0.00954647 -0.00800282
 -0.00645916 -0.00491551 -0.00337185 -0.0018282  -0.00028455]
[ 376  536  715  982 1023 1249 1259 1305 1337 1343] [-0.01572108 -0.01417743 -0.01263378 -0.01109012 -0.00954647 -0.00800282
 -0.00645916 -0.00491551 -0.00337185 -0.0018282  -0.00028455]
-3.05789
2.91099
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.46596
Epoch 1, cost is  3.35091
Epoch 2, cost is  3.3251
Epoch 3, cost is  3.32112
Epoch 4, cost is  3.33057
Training took 0.151511 minutes
Weight histogram
[2278 2329 1412 1451 1143  872  412  197 1878  178] [-0.02798751 -0.02521721 -0.02244692 -0.01967662 -0.01690632 -0.01413603
 -0.01136573 -0.00859544 -0.00582514 -0.00305484 -0.00028455]
[ 895 1399 1169 1015 1138 1240 1237 1325 1363 1369] [-0.02798751 -0.02521721 -0.02244692 -0.01967662 -0.01690632 -0.01413603
 -0.01136573 -0.00859544 -0.00582514 -0.00305484 -0.00028455]
-4.58745
4.93379
fine tuning ...
Epoch 0
Fine tuning took 0.074791 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074224 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073591 minutes
{0: [0.14901477832512317, 0.1539408866995074, 0.18719211822660098, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69088669950738912, 0.66871921182266014, 0.63054187192118227, 0.64408866995073888], 5: [0.16009852216748768, 0.17733990147783252, 0.18226600985221675, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379880 minutes
Weight histogram
[  28  208  811 2081 2453 1828 1387  967  338   24] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
[ 145  222  340  521  718  455  598 1764 2313 3049] [ -3.26082401e-04  -2.15114554e-04  -1.04146707e-04   6.82114041e-06
   1.17788988e-04   2.28756835e-04   3.39724682e-04   4.50692530e-04
   5.61660377e-04   6.72628224e-04   7.83596071e-04]
-0.870602
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.85327
Epoch 1, cost is  2.67808
Epoch 2, cost is  2.61558
Epoch 3, cost is  2.58523
Epoch 4, cost is  2.55868
Training took 0.206466 minutes
Weight histogram
[1934  157 1927  702 1343 1283  810 1545  420    4] [-0.01572108 -0.01417743 -0.01263378 -0.01109012 -0.00954647 -0.00800282
 -0.00645916 -0.00491551 -0.00337185 -0.0018282  -0.00028455]
[ 376  536  715  982 1023 1249 1259 1305 1337 1343] [-0.01572108 -0.01417743 -0.01263378 -0.01109012 -0.00954647 -0.00800282
 -0.00645916 -0.00491551 -0.00337185 -0.0018282  -0.00028455]
-3.05789
2.91099
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.46257
Epoch 1, cost is  2.33988
Epoch 2, cost is  2.30404
Epoch 3, cost is  2.28152
Epoch 4, cost is  2.27105
Training took 0.207542 minutes
Weight histogram
[1634 2563 1863 1368 1020  938  491 1448  817    8] [-0.01806667 -0.01628846 -0.01451025 -0.01273203 -0.01095382 -0.00917561
 -0.0073974  -0.00561918 -0.00384097 -0.00206276 -0.00028455]
[ 642  891 1314 1545 1076 1287 1319 1373 1355 1348] [-0.01806667 -0.01628846 -0.01451025 -0.01273203 -0.01095382 -0.00917561
 -0.0073974  -0.00561918 -0.00384097 -0.00206276 -0.00028455]
-2.85488
2.6856
fine tuning ...
Epoch 0
Fine tuning took 0.079129 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078175 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079075 minutes
{0: [0.17241379310344829, 0.14285714285714285, 0.16379310344827586, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61822660098522164, 0.65886699507389157, 0.63177339901477836, 0.6145320197044335], 5: [0.20935960591133004, 0.19827586206896552, 0.20443349753694581, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379819 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.48399
Epoch 1, cost is  5.33555
Epoch 2, cost is  5.32713
Epoch 3, cost is  5.35192
Epoch 4, cost is  5.39536
Training took 0.107423 minutes
Weight histogram
[1931 1642 1259 1205 1902  197 1977 1624  384   29] [-0.04040445 -0.03638558 -0.03236671 -0.02834784 -0.02432897 -0.0203101
 -0.01629124 -0.01227237 -0.0082535  -0.00423463 -0.00021576]
[ 409  548  834 1209 1325 1415 1485 1631 1732 1562] [-0.04040445 -0.03638558 -0.03236671 -0.02834784 -0.02432897 -0.0203101
 -0.01629124 -0.01227237 -0.0082535  -0.00423463 -0.00021576]
-6.97923
5.83132
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.44036
Epoch 1, cost is  4.34053
Epoch 2, cost is  4.31351
Epoch 3, cost is  4.34036
Epoch 4, cost is  4.35983
Training took 0.072330 minutes
Weight histogram
[2317 5941 1787  882  516  255  176 1672  589   40] [-0.04358585 -0.03924594 -0.03490603 -0.03056612 -0.02622621 -0.02188631
 -0.0175464  -0.01320649 -0.00886658 -0.00452667 -0.00018676]
[ 738  934 1362 1246 1185 1473 1570 1580 2265 1822] [-0.04358585 -0.03924594 -0.03490603 -0.03056612 -0.02622621 -0.02188631
 -0.0175464  -0.01320649 -0.00886658 -0.00452667 -0.00018676]
-5.55632
6.08565
fine tuning ...
Epoch 0
Fine tuning took 0.051312 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053382 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050852 minutes
{0: [0.064039408866995079, 0.059113300492610835, 0.049261083743842367, 0.054187192118226604], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8571428571428571, 0.85591133004926112, 0.88669950738916259, 0.84975369458128081], 5: [0.078817733990147784, 0.084975369458128072, 0.064039408866995079, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379933 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.48399
Epoch 1, cost is  5.33555
Epoch 2, cost is  5.32713
Epoch 3, cost is  5.35192
Epoch 4, cost is  5.39536
Training took 0.109201 minutes
Weight histogram
[1931 1642 1259 1205 1902  197 1977 1624  384   29] [-0.04040445 -0.03638558 -0.03236671 -0.02834784 -0.02432897 -0.0203101
 -0.01629124 -0.01227237 -0.0082535  -0.00423463 -0.00021576]
[ 409  548  834 1209 1325 1415 1485 1631 1732 1562] [-0.04040445 -0.03638558 -0.03236671 -0.02834784 -0.02432897 -0.0203101
 -0.01629124 -0.01227237 -0.0082535  -0.00423463 -0.00021576]
-6.97923
5.83132
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.0988
Epoch 1, cost is  3.03919
Epoch 2, cost is  3.01043
Epoch 3, cost is  2.98767
Epoch 4, cost is  2.99509
Training took 0.086012 minutes
Weight histogram
[3260 1970 2877 1752 1022 1356 1336  487  101   14] [-0.02630998 -0.02369045 -0.02107092 -0.01845139 -0.01583186 -0.01321232
 -0.01059279 -0.00797326 -0.00535373 -0.0027342  -0.00011466]
[ 537  584  823 1164 1898 2077 1690 1784 1915 1703] [-0.02630998 -0.02369045 -0.02107092 -0.01845139 -0.01583186 -0.01321232
 -0.01059279 -0.00797326 -0.00535373 -0.0027342  -0.00011466]
-3.564
3.57128
fine tuning ...
Epoch 0
Fine tuning took 0.052484 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053758 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054503 minutes
{0: [0.097290640394088676, 0.10714285714285714, 0.084975369458128072, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83990147783251234, 0.81280788177339902, 0.8423645320197044, 0.81650246305418717], 5: [0.062807881773399021, 0.080049261083743842, 0.072660098522167482, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.383519 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.48399
Epoch 1, cost is  5.33555
Epoch 2, cost is  5.32713
Epoch 3, cost is  5.35192
Epoch 4, cost is  5.39536
Training took 0.106430 minutes
Weight histogram
[1931 1642 1259 1205 1902  197 1977 1624  384   29] [-0.04040445 -0.03638558 -0.03236671 -0.02834784 -0.02432897 -0.0203101
 -0.01629124 -0.01227237 -0.0082535  -0.00423463 -0.00021576]
[ 409  548  834 1209 1325 1415 1485 1631 1732 1562] [-0.04040445 -0.03638558 -0.03236671 -0.02834784 -0.02432897 -0.0203101
 -0.01629124 -0.01227237 -0.0082535  -0.00423463 -0.00021576]
-6.97923
5.83132
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.17223
Epoch 1, cost is  2.07671
Epoch 2, cost is  2.0421
Epoch 3, cost is  2.01486
Epoch 4, cost is  1.99607
Training took 0.109214 minutes
Weight histogram
[2151 2578 2489  803 2205 2159 1078  431  269   12] [-0.02000716 -0.01802802 -0.01604888 -0.01406974 -0.0120906  -0.01011146
 -0.00813232 -0.00615318 -0.00417404 -0.0021949  -0.00021576]
[ 438  421  587  828 1514 2009 2053 2081 2270 1974] [-0.02000716 -0.01802802 -0.01604888 -0.01406974 -0.0120906  -0.01011146
 -0.00813232 -0.00615318 -0.00417404 -0.0021949  -0.00021576]
-2.11792
1.85945
fine tuning ...
Epoch 0
Fine tuning took 0.057077 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057217 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055614 minutes
{0: [0.068965517241379309, 0.075123152709359611, 0.082512315270935957, 0.078817733990147784], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8423645320197044, 0.79064039408866993, 0.78940886699507384, 0.81650246305418717], 5: [0.088669950738916259, 0.13423645320197045, 0.12807881773399016, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380816 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.06012
Epoch 1, cost is  3.90243
Epoch 2, cost is  3.86517
Epoch 3, cost is  3.85897
Epoch 4, cost is  3.87345
Training took 0.151740 minutes
Weight histogram
[1971 1958 1405  728 1900  131 2003 1442  604    8] [-0.02636126 -0.02375577 -0.02115028 -0.01854479 -0.0159393  -0.01333381
 -0.01072832 -0.00812283 -0.00551734 -0.00291185 -0.00030636]
[ 441  683 1005 1267 1338 1334 1466 1548 1566 1502] [-0.02636126 -0.02375577 -0.02115028 -0.01854479 -0.0159393  -0.01333381
 -0.01072832 -0.00812283 -0.00551734 -0.00291185 -0.00030636]
-5.66571
5.08959
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.74662
Epoch 1, cost is  4.65263
Epoch 2, cost is  4.6431
Epoch 3, cost is  4.6676
Epoch 4, cost is  4.71024
Training took 0.084693 minutes
Weight histogram
[4613 3058 1541 1417  656  405  196  207 1889  193] [-0.04387997 -0.03952261 -0.03516525 -0.03080788 -0.02645052 -0.02209316
 -0.0177358  -0.01337844 -0.00902108 -0.00466372 -0.00030636]
[1072 1764  740 1089 1268 1521 1598 1698 1798 1627] [-0.04387997 -0.03952261 -0.03516525 -0.03080788 -0.02645052 -0.02209316
 -0.0177358  -0.01337844 -0.00902108 -0.00466372 -0.00030636]
-6.7221
5.88039
fine tuning ...
Epoch 0
Fine tuning took 0.060631 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060909 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060061 minutes
{0: [0.091133004926108374, 0.11945812807881774, 0.098522167487684734, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80418719211822665, 0.74384236453201968, 0.74384236453201968, 0.68842364532019706], 5: [0.10467980295566502, 0.13669950738916256, 0.15763546798029557, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379639 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.06012
Epoch 1, cost is  3.90243
Epoch 2, cost is  3.86517
Epoch 3, cost is  3.85897
Epoch 4, cost is  3.87345
Training took 0.151757 minutes
Weight histogram
[1971 1958 1405  728 1900  131 2003 1442  604    8] [-0.02636126 -0.02375577 -0.02115028 -0.01854479 -0.0159393  -0.01333381
 -0.01072832 -0.00812283 -0.00551734 -0.00291185 -0.00030636]
[ 441  683 1005 1267 1338 1334 1466 1548 1566 1502] [-0.02636126 -0.02375577 -0.02115028 -0.01854479 -0.0159393  -0.01333381
 -0.01072832 -0.00812283 -0.00551734 -0.00291185 -0.00030636]
-5.66571
5.08959
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.27907
Epoch 1, cost is  3.18037
Epoch 2, cost is  3.15539
Epoch 3, cost is  3.14221
Epoch 4, cost is  3.16126
Training took 0.110585 minutes
Weight histogram
[2253 3633 2177 1331 1384  699  428 1427  829   14] [-0.02778103 -0.02503357 -0.0222861  -0.01953863 -0.01679116 -0.0140437
 -0.01129623 -0.00854876 -0.00580129 -0.00305383 -0.00030636]
[ 742 1070 1608 1432 1464 1473 1543 1639 1662 1542] [-0.02778103 -0.02503357 -0.0222861  -0.01953863 -0.01679116 -0.0140437
 -0.01129623 -0.00854876 -0.00580129 -0.00305383 -0.00030636]
-4.22064
3.96878
fine tuning ...
Epoch 0
Fine tuning took 0.062360 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061652 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063027 minutes
{0: [0.13177339901477833, 0.12561576354679804, 0.17733990147783252, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.73768472906403937, 0.71798029556650245, 0.68965517241379315], 5: [0.10714285714285714, 0.13669950738916256, 0.10467980295566502, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379865 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.06012
Epoch 1, cost is  3.90243
Epoch 2, cost is  3.86517
Epoch 3, cost is  3.85897
Epoch 4, cost is  3.87345
Training took 0.152691 minutes
Weight histogram
[1971 1958 1405  728 1900  131 2003 1442  604    8] [-0.02636126 -0.02375577 -0.02115028 -0.01854479 -0.0159393  -0.01333381
 -0.01072832 -0.00812283 -0.00551734 -0.00291185 -0.00030636]
[ 441  683 1005 1267 1338 1334 1466 1548 1566 1502] [-0.02636126 -0.02375577 -0.02115028 -0.01854479 -0.0159393  -0.01333381
 -0.01072832 -0.00812283 -0.00551734 -0.00291185 -0.00030636]
-5.66571
5.08959
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.32159
Epoch 1, cost is  2.21532
Epoch 2, cost is  2.17864
Epoch 3, cost is  2.16511
Epoch 4, cost is  2.14447
Training took 0.152816 minutes
Weight histogram
[2811 2956 2293 1256 1054  935 1729 1083   49    9] [-0.01856747 -0.01674136 -0.01491525 -0.01308914 -0.01126303 -0.00943692
 -0.00761081 -0.00578469 -0.00395858 -0.00213247 -0.00030636]
[ 565  720 1135 1778 1962 1645 1602 1598 1599 1571] [-0.01856747 -0.01674136 -0.01491525 -0.01308914 -0.01126303 -0.00943692
 -0.00761081 -0.00578469 -0.00395858 -0.00213247 -0.00030636]
-2.69322
3.12871
fine tuning ...
Epoch 0
Fine tuning took 0.067205 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068607 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068219 minutes
{0: [0.1539408866995074, 0.16625615763546797, 0.16009852216748768, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.66625615763546797, 0.67487684729064035, 0.63300492610837433], 5: [0.1268472906403941, 0.16748768472906403, 0.16502463054187191, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380799 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.8919
Epoch 1, cost is  2.71412
Epoch 2, cost is  2.6494
Epoch 3, cost is  2.61624
Epoch 4, cost is  2.59311
Training took 0.206559 minutes
Weight histogram
[1970 1468  605 2001 1294  767 2003 1201  836    5] [-0.01827629 -0.01647712 -0.01467795 -0.01287877 -0.0110796  -0.00928042
 -0.00748125 -0.00568207 -0.0038829  -0.00208372 -0.00028455]
[ 439  663  898 1202 1330 1434 1501 1536 1562 1585] [-0.01827629 -0.01647712 -0.01467795 -0.01287877 -0.0110796  -0.00928042
 -0.00748125 -0.00568207 -0.0038829  -0.00208372 -0.00028455]
-3.66606
3.42404
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.28212
Epoch 1, cost is  5.15174
Epoch 2, cost is  5.14963
Epoch 3, cost is  5.17753
Epoch 4, cost is  5.2305
Training took 0.107938 minutes
Weight histogram
[4020 3011 1141 1621 1125  694  305  116  251 1891] [-0.04538188 -0.04087215 -0.03636241 -0.03185268 -0.02734295 -0.02283321
 -0.01832348 -0.01381375 -0.00930401 -0.00479428 -0.00028455]
[1668 1271  826 1129 1261 1549 1492 1637 1747 1595] [-0.04538188 -0.04087215 -0.03636241 -0.03185268 -0.02734295 -0.02283321
 -0.01832348 -0.01381375 -0.00930401 -0.00479428 -0.00028455]
-7.59247
6.23858
fine tuning ...
Epoch 0
Fine tuning took 0.068977 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068049 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069248 minutes
{0: [0.10221674876847291, 0.13300492610837439, 0.13793103448275862, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.73275862068965514, 0.68965517241379315, 0.67487684729064035], 5: [0.15517241379310345, 0.13423645320197045, 0.17241379310344829, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380537 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.8919
Epoch 1, cost is  2.71412
Epoch 2, cost is  2.6494
Epoch 3, cost is  2.61624
Epoch 4, cost is  2.59311
Training took 0.208383 minutes
Weight histogram
[1970 1468  605 2001 1294  767 2003 1201  836    5] [-0.01827629 -0.01647712 -0.01467795 -0.01287877 -0.0110796  -0.00928042
 -0.00748125 -0.00568207 -0.0038829  -0.00208372 -0.00028455]
[ 439  663  898 1202 1330 1434 1501 1536 1562 1585] [-0.01827629 -0.01647712 -0.01467795 -0.01287877 -0.0110796  -0.00928042
 -0.00748125 -0.00568207 -0.0038829  -0.00208372 -0.00028455]
-3.66606
3.42404
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.61325
Epoch 1, cost is  3.50328
Epoch 2, cost is  3.47733
Epoch 3, cost is  3.48125
Epoch 4, cost is  3.49646
Training took 0.151835 minutes
Weight histogram
[1985 2810 2289 1421 1621 1131  585  227 1712  394] [-0.03072844 -0.02768405 -0.02463967 -0.02159528 -0.01855089 -0.0155065
 -0.01246211 -0.00941772 -0.00637333 -0.00332894 -0.00028455]
[1066 1781 1021 1302 1401 1424 1521 1581 1569 1509] [-0.03072844 -0.02768405 -0.02463967 -0.02159528 -0.01855089 -0.0155065
 -0.01246211 -0.00941772 -0.00637333 -0.00332894 -0.00028455]
-5.50182
5.28071
fine tuning ...
Epoch 0
Fine tuning took 0.073410 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073602 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073811 minutes
{0: [0.18472906403940886, 0.20689655172413793, 0.21921182266009853, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.59852216748768472, 0.57389162561576357, 0.6145320197044335], 5: [0.18596059113300492, 0.19458128078817735, 0.20689655172413793, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379282 minutes
Weight histogram
[  45  371 1660 3030 2227 1654 1369 1087  604  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 3602] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.00309
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.8919
Epoch 1, cost is  2.71412
Epoch 2, cost is  2.6494
Epoch 3, cost is  2.61624
Epoch 4, cost is  2.59311
Training took 0.208632 minutes
Weight histogram
[1970 1468  605 2001 1294  767 2003 1201  836    5] [-0.01827629 -0.01647712 -0.01467795 -0.01287877 -0.0110796  -0.00928042
 -0.00748125 -0.00568207 -0.0038829  -0.00208372 -0.00028455]
[ 439  663  898 1202 1330 1434 1501 1536 1562 1585] [-0.01827629 -0.01647712 -0.01467795 -0.01287877 -0.0110796  -0.00928042
 -0.00748125 -0.00568207 -0.0038829  -0.00208372 -0.00028455]
-3.66606
3.42404
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.50233
Epoch 1, cost is  2.38097
Epoch 2, cost is  2.33779
Epoch 3, cost is  2.32363
Epoch 4, cost is  2.31029
Training took 0.206665 minutes
Weight histogram
[2731 2261 2544 1317 1272 1099  617 1368  958    8] [-0.01915636 -0.01726917 -0.01538199 -0.01349481 -0.01160763 -0.00972045
 -0.00783327 -0.00594609 -0.00405891 -0.00217173 -0.00028455]
[ 751 1139 1690 1466 1382 1538 1574 1563 1552 1520] [-0.01915636 -0.01726917 -0.01538199 -0.01349481 -0.01160763 -0.00972045
 -0.00783327 -0.00594609 -0.00405891 -0.00217173 -0.00028455]
-3.60468
3.18011
fine tuning ...
Epoch 0
Fine tuning took 0.077895 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078953 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078347 minutes
{0: [0.26354679802955666, 0.2229064039408867, 0.23152709359605911, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51108374384236455, 0.59113300492610843, 0.56527093596059108, 0.55911330049261088], 5: [0.22536945812807882, 0.18596059113300492, 0.20320197044334976, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381079 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.79897
Epoch 1, cost is  5.63223
Epoch 2, cost is  5.59688
Epoch 3, cost is  5.60913
Epoch 4, cost is  5.63742
Training took 0.108350 minutes
Weight histogram
[3949 1196 1278 1626 1786  283 2017 1578  433   29] [-0.04090891 -0.0368396  -0.03277028 -0.02870097 -0.02463165 -0.02056234
 -0.01649302 -0.01242371 -0.00835439 -0.00428508 -0.00021576]
[ 461  679 1075 1397 1580 1649 1749 1958 1791 1836] [-0.04090891 -0.0368396  -0.03277028 -0.02870097 -0.02463165 -0.02056234
 -0.01649302 -0.01242371 -0.00835439 -0.00428508 -0.00021576]
-7.68258
6.81445
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.5858
Epoch 1, cost is  4.49202
Epoch 2, cost is  4.47334
Epoch 3, cost is  4.46969
Epoch 4, cost is  4.51562
Training took 0.071937 minutes
Weight histogram
[2139 5389 3803 1376  640  307  150 1576  779   41] [-0.04608074 -0.04149134 -0.03690195 -0.03231255 -0.02772315 -0.02313375
 -0.01854435 -0.01395496 -0.00936556 -0.00477616 -0.00018676]
[ 816 1123 1648 1213 1409 1749 1765 2390 2050 2037] [-0.04608074 -0.04149134 -0.03690195 -0.03231255 -0.02772315 -0.02313375
 -0.01854435 -0.01395496 -0.00936556 -0.00477616 -0.00018676]
-5.96588
6.7968
fine tuning ...
Epoch 0
Fine tuning took 0.050951 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052143 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051326 minutes
{0: [0.092364532019704432, 0.075123152709359611, 0.10960591133004927, 0.088669950738916259], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81650246305418717, 0.84729064039408863, 0.81896551724137934, 0.8423645320197044], 5: [0.091133004926108374, 0.077586206896551727, 0.071428571428571425, 0.068965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381833 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.79897
Epoch 1, cost is  5.63223
Epoch 2, cost is  5.59688
Epoch 3, cost is  5.60913
Epoch 4, cost is  5.63742
Training took 0.107918 minutes
Weight histogram
[3949 1196 1278 1626 1786  283 2017 1578  433   29] [-0.04090891 -0.0368396  -0.03277028 -0.02870097 -0.02463165 -0.02056234
 -0.01649302 -0.01242371 -0.00835439 -0.00428508 -0.00021576]
[ 461  679 1075 1397 1580 1649 1749 1958 1791 1836] [-0.04090891 -0.0368396  -0.03277028 -0.02870097 -0.02463165 -0.02056234
 -0.01649302 -0.01242371 -0.00835439 -0.00428508 -0.00021576]
-7.68258
6.81445
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.93709
Epoch 1, cost is  2.82653
Epoch 2, cost is  2.81916
Epoch 3, cost is  2.80936
Epoch 4, cost is  2.81304
Training took 0.086708 minutes
Weight histogram
[5163 2092 2877 1752 1022 1356 1336  487  101   14] [-0.02630998 -0.02369045 -0.02107092 -0.01845139 -0.01583186 -0.01321232
 -0.01059279 -0.00797326 -0.00535373 -0.0027342  -0.00011466]
[ 592  678 1008 1648 2228 2021 1868 2186 1903 2068] [-0.02630998 -0.02369045 -0.02107092 -0.01845139 -0.01583186 -0.01321232
 -0.01059279 -0.00797326 -0.00535373 -0.0027342  -0.00011466]
-3.85542
3.8038
fine tuning ...
Epoch 0
Fine tuning took 0.054010 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053235 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053965 minutes
{0: [0.099753694581280791, 0.076354679802955669, 0.093596059113300489, 0.080049261083743842], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81527093596059108, 0.8288177339901478, 0.81650246305418717, 0.81896551724137934], 5: [0.084975369458128072, 0.094827586206896547, 0.089901477832512317, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380438 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.79897
Epoch 1, cost is  5.63223
Epoch 2, cost is  5.59688
Epoch 3, cost is  5.60913
Epoch 4, cost is  5.63742
Training took 0.107608 minutes
Weight histogram
[3949 1196 1278 1626 1786  283 2017 1578  433   29] [-0.04090891 -0.0368396  -0.03277028 -0.02870097 -0.02463165 -0.02056234
 -0.01649302 -0.01242371 -0.00835439 -0.00428508 -0.00021576]
[ 461  679 1075 1397 1580 1649 1749 1958 1791 1836] [-0.04090891 -0.0368396  -0.03277028 -0.02870097 -0.02463165 -0.02056234
 -0.01649302 -0.01242371 -0.00835439 -0.00428508 -0.00021576]
-7.68258
6.81445
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.12259
Epoch 1, cost is  2.02649
Epoch 2, cost is  2.0047
Epoch 3, cost is  1.97598
Epoch 4, cost is  1.95581
Training took 0.109361 minutes
Weight histogram
[2681 3283 2913 1057 1848 2167 1463  480  293   15] [-0.02090257 -0.01883389 -0.01676521 -0.01469653 -0.01262784 -0.01055916
 -0.00849048 -0.0064218  -0.00435312 -0.00228444 -0.00021576]
[ 476  476  713 1017 1919 2374 2247 2458 2273 2247] [-0.02090257 -0.01883389 -0.01676521 -0.01469653 -0.01262784 -0.01055916
 -0.00849048 -0.0064218  -0.00435312 -0.00228444 -0.00021576]
-2.31678
2.02581
fine tuning ...
Epoch 0
Fine tuning took 0.057357 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057392 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057122 minutes
{0: [0.11576354679802955, 0.10344827586206896, 0.12192118226600986, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77339901477832518, 0.79926108374384242, 0.78201970443349755, 0.80665024630541871], 5: [0.11083743842364532, 0.097290640394088676, 0.096059113300492605, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380231 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.24139
Epoch 1, cost is  4.05905
Epoch 2, cost is  4.00727
Epoch 3, cost is  3.99065
Epoch 4, cost is  3.99704
Training took 0.153096 minutes
Weight histogram
[1963 2032 2014 1899  266 1927 1661 1411  993    9] [-0.0298155  -0.02686459 -0.02391367 -0.02096276 -0.01801184 -0.01506093
 -0.01211002 -0.0091591  -0.00620819 -0.00325727 -0.00030636]
[ 508  841 1284 1422 1542 1619 1715 1768 1721 1755] [-0.0298155  -0.02686459 -0.02391367 -0.02096276 -0.01801184 -0.01506093
 -0.01211002 -0.0091591  -0.00620819 -0.00325727 -0.00030636]
-6.49586
5.84219
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.28285
Epoch 1, cost is  5.14306
Epoch 2, cost is  5.13877
Epoch 3, cost is  5.15202
Epoch 4, cost is  5.18372
Training took 0.084662 minutes
Weight histogram
[2332 6133 2071 1472 1113  414  337  239 1666  423] [-0.04726029 -0.0425649  -0.03786951 -0.03317411 -0.02847872 -0.02378333
 -0.01908793 -0.01439254 -0.00969715 -0.00500175 -0.00030636]
[1251 1741  909 1366 1602 1762 1840 2039 1854 1836] [-0.04726029 -0.0425649  -0.03786951 -0.03317411 -0.02847872 -0.02378333
 -0.01908793 -0.01439254 -0.00969715 -0.00500175 -0.00030636]
-7.78691
7.34472
fine tuning ...
Epoch 0
Fine tuning took 0.060990 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.058731 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061220 minutes
{0: [0.15147783251231528, 0.14532019704433496, 0.12315270935960591, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.69458128078817738, 0.69950738916256161, 0.73275862068965514], 5: [0.19088669950738915, 0.16009852216748768, 0.17733990147783252, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380514 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.24139
Epoch 1, cost is  4.05905
Epoch 2, cost is  4.00727
Epoch 3, cost is  3.99065
Epoch 4, cost is  3.99704
Training took 0.152368 minutes
Weight histogram
[1963 2032 2014 1899  266 1927 1661 1411  993    9] [-0.0298155  -0.02686459 -0.02391367 -0.02096276 -0.01801184 -0.01506093
 -0.01211002 -0.0091591  -0.00620819 -0.00325727 -0.00030636]
[ 508  841 1284 1422 1542 1619 1715 1768 1721 1755] [-0.0298155  -0.02686459 -0.02391367 -0.02096276 -0.01801184 -0.01506093
 -0.01211002 -0.0091591  -0.00620819 -0.00325727 -0.00030636]
-6.49586
5.84219
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.45049
Epoch 1, cost is  3.32135
Epoch 2, cost is  3.28462
Epoch 3, cost is  3.28033
Epoch 4, cost is  3.27849
Training took 0.111575 minutes
Weight histogram
[2889 4052 2719 1389 1494  911  441 1344  946   15] [-0.02898567 -0.02611774 -0.02324981 -0.02038188 -0.01751394 -0.01464601
 -0.01177808 -0.00891015 -0.00604222 -0.00317429 -0.00030636]
[ 846 1326 1779 1612 1684 1720 1805 1878 1757 1793] [-0.02898567 -0.02611774 -0.02324981 -0.02038188 -0.01751394 -0.01464601
 -0.01177808 -0.00891015 -0.00604222 -0.00317429 -0.00030636]
-4.85402
4.40923
fine tuning ...
Epoch 0
Fine tuning took 0.063294 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062584 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063100 minutes
{0: [0.20689655172413793, 0.21551724137931033, 0.19088669950738915, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6354679802955665, 0.60344827586206895, 0.61206896551724133, 0.62438423645320196], 5: [0.15763546798029557, 0.18103448275862069, 0.19704433497536947, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380134 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.24139
Epoch 1, cost is  4.05905
Epoch 2, cost is  4.00727
Epoch 3, cost is  3.99065
Epoch 4, cost is  3.99704
Training took 0.152632 minutes
Weight histogram
[1963 2032 2014 1899  266 1927 1661 1411  993    9] [-0.0298155  -0.02686459 -0.02391367 -0.02096276 -0.01801184 -0.01506093
 -0.01211002 -0.0091591  -0.00620819 -0.00325727 -0.00030636]
[ 508  841 1284 1422 1542 1619 1715 1768 1721 1755] [-0.0298155  -0.02686459 -0.02391367 -0.02096276 -0.01801184 -0.01506093
 -0.01211002 -0.0091591  -0.00620819 -0.00325727 -0.00030636]
-6.49586
5.84219
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.20997
Epoch 1, cost is  2.10507
Epoch 2, cost is  2.06526
Epoch 3, cost is  2.03858
Epoch 4, cost is  2.02363
Training took 0.151550 minutes
Weight histogram
[3168 3736 2739 1189 1307 1015 1713 1230   94    9] [-0.01946555 -0.01754963 -0.01563372 -0.0137178  -0.01180188 -0.00988596
 -0.00797004 -0.00605412 -0.0041382  -0.00222228 -0.00030636]
[ 628  863 1407 2140 2048 1780 1757 1781 1743 2053] [-0.01946555 -0.01754963 -0.01563372 -0.0137178  -0.01180188 -0.00988596
 -0.00797004 -0.00605412 -0.0041382  -0.00222228 -0.00030636]
-2.94392
3.43397
fine tuning ...
Epoch 0
Fine tuning took 0.068390 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068534 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068394 minutes
{0: [0.21674876847290642, 0.24630541871921183, 0.17980295566502463, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.51970443349753692, 0.59975369458128081, 0.58743842364532017], 5: [0.20566502463054187, 0.23399014778325122, 0.22044334975369459, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382161 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.91951
Epoch 1, cost is  2.75234
Epoch 2, cost is  2.67959
Epoch 3, cost is  2.64068
Epoch 4, cost is  2.61266
Training took 0.207476 minutes
Weight histogram
[2261 1769 1988 1033 1101 1959 1920 1081 1058    5] [-0.01981923 -0.01786576 -0.01591229 -0.01395883 -0.01200536 -0.01005189
 -0.00809842 -0.00614495 -0.00419148 -0.00223802 -0.00028455]
[ 502  787 1144 1308 1615 1649 1684 1735 1780 1971] [-0.01981923 -0.01786576 -0.01591229 -0.01395883 -0.01200536 -0.01005189
 -0.00809842 -0.00614495 -0.00419148 -0.00223802 -0.00028455]
-4.09595
3.91496
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.69633
Epoch 1, cost is  5.56904
Epoch 2, cost is  5.55915
Epoch 3, cost is  5.59167
Epoch 4, cost is  5.63351
Training took 0.109130 minutes
Weight histogram
[3449 4147 2440 1170 1426  872  387  156  108 2045] [-0.04803665 -0.04326144 -0.03848623 -0.03371102 -0.02893581 -0.0241606
 -0.01938539 -0.01461018 -0.00983497 -0.00505976 -0.00028455]
[1958 1165 1037 1321 1628 1700 1757 1978 1812 1844] [-0.04803665 -0.04326144 -0.03848623 -0.03371102 -0.02893581 -0.0241606
 -0.01938539 -0.01461018 -0.00983497 -0.00505976 -0.00028455]
-8.10318
7.00927
fine tuning ...
Epoch 0
Fine tuning took 0.069200 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067966 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068581 minutes
{0: [0.14778325123152711, 0.14655172413793102, 0.16379310344827586, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71921182266009853, 0.72660098522167482, 0.67980295566502458, 0.6428571428571429], 5: [0.13300492610837439, 0.1268472906403941, 0.15640394088669951, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382224 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.91951
Epoch 1, cost is  2.75234
Epoch 2, cost is  2.67959
Epoch 3, cost is  2.64068
Epoch 4, cost is  2.61266
Training took 0.205029 minutes
Weight histogram
[2261 1769 1988 1033 1101 1959 1920 1081 1058    5] [-0.01981923 -0.01786576 -0.01591229 -0.01395883 -0.01200536 -0.01005189
 -0.00809842 -0.00614495 -0.00419148 -0.00223802 -0.00028455]
[ 502  787 1144 1308 1615 1649 1684 1735 1780 1971] [-0.01981923 -0.01786576 -0.01591229 -0.01395883 -0.01200536 -0.01005189
 -0.00809842 -0.00614495 -0.00419148 -0.00223802 -0.00028455]
-4.09595
3.91496
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.83433
Epoch 1, cost is  3.71261
Epoch 2, cost is  3.68151
Epoch 3, cost is  3.68632
Epoch 4, cost is  3.70418
Training took 0.151308 minutes
Weight histogram
[2636 2508 2992 1891 1832 1119  796  287 1553  586] [-0.03275044 -0.02950385 -0.02625726 -0.02301067 -0.01976408 -0.01651749
 -0.0132709  -0.01002431 -0.00677773 -0.00353114 -0.00028455]
[1255 1916 1218 1494 1627 1697 1772 1781 1724 1716] [-0.03275044 -0.02950385 -0.02625726 -0.02301067 -0.01976408 -0.01651749
 -0.0132709  -0.01002431 -0.00677773 -0.00353114 -0.00028455]
-6.1348
5.64601
fine tuning ...
Epoch 0
Fine tuning took 0.073594 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074125 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073686 minutes
{0: [0.21921182266009853, 0.24261083743842365, 0.25, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.57266009852216748, 0.54433497536945807, 0.5431034482758621], 5: [0.20073891625615764, 0.18472906403940886, 0.20566502463054187, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380417 minutes
Weight histogram
[  45  371 1660 3030 2260 2382 2411 1301  612  103] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 152  245  358  579  752  435  743 2166 3118 5627] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.06448
1.16684
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.91951
Epoch 1, cost is  2.75234
Epoch 2, cost is  2.67959
Epoch 3, cost is  2.64068
Epoch 4, cost is  2.61266
Training took 0.207119 minutes
Weight histogram
[2261 1769 1988 1033 1101 1959 1920 1081 1058    5] [-0.01981923 -0.01786576 -0.01591229 -0.01395883 -0.01200536 -0.01005189
 -0.00809842 -0.00614495 -0.00419148 -0.00223802 -0.00028455]
[ 502  787 1144 1308 1615 1649 1684 1735 1780 1971] [-0.01981923 -0.01786576 -0.01591229 -0.01395883 -0.01200536 -0.01005189
 -0.00809842 -0.00614495 -0.00419148 -0.00223802 -0.00028455]
-4.09595
3.91496
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.48524
Epoch 1, cost is  2.36759
Epoch 2, cost is  2.33155
Epoch 3, cost is  2.30691
Epoch 4, cost is  2.30082
Training took 0.207498 minutes
Weight histogram
[2932 2762 2895 1677 1711 1053  735 1255 1170   10] [-0.02027044 -0.01827185 -0.01627326 -0.01427467 -0.01227608 -0.01027749
 -0.0082789  -0.00628031 -0.00428172 -0.00228314 -0.00028455]
[ 859 1392 2002 1397 1708 1754 1741 1740 1729 1878] [-0.02027044 -0.01827185 -0.01627326 -0.01427467 -0.01227608 -0.01027749
 -0.0082789  -0.00628031 -0.00428172 -0.00228314 -0.00028455]
-3.91341
3.61481
fine tuning ...
Epoch 0
Fine tuning took 0.078499 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079395 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078892 minutes
{0: [0.24630541871921183, 0.25985221674876846, 0.20689655172413793, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52216748768472909, 0.52586206896551724, 0.55418719211822665, 0.5357142857142857], 5: [0.23152709359605911, 0.21428571428571427, 0.23891625615763548, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380481 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.08784
Epoch 1, cost is  5.88122
Epoch 2, cost is  5.83395
Epoch 3, cost is  5.82589
Epoch 4, cost is  5.84812
Training took 0.106272 minutes
Weight histogram
[1835  735 3452 2059 1989 2003 1556 1479 1059   33] [-0.04972044 -0.04476997 -0.0398195  -0.03486904 -0.02991857 -0.0249681
 -0.02001763 -0.01506717 -0.0101167  -0.00516623 -0.00021576]
[ 520  825 1344 1633 1776 1829 2217 2000 2080 1976] [-0.04972044 -0.04476997 -0.0398195  -0.03486904 -0.02991857 -0.0249681
 -0.02001763 -0.01506717 -0.0101167  -0.00516623 -0.00021576]
-9.09455
7.88069
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.84247
Epoch 1, cost is  4.72682
Epoch 2, cost is  4.703
Epoch 3, cost is  4.71937
Epoch 4, cost is  4.75225
Training took 0.071893 minutes
Weight histogram
[3155 6398 3803 1376  640  307  150 1576  779   41] [-0.04608074 -0.04149134 -0.03690195 -0.03231255 -0.02772315 -0.02313375
 -0.01854435 -0.01395496 -0.00936556 -0.00477616 -0.00018676]
[ 912 1354 1684 1458 1818 1938 2488 2338 2263 1972] [-0.04608074 -0.04149134 -0.03690195 -0.03231255 -0.02772315 -0.02313375
 -0.01854435 -0.01395496 -0.00936556 -0.00477616 -0.00018676]
-6.95972
7.66166
fine tuning ...
Epoch 0
Fine tuning took 0.050791 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051944 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051496 minutes
{0: [0.056650246305418719, 0.071428571428571425, 0.080049261083743842, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87315270935960587, 0.84359605911330049, 0.83004926108374388, 0.82019704433497542], 5: [0.070197044334975367, 0.084975369458128072, 0.089901477832512317, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380066 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.08784
Epoch 1, cost is  5.88122
Epoch 2, cost is  5.83395
Epoch 3, cost is  5.82589
Epoch 4, cost is  5.84812
Training took 0.106725 minutes
Weight histogram
[1835  735 3452 2059 1989 2003 1556 1479 1059   33] [-0.04972044 -0.04476997 -0.0398195  -0.03486904 -0.02991857 -0.0249681
 -0.02001763 -0.01506717 -0.0101167  -0.00516623 -0.00021576]
[ 520  825 1344 1633 1776 1829 2217 2000 2080 1976] [-0.04972044 -0.04476997 -0.0398195  -0.03486904 -0.02991857 -0.0249681
 -0.02001763 -0.01506717 -0.0101167  -0.00516623 -0.00021576]
-9.09455
7.88069
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.04313
Epoch 1, cost is  2.95043
Epoch 2, cost is  2.90717
Epoch 3, cost is  2.88027
Epoch 4, cost is  2.891
Training took 0.086438 minutes
Weight histogram
[3700 4330 3234 2190 1290  978 1665  646  176   16] [-0.02768045 -0.02492387 -0.02216729 -0.01941071 -0.01665413 -0.01389756
 -0.01114098 -0.0083844  -0.00562782 -0.00287124 -0.00011466]
[ 654  782 1240 2097 2480 2059 2294 2161 2248 2210] [-0.02768045 -0.02492387 -0.02216729 -0.01941071 -0.01665413 -0.01389756
 -0.01114098 -0.0083844  -0.00562782 -0.00287124 -0.00011466]
-4.11982
4.06594
fine tuning ...
Epoch 0
Fine tuning took 0.052931 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054012 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052978 minutes
{0: [0.092364532019704432, 0.10960591133004927, 0.088669950738916259, 0.12807881773399016], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81157635467980294, 0.7931034482758621, 0.79679802955665024, 0.75], 5: [0.096059113300492605, 0.097290640394088676, 0.1145320197044335, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379799 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.08784
Epoch 1, cost is  5.88122
Epoch 2, cost is  5.83395
Epoch 3, cost is  5.82589
Epoch 4, cost is  5.84812
Training took 0.108271 minutes
Weight histogram
[1835  735 3452 2059 1989 2003 1556 1479 1059   33] [-0.04972044 -0.04476997 -0.0398195  -0.03486904 -0.02991857 -0.0249681
 -0.02001763 -0.01506717 -0.0101167  -0.00516623 -0.00021576]
[ 520  825 1344 1633 1776 1829 2217 2000 2080 1976] [-0.04972044 -0.04476997 -0.0398195  -0.03486904 -0.02991857 -0.0249681
 -0.02001763 -0.01506717 -0.0101167  -0.00516623 -0.00021576]
-9.09455
7.88069
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.16543
Epoch 1, cost is  2.09856
Epoch 2, cost is  2.05481
Epoch 3, cost is  2.05438
Epoch 4, cost is  2.0245
Training took 0.107649 minutes
Weight histogram
[2824 3881 2659 2326 1263 2489 1900  546  318   19] [-0.02216093 -0.01996641 -0.0177719  -0.01557738 -0.01338286 -0.01118835
 -0.00899383 -0.00679931 -0.0046048  -0.00241028 -0.00021576]
[ 521  566  877 1630 2401 2479 2644 2593 2469 2045] [-0.02216093 -0.01996641 -0.0177719  -0.01557738 -0.01338286 -0.01118835
 -0.00899383 -0.00679931 -0.0046048  -0.00241028 -0.00021576]
-2.64177
2.28465
fine tuning ...
Epoch 0
Fine tuning took 0.056157 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057970 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056835 minutes
{0: [0.096059113300492605, 0.11576354679802955, 0.1206896551724138, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79187192118226601, 0.75862068965517238, 0.76724137931034486, 0.73645320197044339], 5: [0.11206896551724138, 0.12561576354679804, 0.11206896551724138, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378985 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.50484
Epoch 1, cost is  4.30029
Epoch 2, cost is  4.24553
Epoch 3, cost is  4.21825
Epoch 4, cost is  4.22266
Training took 0.151667 minutes
Weight histogram
[1951 1875 2152 2073 2018 1330  789 2101 1900   11] [-0.0342838  -0.03088606 -0.02748831 -0.02409057 -0.02069282 -0.01729508
 -0.01389734 -0.01049959 -0.00710185 -0.0037041  -0.00030636]
[ 586 1025 1536 1674 1757 1857 2001 1928 1970 1866] [-0.0342838  -0.03088606 -0.02748831 -0.02409057 -0.02069282 -0.01729508
 -0.01389734 -0.01049959 -0.00710185 -0.0037041  -0.00030636]
-7.06161
6.3196
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.61065
Epoch 1, cost is  5.46777
Epoch 2, cost is  5.45001
Epoch 3, cost is  5.47324
Epoch 4, cost is  5.51698
Training took 0.085808 minutes
Weight histogram
[1691 3553 5934 1985 1570  739  369  271 1508  605] [-0.05208332 -0.04690562 -0.04172793 -0.03655023 -0.03137254 -0.02619484
 -0.02101714 -0.01583945 -0.01066175 -0.00548406 -0.00030636]
[1453 1721 1188 1568 1903 1984 2272 2074 2077 1985] [-0.05208332 -0.04690562 -0.04172793 -0.03655023 -0.03137254 -0.02619484
 -0.02101714 -0.01583945 -0.01066175 -0.00548406 -0.00030636]
-8.54028
8.2387
fine tuning ...
Epoch 0
Fine tuning took 0.059667 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061088 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060403 minutes
{0: [0.15024630541871922, 0.1539408866995074, 0.14408866995073891, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69581280788177335, 0.69088669950738912, 0.67733990147783252, 0.72413793103448276], 5: [0.1539408866995074, 0.15517241379310345, 0.17857142857142858, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379853 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.50484
Epoch 1, cost is  4.30029
Epoch 2, cost is  4.24553
Epoch 3, cost is  4.21825
Epoch 4, cost is  4.22266
Training took 0.151773 minutes
Weight histogram
[1951 1875 2152 2073 2018 1330  789 2101 1900   11] [-0.0342838  -0.03088606 -0.02748831 -0.02409057 -0.02069282 -0.01729508
 -0.01389734 -0.01049959 -0.00710185 -0.0037041  -0.00030636]
[ 586 1025 1536 1674 1757 1857 2001 1928 1970 1866] [-0.0342838  -0.03088606 -0.02748831 -0.02409057 -0.02069282 -0.01729508
 -0.01389734 -0.01049959 -0.00710185 -0.0037041  -0.00030636]
-7.06161
6.3196
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.64256
Epoch 1, cost is  3.50611
Epoch 2, cost is  3.46252
Epoch 3, cost is  3.46497
Epoch 4, cost is  3.47211
Training took 0.111108 minutes
Weight histogram
[2391 3889 4128 1777 1826 1178  688 1024 1307   17] [-0.03122503 -0.02813316 -0.02504129 -0.02194943 -0.01885756 -0.01576569
 -0.01267383 -0.00958196 -0.00649009 -0.00339823 -0.00030636]
[ 969 1619 1958 1844 1882 1940 2166 1969 2007 1871] [-0.03122503 -0.02813316 -0.02504129 -0.02194943 -0.01885756 -0.01576569
 -0.01267383 -0.00958196 -0.00649009 -0.00339823 -0.00030636]
-5.63041
4.83283
fine tuning ...
Epoch 0
Fine tuning took 0.062759 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063331 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062503 minutes
{0: [0.18349753694581281, 0.18472906403940886, 0.21551724137931033, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65394088669950734, 0.66502463054187189, 0.61330049261083741, 0.61083743842364535], 5: [0.1625615763546798, 0.15024630541871922, 0.17118226600985223, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380666 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.50484
Epoch 1, cost is  4.30029
Epoch 2, cost is  4.24553
Epoch 3, cost is  4.21825
Epoch 4, cost is  4.22266
Training took 0.152334 minutes
Weight histogram
[1951 1875 2152 2073 2018 1330  789 2101 1900   11] [-0.0342838  -0.03088606 -0.02748831 -0.02409057 -0.02069282 -0.01729508
 -0.01389734 -0.01049959 -0.00710185 -0.0037041  -0.00030636]
[ 586 1025 1536 1674 1757 1857 2001 1928 1970 1866] [-0.0342838  -0.03088606 -0.02748831 -0.02409057 -0.02069282 -0.01729508
 -0.01389734 -0.01049959 -0.00710185 -0.0037041  -0.00030636]
-7.06161
6.3196
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.34999
Epoch 1, cost is  2.24786
Epoch 2, cost is  2.21835
Epoch 3, cost is  2.20327
Epoch 4, cost is  2.19108
Training took 0.150595 minutes
Weight histogram
[4687 3956 2741 1264 1475  990 1701 1285  117    9] [-0.01980481 -0.01785496 -0.01590512 -0.01395527 -0.01200543 -0.01005558
 -0.00810574 -0.00615589 -0.00420605 -0.0022562  -0.00030636]
[ 705 1065 1888 2425 2056 1965 2001 1967 2256 1897] [-0.01980481 -0.01785496 -0.01590512 -0.01395527 -0.01200543 -0.01005558
 -0.00810574 -0.00615589 -0.00420605 -0.0022562  -0.00030636]
-3.18043
3.87071
fine tuning ...
Epoch 0
Fine tuning took 0.067712 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067284 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068126 minutes
{0: [0.24876847290640394, 0.21798029556650247, 0.21674876847290642, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57266009852216748, 0.55418719211822665, 0.60467980295566504, 0.52955665024630538], 5: [0.17857142857142858, 0.22783251231527094, 0.17857142857142858, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380018 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.99119
Epoch 1, cost is  2.82293
Epoch 2, cost is  2.74905
Epoch 3, cost is  2.71587
Epoch 4, cost is  2.69699
Training took 0.205938 minutes
Weight histogram
[1914 2269 1888 2000 1992 1720  902 1766 1742    7] [-0.02239203 -0.02018129 -0.01797054 -0.01575979 -0.01354904 -0.01133829
 -0.00912754 -0.00691679 -0.00470604 -0.0024953  -0.00028455]
[ 577  935 1372 1606 1816 1839 1936 1983 2187 1949] [-0.02239203 -0.02018129 -0.01797054 -0.01575979 -0.01354904 -0.01133829
 -0.00912754 -0.00691679 -0.00470604 -0.0024953  -0.00028455]
-4.8651
4.33083
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.26577
Epoch 1, cost is  6.13057
Epoch 2, cost is  6.12648
Epoch 3, cost is  6.16388
Epoch 4, cost is  6.21485
Training took 0.110005 minutes
Weight histogram
[3228 5373 2928 1118 1677 1107  464  165  111 2054] [-0.05057943 -0.04554994 -0.04052045 -0.03549096 -0.03046148 -0.02543199
 -0.0204025  -0.01537301 -0.01034352 -0.00531404 -0.00028455]
[2302 1030 1296 1579 1936 1845 2250 2046 2071 1870] [-0.05057943 -0.04554994 -0.04052045 -0.03549096 -0.03046148 -0.02543199
 -0.0204025  -0.01537301 -0.01034352 -0.00531404 -0.00028455]
-9.09032
8.08231
fine tuning ...
Epoch 0
Fine tuning took 0.067166 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068950 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067220 minutes
{0: [0.21674876847290642, 0.21798029556650247, 0.19211822660098521, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64655172413793105, 0.6280788177339901, 0.66256157635467983, 0.66133004926108374], 5: [0.13669950738916256, 0.1539408866995074, 0.14532019704433496, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381104 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.99119
Epoch 1, cost is  2.82293
Epoch 2, cost is  2.74905
Epoch 3, cost is  2.71587
Epoch 4, cost is  2.69699
Training took 0.204932 minutes
Weight histogram
[1914 2269 1888 2000 1992 1720  902 1766 1742    7] [-0.02239203 -0.02018129 -0.01797054 -0.01575979 -0.01354904 -0.01133829
 -0.00912754 -0.00691679 -0.00470604 -0.0024953  -0.00028455]
[ 577  935 1372 1606 1816 1839 1936 1983 2187 1949] [-0.02239203 -0.02018129 -0.01797054 -0.01575979 -0.01354904 -0.01133829
 -0.00912754 -0.00691679 -0.00470604 -0.0024953  -0.00028455]
-4.8651
4.33083
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.16573
Epoch 1, cost is  4.02511
Epoch 2, cost is  3.99135
Epoch 3, cost is  3.99724
Epoch 4, cost is  4.01276
Training took 0.151171 minutes
Weight histogram
[3540 2681 2986 2547 1593 1508  850  367 1454  699] [-0.03425229 -0.03085551 -0.02745874 -0.02406196 -0.02066519 -0.01726842
 -0.01387164 -0.01047487 -0.0070781  -0.00368132 -0.00028455]
[1480 1939 1522 1773 1869 1958 2018 1946 1926 1794] [-0.03425229 -0.03085551 -0.02745874 -0.02406196 -0.02066519 -0.01726842
 -0.01387164 -0.01047487 -0.0070781  -0.00368132 -0.00028455]
-6.74375
6.24146
fine tuning ...
Epoch 0
Fine tuning took 0.073423 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074114 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073711 minutes
{0: [0.2376847290640394, 0.23522167487684728, 0.21428571428571427, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58743842364532017, 0.5431034482758621, 0.56650246305418717, 0.53940886699507384], 5: [0.1748768472906404, 0.22167487684729065, 0.21921182266009853, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379490 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3565 1850  653  113] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 159  266  423  616  721  554  949 3176 5197 4139] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.13856
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.99119
Epoch 1, cost is  2.82293
Epoch 2, cost is  2.74905
Epoch 3, cost is  2.71587
Epoch 4, cost is  2.69699
Training took 0.204356 minutes
Weight histogram
[1914 2269 1888 2000 1992 1720  902 1766 1742    7] [-0.02239203 -0.02018129 -0.01797054 -0.01575979 -0.01354904 -0.01133829
 -0.00912754 -0.00691679 -0.00470604 -0.0024953  -0.00028455]
[ 577  935 1372 1606 1816 1839 1936 1983 2187 1949] [-0.02239203 -0.02018129 -0.01797054 -0.01575979 -0.01354904 -0.01133829
 -0.00912754 -0.00691679 -0.00470604 -0.0024953  -0.00028455]
-4.8651
4.33083
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.64205
Epoch 1, cost is  2.52767
Epoch 2, cost is  2.48892
Epoch 3, cost is  2.47075
Epoch 4, cost is  2.4625
Training took 0.205265 minutes
Weight histogram
[2146 3479 3000 3030 1609 1314 1033  884 1719   11] [-0.02223296 -0.02003812 -0.01784328 -0.01564844 -0.0134536  -0.01125875
 -0.00906391 -0.00686907 -0.00467423 -0.00247939 -0.00028455]
[ 999 1715 2064 1724 1959 1964 1968 1947 2077 1808] [-0.02223296 -0.02003812 -0.01784328 -0.01564844 -0.0134536  -0.01125875
 -0.00906391 -0.00686907 -0.00467423 -0.00247939 -0.00028455]
-4.42843
4.20745
fine tuning ...
Epoch 0
Fine tuning took 0.078104 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.080518 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078896 minutes
{0: [0.19704433497536947, 0.2105911330049261, 0.26108374384236455, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54064039408866993, 0.52093596059113301, 0.50369458128078815, 0.53817733990147787], 5: [0.26231527093596058, 0.26847290640394089, 0.23522167487684728, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381283 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.82475
Epoch 1, cost is  6.55413
Epoch 2, cost is  6.4645
Epoch 3, cost is  6.44808
Epoch 4, cost is  6.46433
Training took 0.107573 minutes
Weight histogram
[2901 1065 4011 1975 2086  709 1633 2397 1411   37] [-0.05300614 -0.0477271  -0.04244806 -0.03716903 -0.03188999 -0.02661095
 -0.02133191 -0.01605287 -0.01077384 -0.0054948  -0.00021576]
[ 580  975 1589 1875 2002 2237 2276 2243 2186 2262] [-0.05300614 -0.0477271  -0.04244806 -0.03716903 -0.03188999 -0.02661095
 -0.02133191 -0.01605287 -0.01077384 -0.0054948  -0.00021576]
-10.0957
9.0217
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.16592
Epoch 1, cost is  4.98877
Epoch 2, cost is  4.94442
Epoch 3, cost is  4.963
Epoch 4, cost is  4.95987
Training took 0.072140 minutes
Weight histogram
[4824 6754 3803 1376  640  307  150 1576  779   41] [-0.04608074 -0.04149134 -0.03690195 -0.03231255 -0.02772315 -0.02313375
 -0.01854435 -0.01395496 -0.00936556 -0.00477616 -0.00018676]
[1003 1564 1742 1612 2114 2185 2872 2466 2201 2491] [-0.04608074 -0.04149134 -0.03690195 -0.03231255 -0.02772315 -0.02313375
 -0.01854435 -0.01395496 -0.00936556 -0.00477616 -0.00018676]
-7.72858
8.21867
fine tuning ...
Epoch 0
Fine tuning took 0.052774 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051843 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050616 minutes
{0: [0.097290640394088676, 0.096059113300492605, 0.10714285714285714, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83004926108374388, 0.80049261083743839, 0.80172413793103448, 0.81650246305418717], 5: [0.072660098522167482, 0.10344827586206896, 0.091133004926108374, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379958 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.82475
Epoch 1, cost is  6.55413
Epoch 2, cost is  6.4645
Epoch 3, cost is  6.44808
Epoch 4, cost is  6.46433
Training took 0.108181 minutes
Weight histogram
[2901 1065 4011 1975 2086  709 1633 2397 1411   37] [-0.05300614 -0.0477271  -0.04244806 -0.03716903 -0.03188999 -0.02661095
 -0.02133191 -0.01605287 -0.01077384 -0.0054948  -0.00021576]
[ 580  975 1589 1875 2002 2237 2276 2243 2186 2262] [-0.05300614 -0.0477271  -0.04244806 -0.03716903 -0.03188999 -0.02661095
 -0.02133191 -0.01605287 -0.01077384 -0.0054948  -0.00021576]
-10.0957
9.0217
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.06785
Epoch 1, cost is  2.98534
Epoch 2, cost is  2.92989
Epoch 3, cost is  2.94237
Epoch 4, cost is  2.94962
Training took 0.086806 minutes
Weight histogram
[5716 4339 3234 2190 1290  978 1665  646  176   16] [-0.02768045 -0.02492387 -0.02216729 -0.01941071 -0.01665413 -0.01389756
 -0.01114098 -0.0083844  -0.00562782 -0.00287124 -0.00011466]
[ 719  899 1460 2511 2532 2334 2498 2376 2411 2510] [-0.02768045 -0.02492387 -0.02216729 -0.01941071 -0.01665413 -0.01389756
 -0.01114098 -0.0083844  -0.00562782 -0.00287124 -0.00011466]
-4.38624
4.5299
fine tuning ...
Epoch 0
Fine tuning took 0.054206 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052391 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054356 minutes
{0: [0.11699507389162561, 0.11945812807881774, 0.098522167487684734, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78078817733990147, 0.74876847290640391, 0.79802955665024633, 0.76724137931034486], 5: [0.10221674876847291, 0.13177339901477833, 0.10344827586206896, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379492 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.82475
Epoch 1, cost is  6.55413
Epoch 2, cost is  6.4645
Epoch 3, cost is  6.44808
Epoch 4, cost is  6.46433
Training took 0.109409 minutes
Weight histogram
[2901 1065 4011 1975 2086  709 1633 2397 1411   37] [-0.05300614 -0.0477271  -0.04244806 -0.03716903 -0.03188999 -0.02661095
 -0.02133191 -0.01605287 -0.01077384 -0.0054948  -0.00021576]
[ 580  975 1589 1875 2002 2237 2276 2243 2186 2262] [-0.05300614 -0.0477271  -0.04244806 -0.03716903 -0.03188999 -0.02661095
 -0.02133191 -0.01605287 -0.01077384 -0.0054948  -0.00021576]
-10.0957
9.0217
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.08286
Epoch 1, cost is  2.00541
Epoch 2, cost is  1.97613
Epoch 3, cost is  1.95401
Epoch 4, cost is  1.92826
Training took 0.107546 minutes
Weight histogram
[4727 3873 2710 2362 1259 2498 1931  551  320   19] [-0.02225316 -0.02004942 -0.01784568 -0.01564194 -0.0134382  -0.01123446
 -0.00903072 -0.00682698 -0.00462324 -0.0024195  -0.00021576]
[ 561  653 1029 2018 2766 2711 3044 2640 2335 2493] [-0.02225316 -0.02004942 -0.01784568 -0.01564194 -0.0134382  -0.01123446
 -0.00903072 -0.00682698 -0.00462324 -0.0024195  -0.00021576]
-2.95001
2.45081
fine tuning ...
Epoch 0
Fine tuning took 0.056148 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056333 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055976 minutes
{0: [0.14778325123152711, 0.11576354679802955, 0.14162561576354679, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.77093596059113301, 0.75492610837438423, 0.74384236453201968], 5: [0.10344827586206896, 0.11330049261083744, 0.10344827586206896, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380465 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.64385
Epoch 1, cost is  4.42788
Epoch 2, cost is  4.34882
Epoch 3, cost is  4.32164
Epoch 4, cost is  4.30825
Training took 0.152091 minutes
Weight histogram
[2909 1318 2209 2578 2547  825 1776 2032 2018   13] [-0.03652919 -0.0329069  -0.02928462 -0.02566234 -0.02204006 -0.01841777
 -0.01479549 -0.01117321 -0.00755092 -0.00392864 -0.00030636]
[ 657 1203 1744 1869 1976 2145 2135 2158 2066 2272] [-0.03652919 -0.0329069  -0.02928462 -0.02566234 -0.02204006 -0.01841777
 -0.01479549 -0.01117321 -0.00755092 -0.00392864 -0.00030636]
-7.54253
7.22331
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.00204
Epoch 1, cost is  5.83338
Epoch 2, cost is  5.80141
Epoch 3, cost is  5.80593
Epoch 4, cost is  5.83257
Training took 0.084804 minutes
Weight histogram
[3716 3553 5934 1985 1570  739  369  271 1508  605] [-0.05208332 -0.04690562 -0.04172793 -0.03655023 -0.03137254 -0.02619484
 -0.02101714 -0.01583945 -0.01066175 -0.00548406 -0.00030636]
[1649 1703 1434 1863 2136 2326 2360 2259 2187 2333] [-0.05208332 -0.04690562 -0.04172793 -0.03655023 -0.03137254 -0.02619484
 -0.02101714 -0.01583945 -0.01066175 -0.00548406 -0.00030636]
-9.20652
8.9583
fine tuning ...
Epoch 0
Fine tuning took 0.059665 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059047 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060215 minutes
{0: [0.19827586206896552, 0.15886699507389163, 0.15763546798029557, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65886699507389157, 0.68965517241379315, 0.71059113300492616, 0.67610837438423643], 5: [0.14285714285714285, 0.15147783251231528, 0.13177339901477833, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378724 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.64385
Epoch 1, cost is  4.42788
Epoch 2, cost is  4.34882
Epoch 3, cost is  4.32164
Epoch 4, cost is  4.30825
Training took 0.151411 minutes
Weight histogram
[2909 1318 2209 2578 2547  825 1776 2032 2018   13] [-0.03652919 -0.0329069  -0.02928462 -0.02566234 -0.02204006 -0.01841777
 -0.01479549 -0.01117321 -0.00755092 -0.00392864 -0.00030636]
[ 657 1203 1744 1869 1976 2145 2135 2158 2066 2272] [-0.03652919 -0.0329069  -0.02928462 -0.02566234 -0.02204006 -0.01841777
 -0.01479549 -0.01117321 -0.00755092 -0.00392864 -0.00030636]
-7.54253
7.22331
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.85752
Epoch 1, cost is  3.71303
Epoch 2, cost is  3.66706
Epoch 3, cost is  3.65912
Epoch 4, cost is  3.65768
Training took 0.109033 minutes
Weight histogram
[2930 3780 4392 2898 1614 1447  772  661 1738   18] [-0.03287425 -0.02961746 -0.02636067 -0.02310388 -0.01984709 -0.0165903
 -0.01333351 -0.01007672 -0.00681994 -0.00356315 -0.00030636]
[1083 1924 2067 2045 2114 2279 2215 2207 2079 2237] [-0.03287425 -0.02961746 -0.02636067 -0.02310388 -0.01984709 -0.0165903
 -0.01333351 -0.01007672 -0.00681994 -0.00356315 -0.00030636]
-6.35613
5.30192
fine tuning ...
Epoch 0
Fine tuning took 0.062785 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.064202 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063842 minutes
{0: [0.23891625615763548, 0.26724137931034481, 0.2105911330049261, 0.22044334975369459], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58866995073891626, 0.56280788177339902, 0.59729064039408863, 0.60467980295566504], 5: [0.17241379310344829, 0.16995073891625614, 0.19211822660098521, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379251 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.64385
Epoch 1, cost is  4.42788
Epoch 2, cost is  4.34882
Epoch 3, cost is  4.32164
Epoch 4, cost is  4.30825
Training took 0.151246 minutes
Weight histogram
[2909 1318 2209 2578 2547  825 1776 2032 2018   13] [-0.03652919 -0.0329069  -0.02928462 -0.02566234 -0.02204006 -0.01841777
 -0.01479549 -0.01117321 -0.00755092 -0.00392864 -0.00030636]
[ 657 1203 1744 1869 1976 2145 2135 2158 2066 2272] [-0.03652919 -0.0329069  -0.02928462 -0.02566234 -0.02204006 -0.01841777
 -0.01479549 -0.01117321 -0.00755092 -0.00392864 -0.00030636]
-7.54253
7.22331
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.53486
Epoch 1, cost is  2.41873
Epoch 2, cost is  2.37825
Epoch 3, cost is  2.3538
Epoch 4, cost is  2.34815
Training took 0.151072 minutes
Weight histogram
[3636 5035 3505 2026 1680 1029 1491 1667  171   10] [-0.02097462 -0.01890779 -0.01684097 -0.01477414 -0.01270732 -0.01064049
 -0.00857366 -0.00650684 -0.00444001 -0.00237319 -0.00030636]
[ 787 1288 2329 2576 2188 2178 2182 2425 2123 2174] [-0.02097462 -0.01890779 -0.01684097 -0.01477414 -0.01270732 -0.01064049
 -0.00857366 -0.00650684 -0.00444001 -0.00237319 -0.00030636]
-3.78367
4.46223
fine tuning ...
Epoch 0
Fine tuning took 0.068032 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067767 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067233 minutes
{0: [0.24261083743842365, 0.23891625615763548, 0.24753694581280788, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60591133004926112, 0.58990147783251234, 0.55172413793103448, 0.56280788177339902], 5: [0.15147783251231528, 0.17118226600985223, 0.20073891625615764, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379940 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.00879
Epoch 1, cost is  2.83146
Epoch 2, cost is  2.76342
Epoch 3, cost is  2.72048
Epoch 4, cost is  2.69672
Training took 0.204906 minutes
Weight histogram
[3014 2309 2687 1992 1179 1522 1518 2021 1975    8] [-0.02362386 -0.02128992 -0.01895599 -0.01662206 -0.01428813 -0.0119542
 -0.00962027 -0.00728634 -0.00495241 -0.00261848 -0.00028455]
[ 645 1078 1568 1877 2002 2072 2147 2329 2171 2336] [-0.02362386 -0.02128992 -0.01895599 -0.01662206 -0.01428813 -0.0119542
 -0.00962027 -0.00728634 -0.00495241 -0.00261848 -0.00028455]
-5.27778
4.70606
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.81504
Epoch 1, cost is  6.58176
Epoch 2, cost is  6.53862
Epoch 3, cost is  6.54188
Epoch 4, cost is  6.57833
Training took 0.105605 minutes
Weight histogram
[4964 4930 3472 1156 1770 1121  503  168  112 2054] [-0.0511629  -0.04607506 -0.04098723 -0.03589939 -0.03081156 -0.02572372
 -0.02063589 -0.01554805 -0.01046022 -0.00537238 -0.00028455]
[2572  986 1522 1920 2089 2292 2313 2282 2092 2182] [-0.0511629  -0.04607506 -0.04098723 -0.03589939 -0.03081156 -0.02572372
 -0.02063589 -0.01554805 -0.01046022 -0.00537238 -0.00028455]
-10.3508
8.69606
fine tuning ...
Epoch 0
Fine tuning took 0.069397 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067851 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069041 minutes
{0: [0.21798029556650247, 0.17610837438423646, 0.18596059113300492, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64655172413793105, 0.6576354679802956, 0.69088669950738912, 0.63054187192118227], 5: [0.1354679802955665, 0.16625615763546797, 0.12315270935960591, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379892 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.00879
Epoch 1, cost is  2.83146
Epoch 2, cost is  2.76342
Epoch 3, cost is  2.72048
Epoch 4, cost is  2.69672
Training took 0.206129 minutes
Weight histogram
[3014 2309 2687 1992 1179 1522 1518 2021 1975    8] [-0.02362386 -0.02128992 -0.01895599 -0.01662206 -0.01428813 -0.0119542
 -0.00962027 -0.00728634 -0.00495241 -0.00261848 -0.00028455]
[ 645 1078 1568 1877 2002 2072 2147 2329 2171 2336] [-0.02362386 -0.02128992 -0.01895599 -0.01662206 -0.01428813 -0.0119542
 -0.00962027 -0.00728634 -0.00495241 -0.00261848 -0.00028455]
-5.27778
4.70606
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.31389
Epoch 1, cost is  4.14649
Epoch 2, cost is  4.09474
Epoch 3, cost is  4.08429
Epoch 4, cost is  4.07974
Training took 0.150833 minutes
Weight histogram
[2833 4053 3117 2989 1673 1828 1106  478 1317  856] [-0.03640682 -0.03279459 -0.02918237 -0.02557014 -0.02195791 -0.01834568
 -0.01473346 -0.01112123 -0.007509   -0.00389677 -0.00028455]
[1698 1972 1789 1996 2104 2225 2155 2147 1999 2165] [-0.03640682 -0.03279459 -0.02918237 -0.02557014 -0.02195791 -0.01834568
 -0.01473346 -0.01112123 -0.007509   -0.00389677 -0.00028455]
-7.29097
6.81893
fine tuning ...
Epoch 0
Fine tuning took 0.074910 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073828 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073214 minutes
{0: [0.25492610837438423, 0.23029556650246305, 0.2857142857142857, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.52955665024630538, 0.52216748768472909, 0.54679802955665024], 5: [0.18596059113300492, 0.24014778325123154, 0.19211822660098521, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380406 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3615 2572 1793  226] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 161  277  437  659  679  610 1434 2786 6347 4835] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.00879
Epoch 1, cost is  2.83146
Epoch 2, cost is  2.76342
Epoch 3, cost is  2.72048
Epoch 4, cost is  2.69672
Training took 0.210485 minutes
Weight histogram
[3014 2309 2687 1992 1179 1522 1518 2021 1975    8] [-0.02362386 -0.02128992 -0.01895599 -0.01662206 -0.01428813 -0.0119542
 -0.00962027 -0.00728634 -0.00495241 -0.00261848 -0.00028455]
[ 645 1078 1568 1877 2002 2072 2147 2329 2171 2336] [-0.02362386 -0.02128992 -0.01895599 -0.01662206 -0.01428813 -0.0119542
 -0.00962027 -0.00728634 -0.00495241 -0.00261848 -0.00028455]
-5.27778
4.70606
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.73699
Epoch 1, cost is  2.60793
Epoch 2, cost is  2.57349
Epoch 3, cost is  2.55414
Epoch 4, cost is  2.54556
Training took 0.205787 minutes
Weight histogram
[3592 3191 3046 3278 1810 1474 1182  741 1925   11] [-0.02298361 -0.0207137  -0.0184438  -0.01617389 -0.01390399 -0.01163408
 -0.00936417 -0.00709427 -0.00482436 -0.00255445 -0.00028455]
[1135 2034 2098 2052 2172 2175 2138 2315 2006 2125] [-0.02298361 -0.0207137  -0.0184438  -0.01617389 -0.01390399 -0.01163408
 -0.00936417 -0.00709427 -0.00482436 -0.00255445 -0.00028455]
-4.81888
4.66545
fine tuning ...
Epoch 0
Fine tuning took 0.077810 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079523 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078225 minutes
{0: [0.27586206896551724, 0.29064039408866993, 0.28325123152709358, 0.33866995073891626], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56034482758620685, 0.52586206896551724, 0.53448275862068961, 0.49384236453201968], 5: [0.16379310344827586, 0.18349753694581281, 0.18226600985221675, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379765 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.16633
Epoch 1, cost is  6.90973
Epoch 2, cost is  6.8277
Epoch 3, cost is  6.82353
Epoch 4, cost is  6.83805
Training took 0.109909 minutes
Weight histogram
[1986 3394  634 4048 2080 2007 2031 2059 1968   43] [-0.05859754 -0.05275936 -0.04692118 -0.04108301 -0.03524483 -0.02940665
 -0.02356847 -0.01773029 -0.01189212 -0.00605394 -0.00021576]
[ 645 1151 1839 2120 2196 2661 2435 2416 2455 2332] [-0.05859754 -0.05275936 -0.04692118 -0.04108301 -0.03524483 -0.02940665
 -0.02356847 -0.01773029 -0.01189212 -0.00605394 -0.00021576]
-10.4553
9.47143
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.53926
Epoch 1, cost is  5.33398
Epoch 2, cost is  5.27019
Epoch 3, cost is  5.24131
Epoch 4, cost is  5.25742
Training took 0.072433 minutes
Weight histogram
[6785 6818 3803 1376  640  307  150 1576  779   41] [-0.04608074 -0.04149134 -0.03690195 -0.03231255 -0.02772315 -0.02313375
 -0.01854435 -0.01395496 -0.00936556 -0.00477616 -0.00018676]
[1116 1822 1817 1975 2309 2912 2774 2499 2652 2399] [-0.04608074 -0.04149134 -0.03690195 -0.03231255 -0.02772315 -0.02313375
 -0.01854435 -0.01395496 -0.00936556 -0.00477616 -0.00018676]
-8.23945
8.68349
fine tuning ...
Epoch 0
Fine tuning took 0.051228 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050566 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051573 minutes
{0: [0.086206896551724144, 0.060344827586206899, 0.1145320197044335, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8288177339901478, 0.83743842364532017, 0.80295566502463056, 0.80418719211822665], 5: [0.084975369458128072, 0.10221674876847291, 0.082512315270935957, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379752 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.16633
Epoch 1, cost is  6.90973
Epoch 2, cost is  6.8277
Epoch 3, cost is  6.82353
Epoch 4, cost is  6.83805
Training took 0.109045 minutes
Weight histogram
[1986 3394  634 4048 2080 2007 2031 2059 1968   43] [-0.05859754 -0.05275936 -0.04692118 -0.04108301 -0.03524483 -0.02940665
 -0.02356847 -0.01773029 -0.01189212 -0.00605394 -0.00021576]
[ 645 1151 1839 2120 2196 2661 2435 2416 2455 2332] [-0.05859754 -0.05275936 -0.04692118 -0.04108301 -0.03524483 -0.02940665
 -0.02356847 -0.01773029 -0.01189212 -0.00605394 -0.00021576]
-10.4553
9.47143
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.0662
Epoch 1, cost is  2.96207
Epoch 2, cost is  2.92088
Epoch 3, cost is  2.91121
Epoch 4, cost is  2.90769
Training took 0.084493 minutes
Weight histogram
[4059 7632 3090 2399 1454  876 1805  729  215   16] [-0.02867506 -0.02581902 -0.02296298 -0.02010694 -0.0172509  -0.01439486
 -0.01153882 -0.00868278 -0.00582674 -0.0029707  -0.00011466]
[ 781 1031 1855 2909 2462 2653 2519 2663 2693 2709] [-0.02867506 -0.02581902 -0.02296298 -0.02010694 -0.0172509  -0.01439486
 -0.01153882 -0.00868278 -0.00582674 -0.0029707  -0.00011466]
-4.50273
4.69612
fine tuning ...
Epoch 0
Fine tuning took 0.054915 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053950 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055044 minutes
{0: [0.14285714285714285, 0.13054187192118227, 0.1268472906403941, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.75246305418719217, 0.76847290640394084, 0.72660098522167482], 5: [0.10344827586206896, 0.11699507389162561, 0.10467980295566502, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379837 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.16633
Epoch 1, cost is  6.90973
Epoch 2, cost is  6.8277
Epoch 3, cost is  6.82353
Epoch 4, cost is  6.83805
Training took 0.108279 minutes
Weight histogram
[1986 3394  634 4048 2080 2007 2031 2059 1968   43] [-0.05859754 -0.05275936 -0.04692118 -0.04108301 -0.03524483 -0.02940665
 -0.02356847 -0.01773029 -0.01189212 -0.00605394 -0.00021576]
[ 645 1151 1839 2120 2196 2661 2435 2416 2455 2332] [-0.05859754 -0.05275936 -0.04692118 -0.04108301 -0.03524483 -0.02940665
 -0.02356847 -0.01773029 -0.01189212 -0.00605394 -0.00021576]
-10.4553
9.47143
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  1.96723
Epoch 1, cost is  1.88048
Epoch 2, cost is  1.84299
Epoch 3, cost is  1.82929
Epoch 4, cost is  1.81572
Training took 0.107555 minutes
Weight histogram
[3107 5270 3782 3070  956 2684 2310  676  394   26] [-0.0237325  -0.02138082 -0.01902915 -0.01667748 -0.0143258  -0.01197413
 -0.00962246 -0.00727078 -0.00491911 -0.00256744 -0.00021576]
[ 601  746 1200 2418 3116 3053 2966 2798 2560 2817] [-0.0237325  -0.02138082 -0.01902915 -0.01667748 -0.0143258  -0.01197413
 -0.00962246 -0.00727078 -0.00491911 -0.00256744 -0.00021576]
-3.14446
2.79073
fine tuning ...
Epoch 0
Fine tuning took 0.057523 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.058093 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057254 minutes
{0: [0.14655172413793102, 0.15024630541871922, 0.14532019704433496, 0.12807881773399016], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.71798029556650245, 0.75985221674876846, 0.71305418719211822], 5: [0.12807881773399016, 0.13177339901477833, 0.094827586206896547, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380426 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.05406
Epoch 1, cost is  4.81907
Epoch 2, cost is  4.73256
Epoch 3, cost is  4.67942
Epoch 4, cost is  4.66789
Training took 0.152459 minutes
Weight histogram
[2001 3313 1766 2774 2269 2031 2020 2007 2050   19] [-0.04045808 -0.03644291 -0.03242774 -0.02841256 -0.02439739 -0.02038222
 -0.01636705 -0.01235188 -0.0083367  -0.00432153 -0.00030636]
[ 745 1429 1930 2067 2230 2388 2332 2286 2427 2416] [-0.04045808 -0.03644291 -0.03242774 -0.02841256 -0.02439739 -0.02038222
 -0.01636705 -0.01235188 -0.0083367  -0.00432153 -0.00030636]
-8.35247
7.85855
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.97889
Epoch 1, cost is  6.8044
Epoch 2, cost is  6.78504
Epoch 3, cost is  6.80366
Epoch 4, cost is  6.85315
Training took 0.084614 minutes
Weight histogram
[4690 3593 6511 2262 1594  814  410  248 1482  671] [-0.05331378 -0.04801304 -0.0427123  -0.03741155 -0.03211081 -0.02681007
 -0.02150933 -0.01620859 -0.01090784 -0.0056071  -0.00030636]
[1890 1698 1724 2195 2384 2747 2496 2430 2550 2161] [-0.05331378 -0.04801304 -0.0427123  -0.03741155 -0.03211081 -0.02681007
 -0.02150933 -0.01620859 -0.01090784 -0.0056071  -0.00030636]
-10.1669
9.54912
fine tuning ...
Epoch 0
Fine tuning took 0.060907 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060251 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061225 minutes
{0: [0.13177339901477833, 0.16625615763546797, 0.16133004926108374, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.66995073891625612, 0.69704433497536944, 0.62931034482758619], 5: [0.15886699507389163, 0.16379310344827586, 0.14162561576354679, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.378907 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.05406
Epoch 1, cost is  4.81907
Epoch 2, cost is  4.73256
Epoch 3, cost is  4.67942
Epoch 4, cost is  4.66789
Training took 0.151418 minutes
Weight histogram
[2001 3313 1766 2774 2269 2031 2020 2007 2050   19] [-0.04045808 -0.03644291 -0.03242774 -0.02841256 -0.02439739 -0.02038222
 -0.01636705 -0.01235188 -0.0083367  -0.00432153 -0.00030636]
[ 745 1429 1930 2067 2230 2388 2332 2286 2427 2416] [-0.04045808 -0.03644291 -0.03242774 -0.02841256 -0.02439739 -0.02038222
 -0.01636705 -0.01235188 -0.0083367  -0.00432153 -0.00030636]
-8.35247
7.85855
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.09968
Epoch 1, cost is  3.94335
Epoch 2, cost is  3.90497
Epoch 3, cost is  3.89194
Epoch 4, cost is  3.9044
Training took 0.110289 minutes
Weight histogram
[3936 3382 5109 3108 1688 1706  842  531 1954   19] [-0.03417124 -0.03078475 -0.02739827 -0.02401178 -0.02062529 -0.0172388
 -0.01385231 -0.01046582 -0.00707934 -0.00369285 -0.00030636]
[1222 2276 2148 2264 2362 2559 2396 2327 2411 2310] [-0.03417124 -0.03078475 -0.02739827 -0.02401178 -0.02062529 -0.0172388
 -0.01385231 -0.01046582 -0.00707934 -0.00369285 -0.00030636]
-6.49909
5.8974
fine tuning ...
Epoch 0
Fine tuning took 0.062925 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063707 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063511 minutes
{0: [0.21798029556650247, 0.26108374384236455, 0.22536945812807882, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61945812807881773, 0.55049261083743839, 0.59605911330049266, 0.61945812807881773], 5: [0.1625615763546798, 0.18842364532019704, 0.17857142857142858, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380842 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.05406
Epoch 1, cost is  4.81907
Epoch 2, cost is  4.73256
Epoch 3, cost is  4.67942
Epoch 4, cost is  4.66789
Training took 0.154089 minutes
Weight histogram
[2001 3313 1766 2774 2269 2031 2020 2007 2050   19] [-0.04045808 -0.03644291 -0.03242774 -0.02841256 -0.02439739 -0.02038222
 -0.01636705 -0.01235188 -0.0083367  -0.00432153 -0.00030636]
[ 745 1429 1930 2067 2230 2388 2332 2286 2427 2416] [-0.04045808 -0.03644291 -0.03242774 -0.02841256 -0.02439739 -0.02038222
 -0.01636705 -0.01235188 -0.0083367  -0.00432153 -0.00030636]
-8.35247
7.85855
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.37276
Epoch 1, cost is  2.2565
Epoch 2, cost is  2.20003
Epoch 3, cost is  2.18507
Epoch 4, cost is  2.17134
Training took 0.150488 minutes
Weight histogram
[2916 5716 4263 2978 1502 1275 1197 2041  376   11] [-0.02233789 -0.02013474 -0.01793159 -0.01572843 -0.01352528 -0.01132213
 -0.00911897 -0.00691582 -0.00471267 -0.00250951 -0.00030636]
[ 867 1504 2694 2636 2352 2391 2474 2459 2335 2563] [-0.02233789 -0.02013474 -0.01793159 -0.01572843 -0.01352528 -0.01132213
 -0.00911897 -0.00691582 -0.00471267 -0.00250951 -0.00030636]
-4.08879
4.81255
fine tuning ...
Epoch 0
Fine tuning took 0.068060 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068312 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068238 minutes
{0: [0.21428571428571427, 0.2376847290640394, 0.27093596059113301, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62931034482758619, 0.56527093596059108, 0.57635467980295563, 0.53694581280788178], 5: [0.15640394088669951, 0.19704433497536947, 0.15270935960591134, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380657 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.19411
Epoch 1, cost is  2.98998
Epoch 2, cost is  2.91174
Epoch 3, cost is  2.86839
Epoch 4, cost is  2.84162
Training took 0.206765 minutes
Weight histogram
[1986 2007 2305 3686 2107 2033 2057 2026 1887  156] [-0.0277396  -0.0249941  -0.02224859 -0.01950309 -0.01675758 -0.01401208
 -0.01126657 -0.00852106 -0.00577556 -0.00303005 -0.00028455]
[ 719 1228 1749 2122 2184 2307 2424 2456 2464 2597] [-0.0277396  -0.0249941  -0.02224859 -0.01950309 -0.01675758 -0.01401208
 -0.01126657 -0.00852106 -0.00577556 -0.00303005 -0.00028455]
-5.64369
5.22871
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.42413
Epoch 1, cost is  7.19539
Epoch 2, cost is  7.12471
Epoch 3, cost is  7.13731
Epoch 4, cost is  7.17811
Training took 0.106787 minutes
Weight histogram
[1897 5948 5490 2776 1541 1404  798  215  147 2059] [-0.05598659 -0.05041638 -0.04484618 -0.03927598 -0.03370577 -0.02813557
 -0.02256536 -0.01699516 -0.01142496 -0.00585475 -0.00028455]
[2645 1171 1778 2270 2256 2719 2504 2359 2379 2194] [-0.05598659 -0.05041638 -0.04484618 -0.03927598 -0.03370577 -0.02813557
 -0.02256536 -0.01699516 -0.01142496 -0.00585475 -0.00028455]
-11.5457
9.6875
fine tuning ...
Epoch 0
Fine tuning took 0.068950 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068109 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067151 minutes
{0: [0.24384236453201971, 0.21551724137931033, 0.17610837438423646, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.57635467980295563, 0.68596059113300489, 0.60837438423645318], 5: [0.14655172413793102, 0.20812807881773399, 0.13793103448275862, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382933 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.19411
Epoch 1, cost is  2.98998
Epoch 2, cost is  2.91174
Epoch 3, cost is  2.86839
Epoch 4, cost is  2.84162
Training took 0.205843 minutes
Weight histogram
[1986 2007 2305 3686 2107 2033 2057 2026 1887  156] [-0.0277396  -0.0249941  -0.02224859 -0.01950309 -0.01675758 -0.01401208
 -0.01126657 -0.00852106 -0.00577556 -0.00303005 -0.00028455]
[ 719 1228 1749 2122 2184 2307 2424 2456 2464 2597] [-0.0277396  -0.0249941  -0.02224859 -0.01950309 -0.01675758 -0.01401208
 -0.01126657 -0.00852106 -0.00577556 -0.00303005 -0.00028455]
-5.64369
5.22871
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.57095
Epoch 1, cost is  4.40656
Epoch 2, cost is  4.3477
Epoch 3, cost is  4.33954
Epoch 4, cost is  4.34125
Training took 0.152613 minutes
Weight histogram
[3217 4324 3045 3313 2292 2030 1326  528 1194 1006] [-0.03855889 -0.03473146 -0.03090402 -0.02707659 -0.02324915 -0.01942172
 -0.01559429 -0.01176685 -0.00793942 -0.00411198 -0.00028455]
[1930 2024 2054 2234 2370 2416 2350 2233 2324 2340] [-0.03855889 -0.03473146 -0.03090402 -0.02707659 -0.02324915 -0.01942172
 -0.01559429 -0.01176685 -0.00793942 -0.00411198 -0.00028455]
-7.71484
7.40377
fine tuning ...
Epoch 0
Fine tuning took 0.079599 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074657 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074169 minutes
{0: [0.23152709359605911, 0.26354679802955666, 0.30172413793103448, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47290640394088668, 0.47906403940886699, 0.48891625615763545, 0.49384236453201968], 5: [0.29556650246305421, 0.25738916256157635, 0.20935960591133004, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379946 minutes
Weight histogram
[  45  371 1660 3030 2265 2648 3760 3873 2360  238] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
[ 162  279  440  665  678  617 1505 2764 6506 6634] [ -3.26082401e-04  -1.94026303e-04  -6.19702041e-05   7.00858945e-05
   2.02141993e-04   3.34198092e-04   4.66254191e-04   5.98310289e-04
   7.30366388e-04   8.62422487e-04   9.94478585e-04]
-1.33128
1.23083
training layer 1, rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.19411
Epoch 1, cost is  2.98998
Epoch 2, cost is  2.91174
Epoch 3, cost is  2.86839
Epoch 4, cost is  2.84162
Training took 0.205809 minutes
Weight histogram
[1986 2007 2305 3686 2107 2033 2057 2026 1887  156] [-0.0277396  -0.0249941  -0.02224859 -0.01950309 -0.01675758 -0.01401208
 -0.01126657 -0.00852106 -0.00577556 -0.00303005 -0.00028455]
[ 719 1228 1749 2122 2184 2307 2424 2456 2464 2597] [-0.0277396  -0.0249941  -0.02224859 -0.01950309 -0.01675758 -0.01401208
 -0.01126657 -0.00852106 -0.00577556 -0.00303005 -0.00028455]
-5.64369
5.22871
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.81597
Epoch 1, cost is  2.66993
Epoch 2, cost is  2.62245
Epoch 3, cost is  2.59561
Epoch 4, cost is  2.58392
Training took 0.207328 minutes
Weight histogram
[2578 3687 4238 2887 2818 1914 1263  730 2139   21] [-0.02491765 -0.02245434 -0.01999103 -0.01752772 -0.01506441 -0.0126011
 -0.01013779 -0.00767448 -0.00521117 -0.00274786 -0.00028455]
[1277 2387 2097 2332 2394 2361 2440 2299 2286 2402] [-0.02491765 -0.02245434 -0.01999103 -0.01752772 -0.01506441 -0.0126011
 -0.01013779 -0.00767448 -0.00521117 -0.00274786 -0.00028455]
-5.34359
5.25946
fine tuning ...
Epoch 0
Fine tuning took 0.078599 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079672 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078654 minutes
{0: [0.28694581280788178, 0.34359605911330049, 0.32635467980295568, 0.35837438423645318], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.43842364532019706, 0.36699507389162561, 0.43103448275862066, 0.4211822660098522], 5: [0.27463054187192121, 0.2894088669950739, 0.24261083743842365, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}
