Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentDBN3
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentDBN3
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235854 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.02513
Epoch 1, cost is  5.17535
Epoch 2, cost is  4.86768
Epoch 3, cost is  4.7223
Epoch 4, cost is  4.64733
Training took 0.086712 minutes
Weight histogram
[ 68 451 888 386 194  18   9   4   4   3] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
[156 104 145 162 180 214 236 254 267 307] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
-1.78439
1.71694
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.67235
Epoch 1, cost is  4.35631
Epoch 2, cost is  3.95357
Epoch 3, cost is  3.72561
Epoch 4, cost is  3.62768
Training took 0.072621 minutes
Weight histogram
[ 444  644  180  220  126  131 1019 1224   48   14] [-0.02043821 -0.01836798 -0.01629775 -0.01422753 -0.0121573  -0.01008707
 -0.00801685 -0.00594662 -0.0038764  -0.00180617  0.00026406]
[335 195 253 317 379 460 532 552 615 412] [-0.02043821 -0.01836798 -0.01629775 -0.01422753 -0.0121573  -0.01008707
 -0.00801685 -0.00594662 -0.0038764  -0.00180617  0.00026406]
-1.78439
1.71694
fine tuning ...
Epoch 0
Fine tuning took 0.033827 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.032255 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.034472 minutes
{0: [0.0073891625615763543, 0.011083743842364532, 0.009852216748768473, 0.013546798029556651], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97783251231527091, 0.97906403940886699, 0.97413793103448276, 0.98029556650246308], 5: [0.014778325123152709, 0.009852216748768473, 0.01600985221674877, 0.0061576354679802959], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234799 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.02513
Epoch 1, cost is  5.17535
Epoch 2, cost is  4.86768
Epoch 3, cost is  4.7223
Epoch 4, cost is  4.64733
Training took 0.085616 minutes
Weight histogram
[ 68 451 888 386 194  18   9   4   4   3] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
[156 104 145 162 180 214 236 254 267 307] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
-1.78439
1.71694
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.26257
Epoch 1, cost is  3.80712
Epoch 2, cost is  3.31691
Epoch 3, cost is  3.10603
Epoch 4, cost is  2.95456
Training took 0.087455 minutes
Weight histogram
[ 183  615  451  580 1232  637  257   79    9    7] [-0.01105958 -0.00992721 -0.00879485 -0.00766249 -0.00653012 -0.00539776
 -0.0042654  -0.00313303 -0.00200067 -0.00086831  0.00026406]
[341 245 382 470 560 748 476 254 267 307] [-0.01105958 -0.00992721 -0.00879485 -0.00766249 -0.00653012 -0.00539776
 -0.0042654  -0.00313303 -0.00200067 -0.00086831  0.00026406]
-1.78439
1.71694
fine tuning ...
Epoch 0
Fine tuning took 0.033058 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.035840 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033283 minutes
{0: [0.011083743842364532, 0.011083743842364532, 0.0073891625615763543, 0.011083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97906403940886699, 0.97044334975369462, 0.98522167487684731, 0.97290640394088668], 5: [0.009852216748768473, 0.018472906403940888, 0.0073891625615763543, 0.01600985221674877], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234697 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.02513
Epoch 1, cost is  5.17535
Epoch 2, cost is  4.86768
Epoch 3, cost is  4.7223
Epoch 4, cost is  4.64733
Training took 0.085229 minutes
Weight histogram
[ 68 451 888 386 194  18   9   4   4   3] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
[156 104 145 162 180 214 236 254 267 307] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
-1.78439
1.71694
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.96679
Epoch 1, cost is  3.41994
Epoch 2, cost is  2.9199
Epoch 3, cost is  2.6975
Epoch 4, cost is  2.51347
Training took 0.111707 minutes
Weight histogram
[  68  499 1603  782  559  247  149  122   16    5] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
[390 421 606 901 454 214 236 254 267 307] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
-1.78439
1.71694
fine tuning ...
Epoch 0
Fine tuning took 0.035887 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036079 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036873 minutes
{0: [0.01600985221674877, 0.0036945812807881772, 0.011083743842364532, 0.012315270935960592], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96551724137931039, 0.98399014778325122, 0.98152709359605916, 0.98152709359605916], 5: [0.018472906403940888, 0.012315270935960592, 0.0073891625615763543, 0.0061576354679802959], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234860 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.64392
Epoch 1, cost is  4.53561
Epoch 2, cost is  4.1419
Epoch 3, cost is  3.95626
Epoch 4, cost is  3.81963
Training took 0.112027 minutes
Weight histogram
[116 503 274 375 304 284 145  16   5   3] [-0.00558492 -0.00504349 -0.00450206 -0.00396063 -0.00341919 -0.00287776
 -0.00233633 -0.0017949  -0.00125347 -0.00071203 -0.0001706 ]
[143 107 149 155 171 202 227 255 295 321] [-0.00558492 -0.00504349 -0.00450206 -0.00396063 -0.00341919 -0.00287776
 -0.00233633 -0.0017949  -0.00125347 -0.00071203 -0.0001706 ]
-1.22238
1.21991
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.45472
Epoch 1, cost is  4.51647
Epoch 2, cost is  4.21953
Epoch 3, cost is  4.08694
Epoch 4, cost is  4.03283
Training took 0.086025 minutes
Weight histogram
[ 623  631  311  151   97  154   17 1033  992   41] [ -1.89242121e-02  -1.70358345e-02  -1.51474570e-02  -1.32590794e-02
  -1.13707019e-02  -9.48232435e-03  -7.59394680e-03  -5.70556926e-03
  -3.81719171e-03  -1.92881416e-03  -4.04366147e-05]
[326 318 395 499 606 714 285 269 295 343] [ -1.89242121e-02  -1.70358345e-02  -1.51474570e-02  -1.32590794e-02
  -1.13707019e-02  -9.48232435e-03  -7.59394680e-03  -5.70556926e-03
  -3.81719171e-03  -1.92881416e-03  -4.04366147e-05]
-1.39687
1.65703
fine tuning ...
Epoch 0
Fine tuning took 0.036366 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037932 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038014 minutes
{0: [0.022167487684729065, 0.019704433497536946, 0.024630541871921183, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9642857142857143, 0.95935960591133007, 0.95443349753694584, 0.95812807881773399], 5: [0.013546798029556651, 0.020935960591133004, 0.020935960591133004, 0.024630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235628 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.64392
Epoch 1, cost is  4.53561
Epoch 2, cost is  4.1419
Epoch 3, cost is  3.95626
Epoch 4, cost is  3.81963
Training took 0.109888 minutes
Weight histogram
[116 503 274 375 304 284 145  16   5   3] [-0.00558492 -0.00504349 -0.00450206 -0.00396063 -0.00341919 -0.00287776
 -0.00233633 -0.0017949  -0.00125347 -0.00071203 -0.0001706 ]
[143 107 149 155 171 202 227 255 295 321] [-0.00558492 -0.00504349 -0.00450206 -0.00396063 -0.00341919 -0.00287776
 -0.00233633 -0.0017949  -0.00125347 -0.00071203 -0.0001706 ]
-1.22238
1.21991
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.98489
Epoch 1, cost is  3.7693
Epoch 2, cost is  3.41422
Epoch 3, cost is  3.27255
Epoch 4, cost is  3.18532
Training took 0.113577 minutes
Weight histogram
[402 615 492 225 139 819 676 545 127  10] [-0.01053993 -0.00950299 -0.00846606 -0.00742913 -0.0063922  -0.00535526
 -0.00431833 -0.0032814  -0.00224447 -0.00120753 -0.0001706 ]
[256 197 274 300 335 401 462 520 617 688] [-0.01053993 -0.00950299 -0.00846606 -0.00742913 -0.0063922  -0.00535526
 -0.00431833 -0.0032814  -0.00224447 -0.00120753 -0.0001706 ]
-1.22238
1.21991
fine tuning ...
Epoch 0
Fine tuning took 0.039365 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.041812 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040874 minutes
{0: [0.019704433497536946, 0.01600985221674877, 0.011083743842364532, 0.020935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94950738916256161, 0.94950738916256161, 0.9642857142857143, 0.94950738916256161], 5: [0.030788177339901478, 0.034482758620689655, 0.024630541871921183, 0.029556650246305417], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234996 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.64392
Epoch 1, cost is  4.53561
Epoch 2, cost is  4.1419
Epoch 3, cost is  3.95626
Epoch 4, cost is  3.81963
Training took 0.109674 minutes
Weight histogram
[116 503 274 375 304 284 145  16   5   3] [-0.00558492 -0.00504349 -0.00450206 -0.00396063 -0.00341919 -0.00287776
 -0.00233633 -0.0017949  -0.00125347 -0.00071203 -0.0001706 ]
[143 107 149 155 171 202 227 255 295 321] [-0.00558492 -0.00504349 -0.00450206 -0.00396063 -0.00341919 -0.00287776
 -0.00233633 -0.0017949  -0.00125347 -0.00071203 -0.0001706 ]
-1.22238
1.21991
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.6338
Epoch 1, cost is  3.29606
Epoch 2, cost is  2.96055
Epoch 3, cost is  2.84093
Epoch 4, cost is  2.77011
Training took 0.156828 minutes
Weight histogram
[247 453 720 909 682 516 401 108   8   6] [-0.00687197 -0.00620184 -0.0055317  -0.00486156 -0.00419142 -0.00352129
 -0.00285115 -0.00218101 -0.00151087 -0.00084074 -0.0001706 ]
[296 275 372 439 584 783 430 255 295 321] [-0.00687197 -0.00620184 -0.0055317  -0.00486156 -0.00419142 -0.00352129
 -0.00285115 -0.00218101 -0.00151087 -0.00084074 -0.0001706 ]
-1.22238
1.21991
fine tuning ...
Epoch 0
Fine tuning took 0.044782 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046813 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046863 minutes
{0: [0.023399014778325122, 0.01600985221674877, 0.020935960591133004, 0.019704433497536946], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94704433497536944, 0.96674876847290636, 0.95073891625615758, 0.95566502463054193], 5: [0.029556650246305417, 0.017241379310344827, 0.02832512315270936, 0.024630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.235033 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.34189
Epoch 1, cost is  4.03658
Epoch 2, cost is  3.61675
Epoch 3, cost is  3.41055
Epoch 4, cost is  3.25016
Training took 0.151646 minutes
Weight histogram
[ 69 317 354 250 246 257 339 185   4   4] [-0.00471885 -0.00425925 -0.00379964 -0.00334004 -0.00288043 -0.00242083
 -0.00196122 -0.00150162 -0.00104202 -0.00058241 -0.00012281]
[136 119 132 153 165 209 241 264 287 319] [-0.00471885 -0.00425925 -0.00379964 -0.00334004 -0.00288043 -0.00242083
 -0.00196122 -0.00150162 -0.00104202 -0.00058241 -0.00012281]
-1.0115
0.860642
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.52072
Epoch 1, cost is  4.83487
Epoch 2, cost is  4.6173
Epoch 3, cost is  4.55025
Epoch 4, cost is  4.48612
Training took 0.110790 minutes
Weight histogram
[ 620  773  235  153   70   71   57  181 1273  617] [-0.02022163 -0.01821175 -0.01620187 -0.01419199 -0.0121821  -0.01017222
 -0.00816234 -0.00615245 -0.00414257 -0.00213269 -0.00012281]
[375 393 562 740 618 205 251 272 294 340] [-0.02022163 -0.01821175 -0.01620187 -0.01419199 -0.0121821  -0.01017222
 -0.00816234 -0.00615245 -0.00414257 -0.00213269 -0.00012281]
-1.46628
1.45903
fine tuning ...
Epoch 0
Fine tuning took 0.046166 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.047402 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046991 minutes
{0: [0.01600985221674877, 0.018472906403940888, 0.018472906403940888, 0.022167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96551724137931039, 0.95073891625615758, 0.95566502463054193, 0.95320197044334976], 5: [0.018472906403940888, 0.030788177339901478, 0.025862068965517241, 0.024630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234828 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.34189
Epoch 1, cost is  4.03658
Epoch 2, cost is  3.61675
Epoch 3, cost is  3.41055
Epoch 4, cost is  3.25016
Training took 0.151595 minutes
Weight histogram
[ 69 317 354 250 246 257 339 185   4   4] [-0.00471885 -0.00425925 -0.00379964 -0.00334004 -0.00288043 -0.00242083
 -0.00196122 -0.00150162 -0.00104202 -0.00058241 -0.00012281]
[136 119 132 153 165 209 241 264 287 319] [-0.00471885 -0.00425925 -0.00379964 -0.00334004 -0.00288043 -0.00242083
 -0.00196122 -0.00150162 -0.00104202 -0.00058241 -0.00012281]
-1.0115
0.860642
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.07939
Epoch 1, cost is  4.16624
Epoch 2, cost is  3.91041
Epoch 3, cost is  3.82849
Epoch 4, cost is  3.77503
Training took 0.156212 minutes
Weight histogram
[639 642 472  90  93  74 641 653 711  35] [-0.0112317  -0.01012081 -0.00900992 -0.00789903 -0.00678814 -0.00567725
 -0.00456637 -0.00345548 -0.00234459 -0.0012337  -0.00012281]
[265 262 310 359 453 528 597 564 327 385] [-0.0112317  -0.01012081 -0.00900992 -0.00789903 -0.00678814 -0.00567725
 -0.00456637 -0.00345548 -0.00234459 -0.0012337  -0.00012281]
-1.02445
1.26792
fine tuning ...
Epoch 0
Fine tuning took 0.052382 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051460 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051373 minutes
{0: [0.027093596059113302, 0.025862068965517241, 0.024630541871921183, 0.02832512315270936], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94581280788177335, 0.95073891625615758, 0.94211822660098521, 0.93719211822660098], 5: [0.027093596059113302, 0.023399014778325122, 0.033251231527093597, 0.034482758620689655], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.234836 minutes
Weight histogram
[ 39 109 227 383 446 392 246 136  34  13] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
[ 76  75  84 128 144 174 215 288 379 462] [ -4.62158729e-04  -3.57846855e-04  -2.53534981e-04  -1.49223107e-04
  -4.49112325e-05   5.94006415e-05   1.63712515e-04   2.68024390e-04
   3.72336264e-04   4.76648138e-04   5.80960012e-04]
-0.790774
0.834471
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.34189
Epoch 1, cost is  4.03658
Epoch 2, cost is  3.61675
Epoch 3, cost is  3.41055
Epoch 4, cost is  3.25016
Training took 0.151620 minutes
Weight histogram
[ 69 317 354 250 246 257 339 185   4   4] [-0.00471885 -0.00425925 -0.00379964 -0.00334004 -0.00288043 -0.00242083
 -0.00196122 -0.00150162 -0.00104202 -0.00058241 -0.00012281]
[136 119 132 153 165 209 241 264 287 319] [-0.00471885 -0.00425925 -0.00379964 -0.00334004 -0.00288043 -0.00242083
 -0.00196122 -0.00150162 -0.00104202 -0.00058241 -0.00012281]
-1.0115
0.860642
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.73467
Epoch 1, cost is  3.73791
Epoch 2, cost is  3.51043
Epoch 3, cost is  3.44245
Epoch 4, cost is  3.39398
Training took 0.218158 minutes
Weight histogram
[471 577 412 378 539 559 379 488 239   8] [-0.00747037 -0.00673561 -0.00600086 -0.0052661  -0.00453135 -0.00379659
 -0.00306183 -0.00232708 -0.00159232 -0.00085756 -0.00012281]
[250 245 277 340 408 536 664 724 287 319] [-0.00747037 -0.00673561 -0.00600086 -0.0052661  -0.00453135 -0.00379659
 -0.00306183 -0.00232708 -0.00159232 -0.00085756 -0.00012281]
-1.0115
0.860642
fine tuning ...
Epoch 0
Fine tuning took 0.055680 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056201 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056325 minutes
{0: [0.030788177339901478, 0.018472906403940888, 0.025862068965517241, 0.043103448275862072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93719211822660098, 0.94334975369458129, 0.94950738916256161, 0.91379310344827591], 5: [0.032019704433497539, 0.038177339901477834, 0.024630541871921183, 0.043103448275862072], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222448 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.81403
Epoch 1, cost is  4.74667
Epoch 2, cost is  4.74296
Epoch 3, cost is  4.76078
Epoch 4, cost is  4.79581
Training took 0.083671 minutes
Weight histogram
[ 410 1597 1402  409  194   18    9    4    4    3] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
[203 203 248 309 361 399 509 585 602 631] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
-2.73186
2.86978
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.86027
Epoch 1, cost is  3.76668
Epoch 2, cost is  3.72379
Epoch 3, cost is  3.69331
Epoch 4, cost is  3.68454
Training took 0.071020 minutes
Weight histogram
[ 803 1810  603  146  244  125  503 1700  124   17] [-0.0223585  -0.02009625 -0.01783399 -0.01557174 -0.01330948 -0.01104722
 -0.00878497 -0.00652271 -0.00426045 -0.0019982   0.00026406]
[405 321 442 578 723 795 710 709 692 700] [-0.0223585  -0.02009625 -0.01783399 -0.01557174 -0.01330948 -0.01104722
 -0.00878497 -0.00652271 -0.00426045 -0.0019982   0.00026406]
-2.21515
2.64787
fine tuning ...
Epoch 0
Fine tuning took 0.033164 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.034050 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.032187 minutes
{0: [0.0086206896551724137, 0.023399014778325122, 0.027093596059113302, 0.0073891625615763543], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.98768472906403937, 0.96798029556650245, 0.96305418719211822, 0.98399014778325122], 5: [0.0036945812807881772, 0.0086206896551724137, 0.009852216748768473, 0.0086206896551724137], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222841 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.81403
Epoch 1, cost is  4.74667
Epoch 2, cost is  4.74296
Epoch 3, cost is  4.76078
Epoch 4, cost is  4.79581
Training took 0.083611 minutes
Weight histogram
[ 410 1597 1402  409  194   18    9    4    4    3] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
[203 203 248 309 361 399 509 585 602 631] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
-2.73186
2.86978
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.0197
Epoch 1, cost is  2.83971
Epoch 2, cost is  2.75845
Epoch 3, cost is  2.70071
Epoch 4, cost is  2.66606
Training took 0.084323 minutes
Weight histogram
[ 967 1706  564  490 1246  715  265  104    9    9] [-0.01131318 -0.01015545 -0.00899773 -0.00784001 -0.00668228 -0.00552456
 -0.00436684 -0.00320911 -0.00205139 -0.00089367  0.00026406]
[ 341  245  382  470  560  748  995  959 1065  310] [-0.01131318 -0.01015545 -0.00899773 -0.00784001 -0.00668228 -0.00552456
 -0.00436684 -0.00320911 -0.00205139 -0.00089367  0.00026406]
-1.78439
1.71694
fine tuning ...
Epoch 0
Fine tuning took 0.032648 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.032772 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.034495 minutes
{0: [0.013546798029556651, 0.014778325123152709, 0.011083743842364532, 0.018472906403940888], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97660098522167482, 0.97167487684729059, 0.97290640394088668, 0.96674876847290636], 5: [0.009852216748768473, 0.013546798029556651, 0.01600985221674877, 0.014778325123152709], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223677 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.81403
Epoch 1, cost is  4.74667
Epoch 2, cost is  4.74296
Epoch 3, cost is  4.76078
Epoch 4, cost is  4.79581
Training took 0.083599 minutes
Weight histogram
[ 410 1597 1402  409  194   18    9    4    4    3] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
[203 203 248 309 361 399 509 585 602 631] [-0.00778381 -0.00697902 -0.00617424 -0.00536945 -0.00456466 -0.00375988
 -0.00295509 -0.0021503  -0.00134552 -0.00054073  0.00026406]
-2.73186
2.86978
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.56728
Epoch 1, cost is  2.39445
Epoch 2, cost is  2.30822
Epoch 3, cost is  2.23665
Epoch 4, cost is  2.19481
Training took 0.106860 minutes
Weight histogram
[ 540 1043 2000 1141  706  304  176  140   19    6] [-0.00818319 -0.00733846 -0.00649374 -0.00564901 -0.00480429 -0.00395956
 -0.00311484 -0.00227012 -0.00142539 -0.00058067  0.00026406]
[ 390  421  606  901 1252 1441  236  254  267  307] [-0.00818319 -0.00733846 -0.00649374 -0.00564901 -0.00480429 -0.00395956
 -0.00311484 -0.00227012 -0.00142539 -0.00058067  0.00026406]
-1.78439
1.71694
fine tuning ...
Epoch 0
Fine tuning took 0.038212 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037962 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037118 minutes
{0: [0.019704433497536946, 0.014778325123152709, 0.023399014778325122, 0.017241379310344827], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.97044334975369462, 0.97660098522167482, 0.95935960591133007, 0.97290640394088668], 5: [0.009852216748768473, 0.0086206896551724137, 0.017241379310344827, 0.009852216748768473], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223315 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.04441
Epoch 1, cost is  3.90428
Epoch 2, cost is  3.8453
Epoch 3, cost is  3.79588
Epoch 4, cost is  3.77379
Training took 0.109290 minutes
Weight histogram
[ 503 1383  825  403  366  308  225   29    5    3] [-0.00603433 -0.00544796 -0.00486159 -0.00427521 -0.00368884 -0.00310247
 -0.00251609 -0.00192972 -0.00134335 -0.00075697 -0.0001706 ]
[187 208 233 288 347 419 548 625 601 594] [-0.00603433 -0.00544796 -0.00486159 -0.00427521 -0.00368884 -0.00310247
 -0.00251609 -0.00192972 -0.00134335 -0.00075697 -0.0001706 ]
-1.83082
1.86554
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.04194
Epoch 1, cost is  3.95289
Epoch 2, cost is  3.93796
Epoch 3, cost is  3.94944
Epoch 4, cost is  3.95626
Training took 0.083616 minutes
Weight histogram
[1339 1554  493  244  188  128   62  794 1193   80] [ -2.04335470e-02  -1.83942359e-02  -1.63549249e-02  -1.43156139e-02
  -1.22763028e-02  -1.02369918e-02  -8.19768076e-03  -6.15836972e-03
  -4.11905869e-03  -2.07974765e-03  -4.04366147e-05]
[467 541 762 983 474 421 522 596 629 680] [ -2.04335470e-02  -1.83942359e-02  -1.63549249e-02  -1.43156139e-02
  -1.22763028e-02  -1.02369918e-02  -8.19768076e-03  -6.15836972e-03
  -4.11905869e-03  -2.07974765e-03  -4.04366147e-05]
-2.34942
2.58777
fine tuning ...
Epoch 0
Fine tuning took 0.036202 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038986 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036884 minutes
{0: [0.024630541871921183, 0.023399014778325122, 0.036945812807881777, 0.029556650246305417], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96305418719211822, 0.94581280788177335, 0.92980295566502458, 0.94827586206896552], 5: [0.012315270935960592, 0.030788177339901478, 0.033251231527093597, 0.022167487684729065], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223274 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.04441
Epoch 1, cost is  3.90428
Epoch 2, cost is  3.8453
Epoch 3, cost is  3.79588
Epoch 4, cost is  3.77379
Training took 0.111256 minutes
Weight histogram
[ 503 1383  825  403  366  308  225   29    5    3] [-0.00603433 -0.00544796 -0.00486159 -0.00427521 -0.00368884 -0.00310247
 -0.00251609 -0.00192972 -0.00134335 -0.00075697 -0.0001706 ]
[187 208 233 288 347 419 548 625 601 594] [-0.00603433 -0.00544796 -0.00486159 -0.00427521 -0.00368884 -0.00310247
 -0.00251609 -0.00192972 -0.00134335 -0.00075697 -0.0001706 ]
-1.83082
1.86554
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.1182
Epoch 1, cost is  3.00037
Epoch 2, cost is  2.94609
Epoch 3, cost is  2.91631
Epoch 4, cost is  2.8955
Training took 0.108896 minutes
Weight histogram
[ 908 1465  701  522  220  489  838  673  247   12] [-0.01181566 -0.01065115 -0.00948665 -0.00832214 -0.00715763 -0.00599313
 -0.00482862 -0.00366412 -0.00249961 -0.00133511 -0.0001706 ]
[325 337 413 501 621 747 918 765 697 751] [-0.01181566 -0.01065115 -0.00948665 -0.00832214 -0.00715763 -0.00599313
 -0.00482862 -0.00366412 -0.00249961 -0.00133511 -0.0001706 ]
-1.56303
1.58741
fine tuning ...
Epoch 0
Fine tuning took 0.040994 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.041132 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.041798 minutes
{0: [0.036945812807881777, 0.046798029556650245, 0.034482758620689655, 0.036945812807881777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.92610837438423643, 0.91995073891625612, 0.94088669950738912, 0.93226600985221675], 5: [0.036945812807881777, 0.033251231527093597, 0.024630541871921183, 0.030788177339901478], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223696 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.04441
Epoch 1, cost is  3.90428
Epoch 2, cost is  3.8453
Epoch 3, cost is  3.79588
Epoch 4, cost is  3.77379
Training took 0.109009 minutes
Weight histogram
[ 503 1383  825  403  366  308  225   29    5    3] [-0.00603433 -0.00544796 -0.00486159 -0.00427521 -0.00368884 -0.00310247
 -0.00251609 -0.00192972 -0.00134335 -0.00075697 -0.0001706 ]
[187 208 233 288 347 419 548 625 601 594] [-0.00603433 -0.00544796 -0.00486159 -0.00427521 -0.00368884 -0.00310247
 -0.00251609 -0.00192972 -0.00134335 -0.00075697 -0.0001706 ]
-1.83082
1.86554
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.74715
Epoch 1, cost is  2.6198
Epoch 2, cost is  2.54806
Epoch 3, cost is  2.50374
Epoch 4, cost is  2.47104
Training took 0.150731 minutes
Weight histogram
[ 632  957  896  785 1079  801  625  273   19    8] [-0.00822171 -0.0074166  -0.00661149 -0.00580638 -0.00500127 -0.00419616
 -0.00339105 -0.00258593 -0.00178082 -0.00097571 -0.0001706 ]
[ 296  275  372  439  584  783 1059 1071  875  321] [-0.00822171 -0.0074166  -0.00661149 -0.00580638 -0.00500127 -0.00419616
 -0.00339105 -0.00258593 -0.00178082 -0.00097571 -0.0001706 ]
-1.22238
1.21991
fine tuning ...
Epoch 0
Fine tuning took 0.046135 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.044854 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045030 minutes
{0: [0.049261083743842367, 0.04064039408866995, 0.044334975369458129, 0.04064039408866995], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91748768472906406, 0.93349753694581283, 0.92980295566502458, 0.92610837438423643], 5: [0.033251231527093597, 0.025862068965517241, 0.025862068965517241, 0.033251231527093597], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223625 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.35565
Epoch 1, cost is  3.16067
Epoch 2, cost is  3.05235
Epoch 3, cost is  2.96251
Epoch 4, cost is  2.89545
Training took 0.150883 minutes
Weight histogram
[ 559 1015  567  430  410  278  350  393   43    5] [-0.00588223 -0.00530629 -0.00473035 -0.0041544  -0.00357846 -0.00300252
 -0.00242658 -0.00185063 -0.00127469 -0.00069875 -0.00012281]
[193 196 234 300 373 428 571 588 579 588] [-0.00588223 -0.00530629 -0.00473035 -0.0041544  -0.00357846 -0.00300252
 -0.00242658 -0.00185063 -0.00127469 -0.00069875 -0.00012281]
-1.69315
1.38212
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.47763
Epoch 1, cost is  4.41793
Epoch 2, cost is  4.42277
Epoch 3, cost is  4.45339
Epoch 4, cost is  4.48492
Training took 0.108341 minutes
Weight histogram
[1212  771  831  777  196   85   89   60 1107  947] [-0.02555717 -0.02301373 -0.0204703  -0.01792686 -0.01538342 -0.01283999
 -0.01029655 -0.00775312 -0.00520968 -0.00266624 -0.00012281]
[ 567  793 1193  359  391  455  535  550  589  643] [-0.02555717 -0.02301373 -0.0204703  -0.01792686 -0.01538342 -0.01283999
 -0.01029655 -0.00775312 -0.00520968 -0.00266624 -0.00012281]
-2.83065
2.57517
fine tuning ...
Epoch 0
Fine tuning took 0.045433 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.047334 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045731 minutes
{0: [0.029556650246305417, 0.030788177339901478, 0.029556650246305417, 0.030788177339901478], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94334975369458129, 0.94704433497536944, 0.94211822660098521, 0.9211822660098522], 5: [0.027093596059113302, 0.022167487684729065, 0.02832512315270936, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223257 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.35565
Epoch 1, cost is  3.16067
Epoch 2, cost is  3.05235
Epoch 3, cost is  2.96251
Epoch 4, cost is  2.89545
Training took 0.151912 minutes
Weight histogram
[ 559 1015  567  430  410  278  350  393   43    5] [-0.00588223 -0.00530629 -0.00473035 -0.0041544  -0.00357846 -0.00300252
 -0.00242658 -0.00185063 -0.00127469 -0.00069875 -0.00012281]
[193 196 234 300 373 428 571 588 579 588] [-0.00588223 -0.00530629 -0.00473035 -0.0041544  -0.00357846 -0.00300252
 -0.00242658 -0.00185063 -0.00127469 -0.00069875 -0.00012281]
-1.69315
1.38212
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.64743
Epoch 1, cost is  3.54452
Epoch 2, cost is  3.51222
Epoch 3, cost is  3.49617
Epoch 4, cost is  3.48105
Training took 0.151582 minutes
Weight histogram
[1159  815  381  886  570  123  100  897  883  261] [-0.01507353 -0.01357846 -0.01208339 -0.01058832 -0.00909324 -0.00759817
 -0.0061031  -0.00460802 -0.00311295 -0.00161788 -0.00012281]
[375 421 537 709 867 631 553 643 638 701] [-0.01507353 -0.01357846 -0.01208339 -0.01058832 -0.00909324 -0.00759817
 -0.0061031  -0.00460802 -0.00311295 -0.00161788 -0.00012281]
-1.80507
1.83041
fine tuning ...
Epoch 0
Fine tuning took 0.052958 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052235 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051331 minutes
{0: [0.059113300492610835, 0.041871921182266007, 0.050492610837438424, 0.078817733990147784], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9076354679802956, 0.90270935960591137, 0.90394088669950734, 0.87438423645320196], 5: [0.033251231527093597, 0.055418719211822662, 0.045566502463054187, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223508 minutes
Weight histogram
[ 43 142 311 569 802 937 836 313  76  21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[106 117 186 218 302 441 637 467 612 964] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.08951
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.35565
Epoch 1, cost is  3.16067
Epoch 2, cost is  3.05235
Epoch 3, cost is  2.96251
Epoch 4, cost is  2.89545
Training took 0.152393 minutes
Weight histogram
[ 559 1015  567  430  410  278  350  393   43    5] [-0.00588223 -0.00530629 -0.00473035 -0.0041544  -0.00357846 -0.00300252
 -0.00242658 -0.00185063 -0.00127469 -0.00069875 -0.00012281]
[193 196 234 300 373 428 571 588 579 588] [-0.00588223 -0.00530629 -0.00473035 -0.0041544  -0.00357846 -0.00300252
 -0.00242658 -0.00185063 -0.00127469 -0.00069875 -0.00012281]
-1.69315
1.38212
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.24247
Epoch 1, cost is  3.11786
Epoch 2, cost is  3.06016
Epoch 3, cost is  3.02093
Epoch 4, cost is  2.99328
Training took 0.207258 minutes
Weight histogram
[1079  928  448  691  575  531  748  513  549   13] [-0.00966838 -0.00871382 -0.00775927 -0.00680471 -0.00585015 -0.00489559
 -0.00394104 -0.00298648 -0.00203192 -0.00107736 -0.00012281]
[285 294 334 428 561 740 951 947 874 661] [-0.00966838 -0.00871382 -0.00775927 -0.00680471 -0.00585015 -0.00489559
 -0.00394104 -0.00298648 -0.00203192 -0.00107736 -0.00012281]
-1.23945
1.07953
fine tuning ...
Epoch 0
Fine tuning took 0.055505 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057147 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056993 minutes
{0: [0.087438423645320201, 0.045566502463054187, 0.036945812807881777, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.85960591133004927, 0.91748768472906406, 0.90886699507389157, 0.87561576354679804], 5: [0.05295566502463054, 0.036945812807881777, 0.054187192118226604, 0.062807881773399021], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222224 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.98375
Epoch 1, cost is  4.89442
Epoch 2, cost is  4.91297
Epoch 3, cost is  4.94252
Epoch 4, cost is  4.98051
Training took 0.085388 minutes
Weight histogram
[ 811 1408 1997 1312  430   92   10    8    3    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[250 289 367 469 529 721 771 868 929 882] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-3.49398
3.73241
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.80158
Epoch 1, cost is  3.69683
Epoch 2, cost is  3.67889
Epoch 3, cost is  3.68043
Epoch 4, cost is  3.67941
Training took 0.071388 minutes
Weight histogram
[1979 2493  750  143  259  123  451 1744  141   17] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
[ 483  479  676  923 1055  845  891  927  911  910] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
-2.98042
3.73527
fine tuning ...
Epoch 0
Fine tuning took 0.030678 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.031596 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.031708 minutes
{0: [0.014778325123152709, 0.023399014778325122, 0.020935960591133004, 0.027093596059113302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96182266009852213, 0.95566502463054193, 0.95812807881773399, 0.95566502463054193], 5: [0.023399014778325122, 0.020935960591133004, 0.020935960591133004, 0.017241379310344827], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221742 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.98375
Epoch 1, cost is  4.89442
Epoch 2, cost is  4.91297
Epoch 3, cost is  4.94252
Epoch 4, cost is  4.98051
Training took 0.087168 minutes
Weight histogram
[ 811 1408 1997 1312  430   92   10    8    3    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[250 289 367 469 529 721 771 868 929 882] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-3.49398
3.73241
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.75193
Epoch 1, cost is  2.66943
Epoch 2, cost is  2.63176
Epoch 3, cost is  2.61472
Epoch 4, cost is  2.59053
Training took 0.087623 minutes
Weight histogram
[1060 2654 1346  391 1013 1050  411  153   13    9] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
[ 362  295  440  544  703 1019 1047 1150 1408 1132] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
-1.78439
1.90794
fine tuning ...
Epoch 0
Fine tuning took 0.035811 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.034809 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033148 minutes
{0: [0.02832512315270936, 0.012315270935960592, 0.027093596059113302, 0.018472906403940888], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94950738916256161, 0.96674876847290636, 0.9568965517241379, 0.95566502463054193], 5: [0.022167487684729065, 0.020935960591133004, 0.01600985221674877, 0.025862068965517241], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221750 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.98375
Epoch 1, cost is  4.89442
Epoch 2, cost is  4.91297
Epoch 3, cost is  4.94252
Epoch 4, cost is  4.98051
Training took 0.087171 minutes
Weight histogram
[ 811 1408 1997 1312  430   92   10    8    3    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[250 289 367 469 529 721 771 868 929 882] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-3.49398
3.73241
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.30851
Epoch 1, cost is  2.20716
Epoch 2, cost is  2.14723
Epoch 3, cost is  2.12075
Epoch 4, cost is  2.07865
Training took 0.106727 minutes
Weight histogram
[ 712 1204 1367 2157 1394  763  264  177   55    7] [-0.00970506 -0.00870815 -0.00771124 -0.00671432 -0.00571741 -0.0047205
 -0.00372359 -0.00272668 -0.00172977 -0.00073285  0.00026406]
[ 390  421  606  901 1252 1686 2016  254  267  307] [-0.00970506 -0.00870815 -0.00771124 -0.00671432 -0.00571741 -0.0047205
 -0.00372359 -0.00272668 -0.00172977 -0.00073285  0.00026406]
-1.78439
1.71694
fine tuning ...
Epoch 0
Fine tuning took 0.035651 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038511 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036897 minutes
{0: [0.017241379310344827, 0.023399014778325122, 0.029556650246305417, 0.030788177339901478], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.9568965517241379, 0.94827586206896552, 0.95443349753694584, 0.93719211822660098], 5: [0.025862068965517241, 0.02832512315270936, 0.01600985221674877, 0.032019704433497539], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221477 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8358
Epoch 1, cost is  3.72567
Epoch 2, cost is  3.68095
Epoch 3, cost is  3.67382
Epoch 4, cost is  3.6588
Training took 0.109161 minutes
Weight histogram
[ 639 1378 1299 1377  455  427  356  134    6    4] [-0.0071789  -0.00647807 -0.00577724 -0.00507641 -0.00437558 -0.00367475
 -0.00297392 -0.00227309 -0.00157226 -0.00087143 -0.0001706 ]
[242 291 358 462 579 828 784 840 855 836] [-0.0071789  -0.00647807 -0.00577724 -0.00507641 -0.00437558 -0.00367475
 -0.00297392 -0.00227309 -0.00157226 -0.00087143 -0.0001706 ]
-2.5264
2.37025
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.17161
Epoch 1, cost is  4.12439
Epoch 2, cost is  4.13358
Epoch 3, cost is  4.15017
Epoch 4, cost is  4.18814
Training took 0.084617 minutes
Weight histogram
[1231  888 2053 1143  316  216  170  138 1669  276] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
[ 592  786 1146  722  538  718  777  869  978  974] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
-3.45489
3.45803
fine tuning ...
Epoch 0
Fine tuning took 0.038745 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036863 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037716 minutes
{0: [0.013546798029556651, 0.025862068965517241, 0.043103448275862072, 0.032019704433497539], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.96921182266009853, 0.94334975369458129, 0.91502463054187189, 0.94211822660098521], 5: [0.017241379310344827, 0.030788177339901478, 0.041871921182266007, 0.025862068965517241], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221221 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8358
Epoch 1, cost is  3.72567
Epoch 2, cost is  3.68095
Epoch 3, cost is  3.67382
Epoch 4, cost is  3.6588
Training took 0.108839 minutes
Weight histogram
[ 639 1378 1299 1377  455  427  356  134    6    4] [-0.0071789  -0.00647807 -0.00577724 -0.00507641 -0.00437558 -0.00367475
 -0.00297392 -0.00227309 -0.00157226 -0.00087143 -0.0001706 ]
[242 291 358 462 579 828 784 840 855 836] [-0.0071789  -0.00647807 -0.00577724 -0.00507641 -0.00437558 -0.00367475
 -0.00297392 -0.00227309 -0.00157226 -0.00087143 -0.0001706 ]
-2.5264
2.37025
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.10537
Epoch 1, cost is  3.01703
Epoch 2, cost is  2.98379
Epoch 3, cost is  2.9696
Epoch 4, cost is  2.96662
Training took 0.111385 minutes
Weight histogram
[1208 1159 1589 1113  565  231  879  869  473   14] [-0.0139726 -0.0125924 -0.0112122 -0.009832  -0.0084518 -0.0070716
 -0.0056914 -0.0043112 -0.002931  -0.0015508 -0.0001706]
[ 399  475  592  770  981 1126  874  956  977  950] [-0.0139726 -0.0125924 -0.0112122 -0.009832  -0.0084518 -0.0070716
 -0.0056914 -0.0043112 -0.002931  -0.0015508 -0.0001706]
-2.21761
2.0955
fine tuning ...
Epoch 0
Fine tuning took 0.039118 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.039585 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.041456 minutes
{0: [0.035714285714285712, 0.032019704433497539, 0.049261083743842367, 0.035714285714285712], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94334975369458129, 0.93965517241379315, 0.92733990147783252, 0.91748768472906406], 5: [0.020935960591133004, 0.02832512315270936, 0.023399014778325122, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222755 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8358
Epoch 1, cost is  3.72567
Epoch 2, cost is  3.68095
Epoch 3, cost is  3.67382
Epoch 4, cost is  3.6588
Training took 0.108950 minutes
Weight histogram
[ 639 1378 1299 1377  455  427  356  134    6    4] [-0.0071789  -0.00647807 -0.00577724 -0.00507641 -0.00437558 -0.00367475
 -0.00297392 -0.00227309 -0.00157226 -0.00087143 -0.0001706 ]
[242 291 358 462 579 828 784 840 855 836] [-0.0071789  -0.00647807 -0.00577724 -0.00507641 -0.00437558 -0.00367475
 -0.00297392 -0.00227309 -0.00157226 -0.00087143 -0.0001706 ]
-2.5264
2.37025
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.50793
Epoch 1, cost is  2.38685
Epoch 2, cost is  2.3359
Epoch 3, cost is  2.29876
Epoch 4, cost is  2.27636
Training took 0.153893 minutes
Weight histogram
[1093 1077 1135 1124  895 1224  863  603   76   10] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
[ 312  308  410  507  705 1005 1131 1209 1418 1095] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
-1.41016
1.39769
fine tuning ...
Epoch 0
Fine tuning took 0.044485 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.044716 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.044764 minutes
{0: [0.04064039408866995, 0.029556650246305417, 0.049261083743842367, 0.049261083743842367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93103448275862066, 0.9211822660098522, 0.90394088669950734, 0.90517241379310343], 5: [0.02832512315270936, 0.049261083743842367, 0.046798029556650245, 0.045566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222572 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.11539
Epoch 1, cost is  2.95354
Epoch 2, cost is  2.87848
Epoch 3, cost is  2.81269
Epoch 4, cost is  2.772
Training took 0.152646 minutes
Weight histogram
[ 654 1423 1287  764  529  397  398  437  180    6] [-0.0069082  -0.00622966 -0.00555112 -0.00487258 -0.00419404 -0.0035155
 -0.00283696 -0.00215842 -0.00147988 -0.00080135 -0.00012281]
[249 274 359 489 582 798 746 835 890 853] [-0.0069082  -0.00622966 -0.00555112 -0.00487258 -0.00419404 -0.0035155
 -0.00283696 -0.00215842 -0.00147988 -0.00080135 -0.00012281]
-2.22959
1.78398
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.52274
Epoch 1, cost is  4.4359
Epoch 2, cost is  4.44152
Epoch 3, cost is  4.46843
Epoch 4, cost is  4.50605
Training took 0.105994 minutes
Weight histogram
[1261  919 1663  622 1135  228  113   84  871 1204] [-0.03099088 -0.02790407 -0.02481726 -0.02173046 -0.01864365 -0.01555684
 -0.01247004 -0.00938323 -0.00629642 -0.00320961 -0.00012281]
[ 750 1248  862  503  607  704  749  861  908  908] [-0.03099088 -0.02790407 -0.02481726 -0.02173046 -0.01864365 -0.01555684
 -0.01247004 -0.00938323 -0.00629642 -0.00320961 -0.00012281]
-4.17352
3.23099
fine tuning ...
Epoch 0
Fine tuning took 0.045328 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046820 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046659 minutes
{0: [0.071428571428571425, 0.054187192118226604, 0.051724137931034482, 0.051724137931034482], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89532019704433496, 0.91009852216748766, 0.89655172413793105, 0.90024630541871919], 5: [0.033251231527093597, 0.035714285714285712, 0.051724137931034482, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221427 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.11539
Epoch 1, cost is  2.95354
Epoch 2, cost is  2.87848
Epoch 3, cost is  2.81269
Epoch 4, cost is  2.772
Training took 0.150891 minutes
Weight histogram
[ 654 1423 1287  764  529  397  398  437  180    6] [-0.0069082  -0.00622966 -0.00555112 -0.00487258 -0.00419404 -0.0035155
 -0.00283696 -0.00215842 -0.00147988 -0.00080135 -0.00012281]
[249 274 359 489 582 798 746 835 890 853] [-0.0069082  -0.00622966 -0.00555112 -0.00487258 -0.00419404 -0.0035155
 -0.00283696 -0.00215842 -0.00147988 -0.00080135 -0.00012281]
-2.22959
1.78398
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.55691
Epoch 1, cost is  3.47015
Epoch 2, cost is  3.44948
Epoch 3, cost is  3.45029
Epoch 4, cost is  3.45679
Training took 0.150390 minutes
Weight histogram
[1099 1317 1438  350 1015  642  140  503 1064  532] [-0.0181754  -0.01637014 -0.01456488 -0.01275962 -0.01095436 -0.0091491
 -0.00734385 -0.00553859 -0.00373333 -0.00192807 -0.00012281]
[ 495  613  875 1156  687  811  856  905  841  861] [-0.0181754  -0.01637014 -0.01456488 -0.01275962 -0.01095436 -0.0091491
 -0.00734385 -0.00553859 -0.00373333 -0.00192807 -0.00012281]
-2.44199
2.36464
fine tuning ...
Epoch 0
Fine tuning took 0.050515 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052045 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052423 minutes
{0: [0.075123152709359611, 0.096059113300492605, 0.071428571428571425, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88793103448275867, 0.84852216748768472, 0.86083743842364535, 0.84605911330049266], 5: [0.036945812807881777, 0.055418719211822662, 0.067733990147783252, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223782 minutes
Weight histogram
[  43  142  392  905 1402 1567 1159  361   83   21] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
[ 119  139  224  295  418  656  581  676 1425 1542] [ -4.62158729e-04  -3.45243010e-04  -2.28327292e-04  -1.11411573e-04
   5.50414552e-06   1.22419864e-04   2.39335583e-04   3.56251301e-04
   4.73167020e-04   5.90082738e-04   7.06998457e-04]
-1.26117
1.24695
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.11539
Epoch 1, cost is  2.95354
Epoch 2, cost is  2.87848
Epoch 3, cost is  2.81269
Epoch 4, cost is  2.772
Training took 0.152397 minutes
Weight histogram
[ 654 1423 1287  764  529  397  398  437  180    6] [-0.0069082  -0.00622966 -0.00555112 -0.00487258 -0.00419404 -0.0035155
 -0.00283696 -0.00215842 -0.00147988 -0.00080135 -0.00012281]
[249 274 359 489 582 798 746 835 890 853] [-0.0069082  -0.00622966 -0.00555112 -0.00487258 -0.00419404 -0.0035155
 -0.00283696 -0.00215842 -0.00147988 -0.00080135 -0.00012281]
-2.22959
1.78398
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.93828
Epoch 1, cost is  2.83052
Epoch 2, cost is  2.78114
Epoch 3, cost is  2.75943
Epoch 4, cost is  2.74501
Training took 0.205855 minutes
Weight histogram
[1672 1485  887  656  737  497  870  602  668   26] [-0.0108351  -0.00976387 -0.00869264 -0.00762141 -0.00655018 -0.00547895
 -0.00440772 -0.00333649 -0.00226527 -0.00119404 -0.00012281]
[ 361  398  524  726 1038 1240 1148  896  880  889] [-0.0108351  -0.00976387 -0.00869264 -0.00762141 -0.00655018 -0.00547895
 -0.00440772 -0.00333649 -0.00226527 -0.00119404 -0.00012281]
-1.52338
1.54343
fine tuning ...
Epoch 0
Fine tuning took 0.057159 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055781 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055619 minutes
{0: [0.087438423645320201, 0.071428571428571425, 0.091133004926108374, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84482758620689657, 0.85837438423645318, 0.83128078817733986, 0.79926108374384242], 5: [0.067733990147783252, 0.070197044334975367, 0.077586206896551727, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223865 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.20044
Epoch 1, cost is  5.11966
Epoch 2, cost is  5.12658
Epoch 3, cost is  5.18256
Epoch 4, cost is  5.23736
Training took 0.083327 minutes
Weight histogram
[ 816 1416 2003 1344  721 1047  653   93    3    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 311  393  525  641  897  965 1149 1107 1069 1043] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-4.48499
4.86466
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.08389
Epoch 1, cost is  4.05375
Epoch 2, cost is  4.08389
Epoch 3, cost is  4.10617
Epoch 4, cost is  4.14996
Training took 0.069994 minutes
Weight histogram
[1998 3159 1863  370  259  123  451 1744  141   17] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
[ 566  659  984 1228 1079 1086 1130 1094 1132 1167] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
-3.63977
4.31804
fine tuning ...
Epoch 0
Fine tuning took 0.030713 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033438 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.032933 minutes
{0: [0.04064039408866995, 0.050492610837438424, 0.046798029556650245, 0.041871921182266007], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91995073891625612, 0.9285714285714286, 0.91871921182266014, 0.94458128078817738], 5: [0.039408866995073892, 0.020935960591133004, 0.034482758620689655, 0.013546798029556651], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.224322 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.20044
Epoch 1, cost is  5.11966
Epoch 2, cost is  5.12658
Epoch 3, cost is  5.18256
Epoch 4, cost is  5.23736
Training took 0.083593 minutes
Weight histogram
[ 816 1416 2003 1344  721 1047  653   93    3    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 311  393  525  641  897  965 1149 1107 1069 1043] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-4.48499
4.86466
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.7428
Epoch 1, cost is  2.67163
Epoch 2, cost is  2.6502
Epoch 3, cost is  2.63982
Epoch 4, cost is  2.62832
Training took 0.084120 minutes
Weight histogram
[1076 3734 2261  405 1013 1050  411  153   13    9] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
[ 401  394  585  737 1103 1259 1399 1568 1356 1323] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
-2.04905
2.27096
fine tuning ...
Epoch 0
Fine tuning took 0.032550 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.035951 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033951 minutes
{0: [0.05295566502463054, 0.050492610837438424, 0.045566502463054187, 0.044334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91748768472906406, 0.92487684729064035, 0.93103448275862066, 0.9285714285714286], 5: [0.029556650246305417, 0.024630541871921183, 0.023399014778325122, 0.027093596059113302], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223462 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.20044
Epoch 1, cost is  5.11966
Epoch 2, cost is  5.12658
Epoch 3, cost is  5.18256
Epoch 4, cost is  5.23736
Training took 0.083342 minutes
Weight histogram
[ 816 1416 2003 1344  721 1047  653   93    3    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 311  393  525  641  897  965 1149 1107 1069 1043] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-4.48499
4.86466
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.23458
Epoch 1, cost is  2.14453
Epoch 2, cost is  2.10885
Epoch 3, cost is  2.07993
Epoch 4, cost is  2.041
Training took 0.108893 minutes
Weight histogram
[1431 2479 1385 2150 1408  763  267  180   55    7] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
[ 390  421  606  901 1252 1686 2180 2042  340  307] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
-1.78439
1.71694
fine tuning ...
Epoch 0
Fine tuning took 0.035566 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036254 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038272 minutes
{0: [0.064039408866995079, 0.054187192118226604, 0.057881773399014777, 0.064039408866995079], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89778325123152714, 0.9211822660098522, 0.90517241379310343, 0.91009852216748766], 5: [0.038177339901477834, 0.024630541871921183, 0.036945812807881777, 0.025862068965517241], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223582 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.88059
Epoch 1, cost is  3.75901
Epoch 2, cost is  3.72051
Epoch 3, cost is  3.71472
Epoch 4, cost is  3.72945
Training took 0.108891 minutes
Weight histogram
[ 649 1648 1636 1467 1389  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 308  381  513  683 1000  959 1071 1034 1099 1052] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-3.20282
2.85614
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.45155
Epoch 1, cost is  4.37806
Epoch 2, cost is  4.3862
Epoch 3, cost is  4.42246
Epoch 4, cost is  4.46971
Training took 0.083494 minutes
Weight histogram
[1320 2824 2053 1143  316  216  170  138 1669  276] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
[ 717 1072 1263  613  833  942 1098 1164 1237 1186] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
-4.38189
4.05024
fine tuning ...
Epoch 0
Fine tuning took 0.036095 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038425 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038439 minutes
{0: [0.036945812807881777, 0.035714285714285712, 0.024630541871921183, 0.023399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93965517241379315, 0.93596059113300489, 0.95935960591133007, 0.94088669950738912], 5: [0.023399014778325122, 0.02832512315270936, 0.01600985221674877, 0.035714285714285712], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223555 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.88059
Epoch 1, cost is  3.75901
Epoch 2, cost is  3.72051
Epoch 3, cost is  3.71472
Epoch 4, cost is  3.72945
Training took 0.111180 minutes
Weight histogram
[ 649 1648 1636 1467 1389  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 308  381  513  683 1000  959 1071 1034 1099 1052] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-3.20282
2.85614
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.18017
Epoch 1, cost is  3.08881
Epoch 2, cost is  3.07894
Epoch 3, cost is  3.08169
Epoch 4, cost is  3.10516
Training took 0.111420 minutes
Weight histogram
[2384 1849 1566 1239  605  236  874  860  498   14] [-0.01419994 -0.01279701 -0.01139407 -0.00999114 -0.00858821 -0.00718527
 -0.00578234 -0.0043794  -0.00297647 -0.00157353 -0.0001706 ]
[ 484  621  827 1124 1345 1064 1195 1156 1178 1131] [-0.01419994 -0.01279701 -0.01139407 -0.00999114 -0.00858821 -0.00718527
 -0.00578234 -0.0043794  -0.00297647 -0.00157353 -0.0001706 ]
-2.91909
2.80615
fine tuning ...
Epoch 0
Fine tuning took 0.040727 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040461 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.039920 minutes
{0: [0.067733990147783252, 0.060344827586206899, 0.060344827586206899, 0.065270935960591137], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89039408866995073, 0.90886699507389157, 0.88669950738916259, 0.89162561576354682], 5: [0.041871921182266007, 0.030788177339901478, 0.05295566502463054, 0.043103448275862072], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222321 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.88059
Epoch 1, cost is  3.75901
Epoch 2, cost is  3.72051
Epoch 3, cost is  3.71472
Epoch 4, cost is  3.72945
Training took 0.110067 minutes
Weight histogram
[ 649 1648 1636 1467 1389  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 308  381  513  683 1000  959 1071 1034 1099 1052] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-3.20282
2.85614
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.423
Epoch 1, cost is  2.33192
Epoch 2, cost is  2.29856
Epoch 3, cost is  2.2781
Epoch 4, cost is  2.2715
Training took 0.151756 minutes
Weight histogram
[2261 1934 1135 1124  895 1224  863  603   76   10] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
[ 367  418  555  784 1197 1378 1614 1489 1189 1134] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
-1.78033
1.82663
fine tuning ...
Epoch 0
Fine tuning took 0.044380 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046596 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045557 minutes
{0: [0.073891625615763554, 0.083743842364532015, 0.071428571428571425, 0.066502463054187194], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90024630541871919, 0.87438423645320196, 0.86699507389162567, 0.88669950738916259], 5: [0.025862068965517241, 0.041871921182266007, 0.061576354679802957, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222367 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.01339
Epoch 1, cost is  2.87103
Epoch 2, cost is  2.80977
Epoch 3, cost is  2.78129
Epoch 4, cost is  2.75201
Training took 0.151451 minutes
Weight histogram
[ 819 2142 1586 1280  602  547  381  492  245    6] [-0.00756304 -0.00681902 -0.00607499 -0.00533097 -0.00458695 -0.00384292
 -0.0030989  -0.00235488 -0.00161085 -0.00086683 -0.00012281]
[ 308  360  537  689  955  922 1093 1052 1116 1068] [-0.00756304 -0.00681902 -0.00607499 -0.00533097 -0.00458695 -0.00384292
 -0.0030989  -0.00235488 -0.00161085 -0.00086683 -0.00012281]
-2.45794
2.18323
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.87338
Epoch 1, cost is  4.80352
Epoch 2, cost is  4.82905
Epoch 3, cost is  4.88885
Epoch 4, cost is  4.95634
Training took 0.108465 minutes
Weight histogram
[2216 1633 1489  891 1211  390  124   85  756 1330] [-0.03255278 -0.02930978 -0.02606678 -0.02282379 -0.01958079 -0.01633779
 -0.0130948  -0.0098518  -0.0066088  -0.0033658  -0.00012281]
[ 967 1685  548  715  871  951 1106 1122 1111 1049] [-0.03255278 -0.02930978 -0.02606678 -0.02282379 -0.01958079 -0.01633779
 -0.0130948  -0.0098518  -0.0066088  -0.0033658  -0.00012281]
-4.97862
4.4801
fine tuning ...
Epoch 0
Fine tuning took 0.046856 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.047548 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045520 minutes
{0: [0.056650246305418719, 0.051724137931034482, 0.029556650246305417, 0.070197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8928571428571429, 0.91871921182266014, 0.93965517241379315, 0.89655172413793105], 5: [0.050492610837438424, 0.029556650246305417, 0.030788177339901478, 0.033251231527093597], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223218 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.01339
Epoch 1, cost is  2.87103
Epoch 2, cost is  2.80977
Epoch 3, cost is  2.78129
Epoch 4, cost is  2.75201
Training took 0.152614 minutes
Weight histogram
[ 819 2142 1586 1280  602  547  381  492  245    6] [-0.00756304 -0.00681902 -0.00607499 -0.00533097 -0.00458695 -0.00384292
 -0.0030989  -0.00235488 -0.00161085 -0.00086683 -0.00012281]
[ 308  360  537  689  955  922 1093 1052 1116 1068] [-0.00756304 -0.00681902 -0.00607499 -0.00533097 -0.00458695 -0.00384292
 -0.0030989  -0.00235488 -0.00161085 -0.00086683 -0.00012281]
-2.45794
2.18323
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.38286
Epoch 1, cost is  3.29745
Epoch 2, cost is  3.2719
Epoch 3, cost is  3.26985
Epoch 4, cost is  3.27141
Training took 0.150063 minutes
Weight histogram
[2124 1873 1449  622  976  813  151  381 1185  551] [-0.01900288 -0.01711487 -0.01522686 -0.01333886 -0.01145085 -0.00956284
 -0.00767483 -0.00578683 -0.00389882 -0.00201081 -0.00012281]
[ 618  845 1255 1031  987 1050 1075 1038 1118 1108] [-0.01900288 -0.01711487 -0.01522686 -0.01333886 -0.01145085 -0.00956284
 -0.00767483 -0.00578683 -0.00389882 -0.00201081 -0.00012281]
-2.78019
2.89954
fine tuning ...
Epoch 0
Fine tuning took 0.052167 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051997 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051767 minutes
{0: [0.087438423645320201, 0.10098522167487685, 0.12807881773399016, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.85467980295566504, 0.82635467980295563, 0.80295566502463056, 0.83128078817733986], 5: [0.057881773399014777, 0.072660098522167482, 0.068965517241379309, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223539 minutes
Weight histogram
[  88  409 1024 1496 1580 1655 1309  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 2599] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.01339
Epoch 1, cost is  2.87103
Epoch 2, cost is  2.80977
Epoch 3, cost is  2.78129
Epoch 4, cost is  2.75201
Training took 0.150368 minutes
Weight histogram
[ 819 2142 1586 1280  602  547  381  492  245    6] [-0.00756304 -0.00681902 -0.00607499 -0.00533097 -0.00458695 -0.00384292
 -0.0030989  -0.00235488 -0.00161085 -0.00086683 -0.00012281]
[ 308  360  537  689  955  922 1093 1052 1116 1068] [-0.00756304 -0.00681902 -0.00607499 -0.00533097 -0.00458695 -0.00384292
 -0.0030989  -0.00235488 -0.00161085 -0.00086683 -0.00012281]
-2.45794
2.18323
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.7447
Epoch 1, cost is  2.63353
Epoch 2, cost is  2.6049
Epoch 3, cost is  2.59083
Epoch 4, cost is  2.59186
Training took 0.206448 minutes
Weight histogram
[1555 2508 1536  700  837  695  760  713  761   60] [-0.01183702 -0.0106656  -0.00949418 -0.00832276 -0.00715134 -0.00597992
 -0.00480849 -0.00363707 -0.00246565 -0.00129423 -0.00012281]
[ 450  534  780 1192 1530 1271 1120 1092 1079 1077] [-0.01183702 -0.0106656  -0.00949418 -0.00832276 -0.00715134 -0.00597992
 -0.00480849 -0.00363707 -0.00246565 -0.00129423 -0.00012281]
-1.7947
1.87777
fine tuning ...
Epoch 0
Fine tuning took 0.055319 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056805 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055492 minutes
{0: [0.10344827586206896, 0.089901477832512317, 0.13054187192118227, 0.10467980295566502], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83743842364532017, 0.83866995073891626, 0.80295566502463056, 0.83251231527093594], 5: [0.059113300492610835, 0.071428571428571425, 0.066502463054187194, 0.062807881773399021], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222133 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.71487
Epoch 1, cost is  5.56382
Epoch 2, cost is  5.5315
Epoch 3, cost is  5.55821
Epoch 4, cost is  5.60506
Training took 0.083432 minutes
Weight histogram
[ 816 1416 2003 1344  721 1096 1678  917  130    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 361  481  673  893 1090 1288 1271 1228 1378 1462] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-5.48761
5.90762
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.18161
Epoch 1, cost is  4.06789
Epoch 2, cost is  4.04738
Epoch 3, cost is  4.04158
Epoch 4, cost is  4.04731
Training took 0.070033 minutes
Weight histogram
[1998 3214 2850 1351  261  123  451 1744  141   17] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
[ 622  791 1187 1316 1202 1237 1221 1249 1288 2037] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
-4.23618
5.17817
fine tuning ...
Epoch 0
Fine tuning took 0.030721 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033065 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.031791 minutes
{0: [0.036945812807881777, 0.045566502463054187, 0.04064039408866995, 0.067733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.94334975369458129, 0.93719211822660098, 0.93226600985221675, 0.89655172413793105], 5: [0.019704433497536946, 0.017241379310344827, 0.027093596059113302, 0.035714285714285712], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221836 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.71487
Epoch 1, cost is  5.56382
Epoch 2, cost is  5.5315
Epoch 3, cost is  5.55821
Epoch 4, cost is  5.60506
Training took 0.083397 minutes
Weight histogram
[ 816 1416 2003 1344  721 1096 1678  917  130    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 361  481  673  893 1090 1288 1271 1228 1378 1462] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-5.48761
5.90762
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.83203
Epoch 1, cost is  2.75035
Epoch 2, cost is  2.71838
Epoch 3, cost is  2.7007
Epoch 4, cost is  2.68922
Training took 0.085207 minutes
Weight histogram
[1076 4852 3160  413 1013 1050  411  153   13    9] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
[ 436  498  700  989 1411 1493 1800 1520 1597 1706] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
-2.48809
2.74291
fine tuning ...
Epoch 0
Fine tuning took 0.032593 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.035621 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033623 minutes
{0: [0.087438423645320201, 0.075123152709359611, 0.066502463054187194, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87315270935960587, 0.89778325123152714, 0.88916256157635465, 0.8928571428571429], 5: [0.039408866995073892, 0.027093596059113302, 0.044334975369458129, 0.020935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221723 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.71487
Epoch 1, cost is  5.56382
Epoch 2, cost is  5.5315
Epoch 3, cost is  5.55821
Epoch 4, cost is  5.60506
Training took 0.084981 minutes
Weight histogram
[ 816 1416 2003 1344  721 1096 1678  917  130    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 361  481  673  893 1090 1288 1271 1228 1378 1462] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-5.48761
5.90762
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.16766
Epoch 1, cost is  2.08128
Epoch 2, cost is  2.05268
Epoch 3, cost is  2.01982
Epoch 4, cost is  1.99635
Training took 0.106612 minutes
Weight histogram
[1469 4277 1574 2150 1408  763  267  180   55    7] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
[ 390  421  606  901 1252 1686 2180 2042 2188  484] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
-1.79752
1.71694
fine tuning ...
Epoch 0
Fine tuning took 0.037652 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036343 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036804 minutes
{0: [0.11330049261083744, 0.089901477832512317, 0.067733990147783252, 0.076354679802955669], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.85098522167487689, 0.8719211822660099, 0.8854679802955665, 0.8645320197044335], 5: [0.035714285714285712, 0.038177339901477834, 0.046798029556650245, 0.059113300492610835], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221646 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.17864
Epoch 1, cost is  4.02306
Epoch 2, cost is  3.9622
Epoch 3, cost is  3.95137
Epoch 4, cost is  3.95617
Training took 0.109745 minutes
Weight histogram
[ 650 2122 3123 1530 1389  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 368  481  690 1056 1132 1244 1238 1267 1338 1311] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-3.51176
3.53397
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.46658
Epoch 1, cost is  4.37135
Epoch 2, cost is  4.35237
Epoch 3, cost is  4.38307
Epoch 4, cost is  4.40493
Training took 0.084274 minutes
Weight histogram
[1366 4613 2243 1143  316  216  170  138 1669  276] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
[ 836 1336 1117  798 1051 1196 1326 1401 1479 1610] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
-4.98799
4.61369
fine tuning ...
Epoch 0
Fine tuning took 0.037747 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037725 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036484 minutes
{0: [0.059113300492610835, 0.064039408866995079, 0.073891625615763554, 0.05295566502463054], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88916256157635465, 0.89162561576354682, 0.88793103448275867, 0.89532019704433496], 5: [0.051724137931034482, 0.044334975369458129, 0.038177339901477834, 0.051724137931034482], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222636 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.17864
Epoch 1, cost is  4.02306
Epoch 2, cost is  3.9622
Epoch 3, cost is  3.95137
Epoch 4, cost is  3.95617
Training took 0.108900 minutes
Weight histogram
[ 650 2122 3123 1530 1389  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 368  481  690 1056 1132 1244 1238 1267 1338 1311] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-3.51176
3.53397
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.22478
Epoch 1, cost is  3.1061
Epoch 2, cost is  3.07423
Epoch 3, cost is  3.0671
Epoch 4, cost is  3.07383
Training took 0.108549 minutes
Weight histogram
[3729 2479 1525 1292  628  248  861  857  516   15] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
[ 572  754 1072 1511 1286 1352 1329 1351 1437 1486] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
-3.47017
3.61015
fine tuning ...
Epoch 0
Fine tuning took 0.041219 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040936 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040307 minutes
{0: [0.096059113300492605, 0.082512315270935957, 0.10221674876847291, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8571428571428571, 0.86945812807881773, 0.85591133004926112, 0.85467980295566504], 5: [0.046798029556650245, 0.048029556650246302, 0.041871921182266007, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222910 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.17864
Epoch 1, cost is  4.02306
Epoch 2, cost is  3.9622
Epoch 3, cost is  3.95137
Epoch 4, cost is  3.95617
Training took 0.108916 minutes
Weight histogram
[ 650 2122 3123 1530 1389  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 368  481  690 1056 1132 1244 1238 1267 1338 1311] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-3.51176
3.53397
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.34109
Epoch 1, cost is  2.23821
Epoch 2, cost is  2.20424
Epoch 3, cost is  2.18473
Epoch 4, cost is  2.17174
Training took 0.150278 minutes
Weight histogram
[3748 2472 1135 1124  895 1224  863  603   76   10] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
[ 425  530  723 1141 1604 1842 1695 1364 1418 1408] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
-2.11075
2.21973
fine tuning ...
Epoch 0
Fine tuning took 0.044321 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.044683 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.044445 minutes
{0: [0.11822660098522167, 0.083743842364532015, 0.061576354679802957, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82389162561576357, 0.85591133004926112, 0.89162561576354682, 0.84482758620689657], 5: [0.057881773399014777, 0.060344827586206899, 0.046798029556650245, 0.062807881773399021], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223413 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.98859
Epoch 1, cost is  2.80622
Epoch 2, cost is  2.74586
Epoch 3, cost is  2.70324
Epoch 4, cost is  2.68534
Training took 0.152191 minutes
Weight histogram
[1512 3371 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 359  470  709 1022 1080 1254 1245 1292 1350 1344] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-2.81417
2.46753
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.78257
Epoch 1, cost is  4.68445
Epoch 2, cost is  4.68735
Epoch 3, cost is  4.71794
Epoch 4, cost is  4.75933
Training took 0.108702 minutes
Weight histogram
[3798 2054 1375  880 1299  442  125   88  711 1378] [-0.03304872 -0.02975613 -0.02646354 -0.02317095 -0.01987836 -0.01658576
 -0.01329317 -0.01000058 -0.00670799 -0.0034154  -0.00012281]
[1188 1613  716  936 1063 1252 1289 1272 1359 1462] [-0.03304872 -0.02975613 -0.02646354 -0.02317095 -0.01987836 -0.01658576
 -0.01329317 -0.01000058 -0.00670799 -0.0034154  -0.00012281]
-5.52551
5.16395
fine tuning ...
Epoch 0
Fine tuning took 0.045397 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046171 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046678 minutes
{0: [0.072660098522167482, 0.064039408866995079, 0.060344827586206899, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8854679802955665, 0.88423645320197042, 0.88300492610837433, 0.87438423645320196], 5: [0.041871921182266007, 0.051724137931034482, 0.056650246305418719, 0.064039408866995079], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.225478 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.98859
Epoch 1, cost is  2.80622
Epoch 2, cost is  2.74586
Epoch 3, cost is  2.70324
Epoch 4, cost is  2.68534
Training took 0.151970 minutes
Weight histogram
[1512 3371 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 359  470  709 1022 1080 1254 1245 1292 1350 1344] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-2.81417
2.46753
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.4003
Epoch 1, cost is  3.30759
Epoch 2, cost is  3.28876
Epoch 3, cost is  3.29472
Epoch 4, cost is  3.30681
Training took 0.149931 minutes
Weight histogram
[2300 2967 1554 1227  680 1010  270  268 1264  610] [-0.02016124 -0.01815739 -0.01615355 -0.01414971 -0.01214586 -0.01014202
 -0.00813818 -0.00613434 -0.00413049 -0.00212665 -0.00012281]
[ 742 1114 1518 1054 1218 1257 1239 1308 1366 1334] [-0.02016124 -0.01815739 -0.01615355 -0.01414971 -0.01214586 -0.01014202
 -0.00813818 -0.00613434 -0.00413049 -0.00212665 -0.00012281]
-3.20361
3.07688
fine tuning ...
Epoch 0
Fine tuning took 0.050465 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050923 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052085 minutes
{0: [0.099753694581280791, 0.084975369458128072, 0.12315270935960591, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83251231527093594, 0.8423645320197044, 0.82266009852216748, 0.77463054187192115], 5: [0.067733990147783252, 0.072660098522167482, 0.054187192118226604, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221031 minutes
Weight histogram
[ 100  592 1514 2264 2067 1739 1310  415  101   23] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
[ 128  166  250  341  520  747  614  957 1778 4624] [ -5.06134937e-04  -3.84821597e-04  -2.63508258e-04  -1.42194919e-04
  -2.08815793e-05   1.00431760e-04   2.21745099e-04   3.43058439e-04
   4.64371778e-04   5.85685117e-04   7.06998457e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.98859
Epoch 1, cost is  2.80622
Epoch 2, cost is  2.74586
Epoch 3, cost is  2.70324
Epoch 4, cost is  2.68534
Training took 0.150293 minutes
Weight histogram
[1512 3371 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 359  470  709 1022 1080 1254 1245 1292 1350 1344] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-2.81417
2.46753
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.67675
Epoch 1, cost is  2.57998
Epoch 2, cost is  2.54328
Epoch 3, cost is  2.53249
Epoch 4, cost is  2.52555
Training took 0.205524 minutes
Weight histogram
[2559 2592 1780 1164  801  765  768  814  807  100] [-0.0125279  -0.01128739 -0.01004688 -0.00880637 -0.00756586 -0.00632535
 -0.00508485 -0.00384434 -0.00260383 -0.00136332 -0.00012281]
[ 540  692 1140 1721 1595 1313 1307 1264 1302 1276] [-0.0125279  -0.01128739 -0.01004688 -0.00880637 -0.00756586 -0.00632535
 -0.00508485 -0.00384434 -0.00260383 -0.00136332 -0.00012281]
-2.16209
2.52398
fine tuning ...
Epoch 0
Fine tuning took 0.055874 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056529 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055641 minutes
{0: [0.12438423645320197, 0.082512315270935957, 0.11945812807881774, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80172413793103448, 0.84113300492610843, 0.80665024630541871, 0.76354679802955661], 5: [0.073891625615763554, 0.076354679802955669, 0.073891625615763554, 0.081280788177339899], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222189 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.97796
Epoch 1, cost is  5.80435
Epoch 2, cost is  5.78286
Epoch 3, cost is  5.80915
Epoch 4, cost is  5.85721
Training took 0.086251 minutes
Weight histogram
[ 816 1416 2003 1344  721 1121 2437 2083  205    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 424  603  821 1197 1407 1471 1410 1626 1656 1535] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-6.29787
6.7548
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.35752
Epoch 1, cost is  4.26355
Epoch 2, cost is  4.26324
Epoch 3, cost is  4.24529
Epoch 4, cost is  4.2863
Training took 0.069380 minutes
Weight histogram
[1998 3214 2893 2644  950  123  451 1744  141   17] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
[ 699  954 1437 1401 1352 1397 1395 1430 2257 1853] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
-4.71379
5.66124
fine tuning ...
Epoch 0
Fine tuning took 0.032188 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033039 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.031947 minutes
{0: [0.033251231527093597, 0.043103448275862072, 0.077586206896551727, 0.049261083743842367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.93103448275862066, 0.91871921182266014, 0.89655172413793105, 0.91133004926108374], 5: [0.035714285714285712, 0.038177339901477834, 0.025862068965517241, 0.039408866995073892], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222462 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.97796
Epoch 1, cost is  5.80435
Epoch 2, cost is  5.78286
Epoch 3, cost is  5.80915
Epoch 4, cost is  5.85721
Training took 0.084790 minutes
Weight histogram
[ 816 1416 2003 1344  721 1121 2437 2083  205    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 424  603  821 1197 1407 1471 1410 1626 1656 1535] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-6.29787
6.7548
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.86657
Epoch 1, cost is  2.80097
Epoch 2, cost is  2.78103
Epoch 3, cost is  2.76977
Epoch 4, cost is  2.78047
Training took 0.084752 minutes
Weight histogram
[1076 5008 4765  677 1013 1050  411  153   13    9] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
[ 481  609  868 1365 1604 1972 1740 1779 1928 1829] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
-2.90162
2.94143
fine tuning ...
Epoch 0
Fine tuning took 0.032407 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033700 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.035125 minutes
{0: [0.075123152709359611, 0.094827586206896547, 0.088669950738916259, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8854679802955665, 0.85098522167487689, 0.87068965517241381, 0.85098522167487689], 5: [0.039408866995073892, 0.054187192118226604, 0.04064039408866995, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222257 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.97796
Epoch 1, cost is  5.80435
Epoch 2, cost is  5.78286
Epoch 3, cost is  5.80915
Epoch 4, cost is  5.85721
Training took 0.083262 minutes
Weight histogram
[ 816 1416 2003 1344  721 1121 2437 2083  205    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 424  603  821 1197 1407 1471 1410 1626 1656 1535] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-6.29787
6.7548
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.02239
Epoch 1, cost is  1.95091
Epoch 2, cost is  1.91964
Epoch 3, cost is  1.9115
Epoch 4, cost is  1.88549
Training took 0.106719 minutes
Weight histogram
[1480 5562 2303 2150 1408  763  267  180   55    7] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
[ 392  424  614  916 1261 1744 2176 2057 2197 2394] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
-1.96669
1.97224
fine tuning ...
Epoch 0
Fine tuning took 0.037652 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036388 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036220 minutes
{0: [0.10221674876847291, 0.094827586206896547, 0.084975369458128072, 0.098522167487684734], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83743842364532017, 0.86576354679802958, 0.86576354679802958, 0.85098522167487689], 5: [0.060344827586206899, 0.039408866995073892, 0.049261083743842367, 0.050492610837438424], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222210 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.31151
Epoch 1, cost is  4.17159
Epoch 2, cost is  4.14025
Epoch 3, cost is  4.14553
Epoch 4, cost is  4.1701
Training took 0.112266 minutes
Weight histogram
[ 650 2122 4122 2541 1404  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 437  607  913 1366 1405 1423 1476 1560 1518 1445] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-4.10734
3.91678
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.08381
Epoch 1, cost is  4.98496
Epoch 2, cost is  4.98095
Epoch 3, cost is  5.01313
Epoch 4, cost is  5.08052
Training took 0.085415 minutes
Weight histogram
[1366 4625 3512 1887  316  216  170  138 1669  276] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
[ 989 1703  919 1087 1275 1521 1598 1743 1782 1558] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
-5.45396
5.52838
fine tuning ...
Epoch 0
Fine tuning took 0.037782 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036826 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037610 minutes
{0: [0.084975369458128072, 0.071428571428571425, 0.062807881773399021, 0.064039408866995079], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87315270935960587, 0.88054187192118227, 0.9076354679802956, 0.90270935960591137], 5: [0.041871921182266007, 0.048029556650246302, 0.029556650246305417, 0.033251231527093597], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221680 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.31151
Epoch 1, cost is  4.17159
Epoch 2, cost is  4.14025
Epoch 3, cost is  4.14553
Epoch 4, cost is  4.1701
Training took 0.110597 minutes
Weight histogram
[ 650 2122 4122 2541 1404  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 437  607  913 1366 1405 1423 1476 1560 1518 1445] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-4.10734
3.91678
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.32499
Epoch 1, cost is  3.21636
Epoch 2, cost is  3.19062
Epoch 3, cost is  3.19866
Epoch 4, cost is  3.2001
Training took 0.110907 minutes
Weight histogram
[4405 3828 1525 1292  628  248  861  857  516   15] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
[ 674  940 1413 1686 1507 1546 1547 1695 1655 1512] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
-4.06002
4.10657
fine tuning ...
Epoch 0
Fine tuning took 0.038911 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040287 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040673 minutes
{0: [0.14039408866995073, 0.10960591133004927, 0.11083743842364532, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77955665024630538, 0.83866995073891626, 0.82266009852216748, 0.85591133004926112], 5: [0.080049261083743842, 0.051724137931034482, 0.066502463054187194, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221667 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.31151
Epoch 1, cost is  4.17159
Epoch 2, cost is  4.14025
Epoch 3, cost is  4.14553
Epoch 4, cost is  4.1701
Training took 0.108446 minutes
Weight histogram
[ 650 2122 4122 2541 1404  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 437  607  913 1366 1405 1423 1476 1560 1518 1445] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-4.10734
3.91678
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.39007
Epoch 1, cost is  2.29308
Epoch 2, cost is  2.26151
Epoch 3, cost is  2.23654
Epoch 4, cost is  2.21979
Training took 0.151997 minutes
Weight histogram
[4755 3490 1135 1124  895 1224  863  603   76   10] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
[ 496  652 1002 1665 1990 2057 1574 1653 1592 1494] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
-2.52472
2.55939
fine tuning ...
Epoch 0
Fine tuning took 0.045744 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046600 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045276 minutes
{0: [0.16502463054187191, 0.15024630541871922, 0.17980295566502463, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77586206896551724, 0.78817733990147787, 0.76477832512315269, 0.82635467980295563], 5: [0.059113300492610835, 0.061576354679802957, 0.055418719211822662, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222033 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.04548
Epoch 1, cost is  2.87664
Epoch 2, cost is  2.79747
Epoch 3, cost is  2.77291
Epoch 4, cost is  2.74573
Training took 0.149897 minutes
Weight histogram
[2560 4348 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 420  601  909 1280 1373 1430 1488 1567 1558 1524] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.15148
2.72288
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.37839
Epoch 1, cost is  5.26305
Epoch 2, cost is  5.26061
Epoch 3, cost is  5.30431
Epoch 4, cost is  5.37283
Training took 0.106728 minutes
Weight histogram
[3152 4191 1141 1580 1034  674  191  111  587 1514] [-0.03483066 -0.03135988 -0.02788909 -0.02441831 -0.02094752 -0.01747674
 -0.01400595 -0.01053516 -0.00706438 -0.00359359 -0.00012281]
[1487 1518  931 1184 1397 1500 1465 1636 1641 1416] [-0.03483066 -0.03135988 -0.02788909 -0.02441831 -0.02094752 -0.01747674
 -0.01400595 -0.01053516 -0.00706438 -0.00359359 -0.00012281]
-6.62623
6.1577
fine tuning ...
Epoch 0
Fine tuning took 0.046404 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046319 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046563 minutes
{0: [0.097290640394088676, 0.083743842364532015, 0.073891625615763554, 0.072660098522167482], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82019704433497542, 0.83497536945812811, 0.8571428571428571, 0.85591133004926112], 5: [0.082512315270935957, 0.081280788177339899, 0.068965517241379309, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221331 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.04548
Epoch 1, cost is  2.87664
Epoch 2, cost is  2.79747
Epoch 3, cost is  2.77291
Epoch 4, cost is  2.74573
Training took 0.150104 minutes
Weight histogram
[2560 4348 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 420  601  909 1280 1373 1430 1488 1567 1558 1524] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.15148
2.72288
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.58739
Epoch 1, cost is  3.4762
Epoch 2, cost is  3.4541
Epoch 3, cost is  3.4646
Epoch 4, cost is  3.47658
Training took 0.149641 minutes
Weight histogram
[3419 3512 1521 1560  534 1088  377  186 1338  640] [-0.02081917 -0.01874954 -0.0166799  -0.01461026 -0.01254063 -0.01047099
 -0.00840135 -0.00633172 -0.00426208 -0.00219244 -0.00012281]
[ 889 1461 1490 1388 1472 1443 1524 1603 1495 1410] [-0.02081917 -0.01874954 -0.0166799  -0.01461026 -0.01254063 -0.01047099
 -0.00840135 -0.00633172 -0.00426208 -0.00219244 -0.00012281]
-3.61246
3.48846
fine tuning ...
Epoch 0
Fine tuning took 0.051835 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051233 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051033 minutes
{0: [0.11330049261083744, 0.1145320197044335, 0.10221674876847291, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81157635467980294, 0.80172413793103448, 0.81896551724137934, 0.80418719211822665], 5: [0.075123152709359611, 0.083743842364532015, 0.078817733990147784, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221733 minutes
Weight histogram
[ 174  943 2309 2557 2328 2019 1092  600  117   11] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
[ 132  173  269  347  570  743  669 1310 2140 5797] [ -5.06134937e-04  -3.62940127e-04  -2.19745317e-04  -7.65505072e-05
   6.66443026e-05   2.09839112e-04   3.53033922e-04   4.96228732e-04
   6.39423542e-04   7.82618352e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.04548
Epoch 1, cost is  2.87664
Epoch 2, cost is  2.79747
Epoch 3, cost is  2.77291
Epoch 4, cost is  2.74573
Training took 0.149859 minutes
Weight histogram
[2560 4348 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 420  601  909 1280 1373 1430 1488 1567 1558 1524] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.15148
2.72288
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.73649
Epoch 1, cost is  2.61415
Epoch 2, cost is  2.5822
Epoch 3, cost is  2.56863
Epoch 4, cost is  2.57214
Training took 0.206282 minutes
Weight histogram
[3143 3221 2148 1581  656  828  732  905  822  139] [-0.01302687 -0.01173646 -0.01044605 -0.00915565 -0.00786524 -0.00657484
 -0.00528443 -0.00399402 -0.00270362 -0.00141321 -0.00012281]
[ 640  917 1677 2081 1521 1529 1483 1528 1429 1370] [-0.01302687 -0.01173646 -0.01044605 -0.00915565 -0.00786524 -0.00657484
 -0.00528443 -0.00399402 -0.00270362 -0.00141321 -0.00012281]
-2.58294
2.94769
fine tuning ...
Epoch 0
Fine tuning took 0.056759 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056741 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057117 minutes
{0: [0.10221674876847291, 0.13177339901477833, 0.12561576354679804, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79187192118226601, 0.79802955665024633, 0.78694581280788178, 0.76108374384236455], 5: [0.10591133004926108, 0.070197044334975367, 0.087438423645320201, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222410 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.31534
Epoch 1, cost is  6.14202
Epoch 2, cost is  6.1078
Epoch 3, cost is  6.13708
Epoch 4, cost is  6.18593
Training took 0.084380 minutes
Weight histogram
[ 816 1416 2003 1344  721 1263 3398 2866  344    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 482  722 1035 1402 1668 1599 1706 1879 1737 1945] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-6.95623
6.7548
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.82057
Epoch 1, cost is  4.67839
Epoch 2, cost is  4.68913
Epoch 3, cost is  4.70114
Epoch 4, cost is  4.73416
Training took 0.069923 minutes
Weight histogram
[1998 3214 3340 4036 1136  123  451 1744  141   17] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
[ 787 1167 1710 1448 1547 1532 1590 2354 2103 1962] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
-6.22048
7.42666
fine tuning ...
Epoch 0
Fine tuning took 0.030656 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.032716 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.031725 minutes
{0: [0.043103448275862072, 0.05295566502463054, 0.051724137931034482, 0.070197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.92980295566502458, 0.91871921182266014, 0.93103448275862066, 0.90147783251231528], 5: [0.027093596059113302, 0.02832512315270936, 0.017241379310344827, 0.02832512315270936], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222428 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.31534
Epoch 1, cost is  6.14202
Epoch 2, cost is  6.1078
Epoch 3, cost is  6.13708
Epoch 4, cost is  6.18593
Training took 0.083508 minutes
Weight histogram
[ 816 1416 2003 1344  721 1263 3398 2866  344    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 482  722 1035 1402 1668 1599 1706 1879 1737 1945] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-6.95623
6.7548
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.7283
Epoch 1, cost is  2.64505
Epoch 2, cost is  2.62396
Epoch 3, cost is  2.61439
Epoch 4, cost is  2.60618
Training took 0.083923 minutes
Weight histogram
[1076 5046 5965 1464 1013 1050  411  153   13    9] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
[ 529  713 1044 1669 1877 2097 1872 2147 2038 2214] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
-3.27841
3.31517
fine tuning ...
Epoch 0
Fine tuning took 0.032466 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.035289 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.034062 minutes
{0: [0.080049261083743842, 0.088669950738916259, 0.078817733990147784, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.89532019704433496, 0.87684729064039413, 0.87438423645320196, 0.84975369458128081], 5: [0.024630541871921183, 0.034482758620689655, 0.046798029556650245, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221971 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.31534
Epoch 1, cost is  6.14202
Epoch 2, cost is  6.1078
Epoch 3, cost is  6.13708
Epoch 4, cost is  6.18593
Training took 0.083284 minutes
Weight histogram
[ 816 1416 2003 1344  721 1263 3398 2866  344    4] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
[ 482  722 1035 1402 1668 1599 1706 1879 1737 1945] [-0.00881309 -0.00790538 -0.00699766 -0.00608995 -0.00518223 -0.00427452
 -0.0033668  -0.00245909 -0.00155137 -0.00064366  0.00026406]
-6.95623
6.7548
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.07235
Epoch 1, cost is  2.01293
Epoch 2, cost is  1.98465
Epoch 3, cost is  1.95883
Epoch 4, cost is  1.93953
Training took 0.108081 minutes
Weight histogram
[1521 7503 2346 2150 1408  763  267  180   55    7] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
[ 422  491  754 1137 1517 2275 2321 2359 2595 2329] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
-2.18443
2.19327
fine tuning ...
Epoch 0
Fine tuning took 0.037796 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036111 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037031 minutes
{0: [0.065270935960591137, 0.097290640394088676, 0.10344827586206896, 0.083743842364532015], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91009852216748766, 0.86083743842364535, 0.85098522167487689, 0.86576354679802958], 5: [0.024630541871921183, 0.041871921182266007, 0.045566502463054187, 0.050492610837438424], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222118 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.48198
Epoch 1, cost is  4.29023
Epoch 2, cost is  4.2193
Epoch 3, cost is  4.20218
Epoch 4, cost is  4.21052
Training took 0.111661 minutes
Weight histogram
[ 650 2122 4453 4136 1503  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 501  749 1264 1491 1649 1660 1745 1715 1668 1733] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-4.74838
4.85024
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.42974
Epoch 1, cost is  5.25028
Epoch 2, cost is  5.21301
Epoch 3, cost is  5.23509
Epoch 4, cost is  5.26428
Training took 0.083516 minutes
Weight histogram
[1366 5367 4795 1887  316  216  170  138 1669  276] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
[1139 1865  929 1327 1597 1747 1848 2035 1769 1944] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
-6.12192
6.23369
fine tuning ...
Epoch 0
Fine tuning took 0.036007 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037534 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037267 minutes
{0: [0.073891625615763554, 0.076354679802955669, 0.080049261083743842, 0.094827586206896547], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90147783251231528, 0.88177339901477836, 0.86822660098522164, 0.8645320197044335], 5: [0.024630541871921183, 0.041871921182266007, 0.051724137931034482, 0.04064039408866995], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221929 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.48198
Epoch 1, cost is  4.29023
Epoch 2, cost is  4.2193
Epoch 3, cost is  4.20218
Epoch 4, cost is  4.21052
Training took 0.110324 minutes
Weight histogram
[ 650 2122 4453 4136 1503  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 501  749 1264 1491 1649 1660 1745 1715 1668 1733] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-4.74838
4.85024
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.60669
Epoch 1, cost is  3.48593
Epoch 2, cost is  3.45726
Epoch 3, cost is  3.4622
Epoch 4, cost is  3.48706
Training took 0.109772 minutes
Weight histogram
[5698 4560 1525 1292  628  248  861  857  516   15] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
[ 780 1158 1840 1710 1781 1760 1876 1879 1742 1674] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
-4.56014
4.63164
fine tuning ...
Epoch 0
Fine tuning took 0.039051 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.039739 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040046 minutes
{0: [0.14655172413793102, 0.078817733990147784, 0.068965517241379309, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79802955665024633, 0.85591133004926112, 0.8719211822660099, 0.80911330049261088], 5: [0.055418719211822662, 0.065270935960591137, 0.059113300492610835, 0.045566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221257 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.48198
Epoch 1, cost is  4.29023
Epoch 2, cost is  4.2193
Epoch 3, cost is  4.20218
Epoch 4, cost is  4.21052
Training took 0.108289 minutes
Weight histogram
[ 650 2122 4453 4136 1503  552  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 501  749 1264 1491 1649 1660 1745 1715 1668 1733] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-4.74838
4.85024
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.26568
Epoch 1, cost is  2.16175
Epoch 2, cost is  2.1315
Epoch 3, cost is  2.10869
Epoch 4, cost is  2.09683
Training took 0.151347 minutes
Weight histogram
[6594 3676 1135 1124  895 1224  863  603   76   10] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
[ 554  775 1275 2056 2414 1871 1794 1795 1680 1986] [-0.00977659 -0.00881599 -0.00785539 -0.0068948  -0.0059342  -0.0049736
 -0.004013   -0.0030524  -0.0020918  -0.0011312  -0.0001706 ]
-2.80651
2.97145
fine tuning ...
Epoch 0
Fine tuning took 0.044382 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.044396 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046344 minutes
{0: [0.13300492610837439, 0.10467980295566502, 0.13177339901477833, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80665024630541871, 0.83990147783251234, 0.80172413793103448, 0.7931034482758621], 5: [0.060344827586206899, 0.055418719211822662, 0.066502463054187194, 0.062807881773399021], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221266 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.05939
Epoch 1, cost is  2.88598
Epoch 2, cost is  2.80922
Epoch 3, cost is  2.75963
Epoch 4, cost is  2.73867
Training took 0.149619 minutes
Weight histogram
[2957 5976 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 476  737 1192 1392 1632 1647 1712 1747 1728 1912] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.53007
3.07026
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.88885
Epoch 1, cost is  5.7082
Epoch 2, cost is  5.67791
Epoch 3, cost is  5.70751
Epoch 4, cost is  5.75736
Training took 0.108531 minutes
Weight histogram
[5177 4191 1141 1580 1034  674  191  111  587 1514] [-0.03483066 -0.03135988 -0.02788909 -0.02441831 -0.02094752 -0.01747674
 -0.01400595 -0.01053516 -0.00706438 -0.00359359 -0.00012281]
[1804 1422 1179 1423 1694 1697 1803 1849 1657 1672] [-0.03483066 -0.03135988 -0.02788909 -0.02441831 -0.02094752 -0.01747674
 -0.01400595 -0.01053516 -0.00706438 -0.00359359 -0.00012281]
-7.16483
6.93255
fine tuning ...
Epoch 0
Fine tuning took 0.045126 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046567 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046119 minutes
{0: [0.1206896551724138, 0.086206896551724144, 0.064039408866995079, 0.087438423645320201], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78201970443349755, 0.8645320197044335, 0.85467980295566504, 0.83990147783251234], 5: [0.097290640394088676, 0.049261083743842367, 0.081280788177339899, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222237 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.05939
Epoch 1, cost is  2.88598
Epoch 2, cost is  2.80922
Epoch 3, cost is  2.75963
Epoch 4, cost is  2.73867
Training took 0.151837 minutes
Weight histogram
[2957 5976 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 476  737 1192 1392 1632 1647 1712 1747 1728 1912] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.53007
3.07026
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.70974
Epoch 1, cost is  3.59096
Epoch 2, cost is  3.56357
Epoch 3, cost is  3.57451
Epoch 4, cost is  3.58673
Training took 0.151168 minutes
Weight histogram
[3177 4103 2717 1567  639 1277  517  170 1306  727] [-0.02203394 -0.01984283 -0.01765172 -0.0154606  -0.01326949 -0.01107838
 -0.00888726 -0.00669615 -0.00450503 -0.00231392 -0.00012281]
[1044 1863 1511 1653 1631 1723 1817 1700 1639 1619] [-0.02203394 -0.01984283 -0.01765172 -0.0154606  -0.01326949 -0.01107838
 -0.00888726 -0.00669615 -0.00450503 -0.00231392 -0.00012281]
-4.3276
4.1037
fine tuning ...
Epoch 0
Fine tuning took 0.051390 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051747 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050749 minutes
{0: [0.14408866995073891, 0.1206896551724138, 0.11330049261083744, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.76354679802955661, 0.74753694581280783, 0.72290640394088668], 5: [0.1268472906403941, 0.11576354679802955, 0.13916256157635468, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221173 minutes
Weight histogram
[ 147  912  801 1207 3202 3275 2784 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 134  177  271  369  584  727  690 1504 2248 7471] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.32594
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.05939
Epoch 1, cost is  2.88598
Epoch 2, cost is  2.80922
Epoch 3, cost is  2.75963
Epoch 4, cost is  2.73867
Training took 0.149876 minutes
Weight histogram
[2957 5976 1603 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 476  737 1192 1392 1632 1647 1712 1747 1728 1912] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.53007
3.07026
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.62557
Epoch 1, cost is  2.5101
Epoch 2, cost is  2.46913
Epoch 3, cost is  2.4557
Epoch 4, cost is  2.44887
Training took 0.206185 minutes
Weight histogram
[4040 3437 2731 1771  619  952  691  954  839  166] [-0.01340263 -0.01207465 -0.01074667 -0.00941869 -0.0080907  -0.00676272
 -0.00543474 -0.00410675 -0.00277877 -0.00145079 -0.00012281]
[ 721 1147 2120 2019 1722 1683 1704 1615 1570 1899] [-0.01340263 -0.01207465 -0.01074667 -0.00941869 -0.0080907  -0.00676272
 -0.00543474 -0.00410675 -0.00277877 -0.00145079 -0.00012281]
-2.84179
3.17877
fine tuning ...
Epoch 0
Fine tuning took 0.054967 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056618 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056198 minutes
{0: [0.16995073891625614, 0.13423645320197045, 0.18226600985221675, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.73522167487684731, 0.70935960591133007, 0.71305418719211822], 5: [0.12561576354679804, 0.13054187192118227, 0.10837438423645321, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221773 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.84773
Epoch 1, cost is  6.63683
Epoch 2, cost is  6.5607
Epoch 3, cost is  6.57231
Epoch 4, cost is  6.60713
Training took 0.082910 minutes
Weight histogram
[1403 2557 1761 1464 4536 2427   67   34 1368  583] [-0.00881309 -0.00751026 -0.00620743 -0.0049046  -0.00360177 -0.00229894
 -0.00099611  0.00030671  0.00160954  0.00291237  0.0042152 ]
[ 548  846 1287 1681 1830 1715 2132 1940 2150 2071] [-0.00881309 -0.00751026 -0.00620743 -0.0049046  -0.00360177 -0.00229894
 -0.00099611  0.00030671  0.00160954  0.00291237  0.0042152 ]
-7.20438
7.97132
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.77705
Epoch 1, cost is  4.65574
Epoch 2, cost is  4.65274
Epoch 3, cost is  4.63482
Epoch 4, cost is  4.64675
Training took 0.072524 minutes
Weight histogram
[1998 3214 3347 4642 2423  248  451 1744  141   17] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
[ 868 1375 1764 1665 1686 1711 2268 2374 2179 2335] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
-6.57666
7.74335
fine tuning ...
Epoch 0
Fine tuning took 0.033104 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.031740 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.030904 minutes
{0: [0.045566502463054187, 0.067733990147783252, 0.072660098522167482, 0.045566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91871921182266014, 0.88669950738916259, 0.89532019704433496, 0.8780788177339901], 5: [0.035714285714285712, 0.045566502463054187, 0.032019704433497539, 0.076354679802955669], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221343 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.84773
Epoch 1, cost is  6.63683
Epoch 2, cost is  6.5607
Epoch 3, cost is  6.57231
Epoch 4, cost is  6.60713
Training took 0.083306 minutes
Weight histogram
[1403 2557 1761 1464 4536 2427   67   34 1368  583] [-0.00881309 -0.00751026 -0.00620743 -0.0049046  -0.00360177 -0.00229894
 -0.00099611  0.00030671  0.00160954  0.00291237  0.0042152 ]
[ 548  846 1287 1681 1830 1715 2132 1940 2150 2071] [-0.00881309 -0.00751026 -0.00620743 -0.0049046  -0.00360177 -0.00229894
 -0.00099611  0.00030671  0.00160954  0.00291237  0.0042152 ]
-7.20438
7.97132
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.00666
Epoch 1, cost is  2.89408
Epoch 2, cost is  2.87103
Epoch 3, cost is  2.87852
Epoch 4, cost is  2.86828
Training took 0.086594 minutes
Weight histogram
[1076 5046 5973 1482 1039 1651 1764  172   13    9] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
[ 582  844 1297 1937 2353 2062 2270 2249 2392 2239] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
-3.69479
3.63173
fine tuning ...
Epoch 0
Fine tuning took 0.032332 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033581 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033925 minutes
{0: [0.068965517241379309, 0.087438423645320201, 0.098522167487684734, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.88793103448275867, 0.85467980295566504, 0.83620689655172409, 0.81773399014778325], 5: [0.043103448275862072, 0.057881773399014777, 0.065270935960591137, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221325 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.84773
Epoch 1, cost is  6.63683
Epoch 2, cost is  6.5607
Epoch 3, cost is  6.57231
Epoch 4, cost is  6.60713
Training took 0.083161 minutes
Weight histogram
[1403 2557 1761 1464 4536 2427   67   34 1368  583] [-0.00881309 -0.00751026 -0.00620743 -0.0049046  -0.00360177 -0.00229894
 -0.00099611  0.00030671  0.00160954  0.00291237  0.0042152 ]
[ 548  846 1287 1681 1830 1715 2132 1940 2150 2071] [-0.00881309 -0.00751026 -0.00620743 -0.0049046  -0.00360177 -0.00229894
 -0.00099611  0.00030671  0.00160954  0.00291237  0.0042152 ]
-7.20438
7.97132
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.03902
Epoch 1, cost is  1.98229
Epoch 2, cost is  1.94588
Epoch 3, cost is  1.93004
Epoch 4, cost is  1.93133
Training took 0.106270 minutes
Weight histogram
[1521 7520 3934 2570 1408  763  267  180   55    7] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
[ 464  561  925 1388 2019 2601 2546 2772 2615 2334] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
-2.36847
2.35889
fine tuning ...
Epoch 0
Fine tuning took 0.037518 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036499 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037860 minutes
{0: [0.089901477832512317, 0.12561576354679804, 0.071428571428571425, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82389162561576357, 0.83497536945812811, 0.84359605911330049, 0.83743842364532017], 5: [0.086206896551724144, 0.039408866995073892, 0.084975369458128072, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.220976 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.98677
Epoch 1, cost is  4.7481
Epoch 2, cost is  4.65963
Epoch 3, cost is  4.63819
Epoch 4, cost is  4.64163
Training took 0.108410 minutes
Weight histogram
[ 650 2122 4453 4410 3103  703  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 575  916 1577 1761 1839 1902 1951 1862 1949 1868] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-5.73337
5.63605
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.67569
Epoch 1, cost is  5.5272
Epoch 2, cost is  5.50518
Epoch 3, cost is  5.516
Epoch 4, cost is  5.56978
Training took 0.084865 minutes
Weight histogram
[1366 6548 5580 1946  316  216  170  138 1669  276] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
[1319 1867 1191 1549 1883 1980 2303 1975 2177 1981] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
-7.03169
6.8269
fine tuning ...
Epoch 0
Fine tuning took 0.038106 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038109 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038492 minutes
{0: [0.094827586206896547, 0.13916256157635468, 0.15270935960591134, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.85221674876847286, 0.81034482758620685, 0.80418719211822665, 0.79187192118226601], 5: [0.05295566502463054, 0.050492610837438424, 0.043103448275862072, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221572 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.98677
Epoch 1, cost is  4.7481
Epoch 2, cost is  4.65963
Epoch 3, cost is  4.63819
Epoch 4, cost is  4.64163
Training took 0.108369 minutes
Weight histogram
[ 650 2122 4453 4410 3103  703  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 575  916 1577 1761 1839 1902 1951 1862 1949 1868] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-5.73337
5.63605
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.68455
Epoch 1, cost is  3.53867
Epoch 2, cost is  3.50838
Epoch 3, cost is  3.51252
Epoch 4, cost is  3.52202
Training took 0.108278 minutes
Weight histogram
[5933 6346 1529 1292  628  248  861  857  516   15] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
[ 896 1424 2142 1909 1975 1984 2190 1955 1893 1857] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
-4.96361
5.07379
fine tuning ...
Epoch 0
Fine tuning took 0.039100 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040368 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.039459 minutes
{0: [0.14039408866995073, 0.14901477832512317, 0.17364532019704434, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79187192118226601, 0.76477832512315269, 0.74014778325123154, 0.69581280788177335], 5: [0.067733990147783252, 0.086206896551724144, 0.086206896551724144, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221552 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.98677
Epoch 1, cost is  4.7481
Epoch 2, cost is  4.65963
Epoch 3, cost is  4.63819
Epoch 4, cost is  4.64163
Training took 0.108294 minutes
Weight histogram
[ 650 2122 4453 4410 3103  703  456  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 575  916 1577 1761 1839 1902 1951 1862 1949 1868] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-5.73337
5.63605
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.48814
Epoch 1, cost is  2.38178
Epoch 2, cost is  2.35817
Epoch 3, cost is  2.35929
Epoch 4, cost is  2.3517
Training took 0.151005 minutes
Weight histogram
[6847 5363 1132 1163  860 1240  904  620   86   10] [-0.0099112  -0.00893714 -0.00796308 -0.00698902 -0.00601496 -0.0050409
 -0.00406684 -0.00309278 -0.00211872 -0.00114466 -0.0001706 ]
[ 640  966 1828 2487 2460 1977 2065 1955 2131 1716] [-0.0099112  -0.00893714 -0.00796308 -0.00698902 -0.00601496 -0.0050409
 -0.00406684 -0.00309278 -0.00211872 -0.00114466 -0.0001706 ]
-3.16669
3.61715
fine tuning ...
Epoch 0
Fine tuning took 0.044486 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046505 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046079 minutes
{0: [0.14778325123152711, 0.11330049261083744, 0.15886699507389163, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75246305418719217, 0.77339901477832518, 0.73029556650246308, 0.72167487684729059], 5: [0.099753694581280791, 0.11330049261083744, 0.11083743842364532, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221647 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.29458
Epoch 1, cost is  3.11462
Epoch 2, cost is  3.03094
Epoch 3, cost is  2.99765
Epoch 4, cost is  2.98099
Training took 0.149721 minutes
Weight histogram
[3389 7522 1650 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 547  894 1463 1683 1824 1854 1974 1925 2141 1895] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.85892
3.47761
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.86446
Epoch 1, cost is  5.68327
Epoch 2, cost is  5.65712
Epoch 3, cost is  5.67803
Epoch 4, cost is  5.72516
Training took 0.108522 minutes
Weight histogram
[2211 7482 2254 1649  980 1186  223  135  403 1702] [-0.03736157 -0.03363769 -0.02991381 -0.02618994 -0.02246606 -0.01874219
 -0.01501831 -0.01129443 -0.00757056 -0.00384668 -0.00012281]
[2159 1319 1415 1761 1930 1900 2142 1857 1894 1848] [-0.03736157 -0.03363769 -0.02991381 -0.02618994 -0.02246606 -0.01874219
 -0.01501831 -0.01129443 -0.00757056 -0.00384668 -0.00012281]
-7.95733
7.18299
fine tuning ...
Epoch 0
Fine tuning took 0.046381 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045993 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.047603 minutes
{0: [0.10344827586206896, 0.11330049261083744, 0.15024630541871922, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8214285714285714, 0.78325123152709364, 0.78325123152709364, 0.7931034482758621], 5: [0.075123152709359611, 0.10344827586206896, 0.066502463054187194, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221597 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.29458
Epoch 1, cost is  3.11462
Epoch 2, cost is  3.03094
Epoch 3, cost is  2.99765
Epoch 4, cost is  2.98099
Training took 0.150514 minutes
Weight histogram
[3389 7522 1650 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 547  894 1463 1683 1824 1854 1974 1925 2141 1895] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.85892
3.47761
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.11204
Epoch 1, cost is  3.93364
Epoch 2, cost is  3.90161
Epoch 3, cost is  3.90762
Epoch 4, cost is  3.9292
Training took 0.152181 minutes
Weight histogram
[2257 4674 4022 1765 1408 1000  862  187 1211  839] [-0.02381115 -0.02144231 -0.01907348 -0.01670464 -0.01433581 -0.01196698
 -0.00959814 -0.00722931 -0.00486047 -0.00249164 -0.00012281]
[1237 2166 1679 1900 1933 2040 1956 1871 1803 1640] [-0.02381115 -0.02144231 -0.01907348 -0.01670464 -0.01433581 -0.01196698
 -0.00959814 -0.00722931 -0.00486047 -0.00249164 -0.00012281]
-5.06336
4.68845
fine tuning ...
Epoch 0
Fine tuning took 0.052586 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051622 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052025 minutes
{0: [0.26847290640394089, 0.2229064039408867, 0.19334975369458129, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.66133004926108374, 0.66256157635467983, 0.65024630541871919], 5: [0.1206896551724138, 0.11576354679802955, 0.14408866995073891, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221876 minutes
Weight histogram
[ 147  912  832 1561 4292 3805 2804 1365  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[ 139  188  285  399  679  706  800 1662 3364 7978] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.37173
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.29458
Epoch 1, cost is  3.11462
Epoch 2, cost is  3.03094
Epoch 3, cost is  2.99765
Epoch 4, cost is  2.98099
Training took 0.149633 minutes
Weight histogram
[3389 7522 1650 1318  620  563  378  494  260    6] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
[ 547  894 1463 1683 1824 1854 1974 1925 2141 1895] [-0.0076353  -0.00688405 -0.0061328  -0.00538155 -0.0046303  -0.00387905
 -0.0031278  -0.00237655 -0.00162531 -0.00087406 -0.00012281]
-3.85892
3.47761
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.62619
Epoch 1, cost is  2.51922
Epoch 2, cost is  2.4883
Epoch 3, cost is  2.47281
Epoch 4, cost is  2.4701
Training took 0.203880 minutes
Weight histogram
[3249 5309 2898 2051  932  950  703 1077  850  206] [-0.01411171 -0.01271282 -0.01131393 -0.00991504 -0.00851615 -0.00711726
 -0.00571837 -0.00431948 -0.00292059 -0.0015217  -0.00012281]
[ 836 1484 2590 1997 1917 1911 1849 1778 2100 1763] [-0.01411171 -0.01271282 -0.01131393 -0.00991504 -0.00851615 -0.00711726
 -0.00571837 -0.00431948 -0.00292059 -0.0015217  -0.00012281]
-3.06683
3.43813
fine tuning ...
Epoch 0
Fine tuning took 0.056334 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055514 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056708 minutes
{0: [0.19704433497536947, 0.18842364532019704, 0.21305418719211822, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.66133004926108374, 0.65147783251231528, 0.65886699507389157], 5: [0.19334975369458129, 0.15024630541871922, 0.1354679802955665, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221493 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.60794
Epoch 1, cost is  7.30083
Epoch 2, cost is  7.2162
Epoch 3, cost is  7.20758
Epoch 4, cost is  7.23297
Training took 0.086310 minutes
Weight histogram
[1727 3334 1447 3912 3728   70 1170 2593  177   67] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
[ 615  985 1547 1963 1947 2174 2185 2310 2277 2222] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
-7.99758
8.28356
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.33716
Epoch 1, cost is  5.19954
Epoch 2, cost is  5.19929
Epoch 3, cost is  5.19215
Epoch 4, cost is  5.23321
Training took 0.071036 minutes
Weight histogram
[1998 3214 3347 4648 2615 1839  687 1744  141   17] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
[ 949 1574 1877 1797 1807 1873 2790 2400 2453 2730] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
-6.57666
7.74335
fine tuning ...
Epoch 0
Fine tuning took 0.032375 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033778 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033568 minutes
{0: [0.10960591133004927, 0.078817733990147784, 0.13054187192118227, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81896551724137934, 0.86699507389162567, 0.83497536945812811, 0.83866995073891626], 5: [0.071428571428571425, 0.054187192118226604, 0.034482758620689655, 0.038177339901477834], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221962 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.60794
Epoch 1, cost is  7.30083
Epoch 2, cost is  7.2162
Epoch 3, cost is  7.20758
Epoch 4, cost is  7.23297
Training took 0.083250 minutes
Weight histogram
[1727 3334 1447 3912 3728   70 1170 2593  177   67] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
[ 615  985 1547 1963 1947 2174 2185 2310 2277 2222] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
-7.99758
8.28356
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.03997
Epoch 1, cost is  2.91616
Epoch 2, cost is  2.89938
Epoch 3, cost is  2.85507
Epoch 4, cost is  2.85167
Training took 0.085052 minutes
Weight histogram
[1076 5046 5973 1482 1039 2938 2478  196   13    9] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
[ 637  949 1584 2143 2497 2225 2571 2533 2462 2649] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
-4.03554
3.84038
fine tuning ...
Epoch 0
Fine tuning took 0.032530 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.034740 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.034897 minutes
{0: [0.11822660098522167, 0.099753694581280791, 0.14039408866995073, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81896551724137934, 0.82512315270935965, 0.79187192118226601, 0.82266009852216748], 5: [0.062807881773399021, 0.075123152709359611, 0.067733990147783252, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221622 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.60794
Epoch 1, cost is  7.30083
Epoch 2, cost is  7.2162
Epoch 3, cost is  7.20758
Epoch 4, cost is  7.23297
Training took 0.086643 minutes
Weight histogram
[1727 3334 1447 3912 3728   70 1170 2593  177   67] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
[ 615  985 1547 1963 1947 2174 2185 2310 2277 2222] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
-7.99758
8.28356
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.17987
Epoch 1, cost is  2.10762
Epoch 2, cost is  2.07606
Epoch 3, cost is  2.04459
Epoch 4, cost is  2.05486
Training took 0.106327 minutes
Weight histogram
[1521 7689 5779 2581 1408  763  267  180   55    7] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
[ 497  648 1091 1658 2527 2743 2873 3018 2578 2617] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
-2.36847
2.42442
fine tuning ...
Epoch 0
Fine tuning took 0.036045 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038155 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037087 minutes
{0: [0.14901477832512317, 0.15024630541871922, 0.1539408866995074, 0.1145320197044335], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77339901477832518, 0.75492610837438423, 0.76847290640394084, 0.82266009852216748], 5: [0.077586206896551727, 0.094827586206896547, 0.077586206896551727, 0.062807881773399021], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221332 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.19289
Epoch 1, cost is  4.95086
Epoch 2, cost is  4.87088
Epoch 3, cost is  4.83807
Epoch 4, cost is  4.83675
Training took 0.108191 minutes
Weight histogram
[ 650 2122 4453 4413 4445 1372  467  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 643 1094 1825 1977 2040 2156 2064 2135 2070 2221] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-6.31429
6.07388
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.00178
Epoch 1, cost is  5.82324
Epoch 2, cost is  5.79785
Epoch 3, cost is  5.80239
Epoch 4, cost is  5.83284
Training took 0.084491 minutes
Weight histogram
[1366 6739 7321 2039  316  216  170  138 1669  276] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
[1495 1869 1415 1847 2128 2342 2300 2307 2205 2342] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
-7.9906
7.33719
fine tuning ...
Epoch 0
Fine tuning took 0.036038 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036225 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037951 minutes
{0: [0.11576354679802955, 0.093596059113300489, 0.13054187192118227, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82389162561576357, 0.84729064039408863, 0.80788177339901479, 0.80295566502463056], 5: [0.060344827586206899, 0.059113300492610835, 0.061576354679802957, 0.060344827586206899], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221864 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.19289
Epoch 1, cost is  4.95086
Epoch 2, cost is  4.87088
Epoch 3, cost is  4.83807
Epoch 4, cost is  4.83675
Training took 0.110458 minutes
Weight histogram
[ 650 2122 4453 4413 4445 1372  467  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 643 1094 1825 1977 2040 2156 2064 2135 2070 2221] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-6.31429
6.07388
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.7763
Epoch 1, cost is  3.62716
Epoch 2, cost is  3.58751
Epoch 3, cost is  3.58045
Epoch 4, cost is  3.58797
Training took 0.109668 minutes
Weight histogram
[7356 6948 1529 1292  628  248  861  857  516   15] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
[1010 1673 2296 2157 2155 2372 2184 2106 2052 2245] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
-5.43261
5.79773
fine tuning ...
Epoch 0
Fine tuning took 0.040903 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040658 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.039406 minutes
{0: [0.16995073891625614, 0.097290640394088676, 0.16379310344827586, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.81157635467980294, 0.73891625615763545, 0.72906403940886699], 5: [0.083743842364532015, 0.091133004926108374, 0.097290640394088676, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221180 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.19289
Epoch 1, cost is  4.95086
Epoch 2, cost is  4.87088
Epoch 3, cost is  4.83807
Epoch 4, cost is  4.83675
Training took 0.110570 minutes
Weight histogram
[ 650 2122 4453 4413 4445 1372  467  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 643 1094 1825 1977 2040 2156 2064 2135 2070 2221] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-6.31429
6.07388
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.4568
Epoch 1, cost is  2.33592
Epoch 2, cost is  2.28692
Epoch 3, cost is  2.27104
Epoch 4, cost is  2.25879
Training took 0.151122 minutes
Weight histogram
[8146 6089 1132 1163  860 1240  904  620   86   10] [-0.0099112  -0.00893714 -0.00796308 -0.00698902 -0.00601496 -0.0050409
 -0.00406684 -0.00309278 -0.00211872 -0.00114466 -0.0001706 ]
[ 720 1154 2242 2927 2298 2272 2133 2406 1892 2206] [-0.0099112  -0.00893714 -0.00796308 -0.00698902 -0.00601496 -0.0050409
 -0.00406684 -0.00309278 -0.00211872 -0.00114466 -0.0001706 ]
-3.44084
3.7528
fine tuning ...
Epoch 0
Fine tuning took 0.044210 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045196 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045344 minutes
{0: [0.1354679802955665, 0.15517241379310345, 0.16871921182266009, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.75862068965517238, 0.75492610837438423, 0.74630541871921185], 5: [0.086206896551724144, 0.086206896551724144, 0.076354679802955669, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221305 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.36911
Epoch 1, cost is  3.13629
Epoch 2, cost is  3.03603
Epoch 3, cost is  2.98825
Epoch 4, cost is  2.96094
Training took 0.151101 minutes
Weight histogram
[1362 9941 2661 1542  862  611  410  528  302    6] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
[ 606 1060 1683 1937 2013 2125 2118 2298 2116 2269] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
-4.02888
3.77567
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.64092
Epoch 1, cost is  6.39523
Epoch 2, cost is  6.33942
Epoch 3, cost is  6.32659
Epoch 4, cost is  6.35161
Training took 0.106540 minutes
Weight histogram
[4236 7482 2254 1649  980 1186  223  135  403 1702] [-0.03736157 -0.03363769 -0.02991381 -0.02618994 -0.02246606 -0.01874219
 -0.01501831 -0.01129443 -0.00757056 -0.00384668 -0.00012281]
[2522 1213 1653 2051 2075 2326 2099 2101 2058 2152] [-0.03736157 -0.03363769 -0.02991381 -0.02618994 -0.02246606 -0.01874219
 -0.01501831 -0.01129443 -0.00757056 -0.00384668 -0.00012281]
-8.63517
8.50205
fine tuning ...
Epoch 0
Fine tuning took 0.046472 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.047141 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046637 minutes
{0: [0.16625615763546797, 0.13300492610837439, 0.1268472906403941, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71551724137931039, 0.76847290640394084, 0.76108374384236455, 0.77709359605911332], 5: [0.11822660098522167, 0.098522167487684734, 0.11206896551724138, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221466 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.36911
Epoch 1, cost is  3.13629
Epoch 2, cost is  3.03603
Epoch 3, cost is  2.98825
Epoch 4, cost is  2.96094
Training took 0.149583 minutes
Weight histogram
[1362 9941 2661 1542  862  611  410  528  302    6] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
[ 606 1060 1683 1937 2013 2125 2118 2298 2116 2269] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
-4.02888
3.77567
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.18353
Epoch 1, cost is  4.03017
Epoch 2, cost is  3.98586
Epoch 3, cost is  3.98456
Epoch 4, cost is  3.99285
Training took 0.151693 minutes
Weight histogram
[3449 4040 4576 2280 1793  890  980  187 1172  883] [-0.0245481  -0.02210557 -0.01966304 -0.01722051 -0.01477798 -0.01233545
 -0.00989292 -0.00745039 -0.00500787 -0.00256534 -0.00012281]
[1437 2263 2001 2088 2190 2289 2050 2050 1820 2062] [-0.0245481  -0.02210557 -0.01966304 -0.01722051 -0.01477798 -0.01233545
 -0.00989292 -0.00745039 -0.00500787 -0.00256534 -0.00012281]
-5.63871
5.25891
fine tuning ...
Epoch 0
Fine tuning took 0.050475 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050822 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050424 minutes
{0: [0.28325123152709358, 0.18596059113300492, 0.2376847290640394, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58990147783251234, 0.64655172413793105, 0.58743842364532017, 0.61083743842364535], 5: [0.1268472906403941, 0.16748768472906403, 0.1748768472906404, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221699 minutes
Weight histogram
[ 147  912  832 1571 4589 5011 3293 1388  460   22] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
[  139   188   285   399   679   706   800  1662  3364 10003] [ -9.91381356e-04  -7.99661904e-04  -6.07942452e-04  -4.16223001e-04
  -2.24503549e-04  -3.27840971e-05   1.58935355e-04   3.50654806e-04
   5.42374258e-04   7.34093710e-04   9.25813161e-04]
-1.35836
1.5996
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.36911
Epoch 1, cost is  3.13629
Epoch 2, cost is  3.03603
Epoch 3, cost is  2.98825
Epoch 4, cost is  2.96094
Training took 0.149497 minutes
Weight histogram
[1362 9941 2661 1542  862  611  410  528  302    6] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
[ 606 1060 1683 1937 2013 2125 2118 2298 2116 2269] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
-4.02888
3.77567
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.76768
Epoch 1, cost is  2.63401
Epoch 2, cost is  2.58847
Epoch 3, cost is  2.5703
Epoch 4, cost is  2.55645
Training took 0.205314 minutes
Weight histogram
[2124 5329 4106 2982 1652  887  930 1078  887  275] [-0.01527143 -0.01375657 -0.0122417  -0.01072684 -0.00921198 -0.00769712
 -0.00618226 -0.00466739 -0.00315253 -0.00163767 -0.00012281]
[ 952 1850 2802 2133 2105 2124 1923 2332 1964 2065] [-0.01527143 -0.01375657 -0.0122417  -0.01072684 -0.00921198 -0.00769712
 -0.00618226 -0.00466739 -0.00315253 -0.00163767 -0.00012281]
-3.43759
3.76701
fine tuning ...
Epoch 0
Fine tuning took 0.055236 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056058 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056283 minutes
{0: [0.25492610837438423, 0.22536945812807882, 0.26108374384236455, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56650246305418717, 0.60837438423645318, 0.56280788177339902, 0.58990147783251234], 5: [0.17857142857142858, 0.16625615763546797, 0.17610837438423646, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221317 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.98007
Epoch 1, cost is  7.70073
Epoch 2, cost is  7.62833
Epoch 3, cost is  7.61814
Epoch 4, cost is  7.64043
Training took 0.084163 minutes
Weight histogram
[1727 3334 1447 3912 3728   70 1194 2624 1397  817] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
[ 693 1144 1828 2219 2095 2575 2454 2526 2445 2271] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
-8.56715
9.46688
training layer 2, rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.86125
Epoch 1, cost is  5.69704
Epoch 2, cost is  5.64182
Epoch 3, cost is  5.65262
Epoch 4, cost is  5.66017
Training took 0.072756 minutes
Weight histogram
[1998 3214 3347 4648 2615 1870 1823 2507  236   17] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
[1059 1825 2012 1987 1988 2636 2753 2637 2953 2425] [-0.02261988 -0.02033149 -0.01804309 -0.0157547  -0.01346631 -0.01117791
 -0.00888952 -0.00660112 -0.00431273 -0.00202434  0.00026406]
-7.21125
8.20964
fine tuning ...
Epoch 0
Fine tuning took 0.032937 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.033728 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.032422 minutes
{0: [0.1268472906403941, 0.087438423645320201, 0.12561576354679804, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81773399014778325, 0.84852216748768472, 0.83990147783251234, 0.81157635467980294], 5: [0.055418719211822662, 0.064039408866995079, 0.034482758620689655, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221280 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.98007
Epoch 1, cost is  7.70073
Epoch 2, cost is  7.62833
Epoch 3, cost is  7.61814
Epoch 4, cost is  7.64043
Training took 0.086199 minutes
Weight histogram
[1727 3334 1447 3912 3728   70 1194 2624 1397  817] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
[ 693 1144 1828 2219 2095 2575 2454 2526 2445 2271] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
-8.56715
9.46688
training layer 2, rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.18195
Epoch 1, cost is  3.08782
Epoch 2, cost is  3.06028
Epoch 3, cost is  3.06661
Epoch 4, cost is  3.07289
Training took 0.086891 minutes
Weight histogram
[1076 5046 5973 1482 1039 3673 3754  210   13    9] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
[ 700 1098 1953 2485 2577 2642 2625 2832 2805 2558] [-0.01215053 -0.01090907 -0.00966761 -0.00842615 -0.0071847  -0.00594324
 -0.00470178 -0.00346032 -0.00221886 -0.0009774   0.00026406]
-4.41398
4.07641
fine tuning ...
Epoch 0
Fine tuning took 0.035288 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.032435 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.035273 minutes
{0: [0.12192118226600986, 0.14532019704433496, 0.14408866995073891, 0.11945812807881774], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81280788177339902, 0.79187192118226601, 0.78325123152709364, 0.81527093596059108], 5: [0.065270935960591137, 0.062807881773399021, 0.072660098522167482, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223561 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.98007
Epoch 1, cost is  7.70073
Epoch 2, cost is  7.62833
Epoch 3, cost is  7.61814
Epoch 4, cost is  7.64043
Training took 0.086634 minutes
Weight histogram
[1727 3334 1447 3912 3728   70 1194 2624 1397  817] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
[ 693 1144 1828 2219 2095 2575 2454 2526 2445 2271] [-0.00881309 -0.00725222 -0.00569135 -0.00413048 -0.00256961 -0.00100874
  0.00055213  0.002113    0.00367387  0.00523474  0.00679562]
-8.56715
9.46688
training layer 2, rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.05983
Epoch 1, cost is  1.98331
Epoch 2, cost is  1.95579
Epoch 3, cost is  1.95183
Epoch 4, cost is  1.9275
Training took 0.107395 minutes
Weight histogram
[1521 7689 5908 4460 1425  763  267  180   55    7] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
[ 536  747 1280 1969 3023 2965 3249 2964 2793 2749] [-0.00971721 -0.00871908 -0.00772095 -0.00672283 -0.0057247  -0.00472657
 -0.00372845 -0.00273032 -0.0017322  -0.00073407  0.00026406]
-2.47273
2.59558
fine tuning ...
Epoch 0
Fine tuning took 0.038575 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036408 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.036051 minutes
{0: [0.10221674876847291, 0.16871921182266009, 0.18103448275862069, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82019704433497542, 0.76354679802955661, 0.75, 0.79433497536945807], 5: [0.077586206896551727, 0.067733990147783252, 0.068965517241379309, 0.076354679802955669], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221136 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.24392
Epoch 1, cost is  5.00846
Epoch 2, cost is  4.91635
Epoch 3, cost is  4.87983
Epoch 4, cost is  4.86054
Training took 0.109158 minutes
Weight histogram
[ 650 2122 4453 4460 6218 1577  467  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 719 1287 2064 2223 2285 2327 2277 2293 2387 2388] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-6.799
6.35975
training layer 2, rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.52656
Epoch 1, cost is  6.36919
Epoch 2, cost is  6.32427
Epoch 3, cost is  6.32722
Epoch 4, cost is  6.35226
Training took 0.083469 minutes
Weight histogram
[1366 6756 8986 2382  316  216  170  138 1669  276] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
[1697 1883 1659 2146 2342 2705 2448 2468 2516 2411] [ -2.51068305e-02  -2.26001911e-02  -2.00935517e-02  -1.75869124e-02
  -1.50802730e-02  -1.25736336e-02  -1.00669942e-02  -7.56035479e-03
  -5.05371540e-03  -2.54707601e-03  -4.04366147e-05]
-8.11954
7.84457
fine tuning ...
Epoch 0
Fine tuning took 0.036075 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.037707 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.038270 minutes
{0: [0.14162561576354679, 0.15024630541871922, 0.13669950738916256, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.7931034482758621, 0.80911330049261088, 0.78448275862068961], 5: [0.080049261083743842, 0.056650246305418719, 0.054187192118226604, 0.073891625615763554], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.220891 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.24392
Epoch 1, cost is  5.00846
Epoch 2, cost is  4.91635
Epoch 3, cost is  4.87983
Epoch 4, cost is  4.86054
Training took 0.111920 minutes
Weight histogram
[ 650 2122 4453 4460 6218 1577  467  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 719 1287 2064 2223 2285 2327 2277 2293 2387 2388] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-6.799
6.35975
training layer 2, rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.28833
Epoch 1, cost is  4.11941
Epoch 2, cost is  4.07112
Epoch 3, cost is  4.05461
Epoch 4, cost is  4.06423
Training took 0.110497 minutes
Weight histogram
[8234 8095 1529 1292  628  248  861  857  516   15] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
[1141 2043 2451 2407 2467 2570 2349 2273 2448 2126] [-0.01432425 -0.01290888 -0.01149352 -0.01007815 -0.00866279 -0.00724742
 -0.00583206 -0.00441669 -0.00300133 -0.00158596 -0.0001706 ]
-5.78011
6.61651
fine tuning ...
Epoch 0
Fine tuning took 0.040808 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.041599 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.040219 minutes
{0: [0.15886699507389163, 0.12931034482758622, 0.19334975369458129, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74137931034482762, 0.79556650246305416, 0.7068965517241379, 0.7068965517241379], 5: [0.099753694581280791, 0.075123152709359611, 0.099753694581280791, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222626 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.24392
Epoch 1, cost is  5.00846
Epoch 2, cost is  4.91635
Epoch 3, cost is  4.87983
Epoch 4, cost is  4.86054
Training took 0.111169 minutes
Weight histogram
[ 650 2122 4453 4460 6218 1577  467  279   18    6] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
[ 719 1287 2064 2223 2285 2327 2277 2293 2387 2388] [-0.00830281 -0.00748959 -0.00667637 -0.00586315 -0.00504993 -0.0042367
 -0.00342348 -0.00261026 -0.00179704 -0.00098382 -0.0001706 ]
-6.799
6.35975
training layer 2, rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.41229
Epoch 1, cost is  2.30824
Epoch 2, cost is  2.26962
Epoch 3, cost is  2.24679
Epoch 4, cost is  2.2413
Training took 0.151303 minutes
Weight histogram
[9381 6879 1132 1163  860 1240  904  620   86   10] [-0.0099112  -0.00893714 -0.00796308 -0.00698902 -0.00601496 -0.0050409
 -0.00406684 -0.00309278 -0.00211872 -0.00114466 -0.0001706 ]
[ 799 1378 2642 3114 2404 2449 2471 2254 2313 2451] [-0.0099112  -0.00893714 -0.00796308 -0.00698902 -0.00601496 -0.0050409
 -0.00406684 -0.00309278 -0.00211872 -0.00114466 -0.0001706 ]
-3.74023
4.06135
fine tuning ...
Epoch 0
Fine tuning took 0.044238 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045858 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.046166 minutes
{0: [0.15517241379310345, 0.18472906403940886, 0.19458128078817735, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.70812807881773399, 0.68103448275862066, 0.70197044334975367], 5: [0.084975369458128072, 0.10714285714285714, 0.12438423645320197, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.221555 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.58209
Epoch 1, cost is  3.37166
Epoch 2, cost is  3.29283
Epoch 3, cost is  3.25777
Epoch 4, cost is  3.22192
Training took 0.150811 minutes
Weight histogram
[ 1375 10767  3847  1542   862   611   410   528   302     6] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
[ 680 1255 1908 2181 2250 2369 2425 2397 2440 2345] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
-4.1428
4.1239
training layer 2, rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.21404
Epoch 1, cost is  6.97115
Epoch 2, cost is  6.91862
Epoch 3, cost is  6.9291
Epoch 4, cost is  6.96892
Training took 0.107987 minutes
Weight histogram
[5679 7649 2652 1597 1015 1211  231  135  380 1726] [-0.03768408 -0.03392796 -0.03017183 -0.0264157  -0.02265957 -0.01890344
 -0.01514732 -0.01139119 -0.00763506 -0.00387893 -0.00012281]
[2690 1370 1933 2368 2350 2506 2293 2282 2360 2123] [-0.03768408 -0.03392796 -0.03017183 -0.0264157  -0.02265957 -0.01890344
 -0.01514732 -0.01139119 -0.00763506 -0.00387893 -0.00012281]
-9.23712
9.62529
fine tuning ...
Epoch 0
Fine tuning took 0.045069 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.045677 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.047199 minutes
{0: [0.087438423645320201, 0.12561576354679804, 0.1268472906403941, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83990147783251234, 0.80911330049261088, 0.74630541871921185, 0.72906403940886699], 5: [0.072660098522167482, 0.065270935960591137, 0.1268472906403941, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.223372 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.58209
Epoch 1, cost is  3.37166
Epoch 2, cost is  3.29283
Epoch 3, cost is  3.25777
Epoch 4, cost is  3.22192
Training took 0.151576 minutes
Weight histogram
[ 1375 10767  3847  1542   862   611   410   528   302     6] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
[ 680 1255 1908 2181 2250 2369 2425 2397 2440 2345] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
-4.1428
4.1239
training layer 2, rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.39281
Epoch 1, cost is  4.22703
Epoch 2, cost is  4.17969
Epoch 3, cost is  4.18405
Epoch 4, cost is  4.1889
Training took 0.152861 minutes
Weight histogram
[5183 3939 4535 2517 1949  836 1055  205 1154  902] [-0.02488572 -0.02240943 -0.01993314 -0.01745685 -0.01498055 -0.01250426
 -0.01002797 -0.00755168 -0.00507539 -0.0025991  -0.00012281]
[1653 2412 2311 2318 2480 2338 2288 2051 2229 2195] [-0.02488572 -0.02240943 -0.01993314 -0.01745685 -0.01498055 -0.01250426
 -0.01002797 -0.00755168 -0.00507539 -0.0025991  -0.00012281]
-6.20024
5.88589
fine tuning ...
Epoch 0
Fine tuning took 0.050597 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052087 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051771 minutes
{0: [0.17980295566502463, 0.26600985221674878, 0.19088669950738915, 0.24630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.59605911330049266, 0.63669950738916259, 0.58497536945812811], 5: [0.11822660098522167, 0.13793103448275862, 0.17241379310344829, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.222143 minutes
Weight histogram
[ 316 1538 1670  981 3650 5550 4090 1834  588   33] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
[  139   188   285   399   679   706   800  1662  3364 12028] [-0.0011315  -0.00092577 -0.00072003 -0.0005143  -0.00030857 -0.00010284
  0.00010289  0.00030862  0.00051435  0.00072008  0.00092581]
-1.35836
1.5996
training layer 1, rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.001_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.58209
Epoch 1, cost is  3.37166
Epoch 2, cost is  3.29283
Epoch 3, cost is  3.25777
Epoch 4, cost is  3.22192
Training took 0.150510 minutes
Weight histogram
[ 1375 10767  3847  1542   862   611   410   528   302     6] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
[ 680 1255 1908 2181 2250 2369 2425 2397 2440 2345] [-0.00804465 -0.00725247 -0.00646028 -0.0056681  -0.00487591 -0.00408373
 -0.00329155 -0.00249936 -0.00170718 -0.00091499 -0.00012281]
-4.1428
4.1239
training layer 2, rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.70977
Epoch 1, cost is  2.57894
Epoch 2, cost is  2.52516
Epoch 3, cost is  2.50313
Epoch 4, cost is  2.49484
Training took 0.203652 minutes
Weight histogram
[4149 5329 4106 2982 1652  887  930 1078  887  275] [-0.01527143 -0.01375657 -0.0122417  -0.01072684 -0.00921198 -0.00769712
 -0.00618226 -0.00466739 -0.00315253 -0.00163767 -0.00012281]
[1069 2289 2843 2336 2306 2202 2369 2269 2233 2359] [-0.01527143 -0.01375657 -0.0122417  -0.01072684 -0.00921198 -0.00769712
 -0.00618226 -0.00466739 -0.00315253 -0.00163767 -0.00012281]
-3.86681
4.01987
fine tuning ...
Epoch 0
Fine tuning took 0.055363 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057014 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056559 minutes
{0: [0.20566502463054187, 0.28078817733990147, 0.26847290640394089, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.59359605911330049, 0.58743842364532017, 0.58866995073891626], 5: [0.11699507389162561, 0.12561576354679804, 0.14408866995073891, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}
