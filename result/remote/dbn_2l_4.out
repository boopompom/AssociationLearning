Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentDBN4
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentDBN4
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.419670 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.56164
Epoch 1, cost is  5.63608
Epoch 2, cost is  6.24378
Epoch 3, cost is  7.06008
Epoch 4, cost is  7.97379
Training took 0.111966 minutes
Weight histogram
[286 569 483 276 213 119  53  23   1   2] [-0.05885183 -0.05327221 -0.04769259 -0.04211297 -0.03653335 -0.03095373
 -0.02537411 -0.01979449 -0.01421487 -0.00863525 -0.00305563]
[ 75  97 149 191 226 250 258 261 252 266] [-0.05885183 -0.05327221 -0.04769259 -0.04211297 -0.03653335 -0.03095373
 -0.02537411 -0.01979449 -0.01421487 -0.00863525 -0.00305563]
-11.1723
12.954
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.11209
Epoch 1, cost is  3.41626
Epoch 2, cost is  3.52028
Epoch 3, cost is  3.79386
Epoch 4, cost is  4.04747
Training took 0.076642 minutes
Weight histogram
[ 302  432  476  309  201  180  737 1082  307   24] [-0.15924251 -0.14362382 -0.12800513 -0.11238645 -0.09676776 -0.08114907
 -0.06553038 -0.04991169 -0.03429301 -0.01867432 -0.00305563]
[148 226 330 430 488 560 575 586 441 266] [-0.15924251 -0.14362382 -0.12800513 -0.11238645 -0.09676776 -0.08114907
 -0.06553038 -0.04991169 -0.03429301 -0.01867432 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.053957 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054395 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053947 minutes
{0: [0.041871921182266007, 0.061576354679802957, 0.051724137931034482, 0.032019704433497539], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.91748768472906406, 0.91256157635467983, 0.90517241379310343, 0.93472906403940892], 5: [0.04064039408866995, 0.025862068965517241, 0.043103448275862072, 0.033251231527093597], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.417439 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.56164
Epoch 1, cost is  5.63608
Epoch 2, cost is  6.24378
Epoch 3, cost is  7.06008
Epoch 4, cost is  7.97379
Training took 0.108974 minutes
Weight histogram
[286 569 483 276 213 119  53  23   1   2] [-0.05885183 -0.05327221 -0.04769259 -0.04211297 -0.03653335 -0.03095373
 -0.02537411 -0.01979449 -0.01421487 -0.00863525 -0.00305563]
[ 75  97 149 191 226 250 258 261 252 266] [-0.05885183 -0.05327221 -0.04769259 -0.04211297 -0.03653335 -0.03095373
 -0.02537411 -0.01979449 -0.01421487 -0.00863525 -0.00305563]
-11.1723
12.954
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.79055
Epoch 1, cost is  2.73751
Epoch 2, cost is  2.53374
Epoch 3, cost is  2.49374
Epoch 4, cost is  2.56749
Training took 0.095459 minutes
Weight histogram
[ 388  392  412  338  326 1139  669  310   71    5] [-0.10782639 -0.09734931 -0.08687224 -0.07639516 -0.06591809 -0.05544101
 -0.04496393 -0.03448686 -0.02400978 -0.01353271 -0.00305563]
[218 370 570 720 842 293 258 261 252 266] [-0.10782639 -0.09734931 -0.08687224 -0.07639516 -0.06591809 -0.05544101
 -0.04496393 -0.03448686 -0.02400978 -0.01353271 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.055235 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.058647 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055743 minutes
{0: [0.041871921182266007, 0.056650246305418719, 0.062807881773399021, 0.048029556650246302], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.90024630541871919, 0.8928571428571429, 0.88793103448275867, 0.90517241379310343], 5: [0.057881773399014777, 0.050492610837438424, 0.049261083743842367, 0.046798029556650245], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.420346 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.56164
Epoch 1, cost is  5.63608
Epoch 2, cost is  6.24378
Epoch 3, cost is  7.06008
Epoch 4, cost is  7.97379
Training took 0.109162 minutes
Weight histogram
[286 569 483 276 213 119  53  23   1   2] [-0.05885183 -0.05327221 -0.04769259 -0.04211297 -0.03653335 -0.03095373
 -0.02537411 -0.01979449 -0.01421487 -0.00863525 -0.00305563]
[ 75  97 149 191 226 250 258 261 252 266] [-0.05885183 -0.05327221 -0.04769259 -0.04211297 -0.03653335 -0.03095373
 -0.02537411 -0.01979449 -0.01421487 -0.00863525 -0.00305563]
-11.1723
12.954
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.68981
Epoch 1, cost is  2.36793
Epoch 2, cost is  2.06114
Epoch 3, cost is  1.94269
Epoch 4, cost is  1.88872
Training took 0.110948 minutes
Weight histogram
[450 462 343 797 985 584 281 107  38   3] [-0.08340102 -0.07536648 -0.06733195 -0.05929741 -0.05126287 -0.04322833
 -0.03519379 -0.02715925 -0.01912471 -0.01109017 -0.00305563]
[ 313  661 1038  525  226  250  258  261  252  266] [-0.08340102 -0.07536648 -0.06733195 -0.05929741 -0.05126287 -0.04322833
 -0.03519379 -0.02715925 -0.01912471 -0.01109017 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.056668 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055970 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057362 minutes
{0: [0.071428571428571425, 0.056650246305418719, 0.066502463054187194, 0.059113300492610835], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8780788177339901, 0.8854679802955665, 0.88054187192118227, 0.90024630541871919], 5: [0.050492610837438424, 0.057881773399014777, 0.05295566502463054, 0.04064039408866995], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.405909 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.4676
Epoch 1, cost is  5.20178
Epoch 2, cost is  5.49426
Epoch 3, cost is  6.02201
Epoch 4, cost is  6.48347
Training took 0.152569 minutes
Weight histogram
[275 303 301 392 313 241 151  47   1   1] [-0.06721411 -0.06086817 -0.05452224 -0.0481763  -0.04183036 -0.03548443
 -0.02913849 -0.02279255 -0.01644662 -0.01010068 -0.00375474]
[ 92 116 172 201 217 242 233 246 252 254] [-0.06721411 -0.06086817 -0.05452224 -0.0481763  -0.04183036 -0.03548443
 -0.02913849 -0.02279255 -0.01644662 -0.01010068 -0.00375474]
-7.6593
9.26404
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.17939
Epoch 1, cost is  2.82139
Epoch 2, cost is  2.99872
Epoch 3, cost is  3.27531
Epoch 4, cost is  3.61782
Training took 0.088876 minutes
Weight histogram
[ 421  383  427  322  248  122   56  876 1069  126] [-0.23014906 -0.20750963 -0.1848702  -0.16223077 -0.13959133 -0.1169519
 -0.09431247 -0.07167304 -0.04903361 -0.02639417 -0.00375474]
[180 272 389 453 515 531 561 577 299 273] [-0.23014906 -0.20750963 -0.1848702  -0.16223077 -0.13959133 -0.1169519
 -0.09431247 -0.07167304 -0.04903361 -0.02639417 -0.00375474]
-7.6593
9.26404
fine tuning ...
Epoch 0
Fine tuning took 0.060093 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060672 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059252 minutes
{0: [0.057881773399014777, 0.083743842364532015, 0.072660098522167482, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.87931034482758619, 0.8645320197044335, 0.86822660098522164, 0.8571428571428571], 5: [0.062807881773399021, 0.051724137931034482, 0.059113300492610835, 0.065270935960591137], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.407017 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.4676
Epoch 1, cost is  5.20178
Epoch 2, cost is  5.49426
Epoch 3, cost is  6.02201
Epoch 4, cost is  6.48347
Training took 0.152164 minutes
Weight histogram
[275 303 301 392 313 241 151  47   1   1] [-0.06721411 -0.06086817 -0.05452224 -0.0481763  -0.04183036 -0.03548443
 -0.02913849 -0.02279255 -0.01644662 -0.01010068 -0.00375474]
[ 92 116 172 201 217 242 233 246 252 254] [-0.06721411 -0.06086817 -0.05452224 -0.0481763  -0.04183036 -0.03548443
 -0.02913849 -0.02279255 -0.01644662 -0.01010068 -0.00375474]
-7.6593
9.26404
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.7365
Epoch 1, cost is  2.02614
Epoch 2, cost is  1.9471
Epoch 3, cost is  2.01016
Epoch 4, cost is  2.07513
Training took 0.112518 minutes
Weight histogram
[485 436 367 278 216 410 742 799 313   4] [-0.14717531 -0.13283326 -0.1184912  -0.10414914 -0.08980708 -0.07546503
 -0.06112297 -0.04678091 -0.03243886 -0.0180968  -0.00375474]
[194 278 406 484 556 600 627 399 252 254] [-0.14717531 -0.13283326 -0.1184912  -0.10414914 -0.08980708 -0.07546503
 -0.06112297 -0.04678091 -0.03243886 -0.0180968  -0.00375474]
-7.6593
9.26404
fine tuning ...
Epoch 0
Fine tuning took 0.062157 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063607 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063098 minutes
{0: [0.099753694581280791, 0.087438423645320201, 0.096059113300492605, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82512315270935965, 0.84482758620689657, 0.81773399014778325, 0.72783251231527091], 5: [0.075123152709359611, 0.067733990147783252, 0.086206896551724144, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.405130 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.4676
Epoch 1, cost is  5.20178
Epoch 2, cost is  5.49426
Epoch 3, cost is  6.02201
Epoch 4, cost is  6.48347
Training took 0.152316 minutes
Weight histogram
[275 303 301 392 313 241 151  47   1   1] [-0.06721411 -0.06086817 -0.05452224 -0.0481763  -0.04183036 -0.03548443
 -0.02913849 -0.02279255 -0.01644662 -0.01010068 -0.00375474]
[ 92 116 172 201 217 242 233 246 252 254] [-0.06721411 -0.06086817 -0.05452224 -0.0481763  -0.04183036 -0.03548443
 -0.02913849 -0.02279255 -0.01644662 -0.01010068 -0.00375474]
-7.6593
9.26404
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.63857
Epoch 1, cost is  1.74197
Epoch 2, cost is  1.58424
Epoch 3, cost is  1.53345
Epoch 4, cost is  1.50377
Training took 0.156539 minutes
Weight histogram
[536 408 384 415 756 666 527 289  66   3] [-0.10232662 -0.09246943 -0.08261224 -0.07275505 -0.06289787 -0.05304068
 -0.04318349 -0.0333263  -0.02346912 -0.01361193 -0.00375474]
[262 418 608 742 793 242 233 246 252 254] [-0.10232662 -0.09246943 -0.08261224 -0.07275505 -0.06289787 -0.05304068
 -0.04318349 -0.0333263  -0.02346912 -0.01361193 -0.00375474]
-7.6593
9.26404
fine tuning ...
Epoch 0
Fine tuning took 0.067901 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068592 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068997 minutes
{0: [0.089901477832512317, 0.093596059113300489, 0.10714285714285714, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81157635467980294, 0.84729064039408863, 0.80049261083743839, 0.78694581280788178], 5: [0.098522167487684734, 0.059113300492610835, 0.092364532019704432, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.405224 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.51562
Epoch 1, cost is  5.30233
Epoch 2, cost is  5.62101
Epoch 3, cost is  6.10417
Epoch 4, cost is  6.55452
Training took 0.210333 minutes
Weight histogram
[399 381 438 459 332  14   0   1   0   1] [-0.05370027 -0.04867463 -0.04364898 -0.03862333 -0.03359769 -0.02857204
 -0.02354639 -0.01852075 -0.0134951  -0.00846945 -0.00344381]
[109 135 172 199 219 237 239 229 241 245] [-0.05370027 -0.04867463 -0.04364898 -0.03862333 -0.03359769 -0.02857204
 -0.02354639 -0.01852075 -0.0134951  -0.00846945 -0.00344381]
-10.554
10.7261
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  1.89507
Epoch 1, cost is  1.64458
Epoch 2, cost is  1.73797
Epoch 3, cost is  1.89665
Epoch 4, cost is  2.09868
Training took 0.108645 minutes
Weight histogram
[ 635  572  382  186  132   54   42   15 1909  123] [-0.27156261 -0.24475073 -0.21793885 -0.19112697 -0.16431509 -0.13750321
 -0.11069133 -0.08387945 -0.05706757 -0.03025569 -0.00344381]
[209 300 413 470 523 539 568 499 262 267] [-0.27156261 -0.24475073 -0.21793885 -0.19112697 -0.16431509 -0.13750321
 -0.11069133 -0.08387945 -0.05706757 -0.03025569 -0.00344381]
-10.554
10.7261
fine tuning ...
Epoch 0
Fine tuning took 0.068080 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067695 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068627 minutes
{0: [0.10714285714285714, 0.14778325123152711, 0.13423645320197045, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83497536945812811, 0.79926108374384242, 0.81527093596059108, 0.75985221674876846], 5: [0.057881773399014777, 0.05295566502463054, 0.050492610837438424, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.405274 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.51562
Epoch 1, cost is  5.30233
Epoch 2, cost is  5.62101
Epoch 3, cost is  6.10417
Epoch 4, cost is  6.55452
Training took 0.211609 minutes
Weight histogram
[399 381 438 459 332  14   0   1   0   1] [-0.05370027 -0.04867463 -0.04364898 -0.03862333 -0.03359769 -0.02857204
 -0.02354639 -0.01852075 -0.0134951  -0.00846945 -0.00344381]
[109 135 172 199 219 237 239 229 241 245] [-0.05370027 -0.04867463 -0.04364898 -0.03862333 -0.03359769 -0.02857204
 -0.02354639 -0.01852075 -0.0134951  -0.00846945 -0.00344381]
-10.554
10.7261
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  1.56409
Epoch 1, cost is  1.17171
Epoch 2, cost is  1.13966
Epoch 3, cost is  1.15011
Epoch 4, cost is  1.20009
Training took 0.155098 minutes
Weight histogram
[ 752  533  310  201  123   62  216 1373  478    2] [-0.16273433 -0.14680528 -0.13087622 -0.11494717 -0.09901812 -0.08308907
 -0.06716002 -0.05123096 -0.03530191 -0.01937286 -0.00344381]
[208 283 395 467 536 579 608 488 241 245] [-0.16273433 -0.14680528 -0.13087622 -0.11494717 -0.09901812 -0.08308907
 -0.06716002 -0.05123096 -0.03530191 -0.01937286 -0.00344381]
-10.554
10.7261
fine tuning ...
Epoch 0
Fine tuning took 0.074376 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.072612 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074451 minutes
{0: [0.13177339901477833, 0.16625615763546797, 0.1539408866995074, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80049261083743839, 0.77586206896551724, 0.80788177339901479, 0.75492610837438423], 5: [0.067733990147783252, 0.057881773399014777, 0.038177339901477834, 0.068965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.406912 minutes
Weight histogram
[ 13 111 268 497 579 370 127  44   7   9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 73  73 106 131 168 176 265 264 345 424] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.610014
0.538845
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.51562
Epoch 1, cost is  5.30233
Epoch 2, cost is  5.62101
Epoch 3, cost is  6.10417
Epoch 4, cost is  6.55452
Training took 0.210117 minutes
Weight histogram
[399 381 438 459 332  14   0   1   0   1] [-0.05370027 -0.04867463 -0.04364898 -0.03862333 -0.03359769 -0.02857204
 -0.02354639 -0.01852075 -0.0134951  -0.00846945 -0.00344381]
[109 135 172 199 219 237 239 229 241 245] [-0.05370027 -0.04867463 -0.04364898 -0.03862333 -0.03359769 -0.02857204
 -0.02354639 -0.01852075 -0.0134951  -0.00846945 -0.00344381]
-10.554
10.7261
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  1.44939
Epoch 1, cost is  0.962559
Epoch 2, cost is  0.86158
Epoch 3, cost is  0.825634
Epoch 4, cost is  0.822152
Training took 0.215613 minutes
Weight histogram
[770 526 313 197 127 595 958 560   3   1] [-0.11249303 -0.10158811 -0.09068319 -0.07977826 -0.06887334 -0.05796842
 -0.0470635  -0.03615857 -0.02525365 -0.01434873 -0.00344381]
[280 433 603 764 779 237 239 229 241 245] [-0.11249303 -0.10158811 -0.09068319 -0.07977826 -0.06887334 -0.05796842
 -0.0470635  -0.03615857 -0.02525365 -0.01434873 -0.00344381]
-10.554
10.7261
fine tuning ...
Epoch 0
Fine tuning took 0.079133 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078968 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078794 minutes
{0: [0.14162561576354679, 0.14162561576354679, 0.17857142857142858, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.74137931034482762, 0.74137931034482762, 0.73275862068965514], 5: [0.094827586206896547, 0.11699507389162561, 0.080049261083743842, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.384186 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.02994
Epoch 1, cost is  8.33846
Epoch 2, cost is  9.06077
Epoch 3, cost is  9.85335
Epoch 4, cost is  10.7468
Training took 0.107774 minutes
Weight histogram
[387 708 775 465 800 512 267 111  23   2] [-0.08630642 -0.07798134 -0.06965626 -0.06133118 -0.0530061  -0.04468102
 -0.03635594 -0.02803087 -0.01970579 -0.01138071 -0.00305563]
[146 275 394 460 459 508 496 455 434 423] [-0.08630642 -0.07798134 -0.06965626 -0.06133118 -0.0530061  -0.04468102
 -0.03635594 -0.02803087 -0.01970579 -0.01138071 -0.00305563]
-20.6984
24.0213
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.56733
Epoch 1, cost is  4.5415
Epoch 2, cost is  4.84698
Epoch 3, cost is  5.17381
Epoch 4, cost is  5.58338
Training took 0.075086 minutes
Weight histogram
[ 565  864 1236  550  369  254  149 1534  507   47] [-0.1863322  -0.16800454 -0.14967688 -0.13134923 -0.11302157 -0.09469391
 -0.07636626 -0.0580386  -0.03971094 -0.02138329 -0.00305563]
[232 423 616 761 815 865 972 487 463 441] [-0.1863322  -0.16800454 -0.14967688 -0.13134923 -0.11302157 -0.09469391
 -0.07636626 -0.0580386  -0.03971094 -0.02138329 -0.00305563]
-11.1813
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.051397 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051789 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051447 minutes
{0: [0.10467980295566502, 0.073891625615763554, 0.062807881773399021, 0.068965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.84729064039408863, 0.85344827586206895, 0.87561576354679804, 0.86083743842364535], 5: [0.048029556650246302, 0.072660098522167482, 0.061576354679802957, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382259 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.02994
Epoch 1, cost is  8.33846
Epoch 2, cost is  9.06077
Epoch 3, cost is  9.85335
Epoch 4, cost is  10.7468
Training took 0.105585 minutes
Weight histogram
[387 708 775 465 800 512 267 111  23   2] [-0.08630642 -0.07798134 -0.06965626 -0.06133118 -0.0530061  -0.04468102
 -0.03635594 -0.02803087 -0.01970579 -0.01138071 -0.00305563]
[146 275 394 460 459 508 496 455 434 423] [-0.08630642 -0.07798134 -0.06965626 -0.06133118 -0.0530061  -0.04468102
 -0.03635594 -0.02803087 -0.01970579 -0.01138071 -0.00305563]
-20.6984
24.0213
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.85409
Epoch 1, cost is  2.63742
Epoch 2, cost is  2.642
Epoch 3, cost is  2.65937
Epoch 4, cost is  2.71603
Training took 0.084184 minutes
Weight histogram
[ 808 1435  626  463  280  840 1069  424  123    7] [-0.1216024  -0.10974772 -0.09789305 -0.08603837 -0.07418369 -0.06232902
 -0.05047434 -0.03861966 -0.02676498 -0.01491031 -0.00305563]
[ 218  370  570  720  842 1033  928  876  252  266] [-0.1216024  -0.10974772 -0.09789305 -0.08603837 -0.07418369 -0.06232902
 -0.05047434 -0.03861966 -0.02676498 -0.01491031 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.053632 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054321 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054276 minutes
{0: [0.097290640394088676, 0.094827586206896547, 0.097290640394088676, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83251231527093594, 0.83374384236453203, 0.8214285714285714, 0.81280788177339902], 5: [0.070197044334975367, 0.071428571428571425, 0.081280788177339899, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380740 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.02994
Epoch 1, cost is  8.33846
Epoch 2, cost is  9.06077
Epoch 3, cost is  9.85335
Epoch 4, cost is  10.7468
Training took 0.109178 minutes
Weight histogram
[387 708 775 465 800 512 267 111  23   2] [-0.08630642 -0.07798134 -0.06965626 -0.06133118 -0.0530061  -0.04468102
 -0.03635594 -0.02803087 -0.01970579 -0.01138071 -0.00305563]
[146 275 394 460 459 508 496 455 434 423] [-0.08630642 -0.07798134 -0.06965626 -0.06133118 -0.0530061  -0.04468102
 -0.03635594 -0.02803087 -0.01970579 -0.01138071 -0.00305563]
-20.6984
24.0213
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.17796
Epoch 1, cost is  1.89968
Epoch 2, cost is  1.81005
Epoch 3, cost is  1.76738
Epoch 4, cost is  1.75229
Training took 0.107220 minutes
Weight histogram
[ 829 1458  489  480  936 1090  525  199   65    4] [-0.09766698 -0.08820584 -0.07874471 -0.06928357 -0.05982244 -0.0503613
 -0.04090017 -0.03143903 -0.0219779  -0.01251676 -0.00305563]
[ 313  661 1038 1391 1354  281  258  261  252  266] [-0.09766698 -0.08820584 -0.07874471 -0.06928357 -0.05982244 -0.0503613
 -0.04090017 -0.03143903 -0.0219779  -0.01251676 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.058065 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057417 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055968 minutes
{0: [0.082512315270935957, 0.096059113300492605, 0.096059113300492605, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.86576354679802958, 0.83128078817733986, 0.82635467980295563, 0.81773399014778325], 5: [0.051724137931034482, 0.072660098522167482, 0.077586206896551727, 0.081280788177339899], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380406 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.46908
Epoch 1, cost is  6.3521
Epoch 2, cost is  6.59058
Epoch 3, cost is  6.95915
Epoch 4, cost is  7.38834
Training took 0.155305 minutes
Weight histogram
[461 525 615 712 471 530 442 249  43   2] [-0.09835714 -0.0888969  -0.07943666 -0.06997642 -0.06051618 -0.05105594
 -0.0415957  -0.03213546 -0.02267522 -0.01321498 -0.00375474]
[171 298 378 412 425 489 510 467 457 443] [-0.09835714 -0.0888969  -0.07943666 -0.06997642 -0.06051618 -0.05105594
 -0.0415957  -0.03213546 -0.02267522 -0.01321498 -0.00375474]
-13.1575
16.2427
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.31557
Epoch 1, cost is  4.28402
Epoch 2, cost is  4.57159
Epoch 3, cost is  4.92876
Epoch 4, cost is  5.31634
Training took 0.084996 minutes
Weight histogram
[ 499  592 1064  895  457  313  149  346 1465  295] [-0.28715926 -0.25881881 -0.23047836 -0.20213791 -0.17379746 -0.145457
 -0.11711655 -0.0887761  -0.06043565 -0.03209519 -0.00375474]
[379 685 889 970 797 539 494 457 437 428] [-0.28715926 -0.25881881 -0.23047836 -0.20213791 -0.17379746 -0.145457
 -0.11711655 -0.0887761  -0.06043565 -0.03209519 -0.00375474]
-13.272
16.3168
fine tuning ...
Epoch 0
Fine tuning took 0.062211 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060417 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060765 minutes
{0: [0.18965517241379309, 0.10837438423645321, 0.13300492610837439, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.79679802955665024, 0.76354679802955661, 0.75], 5: [0.098522167487684734, 0.094827586206896547, 0.10344827586206896, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380461 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.46908
Epoch 1, cost is  6.3521
Epoch 2, cost is  6.59058
Epoch 3, cost is  6.95915
Epoch 4, cost is  7.38834
Training took 0.152255 minutes
Weight histogram
[461 525 615 712 471 530 442 249  43   2] [-0.09835714 -0.0888969  -0.07943666 -0.06997642 -0.06051618 -0.05105594
 -0.0415957  -0.03213546 -0.02267522 -0.01321498 -0.00375474]
[171 298 378 412 425 489 510 467 457 443] [-0.09835714 -0.0888969  -0.07943666 -0.06997642 -0.06051618 -0.05105594
 -0.0415957  -0.03213546 -0.02267522 -0.01321498 -0.00375474]
-13.1575
16.2427
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.6244
Epoch 1, cost is  2.37422
Epoch 2, cost is  2.39116
Epoch 3, cost is  2.47128
Epoch 4, cost is  2.55125
Training took 0.109582 minutes
Weight histogram
[ 641  735 1382  459  374  246  695  949  574   20] [-0.17608808 -0.15885475 -0.14162141 -0.12438808 -0.10715474 -0.08992141
 -0.07268808 -0.05545474 -0.03822141 -0.02098808 -0.00375474]
[250 394 557 670 734 778 883 804 525 480] [-0.17608808 -0.15885475 -0.14162141 -0.12438808 -0.10715474 -0.08992141
 -0.07268808 -0.05545474 -0.03822141 -0.02098808 -0.00375474]
-7.6593
9.26404
fine tuning ...
Epoch 0
Fine tuning took 0.063189 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063818 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062822 minutes
{0: [0.18226600985221675, 0.16871921182266009, 0.16625615763546797, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72783251231527091, 0.70812807881773399, 0.69704433497536944, 0.69334975369458129], 5: [0.089901477832512317, 0.12315270935960591, 0.13669950738916256, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382121 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.46908
Epoch 1, cost is  6.3521
Epoch 2, cost is  6.59058
Epoch 3, cost is  6.95915
Epoch 4, cost is  7.38834
Training took 0.154063 minutes
Weight histogram
[461 525 615 712 471 530 442 249  43   2] [-0.09835714 -0.0888969  -0.07943666 -0.06997642 -0.06051618 -0.05105594
 -0.0415957  -0.03213546 -0.02267522 -0.01321498 -0.00375474]
[171 298 378 412 425 489 510 467 457 443] [-0.09835714 -0.0888969  -0.07943666 -0.06997642 -0.06051618 -0.05105594
 -0.0415957  -0.03213546 -0.02267522 -0.01321498 -0.00375474]
-13.1575
16.2427
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  1.97273
Epoch 1, cost is  1.67517
Epoch 2, cost is  1.62973
Epoch 3, cost is  1.61909
Epoch 4, cost is  1.64338
Training took 0.150113 minutes
Weight histogram
[ 791  961 1057  486  484  871  775  499  148    3] [-0.12182619 -0.11001905 -0.0982119  -0.08640476 -0.07459761 -0.06279047
 -0.05098332 -0.03917618 -0.02736903 -0.01556189 -0.00375474]
[262 418 608 742 897 876 872 892 254 254] [-0.12182619 -0.11001905 -0.0982119  -0.08640476 -0.07459761 -0.06279047
 -0.05098332 -0.03917618 -0.02736903 -0.01556189 -0.00375474]
-7.6593
9.26404
fine tuning ...
Epoch 0
Fine tuning took 0.068661 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069338 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067725 minutes
{0: [0.15763546798029557, 0.1625615763546798, 0.16995073891625614, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.72413793103448276, 0.68719211822660098, 0.68842364532019706], 5: [0.13916256157635468, 0.11330049261083744, 0.14285714285714285, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.413837 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.41911
Epoch 1, cost is  6.29361
Epoch 2, cost is  6.46339
Epoch 3, cost is  6.72825
Epoch 4, cost is  7.01602
Training took 0.233859 minutes
Weight histogram
[507 524 573 820 582 696 345   1   1   1] [-0.07875437 -0.07122332 -0.06369226 -0.0561612  -0.04863015 -0.04109909
 -0.03356803 -0.02603698 -0.01850592 -0.01097486 -0.00344381]
[213 322 399 424 425 484 466 446 434 437] [-0.07875437 -0.07122332 -0.06369226 -0.0561612  -0.04863015 -0.04109909
 -0.03356803 -0.02603698 -0.01850592 -0.01097486 -0.00344381]
-16.3846
17.8377
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.07333
Epoch 1, cost is  3.08158
Epoch 2, cost is  3.3027
Epoch 3, cost is  3.55662
Epoch 4, cost is  3.84542
Training took 0.112713 minutes
Weight histogram
[ 935 1315 1166  300  176   82   44   24 1733  300] [-0.2934128  -0.2644159  -0.235419   -0.2064221  -0.1774252  -0.1484283
 -0.11943141 -0.09043451 -0.06143761 -0.03244071 -0.00344381]
[ 484  818 1007 1085  529  498  427  415  407  405] [-0.2934128  -0.2644159  -0.235419   -0.2064221  -0.1774252  -0.1484283
 -0.11943141 -0.09043451 -0.06143761 -0.03244071 -0.00344381]
-15.1663
13.3712
fine tuning ...
Epoch 0
Fine tuning took 0.068914 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069551 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.071095 minutes
{0: [0.18719211822660098, 0.13300492610837439, 0.16995073891625614, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.76600985221674878, 0.73399014778325122, 0.70320197044334976], 5: [0.05295566502463054, 0.10098522167487685, 0.096059113300492605, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.414682 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.41911
Epoch 1, cost is  6.29361
Epoch 2, cost is  6.46339
Epoch 3, cost is  6.72825
Epoch 4, cost is  7.01602
Training took 0.221173 minutes
Weight histogram
[507 524 573 820 582 696 345   1   1   1] [-0.07875437 -0.07122332 -0.06369226 -0.0561612  -0.04863015 -0.04109909
 -0.03356803 -0.02603698 -0.01850592 -0.01097486 -0.00344381]
[213 322 399 424 425 484 466 446 434 437] [-0.07875437 -0.07122332 -0.06369226 -0.0561612  -0.04863015 -0.04109909
 -0.03356803 -0.02603698 -0.01850592 -0.01097486 -0.00344381]
-16.3846
17.8377
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  1.89534
Epoch 1, cost is  1.70271
Epoch 2, cost is  1.72117
Epoch 3, cost is  1.7755
Epoch 4, cost is  1.8422
Training took 0.151393 minutes
Weight histogram
[ 969 1565  911  285  166   92   44 1313  727    3] [-0.17593482 -0.15868572 -0.14143662 -0.12418752 -0.10693841 -0.08968931
 -0.07244021 -0.05519111 -0.03794201 -0.02069291 -0.00344381]
[302 486 654 770 834 841 797 506 440 445] [-0.17593482 -0.15868572 -0.14143662 -0.12418752 -0.10693841 -0.08968931
 -0.07244021 -0.05519111 -0.03794201 -0.02069291 -0.00344381]
-10.554
10.7261
fine tuning ...
Epoch 0
Fine tuning took 0.074128 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.077075 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.077298 minutes
{0: [0.17610837438423646, 0.2019704433497537, 0.23029556650246305, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72290640394088668, 0.67364532019704437, 0.60221674876847286, 0.60221674876847286], 5: [0.10098522167487685, 0.12438423645320197, 0.16748768472906403, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.400151 minutes
Weight histogram
[  13  116  300  648 1130 1184  530  112    8    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[109 157 228 309 426 566 427 405 596 827] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-0.738286
0.704568
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.41911
Epoch 1, cost is  6.29361
Epoch 2, cost is  6.46339
Epoch 3, cost is  6.72825
Epoch 4, cost is  7.01602
Training took 0.212310 minutes
Weight histogram
[507 524 573 820 582 696 345   1   1   1] [-0.07875437 -0.07122332 -0.06369226 -0.0561612  -0.04863015 -0.04109909
 -0.03356803 -0.02603698 -0.01850592 -0.01097486 -0.00344381]
[213 322 399 424 425 484 466 446 434 437] [-0.07875437 -0.07122332 -0.06369226 -0.0561612  -0.04863015 -0.04109909
 -0.03356803 -0.02603698 -0.01850592 -0.01097486 -0.00344381]
-16.3846
17.8377
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  1.44529
Epoch 1, cost is  1.20845
Epoch 2, cost is  1.16817
Epoch 3, cost is  1.16296
Epoch 4, cost is  1.16988
Training took 0.225079 minutes
Weight histogram
[ 871 1696  829  309  193  212 1004  956    4    1] [-0.12491462 -0.11276754 -0.10062046 -0.08847338 -0.0763263  -0.06417921
 -0.05203213 -0.03988505 -0.02773797 -0.01559089 -0.00344381]
[280 433 603 764 868 742 771 785 584 245] [-0.12491462 -0.11276754 -0.10062046 -0.08847338 -0.0763263  -0.06417921
 -0.05203213 -0.03988505 -0.02773797 -0.01559089 -0.00344381]
-10.554
10.7261
fine tuning ...
Epoch 0
Fine tuning took 0.081572 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.082178 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079095 minutes
{0: [0.24630541871921183, 0.21921182266009853, 0.23522167487684728, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.62684729064039413, 0.59359605911330049, 0.58743842364532017], 5: [0.10837438423645321, 0.1539408866995074, 0.17118226600985223, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382913 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  12.4961
Epoch 1, cost is  12.3374
Epoch 2, cost is  12.9572
Epoch 3, cost is  13.7709
Epoch 4, cost is  14.6536
Training took 0.108259 minutes
Weight histogram
[ 521 1193  341  844 1044  825  862  352   90    3] [-0.11831721 -0.10679105 -0.09526489 -0.08373874 -0.07221258 -0.06068642
 -0.04916026 -0.0376341  -0.02610795 -0.01458179 -0.00305563]
[248 524 654 688 718 637 633 721 642 610] [-0.11831721 -0.10679105 -0.09526489 -0.08373874 -0.07221258 -0.06068642
 -0.04916026 -0.0376341  -0.02610795 -0.01458179 -0.00305563]
-30.6778
32.9371
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.90196
Epoch 1, cost is  6.85909
Epoch 2, cost is  7.25119
Epoch 3, cost is  7.69926
Epoch 4, cost is  8.19648
Training took 0.070349 minutes
Weight histogram
[ 533 1469 1557 1214  608  341  228 1242  843   65] [-0.21037006 -0.18963862 -0.16890718 -0.14817573 -0.12744429 -0.10671285
 -0.0859814  -0.06524996 -0.04451852 -0.02378707 -0.00305563]
[ 393  810 1107 1216 1292  681  670  706  631  594] [-0.21037006 -0.18963862 -0.16890718 -0.14817573 -0.12744429 -0.10671285
 -0.0859814  -0.06524996 -0.04451852 -0.02378707 -0.00305563]
-17.0693
17.7402
fine tuning ...
Epoch 0
Fine tuning took 0.052960 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052544 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051758 minutes
{0: [0.062807881773399021, 0.078817733990147784, 0.066502463054187194, 0.054187192118226604], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.8645320197044335, 0.8571428571428571, 0.85344827586206895, 0.88177339901477836], 5: [0.072660098522167482, 0.064039408866995079, 0.080049261083743842, 0.064039408866995079], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382347 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  12.4961
Epoch 1, cost is  12.3374
Epoch 2, cost is  12.9572
Epoch 3, cost is  13.7709
Epoch 4, cost is  14.6536
Training took 0.107306 minutes
Weight histogram
[ 521 1193  341  844 1044  825  862  352   90    3] [-0.11831721 -0.10679105 -0.09526489 -0.08373874 -0.07221258 -0.06068642
 -0.04916026 -0.0376341  -0.02610795 -0.01458179 -0.00305563]
[248 524 654 688 718 637 633 721 642 610] [-0.11831721 -0.10679105 -0.09526489 -0.08373874 -0.07221258 -0.06068642
 -0.04916026 -0.0376341  -0.02610795 -0.01458179 -0.00305563]
-30.6778
32.9371
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.59439
Epoch 1, cost is  3.31441
Epoch 2, cost is  3.32894
Epoch 3, cost is  3.37679
Epoch 4, cost is  3.47326
Training took 0.084115 minutes
Weight histogram
[ 784 1730 1711  765  451  351 1402  692  198   16] [-0.1389415  -0.12535291 -0.11176432 -0.09817574 -0.08458715 -0.07099856
 -0.05740998 -0.04382139 -0.0302328  -0.01664422 -0.00305563]
[ 254  460  698  877 1109 1049 1018 1043  939  653] [-0.1389415  -0.12535291 -0.11176432 -0.09817574 -0.08458715 -0.07099856
 -0.05740998 -0.04382139 -0.0302328  -0.01664422 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.055181 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054588 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053969 minutes
{0: [0.12315270935960591, 0.11206896551724138, 0.11330049261083744, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79679802955665024, 0.80049261083743839, 0.81403940886699511, 0.79064039408866993], 5: [0.080049261083743842, 0.087438423645320201, 0.072660098522167482, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382440 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  12.4961
Epoch 1, cost is  12.3374
Epoch 2, cost is  12.9572
Epoch 3, cost is  13.7709
Epoch 4, cost is  14.6536
Training took 0.105920 minutes
Weight histogram
[ 521 1193  341  844 1044  825  862  352   90    3] [-0.11831721 -0.10679105 -0.09526489 -0.08373874 -0.07221258 -0.06068642
 -0.04916026 -0.0376341  -0.02610795 -0.01458179 -0.00305563]
[248 524 654 688 718 637 633 721 642 610] [-0.11831721 -0.10679105 -0.09526489 -0.08373874 -0.07221258 -0.06068642
 -0.04916026 -0.0376341  -0.02610795 -0.01458179 -0.00305563]
-30.6778
32.9371
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.19397
Epoch 1, cost is  1.92623
Epoch 2, cost is  1.84955
Epoch 3, cost is  1.83261
Epoch 4, cost is  1.82969
Training took 0.106349 minutes
Weight histogram
[1166 1672 1504  641  482 1310  901  328   89    7] [-0.10962062 -0.09896412 -0.08830762 -0.07765113 -0.06699463 -0.05633813
 -0.04568163 -0.03502513 -0.02436863 -0.01371213 -0.00305563]
[ 313  661 1038 1391 1354 1525 1039  261  252  266] [-0.10962062 -0.09896412 -0.08830762 -0.07765113 -0.06699463 -0.05633813
 -0.04568163 -0.03502513 -0.02436863 -0.01371213 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.056074 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057264 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057162 minutes
{0: [0.094827586206896547, 0.11699507389162561, 0.091133004926108374, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.82758620689655171, 0.80049261083743839, 0.81773399014778325, 0.76477832512315269], 5: [0.077586206896551727, 0.082512315270935957, 0.091133004926108374, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381291 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  9.22169
Epoch 1, cost is  8.82625
Epoch 2, cost is  9.04616
Epoch 3, cost is  9.43695
Epoch 4, cost is  9.86771
Training took 0.149939 minutes
Weight histogram
[ 646  748  604  472  850 1012  672  763  305    3] [-0.14625193 -0.13200221 -0.11775249 -0.10350277 -0.08925306 -0.07500334
 -0.06075362 -0.0465039  -0.03225418 -0.01800446 -0.00375474]
[287 518 600 646 752 667 664 684 638 619] [-0.14625193 -0.13200221 -0.11775249 -0.10350277 -0.08925306 -0.07500334
 -0.06075362 -0.0465039  -0.03225418 -0.01800446 -0.00375474]
-17.9168
21.3475
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.63261
Epoch 1, cost is  6.54742
Epoch 2, cost is  6.86993
Epoch 3, cost is  7.31877
Epoch 4, cost is  7.76847
Training took 0.085985 minutes
Weight histogram
[ 683  616 1116  846 1539  635  435  143 1399  688] [-0.37391874 -0.33690234 -0.29988594 -0.26286954 -0.22585314 -0.18883674
 -0.15182034 -0.11480394 -0.07778754 -0.04077114 -0.00375474]
[ 691 1264 1477  829  725  662  658  625  595  574] [-0.37391874 -0.33690234 -0.29988594 -0.26286954 -0.22585314 -0.18883674
 -0.15182034 -0.11480394 -0.07778754 -0.04077114 -0.00375474]
-21.8055
23.4367
fine tuning ...
Epoch 0
Fine tuning took 0.060558 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059550 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060667 minutes
{0: [0.1206896551724138, 0.13916256157635468, 0.15024630541871922, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78817733990147787, 0.78201970443349755, 0.76108374384236455, 0.72290640394088668], 5: [0.091133004926108374, 0.078817733990147784, 0.088669950738916259, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380803 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  9.22169
Epoch 1, cost is  8.82625
Epoch 2, cost is  9.04616
Epoch 3, cost is  9.43695
Epoch 4, cost is  9.86771
Training took 0.153354 minutes
Weight histogram
[ 646  748  604  472  850 1012  672  763  305    3] [-0.14625193 -0.13200221 -0.11775249 -0.10350277 -0.08925306 -0.07500334
 -0.06075362 -0.0465039  -0.03225418 -0.01800446 -0.00375474]
[287 518 600 646 752 667 664 684 638 619] [-0.14625193 -0.13200221 -0.11775249 -0.10350277 -0.08925306 -0.07500334
 -0.06075362 -0.0465039  -0.03225418 -0.01800446 -0.00375474]
-17.9168
21.3475
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.38233
Epoch 1, cost is  3.19584
Epoch 2, cost is  3.26707
Epoch 3, cost is  3.4022
Epoch 4, cost is  3.54342
Training took 0.111054 minutes
Weight histogram
[ 689  777 1284 1196 1157  488  315 1041 1045  108] [-0.22283646 -0.20092829 -0.17902012 -0.15711195 -0.13520378 -0.1132956
 -0.09138743 -0.06947926 -0.04757109 -0.02566291 -0.00375474]
[ 404  742  988 1119 1248  971  708  670  632  618] [-0.22283646 -0.20092829 -0.17902012 -0.15711195 -0.13520378 -0.1132956
 -0.09138743 -0.06947926 -0.04757109 -0.02566291 -0.00375474]
-10.5387
11.5005
fine tuning ...
Epoch 0
Fine tuning took 0.064462 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.064275 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062841 minutes
{0: [0.16625615763546797, 0.18226600985221675, 0.23029556650246305, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.71305418719211822, 0.65270935960591137, 0.67364532019704437], 5: [0.13054187192118227, 0.10467980295566502, 0.11699507389162561, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379987 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  9.22169
Epoch 1, cost is  8.82625
Epoch 2, cost is  9.04616
Epoch 3, cost is  9.43695
Epoch 4, cost is  9.86771
Training took 0.152209 minutes
Weight histogram
[ 646  748  604  472  850 1012  672  763  305    3] [-0.14625193 -0.13200221 -0.11775249 -0.10350277 -0.08925306 -0.07500334
 -0.06075362 -0.0465039  -0.03225418 -0.01800446 -0.00375474]
[287 518 600 646 752 667 664 684 638 619] [-0.14625193 -0.13200221 -0.11775249 -0.10350277 -0.08925306 -0.07500334
 -0.06075362 -0.0465039  -0.03225418 -0.01800446 -0.00375474]
-17.9168
21.3475
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.18828
Epoch 1, cost is  1.91036
Epoch 2, cost is  1.85199
Epoch 3, cost is  1.84139
Epoch 4, cost is  1.84169
Training took 0.152256 minutes
Weight histogram
[1040 1447  878 1447  543  702  973  795  272    3] [-0.14315288 -0.12921306 -0.11527325 -0.10133344 -0.08739362 -0.07345381
 -0.059514   -0.04557418 -0.03163437 -0.01769456 -0.00375474]
[ 284  467  669  820  974  933  934 1049 1068  902] [-0.14315288 -0.12921306 -0.11527325 -0.10133344 -0.08739362 -0.07345381
 -0.059514   -0.04557418 -0.03163437 -0.01769456 -0.00375474]
-7.6593
9.49369
fine tuning ...
Epoch 0
Fine tuning took 0.068148 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069614 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068926 minutes
{0: [0.15886699507389163, 0.18596059113300492, 0.17980295566502463, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.7068965517241379, 0.70443349753694584, 0.6785714285714286], 5: [0.1206896551724138, 0.10714285714285714, 0.11576354679802955, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380495 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.14354
Epoch 1, cost is  7.40742
Epoch 2, cost is  7.29457
Epoch 3, cost is  7.32713
Epoch 4, cost is  7.42311
Training took 0.209041 minutes
Weight histogram
[ 709  776  538  705  804 1085  922  533    2    1] [-0.1112758  -0.1004926  -0.0897094  -0.0789262  -0.068143   -0.0573598
 -0.0465766  -0.0357934  -0.0250102  -0.014227   -0.00344381]
[319 497 581 591 667 605 599 766 738 712] [-0.1112758  -0.1004926  -0.0897094  -0.0789262  -0.068143   -0.0573598
 -0.0465766  -0.0357934  -0.0250102  -0.014227   -0.00344381]
-20.7213
22.0437
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.25538
Epoch 1, cost is  5.1686
Epoch 2, cost is  5.44826
Epoch 3, cost is  5.79494
Epoch 4, cost is  6.1801
Training took 0.105180 minutes
Weight histogram
[ 619  984 2128 1649  366  189   80   44 1293  748] [-0.35029092 -0.31560621 -0.2809215  -0.24623679 -0.21155208 -0.17686736
 -0.14218265 -0.10749794 -0.07281323 -0.03812852 -0.00344381]
[ 893 1489 1349  750  647  628  636  594  562  552] [-0.35029092 -0.31560621 -0.2809215  -0.24623679 -0.21155208 -0.17686736
 -0.14218265 -0.10749794 -0.07281323 -0.03812852 -0.00344381]
-20.2427
20.0258
fine tuning ...
Epoch 0
Fine tuning took 0.070241 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068839 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070184 minutes
{0: [0.22167487684729065, 0.16871921182266009, 0.14778325123152711, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65394088669950734, 0.73275862068965514, 0.72044334975369462, 0.68965517241379315], 5: [0.12438423645320197, 0.098522167487684734, 0.13177339901477833, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381510 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.14354
Epoch 1, cost is  7.40742
Epoch 2, cost is  7.29457
Epoch 3, cost is  7.32713
Epoch 4, cost is  7.42311
Training took 0.219002 minutes
Weight histogram
[ 709  776  538  705  804 1085  922  533    2    1] [-0.1112758  -0.1004926  -0.0897094  -0.0789262  -0.068143   -0.0573598
 -0.0465766  -0.0357934  -0.0250102  -0.014227   -0.00344381]
[319 497 581 591 667 605 599 766 738 712] [-0.1112758  -0.1004926  -0.0897094  -0.0789262  -0.068143   -0.0573598
 -0.0465766  -0.0357934  -0.0250102  -0.014227   -0.00344381]
-20.7213
22.0437
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.93007
Epoch 1, cost is  2.72941
Epoch 2, cost is  2.80318
Epoch 3, cost is  2.91588
Epoch 4, cost is  3.05952
Training took 0.151118 minutes
Weight histogram
[ 745  976 2115 1544  365  192   95  672 1392    4] [-0.21249561 -0.19159043 -0.17068525 -0.14978007 -0.12887489 -0.10796971
 -0.08706453 -0.06615935 -0.04525417 -0.02434899 -0.00344381]
[ 544  968 1232 1305 1027  689  651  575  558  551] [-0.21249561 -0.19159043 -0.17068525 -0.14978007 -0.12887489 -0.10796971
 -0.08706453 -0.06615935 -0.04525417 -0.02434899 -0.00344381]
-12.2197
14.0007
fine tuning ...
Epoch 0
Fine tuning took 0.076072 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074169 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.075572 minutes
{0: [0.30172413793103448, 0.2105911330049261, 0.26477832512315269, 0.29310344827586204], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56034482758620685, 0.63916256157635465, 0.58374384236453203, 0.53940886699507384], 5: [0.13793103448275862, 0.15024630541871922, 0.15147783251231528, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381442 minutes
Weight histogram
[  13  116  315  869 1833 1952  804  155    9    9] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 122  177  266  385  484  658  353  587 1416 1627] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.00243
0.820403
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.14354
Epoch 1, cost is  7.40742
Epoch 2, cost is  7.29457
Epoch 3, cost is  7.32713
Epoch 4, cost is  7.42311
Training took 0.211264 minutes
Weight histogram
[ 709  776  538  705  804 1085  922  533    2    1] [-0.1112758  -0.1004926  -0.0897094  -0.0789262  -0.068143   -0.0573598
 -0.0465766  -0.0357934  -0.0250102  -0.014227   -0.00344381]
[319 497 581 591 667 605 599 766 738 712] [-0.1112758  -0.1004926  -0.0897094  -0.0789262  -0.068143   -0.0573598
 -0.0465766  -0.0357934  -0.0250102  -0.014227   -0.00344381]
-20.7213
22.0437
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  1.95745
Epoch 1, cost is  1.67183
Epoch 2, cost is  1.62593
Epoch 3, cost is  1.629
Epoch 4, cost is  1.64049
Training took 0.208133 minutes
Weight histogram
[ 878 1693 2339  601  300  171  796 1177  143    2] [-0.14048484 -0.12678074 -0.11307663 -0.09937253 -0.08566843 -0.07196432
 -0.05826022 -0.04455612 -0.03085201 -0.01714791 -0.00344381]
[ 368  610  882 1063  924  965 1006  969  663  650] [-0.14048484 -0.12678074 -0.11307663 -0.09937253 -0.08566843 -0.07196432
 -0.05826022 -0.04455612 -0.03085201 -0.01714791 -0.00344381]
-10.554
10.7261
fine tuning ...
Epoch 0
Fine tuning took 0.080226 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.080824 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.081415 minutes
{0: [0.27709359605911332, 0.21921182266009853, 0.27216748768472904, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.58004926108374388, 0.53817733990147787, 0.55172413793103448], 5: [0.2019704433497537, 0.20073891625615764, 0.18965517241379309, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381951 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  17.2288
Epoch 1, cost is  16.8125
Epoch 2, cost is  17.3963
Epoch 3, cost is  18.2513
Epoch 4, cost is  19.2359
Training took 0.110078 minutes
Weight histogram
[ 724 1168  482 1470  538 1359 1110  970  259   20] [-0.15463524 -0.13947728 -0.12431931 -0.10916135 -0.09400339 -0.07884543
 -0.06368747 -0.04852951 -0.03337155 -0.01821359 -0.00305563]
[399 814 931 932 840 951 854 849 789 741] [-0.15463524 -0.13947728 -0.12431931 -0.10916135 -0.09400339 -0.07884543
 -0.06368747 -0.04852951 -0.03337155 -0.01821359 -0.00305563]
-37.6689
43.7363
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.1012
Epoch 1, cost is  9.91084
Epoch 2, cost is  10.2944
Epoch 3, cost is  10.8204
Epoch 4, cost is  11.3545
Training took 0.072186 minutes
Weight histogram
[ 620 1006 1818 1947 1468  626  390  538 1577  135] [-0.24982199 -0.22514535 -0.20046872 -0.17579208 -0.15111545 -0.12643881
 -0.10176217 -0.07708554 -0.0524089  -0.02773227 -0.00305563]
[ 652 1364 1647 1488  902  967  843  811  744  707] [-0.24982199 -0.22514535 -0.20046872 -0.17579208 -0.15111545 -0.12643881
 -0.10176217 -0.07708554 -0.0524089  -0.02773227 -0.00305563]
-22.7373
21.0049
fine tuning ...
Epoch 0
Fine tuning took 0.051600 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051944 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051648 minutes
{0: [0.10221674876847291, 0.10098522167487685, 0.099753694581280791, 0.1145320197044335], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83743842364532017, 0.80541871921182262, 0.81403940886699511, 0.79679802955665024], 5: [0.060344827586206899, 0.093596059113300489, 0.086206896551724144, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.383534 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  17.2288
Epoch 1, cost is  16.8125
Epoch 2, cost is  17.3963
Epoch 3, cost is  18.2513
Epoch 4, cost is  19.2359
Training took 0.106838 minutes
Weight histogram
[ 724 1168  482 1470  538 1359 1110  970  259   20] [-0.15463524 -0.13947728 -0.12431931 -0.10916135 -0.09400339 -0.07884543
 -0.06368747 -0.04852951 -0.03337155 -0.01821359 -0.00305563]
[399 814 931 932 840 951 854 849 789 741] [-0.15463524 -0.13947728 -0.12431931 -0.10916135 -0.09400339 -0.07884543
 -0.06368747 -0.04852951 -0.03337155 -0.01821359 -0.00305563]
-37.6689
43.7363
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.49907
Epoch 1, cost is  4.2283
Epoch 2, cost is  4.25186
Epoch 3, cost is  4.34712
Epoch 4, cost is  4.46355
Training took 0.084132 minutes
Weight histogram
[ 982 1743 1894 1901  677  435 1013 1133  324   23] [-0.1570068  -0.14161168 -0.12621657 -0.11082145 -0.09542633 -0.08003121
 -0.0646361  -0.04924098 -0.03384586 -0.01845075 -0.00305563]
[ 367  768 1119 1440 1370 1364 1173  886  843  795] [-0.1570068  -0.14161168 -0.12621657 -0.11082145 -0.09542633 -0.08003121
 -0.0646361  -0.04924098 -0.03384586 -0.01845075 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.052991 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053994 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054400 minutes
{0: [0.1206896551724138, 0.13793103448275862, 0.14162561576354679, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80049261083743839, 0.76724137931034486, 0.77709359605911332, 0.74507389162561577], 5: [0.078817733990147784, 0.094827586206896547, 0.081280788177339899, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382695 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  17.2288
Epoch 1, cost is  16.8125
Epoch 2, cost is  17.3963
Epoch 3, cost is  18.2513
Epoch 4, cost is  19.2359
Training took 0.105612 minutes
Weight histogram
[ 724 1168  482 1470  538 1359 1110  970  259   20] [-0.15463524 -0.13947728 -0.12431931 -0.10916135 -0.09400339 -0.07884543
 -0.06368747 -0.04852951 -0.03337155 -0.01821359 -0.00305563]
[399 814 931 932 840 951 854 849 789 741] [-0.15463524 -0.13947728 -0.12431931 -0.10916135 -0.09400339 -0.07884543
 -0.06368747 -0.04852951 -0.03337155 -0.01821359 -0.00305563]
-37.6689
43.7363
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.29427
Epoch 1, cost is  2.00852
Epoch 2, cost is  1.93789
Epoch 3, cost is  1.91279
Epoch 4, cost is  1.89469
Training took 0.106043 minutes
Weight histogram
[1303 1773 1922 1533  652  953 1322  519  135   13] [-0.12343726 -0.1113991  -0.09936094 -0.08732277 -0.07528461 -0.06324645
 -0.05120828 -0.03917012 -0.02713196 -0.01509379 -0.00305563]
[ 313  661 1038 1391 1354 1525 1546 1536  495  266] [-0.12343726 -0.1113991  -0.09936094 -0.08732277 -0.07528461 -0.06324645
 -0.05120828 -0.03917012 -0.02713196 -0.01509379 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.057072 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056948 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056477 minutes
{0: [0.15517241379310345, 0.15024630541871922, 0.12807881773399016, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75615763546798032, 0.74876847290640391, 0.74753694581280783, 0.70566502463054193], 5: [0.088669950738916259, 0.10098522167487685, 0.12438423645320197, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381963 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  11.0574
Epoch 1, cost is  10.467
Epoch 2, cost is  10.5527
Epoch 3, cost is  10.8104
Epoch 4, cost is  11.1588
Training took 0.153271 minutes
Weight histogram
[ 909 1099  534  959  569 1021 1292  999  678   40] [-0.1912695  -0.17251803 -0.15376655 -0.13501507 -0.1162636  -0.09751212
 -0.07876065 -0.06000917 -0.04125769 -0.02250622 -0.00375474]
[429 733 801 961 854 885 831 855 900 851] [-0.1912695  -0.17251803 -0.15376655 -0.13501507 -0.1162636  -0.09751212
 -0.07876065 -0.06000917 -0.04125769 -0.02250622 -0.00375474]
-23.1511
27.7785
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  9.59005
Epoch 1, cost is  9.41388
Epoch 2, cost is  9.78437
Epoch 3, cost is  10.2564
Epoch 4, cost is  10.7884
Training took 0.086562 minutes
Weight histogram
[ 816  876 1012  864 1303 1727  942  446  833 1306] [-0.48028424 -0.43263129 -0.38497834 -0.33732539 -0.28967244 -0.24201949
 -0.19436654 -0.14671359 -0.09906064 -0.05140769 -0.00375474]
[1103 1922 1313  959  893  860  801  795  752  727] [-0.48028424 -0.43263129 -0.38497834 -0.33732539 -0.28967244 -0.24201949
 -0.19436654 -0.14671359 -0.09906064 -0.05140769 -0.00375474]
-25.5789
29.0786
fine tuning ...
Epoch 0
Fine tuning took 0.059987 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061571 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063187 minutes
{0: [0.24507389162561577, 0.22536945812807882, 0.24261083743842365, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6280788177339901, 0.68596059113300489, 0.61822660098522164, 0.63669950738916259], 5: [0.1268472906403941, 0.088669950738916259, 0.13916256157635468, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.411138 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  11.0574
Epoch 1, cost is  10.467
Epoch 2, cost is  10.5527
Epoch 3, cost is  10.8104
Epoch 4, cost is  11.1588
Training took 0.152362 minutes
Weight histogram
[ 909 1099  534  959  569 1021 1292  999  678   40] [-0.1912695  -0.17251803 -0.15376655 -0.13501507 -0.1162636  -0.09751212
 -0.07876065 -0.06000917 -0.04125769 -0.02250622 -0.00375474]
[429 733 801 961 854 885 831 855 900 851] [-0.1912695  -0.17251803 -0.15376655 -0.13501507 -0.1162636  -0.09751212
 -0.07876065 -0.06000917 -0.04125769 -0.02250622 -0.00375474]
-23.1511
27.7785
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.49745
Epoch 1, cost is  4.24481
Epoch 2, cost is  4.29243
Epoch 3, cost is  4.37883
Epoch 4, cost is  4.52676
Training took 0.111023 minutes
Weight histogram
[ 954  992  983 1349 1287 1569  584  667 1485  255] [-0.2762101  -0.24896456 -0.22171903 -0.19447349 -0.16722796 -0.13998242
 -0.11273689 -0.08549135 -0.05824581 -0.03100028 -0.00375474]
[ 615 1181 1466 1635 1053  910  837  837  806  785] [-0.2762101  -0.24896456 -0.22171903 -0.19447349 -0.16722796 -0.13998242
 -0.11273689 -0.08549135 -0.05824581 -0.03100028 -0.00375474]
-13.4323
15.8413
fine tuning ...
Epoch 0
Fine tuning took 0.066881 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.064925 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.065447 minutes
{0: [0.23891625615763548, 0.25615763546798032, 0.28817733990147781, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61822660098522164, 0.60467980295566504, 0.56650246305418717, 0.56403940886699511], 5: [0.14285714285714285, 0.13916256157635468, 0.14532019704433496, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.383433 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  11.0574
Epoch 1, cost is  10.467
Epoch 2, cost is  10.5527
Epoch 3, cost is  10.8104
Epoch 4, cost is  11.1588
Training took 0.152289 minutes
Weight histogram
[ 909 1099  534  959  569 1021 1292  999  678   40] [-0.1912695  -0.17251803 -0.15376655 -0.13501507 -0.1162636  -0.09751212
 -0.07876065 -0.06000917 -0.04125769 -0.02250622 -0.00375474]
[429 733 801 961 854 885 831 855 900 851] [-0.1912695  -0.17251803 -0.15376655 -0.13501507 -0.1162636  -0.09751212
 -0.07876065 -0.06000917 -0.04125769 -0.02250622 -0.00375474]
-23.1511
27.7785
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.5661
Epoch 1, cost is  2.28816
Epoch 2, cost is  2.23391
Epoch 3, cost is  2.231
Epoch 4, cost is  2.24439
Training took 0.155994 minutes
Weight histogram
[1123 1437 1497 1192 1614  650 1055 1033  518    6] [-0.16837446 -0.15191249 -0.13545052 -0.11898855 -0.10252658 -0.0860646
 -0.06960263 -0.05314066 -0.03667869 -0.02021671 -0.00375474]
[ 390  714  997 1224 1182 1266 1375 1132  926  919] [-0.16837446 -0.15191249 -0.13545052 -0.11898855 -0.10252658 -0.0860646
 -0.06960263 -0.05314066 -0.03667869 -0.02021671 -0.00375474]
-8.47774
11.5271
fine tuning ...
Epoch 0
Fine tuning took 0.070940 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069520 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.071957 minutes
{0: [0.24630541871921183, 0.28078817733990147, 0.28817733990147781, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59729064039408863, 0.54802955665024633, 0.5788177339901478, 0.5714285714285714], 5: [0.15640394088669951, 0.17118226600985223, 0.13300492610837439, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.383737 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.83383
Epoch 1, cost is  8.10126
Epoch 2, cost is  7.95118
Epoch 3, cost is  7.9358
Epoch 4, cost is  8.00823
Training took 0.208723 minutes
Weight histogram
[ 939 1063  672 1050  671  959 1330 1215  199    2] [-0.14394368 -0.12989369 -0.11584371 -0.10179372 -0.08774373 -0.07369374
 -0.05964376 -0.04559377 -0.03154378 -0.01749379 -0.00344381]
[441 700 745 838 770 868 960 932 940 906] [-0.14394368 -0.12989369 -0.11584371 -0.10179372 -0.08774373 -0.07369374
 -0.05964376 -0.04559377 -0.03154378 -0.01749379 -0.00344381]
-24.1025
24.0851
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.87991
Epoch 1, cost is  7.6713
Epoch 2, cost is  8.01639
Epoch 3, cost is  8.44957
Epoch 4, cost is  8.9347
Training took 0.108446 minutes
Weight histogram
[ 701  845 1339 2101 2172  631  206   78  614 1438] [-0.42854261 -0.38603273 -0.34352285 -0.30101297 -0.25850309 -0.21599321
 -0.17348333 -0.13097345 -0.08846357 -0.04595369 -0.00344381]
[1448 2134 1039  892  891  821  778  747  700  675] [-0.42854261 -0.38603273 -0.34352285 -0.30101297 -0.25850309 -0.21599321
 -0.17348333 -0.13097345 -0.08846357 -0.04595369 -0.00344381]
-29.6665
29.4344
fine tuning ...
Epoch 0
Fine tuning took 0.068085 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070605 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070716 minutes
{0: [0.25738916256157635, 0.19581280788177341, 0.23029556650246305, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65394088669950734, 0.66256157635467983, 0.63177339901477836, 0.60960591133004927], 5: [0.088669950738916259, 0.14162561576354679, 0.13793103448275862, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.409668 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.83383
Epoch 1, cost is  8.10126
Epoch 2, cost is  7.95118
Epoch 3, cost is  7.9358
Epoch 4, cost is  8.00823
Training took 0.232150 minutes
Weight histogram
[ 939 1063  672 1050  671  959 1330 1215  199    2] [-0.14394368 -0.12989369 -0.11584371 -0.10179372 -0.08774373 -0.07369374
 -0.05964376 -0.04559377 -0.03154378 -0.01749379 -0.00344381]
[441 700 745 838 770 868 960 932 940 906] [-0.14394368 -0.12989369 -0.11584371 -0.10179372 -0.08774373 -0.07369374
 -0.05964376 -0.04559377 -0.03154378 -0.01749379 -0.00344381]
-24.1025
24.0851
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.13479
Epoch 1, cost is  3.87028
Epoch 2, cost is  3.95151
Epoch 3, cost is  4.09421
Epoch 4, cost is  4.2588
Training took 0.168473 minutes
Weight histogram
[ 790  844 1343 1884 2440  480  226   75 1990   53] [-0.26188454 -0.23604047 -0.21019639 -0.18435232 -0.15850825 -0.13266417
 -0.1068201  -0.08097603 -0.05513195 -0.02928788 -0.00344381]
[ 881 1571 1798 1203  925  794  771  759  720  703] [-0.26188454 -0.23604047 -0.21019639 -0.18435232 -0.15850825 -0.13266417
 -0.1068201  -0.08097603 -0.05513195 -0.02928788 -0.00344381]
-18.1755
17.6305
fine tuning ...
Epoch 0
Fine tuning took 0.074480 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.075355 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.075195 minutes
{0: [0.34729064039408869, 0.29926108374384236, 0.33251231527093594, 0.35837438423645318], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48152709359605911, 0.49137931034482757, 0.44581280788177341, 0.39901477832512317], 5: [0.17118226600985223, 0.20935960591133004, 0.22167487684729065, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379289 minutes
Weight histogram
[  13  116  315  869 1845 2084 1385 1042  368   63] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 132  201  287  433  573  589  473  747 2068 2597] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.02166
0.894647
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.83383
Epoch 1, cost is  8.10126
Epoch 2, cost is  7.95118
Epoch 3, cost is  7.9358
Epoch 4, cost is  8.00823
Training took 0.209450 minutes
Weight histogram
[ 939 1063  672 1050  671  959 1330 1215  199    2] [-0.14394368 -0.12989369 -0.11584371 -0.10179372 -0.08774373 -0.07369374
 -0.05964376 -0.04559377 -0.03154378 -0.01749379 -0.00344381]
[441 700 745 838 770 868 960 932 940 906] [-0.14394368 -0.12989369 -0.11584371 -0.10179372 -0.08774373 -0.07369374
 -0.05964376 -0.04559377 -0.03154378 -0.01749379 -0.00344381]
-24.1025
24.0851
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.4343
Epoch 1, cost is  2.11009
Epoch 2, cost is  2.07084
Epoch 3, cost is  2.07413
Epoch 4, cost is  2.09964
Training took 0.208994 minutes
Weight histogram
[ 941 1651 1752 2505  745  300  317 1406  505    3] [-0.16387872 -0.14783523 -0.13179174 -0.11574825 -0.09970476 -0.08366126
 -0.06761777 -0.05157428 -0.03553079 -0.0194873  -0.00344381]
[ 538  982 1387 1234 1297 1312  888  868  815  804] [-0.16387872 -0.14783523 -0.13179174 -0.11574825 -0.09970476 -0.08366126
 -0.06761777 -0.05157428 -0.03553079 -0.0194873  -0.00344381]
-10.554
10.7261
fine tuning ...
Epoch 0
Fine tuning took 0.079730 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.080467 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079873 minutes
{0: [0.34852216748768472, 0.3288177339901478, 0.31896551724137934, 0.3251231527093596], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45812807881773399, 0.49507389162561577, 0.43226600985221675, 0.49014778325123154], 5: [0.19334975369458129, 0.17610837438423646, 0.24876847290640394, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380824 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  21.7583
Epoch 1, cost is  20.6788
Epoch 2, cost is  20.9818
Epoch 3, cost is  21.612
Epoch 4, cost is  22.389
Training took 0.106606 minutes
Weight histogram
[ 988 1050 1141  919 1533  849 1581 1484  535   45] [-0.18964005 -0.1709816  -0.15232316 -0.13366472 -0.11500628 -0.09634784
 -0.0776894  -0.05903095 -0.04037251 -0.02171407 -0.00305563]
[ 564 1060 1186 1044 1152 1031 1026  926 1115 1021] [-0.18964005 -0.1709816  -0.15232316 -0.13366472 -0.11500628 -0.09634784
 -0.0776894  -0.05903095 -0.04037251 -0.02171407 -0.00305563]
-55.1108
52.1913
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  12.3862
Epoch 1, cost is  11.9066
Epoch 2, cost is  12.1531
Epoch 3, cost is  12.496
Epoch 4, cost is  12.9785
Training took 0.070892 minutes
Weight histogram
[1056 1665 1261 2155 1998 1154  486  287 1891  197] [-0.2781533  -0.25064353 -0.22313377 -0.195624   -0.16811423 -0.14060446
 -0.1130947  -0.08558493 -0.05807516 -0.0305654  -0.00305563]
[ 913 1846 2095 1123 1169 1031  944  880 1122 1027] [-0.2781533  -0.25064353 -0.22313377 -0.195624   -0.16811423 -0.14060446
 -0.1130947  -0.08558493 -0.05807516 -0.0305654  -0.00305563]
-25.8363
28.7613
fine tuning ...
Epoch 0
Fine tuning took 0.050712 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052008 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051049 minutes
{0: [0.089901477832512317, 0.10960591133004927, 0.12561576354679804, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.83866995073891626, 0.8214285714285714, 0.81650246305418717, 0.79679802955665024], 5: [0.071428571428571425, 0.068965517241379309, 0.057881773399014777, 0.067733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381661 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  21.7583
Epoch 1, cost is  20.6788
Epoch 2, cost is  20.9818
Epoch 3, cost is  21.612
Epoch 4, cost is  22.389
Training took 0.105562 minutes
Weight histogram
[ 988 1050 1141  919 1533  849 1581 1484  535   45] [-0.18964005 -0.1709816  -0.15232316 -0.13366472 -0.11500628 -0.09634784
 -0.0776894  -0.05903095 -0.04037251 -0.02171407 -0.00305563]
[ 564 1060 1186 1044 1152 1031 1026  926 1115 1021] [-0.18964005 -0.1709816  -0.15232316 -0.13366472 -0.11500628 -0.09634784
 -0.0776894  -0.05903095 -0.04037251 -0.02171407 -0.00305563]
-55.1108
52.1913
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.22193
Epoch 1, cost is  4.83919
Epoch 2, cost is  4.83974
Epoch 3, cost is  4.87309
Epoch 4, cost is  4.99526
Training took 0.084705 minutes
Weight histogram
[1151 1191 1503 2344 2356  731  518 1724  581   51] [-0.18780422 -0.16932936 -0.1508545  -0.13237964 -0.11390478 -0.09542993
 -0.07695507 -0.05848021 -0.04000535 -0.02153049 -0.00305563]
[ 511 1096 1629 1696 1670 1309 1073  994 1113 1059] [-0.18780422 -0.16932936 -0.1508545  -0.13237964 -0.11390478 -0.09542993
 -0.07695507 -0.05848021 -0.04000535 -0.02153049 -0.00305563]
-14.0319
14.5055
fine tuning ...
Epoch 0
Fine tuning took 0.052546 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052556 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054052 minutes
{0: [0.14408866995073891, 0.1354679802955665, 0.15763546798029557, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.78325123152709364, 0.75492610837438423, 0.74014778325123154], 5: [0.11083743842364532, 0.081280788177339899, 0.087438423645320201, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381586 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  21.7583
Epoch 1, cost is  20.6788
Epoch 2, cost is  20.9818
Epoch 3, cost is  21.612
Epoch 4, cost is  22.389
Training took 0.106226 minutes
Weight histogram
[ 988 1050 1141  919 1533  849 1581 1484  535   45] [-0.18964005 -0.1709816  -0.15232316 -0.13366472 -0.11500628 -0.09634784
 -0.0776894  -0.05903095 -0.04037251 -0.02171407 -0.00305563]
[ 564 1060 1186 1044 1152 1031 1026  926 1115 1021] [-0.18964005 -0.1709816  -0.15232316 -0.13366472 -0.11500628 -0.09634784
 -0.0776894  -0.05903095 -0.04037251 -0.02171407 -0.00305563]
-55.1108
52.1913
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.30792
Epoch 1, cost is  2.04526
Epoch 2, cost is  1.96503
Epoch 3, cost is  1.9223
Epoch 4, cost is  1.92022
Training took 0.106008 minutes
Weight histogram
[1732 1576 2067 2015 1419  639 1639  838  200   25] [-0.13856557 -0.12501458 -0.11146358 -0.09791259 -0.08436159 -0.0708106
 -0.05725961 -0.04370861 -0.03015762 -0.01660662 -0.00305563]
[ 313  661 1038 1391 1354 1525 1546 1536 1653 1133] [-0.13856557 -0.12501458 -0.11146358 -0.09791259 -0.08436159 -0.0708106
 -0.05725961 -0.04370861 -0.03015762 -0.01660662 -0.00305563]
-11.1723
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.055989 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055937 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057043 minutes
{0: [0.15024630541871922, 0.11699507389162561, 0.16133004926108374, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7573891625615764, 0.80541871921182262, 0.75246305418719217, 0.71551724137931039], 5: [0.092364532019704432, 0.077586206896551727, 0.086206896551724144, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380424 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.2972
Epoch 1, cost is  12.5181
Epoch 2, cost is  12.5003
Epoch 3, cost is  12.713
Epoch 4, cost is  13.0117
Training took 0.150441 minutes
Weight histogram
[1123  885 1143  908 1150  876 1307 1475 1111  147] [-0.23940341 -0.21583854 -0.19227368 -0.16870881 -0.14514394 -0.12157908
 -0.09801421 -0.07444934 -0.05088448 -0.02731961 -0.00375474]
[ 586  940 1134 1066 1086 1009 1115 1062 1094 1033] [-0.23940341 -0.21583854 -0.19227368 -0.16870881 -0.14514394 -0.12157908
 -0.09801421 -0.07444934 -0.05088448 -0.02731961 -0.00375474]
-29.4542
33.9159
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  11.8767
Epoch 1, cost is  11.3777
Epoch 2, cost is  11.5067
Epoch 3, cost is  11.8042
Epoch 4, cost is  12.1866
Training took 0.085085 minutes
Weight histogram
[1137 1313 1071 1135 1047 1556 1873  780  507 1731] [-0.56425941 -0.50820894 -0.45215848 -0.39610801 -0.34005754 -0.28400708
 -0.22795661 -0.17190614 -0.11585568 -0.05980521 -0.00375474]
[1546 2228 1283 1118 1062  997  959  911 1051  995] [-0.56425941 -0.50820894 -0.45215848 -0.39610801 -0.34005754 -0.28400708
 -0.22795661 -0.17190614 -0.11585568 -0.05980521 -0.00375474]
-33.5156
33.9974
fine tuning ...
Epoch 0
Fine tuning took 0.059811 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059833 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060711 minutes
{0: [0.19950738916256158, 0.20320197044334976, 0.2019704433497537, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66871921182266014, 0.70073891625615758, 0.66379310344827591, 0.62561576354679804], 5: [0.13177339901477833, 0.096059113300492605, 0.13423645320197045, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379985 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.2972
Epoch 1, cost is  12.5181
Epoch 2, cost is  12.5003
Epoch 3, cost is  12.713
Epoch 4, cost is  13.0117
Training took 0.152472 minutes
Weight histogram
[1123  885 1143  908 1150  876 1307 1475 1111  147] [-0.23940341 -0.21583854 -0.19227368 -0.16870881 -0.14514394 -0.12157908
 -0.09801421 -0.07444934 -0.05088448 -0.02731961 -0.00375474]
[ 586  940 1134 1066 1086 1009 1115 1062 1094 1033] [-0.23940341 -0.21583854 -0.19227368 -0.16870881 -0.14514394 -0.12157908
 -0.09801421 -0.07444934 -0.05088448 -0.02731961 -0.00375474]
-29.4542
33.9159
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.61154
Epoch 1, cost is  5.2967
Epoch 2, cost is  5.31666
Epoch 3, cost is  5.42022
Epoch 4, cost is  5.55316
Training took 0.110204 minutes
Weight histogram
[1144 1353 1097 1275 1553 1729 1337  481 1695  486] [-0.32715985 -0.29481934 -0.26247883 -0.23013832 -0.19779781 -0.1654573
 -0.13311679 -0.10077628 -0.06843576 -0.03609525 -0.00375474]
[ 874 1627 2007 1439 1120 1041 1030  980 1035  997] [-0.32715985 -0.29481934 -0.26247883 -0.23013832 -0.19779781 -0.1654573
 -0.13311679 -0.10077628 -0.06843576 -0.03609525 -0.00375474]
-16.1797
18.9628
fine tuning ...
Epoch 0
Fine tuning took 0.062980 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061816 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063062 minutes
{0: [0.23152709359605911, 0.27463054187192121, 0.26108374384236455, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.58743842364532017, 0.62561576354679804, 0.59359605911330049], 5: [0.1268472906403941, 0.13793103448275862, 0.11330049261083744, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380806 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.2972
Epoch 1, cost is  12.5181
Epoch 2, cost is  12.5003
Epoch 3, cost is  12.713
Epoch 4, cost is  13.0117
Training took 0.150774 minutes
Weight histogram
[1123  885 1143  908 1150  876 1307 1475 1111  147] [-0.23940341 -0.21583854 -0.19227368 -0.16870881 -0.14514394 -0.12157908
 -0.09801421 -0.07444934 -0.05088448 -0.02731961 -0.00375474]
[ 586  940 1134 1066 1086 1009 1115 1062 1094 1033] [-0.23940341 -0.21583854 -0.19227368 -0.16870881 -0.14514394 -0.12157908
 -0.09801421 -0.07444934 -0.05088448 -0.02731961 -0.00375474]
-29.4542
33.9159
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.8102
Epoch 1, cost is  2.48511
Epoch 2, cost is  2.41547
Epoch 3, cost is  2.38013
Epoch 4, cost is  2.36194
Training took 0.152032 minutes
Weight histogram
[1551 1412 1557 1818 1461 1383  860 1326  733   49] [-0.19362906 -0.17464162 -0.15565419 -0.13666676 -0.11767933 -0.0986919
 -0.07970447 -0.06071704 -0.04172961 -0.02274217 -0.00375474]
[ 503  972 1367 1441 1506 1652 1228 1107 1185 1189] [-0.19362906 -0.17464162 -0.15565419 -0.13666676 -0.11767933 -0.0986919
 -0.07970447 -0.06071704 -0.04172961 -0.02274217 -0.00375474]
-10.3893
13.1095
fine tuning ...
Epoch 0
Fine tuning took 0.068292 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068077 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069210 minutes
{0: [0.25123152709359609, 0.27955665024630544, 0.25985221674876846, 0.27955665024630544], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.59605911330049266, 0.62315270935960587, 0.6071428571428571], 5: [0.13177339901477833, 0.12438423645320197, 0.11699507389162561, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.383118 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  9.94003
Epoch 1, cost is  9.01053
Epoch 2, cost is  8.79963
Epoch 3, cost is  8.76316
Epoch 4, cost is  8.79179
Training took 0.209555 minutes
Weight histogram
[1145  876 1179 1002 1172 1022 1271 1644  812    2] [-0.17975786 -0.16212646 -0.14449505 -0.12686365 -0.10923224 -0.09160083
 -0.07396943 -0.05633802 -0.03870662 -0.02107521 -0.00344381]
[ 581  878  986  951 1058 1149 1148 1118 1147 1109] [-0.17975786 -0.16212646 -0.14449505 -0.12686365 -0.10923224 -0.09160083
 -0.07396943 -0.05633802 -0.03870662 -0.02107521 -0.00344381]
-31.1164
29.7029
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.8585
Epoch 1, cost is  10.4033
Epoch 2, cost is  10.6115
Epoch 3, cost is  10.9852
Epoch 4, cost is  11.4162
Training took 0.105893 minutes
Weight histogram
[1023 1383  862 1623 2496 2148  404  134   52 2025] [-0.50376302 -0.4537311  -0.40369918 -0.35366726 -0.30363533 -0.25360341
 -0.20357149 -0.15353957 -0.10350765 -0.05347573 -0.00344381]
[2041 2144 1176 1128 1027  981  896  899  955  903] [-0.50376302 -0.4537311  -0.40369918 -0.35366726 -0.30363533 -0.25360341
 -0.20357149 -0.15353957 -0.10350765 -0.05347573 -0.00344381]
-33.629
37.2612
fine tuning ...
Epoch 0
Fine tuning took 0.067789 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068113 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068772 minutes
{0: [0.31896551724137934, 0.2413793103448276, 0.19704433497536947, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.54556650246305416, 0.64039408866995073, 0.67487684729064035, 0.62438423645320196], 5: [0.1354679802955665, 0.11822660098522167, 0.12807881773399016, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380482 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  9.94003
Epoch 1, cost is  9.01053
Epoch 2, cost is  8.79963
Epoch 3, cost is  8.76316
Epoch 4, cost is  8.79179
Training took 0.207286 minutes
Weight histogram
[1145  876 1179 1002 1172 1022 1271 1644  812    2] [-0.17975786 -0.16212646 -0.14449505 -0.12686365 -0.10923224 -0.09160083
 -0.07396943 -0.05633802 -0.03870662 -0.02107521 -0.00344381]
[ 581  878  986  951 1058 1149 1148 1118 1147 1109] [-0.17975786 -0.16212646 -0.14449505 -0.12686365 -0.10923224 -0.09160083
 -0.07396943 -0.05633802 -0.03870662 -0.02107521 -0.00344381]
-31.1164
29.7029
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.46233
Epoch 1, cost is  5.08147
Epoch 2, cost is  5.12433
Epoch 3, cost is  5.2343
Epoch 4, cost is  5.38732
Training took 0.152650 minutes
Weight histogram
[1034 1357  924 1532 2142 2489  441  165 1653  413] [-0.31234774 -0.28145735 -0.25056695 -0.21967656 -0.18878617 -0.15789577
 -0.12700538 -0.09611499 -0.06522459 -0.0343342  -0.00344381]
[1263 2196 1839 1170 1008  980  926  925  939  904] [-0.31234774 -0.28145735 -0.25056695 -0.21967656 -0.18878617 -0.15789577
 -0.12700538 -0.09611499 -0.06522459 -0.0343342  -0.00344381]
-21.0649
20.6107
fine tuning ...
Epoch 0
Fine tuning took 0.073437 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073935 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.074291 minutes
{0: [0.3682266009852217, 0.35467980295566504, 0.39285714285714285, 0.32266009852216748], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45566502463054187, 0.47783251231527096, 0.43596059113300495, 0.47167487684729065], 5: [0.17610837438423646, 0.16748768472906403, 0.17118226600985223, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380083 minutes
Weight histogram
[  13  116  315  869 1845 2177 1952 1911  797  130] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
[ 144  233  329  536  742  437  668 1928 2599 2509] [ -3.47374647e-04  -2.59417045e-04  -1.71459443e-04  -8.35018407e-05
   4.45576152e-06   9.24133637e-05   1.80370966e-04   2.68328568e-04
   3.56286170e-04   4.44243773e-04   5.32201375e-04]
-1.05275
0.908243
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  9.94003
Epoch 1, cost is  9.01053
Epoch 2, cost is  8.79963
Epoch 3, cost is  8.76316
Epoch 4, cost is  8.79179
Training took 0.206307 minutes
Weight histogram
[1145  876 1179 1002 1172 1022 1271 1644  812    2] [-0.17975786 -0.16212646 -0.14449505 -0.12686365 -0.10923224 -0.09160083
 -0.07396943 -0.05633802 -0.03870662 -0.02107521 -0.00344381]
[ 581  878  986  951 1058 1149 1148 1118 1147 1109] [-0.17975786 -0.16212646 -0.14449505 -0.12686365 -0.10923224 -0.09160083
 -0.07396943 -0.05633802 -0.03870662 -0.02107521 -0.00344381]
-31.1164
29.7029
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.97418
Epoch 1, cost is  2.61285
Epoch 2, cost is  2.55795
Epoch 3, cost is  2.55756
Epoch 4, cost is  2.58494
Training took 0.210379 minutes
Weight histogram
[1058 1665 1541 2146 2758  579  279 1094 1027    3] [-0.18967013 -0.1710475  -0.15242487 -0.13380223 -0.1151796  -0.09655697
 -0.07793434 -0.0593117  -0.04068907 -0.02206644 -0.00344381]
[ 752 1452 1659 1627 1545 1089 1056 1018  999  953] [-0.18967013 -0.1710475  -0.15242487 -0.13380223 -0.1151796  -0.09655697
 -0.07793434 -0.0593117  -0.04068907 -0.02206644 -0.00344381]
-12.0286
12.227
fine tuning ...
Epoch 0
Fine tuning took 0.079329 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.080667 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.080125 minutes
{0: [0.25, 0.3608374384236453, 0.31896551724137934, 0.34852216748768472], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50492610837438423, 0.46182266009852219, 0.44704433497536944, 0.45443349753694579], 5: [0.24507389162561577, 0.17733990147783252, 0.23399014778325122, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380261 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  27.1316
Epoch 1, cost is  25.8207
Epoch 2, cost is  26.0176
Epoch 3, cost is  26.6217
Epoch 4, cost is  27.386
Training took 0.107677 minutes
Weight histogram
[ 842 1029  474 1664 1612 1217 1578 1956 1631  147] [-0.25626042 -0.23093995 -0.20561947 -0.18029899 -0.15497851 -0.12965803
 -0.10433755 -0.07901707 -0.05369659 -0.02837611 -0.00305563]
[ 775 1351 1356 1358 1263 1220 1222 1280 1204 1121] [-0.25626042 -0.23093995 -0.20561947 -0.18029899 -0.15497851 -0.12965803
 -0.10433755 -0.07901707 -0.05369659 -0.02837611 -0.00305563]
-62.6403
57.9424
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  16.2102
Epoch 1, cost is  15.6574
Epoch 2, cost is  15.948
Epoch 3, cost is  16.413
Epoch 4, cost is  16.959
Training took 0.070616 minutes
Weight histogram
[1152 1277 1908 1432 2582 2255  942  467 1772  388] [-0.32964784 -0.29698862 -0.2643294  -0.23167018 -0.19901096 -0.16635173
 -0.13369251 -0.10103329 -0.06837407 -0.03571485 -0.00305563]
[1259 2402 1947 1408 1269 1128 1210 1290 1176 1086] [-0.32964784 -0.29698862 -0.2643294  -0.23167018 -0.19901096 -0.16635173
 -0.13369251 -0.10103329 -0.06837407 -0.03571485 -0.00305563]
-32.6113
32.9883
fine tuning ...
Epoch 0
Fine tuning took 0.052575 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051769 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052072 minutes
{0: [0.1268472906403941, 0.13669950738916256, 0.12192118226600986, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80665024630541871, 0.78694581280788178, 0.80172413793103448, 0.77955665024630538], 5: [0.066502463054187194, 0.076354679802955669, 0.076354679802955669, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382164 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  27.1316
Epoch 1, cost is  25.8207
Epoch 2, cost is  26.0176
Epoch 3, cost is  26.6217
Epoch 4, cost is  27.386
Training took 0.107375 minutes
Weight histogram
[ 842 1029  474 1664 1612 1217 1578 1956 1631  147] [-0.25626042 -0.23093995 -0.20561947 -0.18029899 -0.15497851 -0.12965803
 -0.10433755 -0.07901707 -0.05369659 -0.02837611 -0.00305563]
[ 775 1351 1356 1358 1263 1220 1222 1280 1204 1121] [-0.25626042 -0.23093995 -0.20561947 -0.18029899 -0.15497851 -0.12965803
 -0.10433755 -0.07901707 -0.05369659 -0.02837611 -0.00305563]
-62.6403
57.9424
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.97763
Epoch 1, cost is  5.60807
Epoch 2, cost is  5.58589
Epoch 3, cost is  5.59712
Epoch 4, cost is  5.63227
Training took 0.085329 minutes
Weight histogram
[1507 1902 1315 2119 2438 1801  609 1593  820   71] [-0.20623615 -0.1859181  -0.16560005 -0.145282   -0.12496394 -0.10464589
 -0.08432784 -0.06400979 -0.04369173 -0.02337368 -0.00305563]
[ 668 1488 2058 2001 1645 1264 1223 1300 1280 1248] [-0.20623615 -0.1859181  -0.16560005 -0.145282   -0.12496394 -0.10464589
 -0.08432784 -0.06400979 -0.04369173 -0.02337368 -0.00305563]
-17.1077
16.3675
fine tuning ...
Epoch 0
Fine tuning took 0.053115 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054819 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053846 minutes
{0: [0.20689655172413793, 0.16995073891625614, 0.20320197044334976, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69334975369458129, 0.74384236453201968, 0.71059113300492616, 0.72044334975369462], 5: [0.099753694581280791, 0.086206896551724144, 0.086206896551724144, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382323 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  27.1316
Epoch 1, cost is  25.8207
Epoch 2, cost is  26.0176
Epoch 3, cost is  26.6217
Epoch 4, cost is  27.386
Training took 0.105712 minutes
Weight histogram
[ 842 1029  474 1664 1612 1217 1578 1956 1631  147] [-0.25626042 -0.23093995 -0.20561947 -0.18029899 -0.15497851 -0.12965803
 -0.10433755 -0.07901707 -0.05369659 -0.02837611 -0.00305563]
[ 775 1351 1356 1358 1263 1220 1222 1280 1204 1121] [-0.25626042 -0.23093995 -0.20561947 -0.18029899 -0.15497851 -0.12965803
 -0.10433755 -0.07901707 -0.05369659 -0.02837611 -0.00305563]
-62.6403
57.9424
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.79901
Epoch 1, cost is  2.53836
Epoch 2, cost is  2.47537
Epoch 3, cost is  2.41566
Epoch 4, cost is  2.37003
Training took 0.107453 minutes
Weight histogram
[1710 2193 1801 2757 1927  834 1334 1278  304   37] [-0.15403461 -0.13893672 -0.12383882 -0.10874092 -0.09364302 -0.07854512
 -0.06344722 -0.04834932 -0.03325143 -0.01815353 -0.00305563]
[ 386  849 1376 1547 1687 1721 1775 1888 1664 1282] [-0.15403461 -0.13893672 -0.12383882 -0.10874092 -0.09364302 -0.07854512
 -0.06344722 -0.04834932 -0.03325143 -0.01815353 -0.00305563]
-11.4029
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.055924 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057059 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.058279 minutes
{0: [0.17857142857142858, 0.18596059113300492, 0.2376847290640394, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.72536945812807885, 0.65024630541871919, 0.69334975369458129], 5: [0.094827586206896547, 0.088669950738916259, 0.11206896551724138, 0.076354679802955669], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.413457 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  15.873
Epoch 1, cost is  14.9364
Epoch 2, cost is  14.8524
Epoch 3, cost is  15.0001
Epoch 4, cost is  15.2581
Training took 0.169626 minutes
Weight histogram
[1366  734 1535  978 1457 1219 1183 1864 1490  324] [-0.29411784 -0.26508153 -0.23604522 -0.20700891 -0.1779726  -0.14893629
 -0.11989998 -0.09086367 -0.06182736 -0.03279105 -0.00375474]
[ 753 1163 1371 1288 1217 1321 1285 1269 1257 1226] [-0.29411784 -0.26508153 -0.23604522 -0.20700891 -0.1779726  -0.14893629
 -0.11989998 -0.09086367 -0.06182736 -0.03279105 -0.00375474]
-36.2733
39.7832
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  16.0371
Epoch 1, cost is  15.3505
Epoch 2, cost is  15.4857
Epoch 3, cost is  15.88
Epoch 4, cost is  16.3495
Training took 0.092125 minutes
Weight histogram
[1195 1202 1484 1418 1355 1510 1981 1536  424 2070] [-0.6784693  -0.61099784 -0.54352639 -0.47605493 -0.40858348 -0.34111202
 -0.27364057 -0.20616911 -0.13869765 -0.0712262  -0.00375474]
[2065 2331 1414 1323 1222 1156 1201 1236 1140 1087] [-0.6784693  -0.61099784 -0.54352639 -0.47605493 -0.40858348 -0.34111202
 -0.27364057 -0.20616911 -0.13869765 -0.0712262  -0.00375474]
-38.7877
44.6879
fine tuning ...
Epoch 0
Fine tuning took 0.059881 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060087 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061106 minutes
{0: [0.22413793103448276, 0.20073891625615764, 0.19704433497536947, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.67980295566502458, 0.71798029556650245, 0.69088669950738912], 5: [0.1268472906403941, 0.11945812807881774, 0.084975369458128072, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.415425 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  15.873
Epoch 1, cost is  14.9364
Epoch 2, cost is  14.8524
Epoch 3, cost is  15.0001
Epoch 4, cost is  15.2581
Training took 0.170468 minutes
Weight histogram
[1366  734 1535  978 1457 1219 1183 1864 1490  324] [-0.29411784 -0.26508153 -0.23604522 -0.20700891 -0.1779726  -0.14893629
 -0.11989998 -0.09086367 -0.06182736 -0.03279105 -0.00375474]
[ 753 1163 1371 1288 1217 1321 1285 1269 1257 1226] [-0.29411784 -0.26508153 -0.23604522 -0.20700891 -0.1779726  -0.14893629
 -0.11989998 -0.09086367 -0.06182736 -0.03279105 -0.00375474]
-36.2733
39.7832
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.27018
Epoch 1, cost is  6.90091
Epoch 2, cost is  6.91265
Epoch 3, cost is  7.02773
Epoch 4, cost is  7.18024
Training took 0.122185 minutes
Weight histogram
[1185 1408 1530 1360 1441 1859 2228  844 1525  795] [-0.38957521 -0.35099317 -0.31241112 -0.27382907 -0.23524702 -0.19666498
 -0.15808293 -0.11950088 -0.08091884 -0.04233679 -0.00375474]
[1181 2160 2211 1395 1278 1237 1236 1230 1146 1101] [-0.38957521 -0.35099317 -0.31241112 -0.27382907 -0.23524702 -0.19666498
 -0.15808293 -0.11950088 -0.08091884 -0.04233679 -0.00375474]
-20.6955
23.1969
fine tuning ...
Epoch 0
Fine tuning took 0.062688 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062946 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.065925 minutes
{0: [0.27463054187192121, 0.27463054187192121, 0.2894088669950739, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58743842364532017, 0.62684729064039413, 0.5923645320197044, 0.5923645320197044], 5: [0.13793103448275862, 0.098522167487684734, 0.11822660098522167, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.412251 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  15.873
Epoch 1, cost is  14.9364
Epoch 2, cost is  14.8524
Epoch 3, cost is  15.0001
Epoch 4, cost is  15.2581
Training took 0.166147 minutes
Weight histogram
[1366  734 1535  978 1457 1219 1183 1864 1490  324] [-0.29411784 -0.26508153 -0.23604522 -0.20700891 -0.1779726  -0.14893629
 -0.11989998 -0.09086367 -0.06182736 -0.03279105 -0.00375474]
[ 753 1163 1371 1288 1217 1321 1285 1269 1257 1226] [-0.29411784 -0.26508153 -0.23604522 -0.20700891 -0.1779726  -0.14893629
 -0.11989998 -0.09086367 -0.06182736 -0.03279105 -0.00375474]
-36.2733
39.7832
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.21287
Epoch 1, cost is  2.8533
Epoch 2, cost is  2.77166
Epoch 3, cost is  2.73068
Epoch 4, cost is  2.71163
Training took 0.161085 minutes
Weight histogram
[1566 1677 1674 1823 2001 2001  835 1404 1088  106] [-0.22150509 -0.19973006 -0.17795502 -0.15617999 -0.13440495 -0.11262992
 -0.09085488 -0.06907985 -0.04730481 -0.02552978 -0.00375474]
[ 638 1274 1692 1687 1936 1492 1297 1420 1382 1357] [-0.22150509 -0.19973006 -0.17795502 -0.15617999 -0.13440495 -0.11262992
 -0.09085488 -0.06907985 -0.04730481 -0.02552978 -0.00375474]
-12.3257
15.3729
fine tuning ...
Epoch 0
Fine tuning took 0.068315 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070179 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070234 minutes
{0: [0.29310344827586204, 0.27216748768472904, 0.2857142857142857, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53448275862068961, 0.58990147783251234, 0.59113300492610843, 0.62684729064039413], 5: [0.17241379310344829, 0.13793103448275862, 0.12315270935960591, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.412429 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.7783
Epoch 1, cost is  9.82903
Epoch 2, cost is  9.54281
Epoch 3, cost is  9.46288
Epoch 4, cost is  9.4635
Training took 0.230149 minutes
Weight histogram
[1773  745 1508 1045 1220 1527 1087 1846 1396    3] [-0.21308567 -0.19212148 -0.17115729 -0.15019311 -0.12922892 -0.10826474
 -0.08730055 -0.06633636 -0.04537218 -0.02440799 -0.00344381]
[ 718 1057 1166 1091 1415 1341 1303 1346 1358 1355] [-0.21308567 -0.19212148 -0.17115729 -0.15019311 -0.12922892 -0.10826474
 -0.08730055 -0.06633636 -0.04537218 -0.02440799 -0.00344381]
-35.4338
32.5347
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  14.3779
Epoch 1, cost is  13.7987
Epoch 2, cost is  13.9907
Epoch 3, cost is  14.3873
Epoch 4, cost is  14.8832
Training took 0.115318 minutes
Weight histogram
[1004 1290 1300 1487 1761 2915 1976  330   77 2035] [-0.61174935 -0.5509188  -0.49008824 -0.42925769 -0.36842713 -0.30759658
 -0.24676602 -0.18593547 -0.12510491 -0.06427436 -0.00344381]
[2742 2028 1408 1307 1206 1094 1179 1126 1071 1014] [-0.61174935 -0.5509188  -0.49008824 -0.42925769 -0.36842713 -0.30759658
 -0.24676602 -0.18593547 -0.12510491 -0.06427436 -0.00344381]
-40.672
43.9746
fine tuning ...
Epoch 0
Fine tuning took 0.067690 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068610 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067167 minutes
{0: [0.29802955665024633, 0.25985221674876846, 0.26477832512315269, 0.34113300492610837], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.58743842364532017, 0.57635467980295563, 0.52093596059113301], 5: [0.15147783251231528, 0.15270935960591134, 0.15886699507389163, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380783 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.7783
Epoch 1, cost is  9.82903
Epoch 2, cost is  9.54281
Epoch 3, cost is  9.46288
Epoch 4, cost is  9.4635
Training took 0.211504 minutes
Weight histogram
[1773  745 1508 1045 1220 1527 1087 1846 1396    3] [-0.21308567 -0.19212148 -0.17115729 -0.15019311 -0.12922892 -0.10826474
 -0.08730055 -0.06633636 -0.04537218 -0.02440799 -0.00344381]
[ 718 1057 1166 1091 1415 1341 1303 1346 1358 1355] [-0.21308567 -0.19212148 -0.17115729 -0.15019311 -0.12922892 -0.10826474
 -0.08730055 -0.06633636 -0.04537218 -0.02440799 -0.00344381]
-35.4338
32.5347
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  7.32221
Epoch 1, cost is  6.90715
Epoch 2, cost is  6.96085
Epoch 3, cost is  7.1228
Epoch 4, cost is  7.31686
Training took 0.154204 minutes
Weight histogram
[1105 1137 1230 1548 1733 2651 2274  387 1045 1065] [-0.38245034 -0.34454969 -0.30664903 -0.26874838 -0.23084773 -0.19294707
 -0.15504642 -0.11714577 -0.07924511 -0.04134446 -0.00344381]
[1784 2840 1617 1276 1225 1151 1193 1134  985  970] [-0.38245034 -0.34454969 -0.30664903 -0.26874838 -0.23084773 -0.19294707
 -0.15504642 -0.11714577 -0.07924511 -0.04134446 -0.00344381]
-26.4858
26.1435
fine tuning ...
Epoch 0
Fine tuning took 0.073393 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073943 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.073336 minutes
{0: [0.45689655172413796, 0.41256157635467983, 0.38669950738916259, 0.36699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.38669950738916259, 0.44458128078817732, 0.43472906403940886, 0.4642857142857143], 5: [0.15640394088669951, 0.14285714285714285, 0.17857142857142858, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382579 minutes
Weight histogram
[  27  222  670 2086 2632 2458 1984 1240  713  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 3825] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.14014
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.7783
Epoch 1, cost is  9.82903
Epoch 2, cost is  9.54281
Epoch 3, cost is  9.46288
Epoch 4, cost is  9.4635
Training took 0.208639 minutes
Weight histogram
[1773  745 1508 1045 1220 1527 1087 1846 1396    3] [-0.21308567 -0.19212148 -0.17115729 -0.15019311 -0.12922892 -0.10826474
 -0.08730055 -0.06633636 -0.04537218 -0.02440799 -0.00344381]
[ 718 1057 1166 1091 1415 1341 1303 1346 1358 1355] [-0.21308567 -0.19212148 -0.17115729 -0.15019311 -0.12922892 -0.10826474
 -0.08730055 -0.06633636 -0.04537218 -0.02440799 -0.00344381]
-35.4338
32.5347
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.63039
Epoch 1, cost is  3.21969
Epoch 2, cost is  3.15097
Epoch 3, cost is  3.13565
Epoch 4, cost is  3.1711
Training took 0.209485 minutes
Weight histogram
[1221 1620 1768 1848 2646 2353  485  761 1469    4] [-0.21887781 -0.19733441 -0.17579101 -0.15424761 -0.13270421 -0.11116081
 -0.08961741 -0.06807401 -0.04653061 -0.02498721 -0.00344381]
[1013 1999 1945 1977 1342 1261 1251 1178 1122 1087] [-0.21887781 -0.19733441 -0.17579101 -0.15424761 -0.13270421 -0.11116081
 -0.08961741 -0.06807401 -0.04653061 -0.02498721 -0.00344381]
-19.6966
18.6517
fine tuning ...
Epoch 0
Fine tuning took 0.079155 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079064 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.079397 minutes
{0: [0.42241379310344829, 0.35098522167487683, 0.3854679802955665, 0.38793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42857142857142855, 0.51477832512315269, 0.46182266009852219, 0.45689655172413796], 5: [0.14901477832512317, 0.13423645320197045, 0.15270935960591134, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382803 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  32.8738
Epoch 1, cost is  31.1929
Epoch 2, cost is  31.1994
Epoch 3, cost is  31.6873
Epoch 4, cost is  32.4012
Training took 0.106529 minutes
Weight histogram
[1260  857 1480  485 1939 1890 1966 1887 2122  289] [-0.30855715 -0.278007   -0.24745685 -0.2169067  -0.18635654 -0.15580639
 -0.12525624 -0.09470609 -0.06415593 -0.03360578 -0.00305563]
[1000 1641 1527 1570 1448 1438 1482 1386 1362 1321] [-0.30855715 -0.278007   -0.24745685 -0.2169067  -0.18635654 -0.15580639
 -0.12525624 -0.09470609 -0.06415593 -0.03360578 -0.00305563]
-79.8372
69.5198
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  18.9427
Epoch 1, cost is  18.3265
Epoch 2, cost is  18.5277
Epoch 3, cost is  18.9598
Epoch 4, cost is  19.4325
Training took 0.070577 minutes
Weight histogram
[1261 1785 1470 2126 1902 2869 1891  640 1675  581] [-0.37466598 -0.33750494 -0.30034391 -0.26318287 -0.22602184 -0.1888608
 -0.15169977 -0.11453873 -0.0773777  -0.04021666 -0.00305563]
[1658 3055 1680 1551 1380 1440 1506 1333 1317 1280] [-0.37466598 -0.33750494 -0.30034391 -0.26318287 -0.22602184 -0.1888608
 -0.15169977 -0.11453873 -0.0773777  -0.04021666 -0.00305563]
-32.68
37.4911
fine tuning ...
Epoch 0
Fine tuning took 0.050467 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050291 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050433 minutes
{0: [0.13793103448275862, 0.17733990147783252, 0.17364532019704434, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77216748768472909, 0.71674876847290636, 0.73399014778325122, 0.76354679802955661], 5: [0.089901477832512317, 0.10591133004926108, 0.092364532019704432, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380978 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  32.8738
Epoch 1, cost is  31.1929
Epoch 2, cost is  31.1994
Epoch 3, cost is  31.6873
Epoch 4, cost is  32.4012
Training took 0.107624 minutes
Weight histogram
[1260  857 1480  485 1939 1890 1966 1887 2122  289] [-0.30855715 -0.278007   -0.24745685 -0.2169067  -0.18635654 -0.15580639
 -0.12525624 -0.09470609 -0.06415593 -0.03360578 -0.00305563]
[1000 1641 1527 1570 1448 1438 1482 1386 1362 1321] [-0.30855715 -0.278007   -0.24745685 -0.2169067  -0.18635654 -0.15580639
 -0.12525624 -0.09470609 -0.06415593 -0.03360578 -0.00305563]
-79.8372
69.5198
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  6.7918
Epoch 1, cost is  6.41683
Epoch 2, cost is  6.36721
Epoch 3, cost is  6.44798
Epoch 4, cost is  6.52196
Training took 0.087283 minutes
Weight histogram
[1877 2910 1449 1773 2744 2128  753 1415 1067   84] [-0.21774949 -0.19628011 -0.17481072 -0.15334133 -0.13187195 -0.11040256
 -0.08893317 -0.06746379 -0.0459944  -0.02452502 -0.00305563]
[ 889 2015 2418 2212 1501 1466 1518 1516 1391 1274] [-0.21774949 -0.19628011 -0.17481072 -0.15334133 -0.13187195 -0.11040256
 -0.08893317 -0.06746379 -0.0459944  -0.02452502 -0.00305563]
-18.7574
18.2146
fine tuning ...
Epoch 0
Fine tuning took 0.052207 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053071 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052778 minutes
{0: [0.25615763546798032, 0.26231527093596058, 0.27216748768472904, 0.22783251231527094], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64901477832512311, 0.60098522167487689, 0.61206896551724133, 0.62315270935960587], 5: [0.094827586206896547, 0.13669950738916256, 0.11576354679802955, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380403 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  32.8738
Epoch 1, cost is  31.1929
Epoch 2, cost is  31.1994
Epoch 3, cost is  31.6873
Epoch 4, cost is  32.4012
Training took 0.108964 minutes
Weight histogram
[1260  857 1480  485 1939 1890 1966 1887 2122  289] [-0.30855715 -0.278007   -0.24745685 -0.2169067  -0.18635654 -0.15580639
 -0.12525624 -0.09470609 -0.06415593 -0.03360578 -0.00305563]
[1000 1641 1527 1570 1448 1438 1482 1386 1362 1321] [-0.30855715 -0.278007   -0.24745685 -0.2169067  -0.18635654 -0.15580639
 -0.12525624 -0.09470609 -0.06415593 -0.03360578 -0.00305563]
-79.8372
69.5198
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  2.59343
Epoch 1, cost is  2.33309
Epoch 2, cost is  2.26599
Epoch 3, cost is  2.20847
Epoch 4, cost is  2.18095
Training took 0.107955 minutes
Weight histogram
[2276 2214 2051 2438 2402 1585 1117 1663  411   43] [-0.16644198 -0.15010334 -0.13376471 -0.11742607 -0.10108744 -0.0847488
 -0.06841017 -0.05207153 -0.0357329  -0.01939426 -0.00305563]
[ 462 1058 1672 1791 1888 2027 2067 1911 1497 1827] [-0.16644198 -0.15010334 -0.13376471 -0.11742607 -0.10108744 -0.0847488
 -0.06841017 -0.05207153 -0.0357329  -0.01939426 -0.00305563]
-12.6614
12.954
fine tuning ...
Epoch 0
Fine tuning took 0.055783 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056884 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057742 minutes
{0: [0.29187192118226601, 0.27339901477832512, 0.27216748768472904, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.61699507389162567, 0.59482758620689657, 0.61206896551724133], 5: [0.098522167487684734, 0.10960591133004927, 0.13300492610837439, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382494 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  18.1047
Epoch 1, cost is  16.9812
Epoch 2, cost is  16.7623
Epoch 3, cost is  16.8184
Epoch 4, cost is  16.9978
Training took 0.151906 minutes
Weight histogram
[1720  914 1390 1414 1363 1496 1751 1482 2060  585] [-0.34810257 -0.31366779 -0.279233   -0.24479822 -0.21036344 -0.17592866
 -0.14149387 -0.10705909 -0.07262431 -0.03818952 -0.00375474]
[ 933 1462 1484 1477 1488 1489 1470 1457 1465 1450] [-0.34810257 -0.31366779 -0.279233   -0.24479822 -0.21036344 -0.17592866
 -0.14149387 -0.10705909 -0.07262431 -0.03818952 -0.00375474]
-41.2097
44.0682
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  18.7988
Epoch 1, cost is  17.989
Epoch 2, cost is  18.0156
Epoch 3, cost is  18.2599
Epoch 4, cost is  18.6693
Training took 0.088842 minutes
Weight histogram
[1547 1270 1420 1689 1629 1565 1889 2405  690 2096] [-0.78980404 -0.71119911 -0.63259418 -0.55398925 -0.47538432 -0.39677939
 -0.31817446 -0.23956953 -0.1609646  -0.08235967 -0.00375474]
[2601 2318 1612 1456 1388 1408 1440 1310 1336 1331] [-0.78980404 -0.71119911 -0.63259418 -0.55398925 -0.47538432 -0.39677939
 -0.31817446 -0.23956953 -0.1609646  -0.08235967 -0.00375474]
-46.5453
50.5151
fine tuning ...
Epoch 0
Fine tuning took 0.059500 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059450 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.059777 minutes
{0: [0.30049261083743845, 0.23275862068965517, 0.25123152709359609, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57389162561576357, 0.64532019704433496, 0.62315270935960587, 0.58497536945812811], 5: [0.12561576354679804, 0.12192118226600986, 0.12561576354679804, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380998 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  18.1047
Epoch 1, cost is  16.9812
Epoch 2, cost is  16.7623
Epoch 3, cost is  16.8184
Epoch 4, cost is  16.9978
Training took 0.150826 minutes
Weight histogram
[1720  914 1390 1414 1363 1496 1751 1482 2060  585] [-0.34810257 -0.31366779 -0.279233   -0.24479822 -0.21036344 -0.17592866
 -0.14149387 -0.10705909 -0.07262431 -0.03818952 -0.00375474]
[ 933 1462 1484 1477 1488 1489 1470 1457 1465 1450] [-0.34810257 -0.31366779 -0.279233   -0.24479822 -0.21036344 -0.17592866
 -0.14149387 -0.10705909 -0.07262431 -0.03818952 -0.00375474]
-41.2097
44.0682
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.11756
Epoch 1, cost is  7.64382
Epoch 2, cost is  7.57915
Epoch 3, cost is  7.63672
Epoch 4, cost is  7.75121
Training took 0.112511 minutes
Weight histogram
[1578 1378 1526 1833 1449 1698 2442 1773 1338 1185] [-0.44864771 -0.40415841 -0.35966911 -0.31517982 -0.27069052 -0.22620122
 -0.18171193 -0.13722263 -0.09273334 -0.04824404 -0.00375474]
[1525 2738 2057 1532 1473 1451 1438 1323 1327 1336] [-0.44864771 -0.40415841 -0.35966911 -0.31517982 -0.27069052 -0.22620122
 -0.18171193 -0.13722263 -0.09273334 -0.04824404 -0.00375474]
-26.0572
28.2393
fine tuning ...
Epoch 0
Fine tuning took 0.061632 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061224 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061702 minutes
{0: [0.26231527093596058, 0.28325123152709358, 0.31157635467980294, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.58374384236453203, 0.56650246305418717, 0.5714285714285714], 5: [0.11699507389162561, 0.13300492610837439, 0.12192118226600986, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382651 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  18.1047
Epoch 1, cost is  16.9812
Epoch 2, cost is  16.7623
Epoch 3, cost is  16.8184
Epoch 4, cost is  16.9978
Training took 0.152146 minutes
Weight histogram
[1720  914 1390 1414 1363 1496 1751 1482 2060  585] [-0.34810257 -0.31366779 -0.279233   -0.24479822 -0.21036344 -0.17592866
 -0.14149387 -0.10705909 -0.07262431 -0.03818952 -0.00375474]
[ 933 1462 1484 1477 1488 1489 1470 1457 1465 1450] [-0.34810257 -0.31366779 -0.279233   -0.24479822 -0.21036344 -0.17592866
 -0.14149387 -0.10705909 -0.07262431 -0.03818952 -0.00375474]
-41.2097
44.0682
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.44906
Epoch 1, cost is  3.10764
Epoch 2, cost is  3.00769
Epoch 3, cost is  2.95249
Epoch 4, cost is  2.91811
Training took 0.153697 minutes
Weight histogram
[2192 1720 2004 1647 2027 2197 1580 1375 1297  161] [-0.24249592 -0.21862181 -0.19474769 -0.17087357 -0.14699945 -0.12312533
 -0.09925122 -0.0753771  -0.05150298 -0.02762886 -0.00375474]
[ 781 1555 1957 2068 1946 1486 1589 1569 1591 1658] [-0.24249592 -0.21862181 -0.19474769 -0.17087357 -0.14699945 -0.12312533
 -0.09925122 -0.0753771  -0.05150298 -0.02762886 -0.00375474]
-14.2321
16.0089
fine tuning ...
Epoch 0
Fine tuning took 0.067168 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067740 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067754 minutes
{0: [0.25985221674876846, 0.29433497536945813, 0.3251231527093596, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.58866995073891626, 0.5714285714285714, 0.60467980295566504], 5: [0.13300492610837439, 0.11699507389162561, 0.10344827586206896, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380084 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  11.6616
Epoch 1, cost is  10.5632
Epoch 2, cost is  10.1715
Epoch 3, cost is  9.99169
Epoch 4, cost is  9.89912
Training took 0.206125 minutes
Weight histogram
[1984 1158 1269 1642 1344 1400 1505 1946 1923    4] [-0.24660434 -0.22228828 -0.19797223 -0.17365618 -0.14934013 -0.12502407
 -0.10070802 -0.07639197 -0.05207591 -0.02775986 -0.00344381]
[ 849 1221 1297 1437 1505 1495 1507 1533 1593 1738] [-0.24660434 -0.22228828 -0.19797223 -0.17365618 -0.14934013 -0.12502407
 -0.10070802 -0.07639197 -0.05207591 -0.02775986 -0.00344381]
-37.0737
35.5548
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  18.8739
Epoch 1, cost is  17.969
Epoch 2, cost is  18.0813
Epoch 3, cost is  18.4543
Epoch 4, cost is  18.9707
Training took 0.106027 minutes
Weight histogram
[1395 1070 1307 1654 1741 2031 3897  924  140 2041] [-0.72526759 -0.65308521 -0.58090283 -0.50872045 -0.43653808 -0.3643557
 -0.29217332 -0.21999094 -0.14780856 -0.07562618 -0.00344381]
[3473 1862 1646 1476 1323 1408 1343 1235 1243 1191] [-0.72526759 -0.65308521 -0.58090283 -0.50872045 -0.43653808 -0.3643557
 -0.29217332 -0.21999094 -0.14780856 -0.07562618 -0.00344381]
-52.635
56.3313
fine tuning ...
Epoch 0
Fine tuning took 0.068079 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067032 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.066736 minutes
{0: [0.46305418719211822, 0.35344827586206895, 0.27709359605911332, 0.33251231527093594], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39039408866995073, 0.49753694581280788, 0.58251231527093594, 0.52955665024630538], 5: [0.14655172413793102, 0.14901477832512317, 0.14039408866995073, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380073 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  11.6616
Epoch 1, cost is  10.5632
Epoch 2, cost is  10.1715
Epoch 3, cost is  9.99169
Epoch 4, cost is  9.89912
Training took 0.205552 minutes
Weight histogram
[1984 1158 1269 1642 1344 1400 1505 1946 1923    4] [-0.24660434 -0.22228828 -0.19797223 -0.17365618 -0.14934013 -0.12502407
 -0.10070802 -0.07639197 -0.05207591 -0.02775986 -0.00344381]
[ 849 1221 1297 1437 1505 1495 1507 1533 1593 1738] [-0.24660434 -0.22228828 -0.19797223 -0.17365618 -0.14934013 -0.12502407
 -0.10070802 -0.07639197 -0.05207591 -0.02775986 -0.00344381]
-37.0737
35.5548
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.66333
Epoch 1, cost is  8.12073
Epoch 2, cost is  8.09556
Epoch 3, cost is  8.18477
Epoch 4, cost is  8.3341
Training took 0.152676 minutes
Weight histogram
[1440 1364 1187 1551 1716 1993 3999  759  649 1542] [-0.44289097 -0.39894626 -0.35500154 -0.31105682 -0.26711211 -0.22316739
 -0.17922267 -0.13527796 -0.09133324 -0.04738852 -0.00344381]
[2331 2989 1689 1483 1386 1421 1301 1167 1224 1209] [-0.44289097 -0.39894626 -0.35500154 -0.31105682 -0.26711211 -0.22316739
 -0.17922267 -0.13527796 -0.09133324 -0.04738852 -0.00344381]
-31.3617
31.6518
fine tuning ...
Epoch 0
Fine tuning took 0.071657 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.072035 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.071558 minutes
{0: [0.47167487684729065, 0.44704433497536944, 0.45073891625615764, 0.44211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.38054187192118227, 0.40640394088669951, 0.3891625615763547, 0.41625615763546797], 5: [0.14778325123152711, 0.14655172413793102, 0.16009852216748768, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380401 minutes
Weight histogram
[  27  222  670 2086 2650 2784 2958 1856  804  118] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
[ 147  240  351  556  764  415  736 2022 3094 5850] [ -3.47374647e-04  -2.38957629e-04  -1.30540610e-04  -2.21235910e-05
   8.62934277e-05   1.94710447e-04   3.03127465e-04   4.11544484e-04
   5.19961503e-04   6.28378522e-04   7.36795540e-04]
-1.2056
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  11.6616
Epoch 1, cost is  10.5632
Epoch 2, cost is  10.1715
Epoch 3, cost is  9.99169
Epoch 4, cost is  9.89912
Training took 0.210375 minutes
Weight histogram
[1984 1158 1269 1642 1344 1400 1505 1946 1923    4] [-0.24660434 -0.22228828 -0.19797223 -0.17365618 -0.14934013 -0.12502407
 -0.10070802 -0.07639197 -0.05207591 -0.02775986 -0.00344381]
[ 849 1221 1297 1437 1505 1495 1507 1533 1593 1738] [-0.24660434 -0.22228828 -0.19797223 -0.17365618 -0.14934013 -0.12502407
 -0.10070802 -0.07639197 -0.05207591 -0.02775986 -0.00344381]
-37.0737
35.5548
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.0064
Epoch 1, cost is  3.57228
Epoch 2, cost is  3.47079
Epoch 3, cost is  3.43187
Epoch 4, cost is  3.43292
Training took 0.211602 minutes
Weight histogram
[1673 1417 1729 1995 2152 3574 1263  361 2031    5] [-0.250485   -0.22578088 -0.20107676 -0.17637264 -0.15166852 -0.1269644
 -0.10226028 -0.07755616 -0.05285204 -0.02814793 -0.00344381]
[1291 2347 2347 1824 1512 1455 1371 1293 1358 1402] [-0.250485   -0.22578088 -0.20107676 -0.17637264 -0.15166852 -0.1269644
 -0.10226028 -0.07755616 -0.05285204 -0.02814793 -0.00344381]
-21.9131
22.0389
fine tuning ...
Epoch 0
Fine tuning took 0.077440 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.077515 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.078340 minutes
{0: [0.39285714285714285, 0.41748768472906406, 0.42980295566502463, 0.45443349753694579], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.40517241379310343, 0.40763546798029554, 0.38300492610837439], 5: [0.12438423645320197, 0.17733990147783252, 0.1625615763546798, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381180 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  37.7595
Epoch 1, cost is  35.7734
Epoch 2, cost is  35.6089
Epoch 3, cost is  36.0146
Epoch 4, cost is  36.7146
Training took 0.105227 minutes
Weight histogram
[2553 1326 1200  943 1761 1673 1903 2090 2395  356] [-0.32415828 -0.29204802 -0.25993775 -0.22782749 -0.19571722 -0.16360696
 -0.13149669 -0.09938642 -0.06727616 -0.03516589 -0.00305563]
[1246 1884 1818 1722 1597 1756 1599 1576 1532 1470] [-0.32415828 -0.29204802 -0.25993775 -0.22782749 -0.19571722 -0.16360696
 -0.13149669 -0.09938642 -0.06727616 -0.03516589 -0.00305563]
-89.9388
79.8918
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  22.2904
Epoch 1, cost is  21.1748
Epoch 2, cost is  21.068
Epoch 3, cost is  21.3835
Epoch 4, cost is  21.7713
Training took 0.071338 minutes
Weight histogram
[1189 2053 1963 1684 2299 2763 2921  969 1408  976] [-0.42294878 -0.38095946 -0.33897015 -0.29698083 -0.25499152 -0.2130022
 -0.17101289 -0.12902357 -0.08703426 -0.04504494 -0.00305563]
[2113 3151 1918 1671 1583 1783 1545 1526 1493 1442] [-0.42294878 -0.38095946 -0.33897015 -0.29698083 -0.25499152 -0.2130022
 -0.17101289 -0.12902357 -0.08703426 -0.04504494 -0.00305563]
-36.9019
41.6113
fine tuning ...
Epoch 0
Fine tuning took 0.050682 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.051580 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052566 minutes
{0: [0.16133004926108374, 0.11945812807881774, 0.15886699507389163, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.79679802955665024, 0.75, 0.74753694581280783], 5: [0.16133004926108374, 0.083743842364532015, 0.091133004926108374, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380205 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  37.7595
Epoch 1, cost is  35.7734
Epoch 2, cost is  35.6089
Epoch 3, cost is  36.0146
Epoch 4, cost is  36.7146
Training took 0.106708 minutes
Weight histogram
[2553 1326 1200  943 1761 1673 1903 2090 2395  356] [-0.32415828 -0.29204802 -0.25993775 -0.22782749 -0.19571722 -0.16360696
 -0.13149669 -0.09938642 -0.06727616 -0.03516589 -0.00305563]
[1246 1884 1818 1722 1597 1756 1599 1576 1532 1470] [-0.32415828 -0.29204802 -0.25993775 -0.22782749 -0.19571722 -0.16360696
 -0.13149669 -0.09938642 -0.06727616 -0.03516589 -0.00305563]
-89.9388
79.8918
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  8.42998
Epoch 1, cost is  7.8144
Epoch 2, cost is  7.73094
Epoch 3, cost is  7.78347
Epoch 4, cost is  7.86888
Training took 0.086283 minutes
Weight histogram
[1728  293 2233 3145 1858 3314 2534  802 2101  217] [-0.27571809 -0.24845185 -0.2211856  -0.19391935 -0.16665311 -0.13938686
 -0.11212061 -0.08485437 -0.05758812 -0.03032188 -0.00305563]
[1134 2555 2735 2059 1636 1802 1738 1576 1501 1489] [-0.27571809 -0.24845185 -0.2211856  -0.19391935 -0.16665311 -0.13938686
 -0.11212061 -0.08485437 -0.05758812 -0.03032188 -0.00305563]
-21.571
23.567
fine tuning ...
Epoch 0
Fine tuning took 0.052736 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.053288 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054159 minutes
{0: [0.19581280788177341, 0.22783251231527094, 0.23029556650246305, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65517241379310343, 0.65640394088669951, 0.64408866995073888, 0.62931034482758619], 5: [0.14901477832512317, 0.11576354679802955, 0.12561576354679804, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.416240 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  37.7595
Epoch 1, cost is  35.7734
Epoch 2, cost is  35.6089
Epoch 3, cost is  36.0146
Epoch 4, cost is  36.7146
Training took 0.115236 minutes
Weight histogram
[2553 1326 1200  943 1761 1673 1903 2090 2395  356] [-0.32415828 -0.29204802 -0.25993775 -0.22782749 -0.19571722 -0.16360696
 -0.13149669 -0.09938642 -0.06727616 -0.03516589 -0.00305563]
[1246 1884 1818 1722 1597 1756 1599 1576 1532 1470] [-0.32415828 -0.29204802 -0.25993775 -0.22782749 -0.19571722 -0.16360696
 -0.13149669 -0.09938642 -0.06727616 -0.03516589 -0.00305563]
-89.9388
79.8918
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.25375
Epoch 1, cost is  2.93758
Epoch 2, cost is  2.86022
Epoch 3, cost is  2.81605
Epoch 4, cost is  2.79527
Training took 0.121715 minutes
Weight histogram
[2823 2375 2635 2172 2867 1892  904 1997  507   53] [-0.17707136 -0.15966979 -0.14226822 -0.12486664 -0.10746507 -0.0900635
 -0.07266192 -0.05526035 -0.03785878 -0.0204572  -0.00305563]
[ 569 1357 1992 2128 2269 2325 2213 1760 1971 1641] [-0.17707136 -0.15966979 -0.14226822 -0.12486664 -0.10746507 -0.0900635
 -0.07266192 -0.05526035 -0.03785878 -0.0204572  -0.00305563]
-13.8819
13.2928
fine tuning ...
Epoch 0
Fine tuning took 0.057081 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056966 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063197 minutes
{0: [0.18965517241379309, 0.23522167487684728, 0.23522167487684728, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67487684729064035, 0.60591133004926112, 0.64162561576354682, 0.59605911330049266], 5: [0.1354679802955665, 0.15886699507389163, 0.12315270935960591, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.411433 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  21.1797
Epoch 1, cost is  20.0045
Epoch 2, cost is  19.8014
Epoch 3, cost is  19.8971
Epoch 4, cost is  20.1171
Training took 0.168166 minutes
Weight histogram
[1774 1466 1326 1494 1856 1729 1592 1742 2372  849] [-0.40029192 -0.3606382  -0.32098448 -0.28133077 -0.24167705 -0.20202333
 -0.16236961 -0.1227159  -0.08306218 -0.04340846 -0.00375474]
[1137 1729 1713 1644 1738 1714 1674 1694 1632 1525] [-0.40029192 -0.3606382  -0.32098448 -0.28133077 -0.24167705 -0.20202333
 -0.16236961 -0.1227159  -0.08306218 -0.04340846 -0.00375474]
-48.6792
49.6833
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  23.6329
Epoch 1, cost is  22.6572
Epoch 2, cost is  22.6658
Epoch 3, cost is  22.9781
Epoch 4, cost is  23.4791
Training took 0.103925 minutes
Weight histogram
[1646 2023 1445 1626 1940 1794 2010 2598 1025 2118] [-0.88352054 -0.79554396 -0.70756738 -0.6195908  -0.53161422 -0.44363764
 -0.35566106 -0.26768448 -0.1797079  -0.09173132 -0.00375474]
[3250 2241 1824 1662 1624 1700 1525 1593 1471 1335] [-0.88352054 -0.79554396 -0.70756738 -0.6195908  -0.53161422 -0.44363764
 -0.35566106 -0.26768448 -0.1797079  -0.09173132 -0.00375474]
-55.6513
60.8459
fine tuning ...
Epoch 0
Fine tuning took 0.058850 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.058455 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.060591 minutes
{0: [0.35591133004926107, 0.30172413793103448, 0.3288177339901478, 0.31896551724137934], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.56527093596059108, 0.53694581280788178, 0.56527093596059108], 5: [0.13054187192118227, 0.13300492610837439, 0.13423645320197045, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.412466 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  21.1797
Epoch 1, cost is  20.0045
Epoch 2, cost is  19.8014
Epoch 3, cost is  19.8971
Epoch 4, cost is  20.1171
Training took 0.165051 minutes
Weight histogram
[1774 1466 1326 1494 1856 1729 1592 1742 2372  849] [-0.40029192 -0.3606382  -0.32098448 -0.28133077 -0.24167705 -0.20202333
 -0.16236961 -0.1227159  -0.08306218 -0.04340846 -0.00375474]
[1137 1729 1713 1644 1738 1714 1674 1694 1632 1525] [-0.40029192 -0.3606382  -0.32098448 -0.28133077 -0.24167705 -0.20202333
 -0.16236961 -0.1227159  -0.08306218 -0.04340846 -0.00375474]
-48.6792
49.6833
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.0151
Epoch 1, cost is  9.3352
Epoch 2, cost is  9.24301
Epoch 3, cost is  9.28556
Epoch 4, cost is  9.38541
Training took 0.108958 minutes
Weight histogram
[1788 1568 1607 1802 1988 1849 2285 2511 1316 1511] [-0.51279962 -0.46189513 -0.41099064 -0.36008616 -0.30918167 -0.25827718
 -0.20737269 -0.15646821 -0.10556372 -0.05465923 -0.00375474]
[1905 3243 1913 1734 1669 1676 1532 1547 1523 1483] [-0.51279962 -0.46189513 -0.41099064 -0.36008616 -0.30918167 -0.25827718
 -0.20737269 -0.15646821 -0.10556372 -0.05465923 -0.00375474]
-30.2299
33.6728
fine tuning ...
Epoch 0
Fine tuning took 0.061122 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063058 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.063378 minutes
{0: [0.37438423645320196, 0.3682266009852217, 0.34359605911330049, 0.36945812807881773], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44827586206896552, 0.43103448275862066, 0.46921182266009853, 0.46921182266009853], 5: [0.17733990147783252, 0.20073891625615764, 0.18719211822660098, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.409473 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  21.1797
Epoch 1, cost is  20.0045
Epoch 2, cost is  19.8014
Epoch 3, cost is  19.8971
Epoch 4, cost is  20.1171
Training took 0.173410 minutes
Weight histogram
[1774 1466 1326 1494 1856 1729 1592 1742 2372  849] [-0.40029192 -0.3606382  -0.32098448 -0.28133077 -0.24167705 -0.20202333
 -0.16236961 -0.1227159  -0.08306218 -0.04340846 -0.00375474]
[1137 1729 1713 1644 1738 1714 1674 1694 1632 1525] [-0.40029192 -0.3606382  -0.32098448 -0.28133077 -0.24167705 -0.20202333
 -0.16236961 -0.1227159  -0.08306218 -0.04340846 -0.00375474]
-48.6792
49.6833
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.19273
Epoch 1, cost is  3.81474
Epoch 2, cost is  3.71499
Epoch 3, cost is  3.69318
Epoch 4, cost is  3.69693
Training took 0.168475 minutes
Weight histogram
[1845 2484 2090 2016 2128 2362 2172 1241 1658  229] [-0.27020505 -0.24356002 -0.21691499 -0.19026996 -0.16362493 -0.1369799
 -0.11033487 -0.08368983 -0.0570448  -0.03039977 -0.00375474]
[ 961 1984 2198 2497 1763 1785 1808 1835 1819 1575] [-0.27020505 -0.24356002 -0.21691499 -0.19026996 -0.16362493 -0.1369799
 -0.11033487 -0.08368983 -0.0570448  -0.03039977 -0.00375474]
-15.4061
18.2673
fine tuning ...
Epoch 0
Fine tuning took 0.067267 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.068267 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067520 minutes
{0: [0.40517241379310343, 0.33497536945812806, 0.33374384236453203, 0.32389162561576357], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41502463054187194, 0.48029556650246308, 0.51108374384236455, 0.48275862068965519], 5: [0.17980295566502463, 0.18472906403940886, 0.15517241379310345, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379918 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.0348
Epoch 1, cost is  11.9129
Epoch 2, cost is  11.5525
Epoch 3, cost is  11.4173
Epoch 4, cost is  11.3872
Training took 0.205096 minutes
Weight histogram
[1876 2129 1408 1381 1495 1825 1982 1479 2478  147] [-0.27856505 -0.25105292 -0.2235408  -0.19602868 -0.16851655 -0.14100443
 -0.1134923  -0.08598018 -0.05846805 -0.03095593 -0.00344381]
[1015 1439 1420 1750 1718 1706 1727 1833 1914 1678] [-0.27856505 -0.25105292 -0.2235408  -0.19602868 -0.16851655 -0.14100443
 -0.1134923  -0.08598018 -0.05846805 -0.03095593 -0.00344381]
-39.5883
37.4605
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  22.5292
Epoch 1, cost is  21.4492
Epoch 2, cost is  21.4826
Epoch 3, cost is  21.7995
Epoch 4, cost is  22.2702
Training took 0.106606 minutes
Weight histogram
[1462 1703 1325 1595 2206 1736 3661 2240  247 2050] [-0.82747328 -0.74507034 -0.66266739 -0.58026444 -0.49786149 -0.41545854
 -0.3330556  -0.25065265 -0.1682497  -0.08584675 -0.00344381]
[3834 2090 1857 1627 1629 1580 1443 1458 1383 1324] [-0.82747328 -0.74507034 -0.66266739 -0.58026444 -0.49786149 -0.41545854
 -0.3330556  -0.25065265 -0.1682497  -0.08584675 -0.00344381]
-60.7645
68.0429
fine tuning ...
Epoch 0
Fine tuning took 0.066090 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.066895 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067930 minutes
{0: [0.28448275862068967, 0.26970443349753692, 0.29310344827586204, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.57389162561576357, 0.54187192118226601, 0.52463054187192115], 5: [0.12438423645320197, 0.15640394088669951, 0.16502463054187191, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379939 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.0348
Epoch 1, cost is  11.9129
Epoch 2, cost is  11.5525
Epoch 3, cost is  11.4173
Epoch 4, cost is  11.3872
Training took 0.208907 minutes
Weight histogram
[1876 2129 1408 1381 1495 1825 1982 1479 2478  147] [-0.27856505 -0.25105292 -0.2235408  -0.19602868 -0.16851655 -0.14100443
 -0.1134923  -0.08598018 -0.05846805 -0.03095593 -0.00344381]
[1015 1439 1420 1750 1718 1706 1727 1833 1914 1678] [-0.27856505 -0.25105292 -0.2235408  -0.19602868 -0.16851655 -0.14100443
 -0.1134923  -0.08598018 -0.05846805 -0.03095593 -0.00344381]
-39.5883
37.4605
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.4735
Epoch 1, cost is  9.77942
Epoch 2, cost is  9.66862
Epoch 3, cost is  9.71538
Epoch 4, cost is  9.84665
Training took 0.152692 minutes
Weight histogram
[1500 1658 1665 1501 1941 1878 3409 2363  270 2040] [-0.50673133 -0.45640258 -0.40607383 -0.35574507 -0.30541632 -0.25508757
 -0.20475882 -0.15443006 -0.10410131 -0.05377256 -0.00344381]
[2928 3033 1827 1678 1657 1532 1368 1449 1394 1359] [-0.50673133 -0.45640258 -0.40607383 -0.35574507 -0.30541632 -0.25508757
 -0.20475882 -0.15443006 -0.10410131 -0.05377256 -0.00344381]
-35.8502
34.8272
fine tuning ...
Epoch 0
Fine tuning took 0.072147 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070588 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070816 minutes
{0: [0.4039408866995074, 0.39408866995073893, 0.42364532019704432, 0.39285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.43596059113300495, 0.44704433497536944, 0.41625615763546797, 0.44458128078817732], 5: [0.16009852216748768, 0.15886699507389163, 0.16009852216748768, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382045 minutes
Weight histogram
[  31  284  896 2479 2809 3262 2935 2237 1072  195] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
[ 161  263  420  607  736  553 1042 3286 5077 4055] [ -3.47374647e-04  -2.31976045e-04  -1.16577442e-04  -1.17883901e-06
   1.14219764e-04   2.29618367e-04   3.45016969e-04   4.60415572e-04
   5.75814175e-04   6.91212778e-04   8.06611381e-04]
-1.20566
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.0348
Epoch 1, cost is  11.9129
Epoch 2, cost is  11.5525
Epoch 3, cost is  11.4173
Epoch 4, cost is  11.3872
Training took 0.205276 minutes
Weight histogram
[1876 2129 1408 1381 1495 1825 1982 1479 2478  147] [-0.27856505 -0.25105292 -0.2235408  -0.19602868 -0.16851655 -0.14100443
 -0.1134923  -0.08598018 -0.05846805 -0.03095593 -0.00344381]
[1015 1439 1420 1750 1718 1706 1727 1833 1914 1678] [-0.27856505 -0.25105292 -0.2235408  -0.19602868 -0.16851655 -0.14100443
 -0.1134923  -0.08598018 -0.05846805 -0.03095593 -0.00344381]
-39.5883
37.4605
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.84954
Epoch 1, cost is  4.39921
Epoch 2, cost is  4.3039
Epoch 3, cost is  4.27807
Epoch 4, cost is  4.30406
Training took 0.209683 minutes
Weight histogram
[1599 1909 1628 1976 2169 3161 3128  526 1927  202] [-0.284255   -0.25617388 -0.22809276 -0.20001164 -0.17193052 -0.1438494
 -0.11576828 -0.08768716 -0.05960604 -0.03152492 -0.00344381]
[1658 2763 2640 1804 1712 1613 1509 1633 1521 1372] [-0.284255   -0.25617388 -0.22809276 -0.20001164 -0.17193052 -0.1438494
 -0.11576828 -0.08768716 -0.05960604 -0.03152492 -0.00344381]
-26.0034
25.5157
fine tuning ...
Epoch 0
Fine tuning took 0.076499 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.077739 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.077924 minutes
{0: [0.37192118226600984, 0.37068965517241381, 0.38054187192118227, 0.35960591133004927], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49507389162561577, 0.46921182266009853, 0.41871921182266009, 0.44827586206896552], 5: [0.13300492610837439, 0.16009852216748768, 0.20073891625615764, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380282 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  43.5071
Epoch 1, cost is  41.4275
Epoch 2, cost is  41.1436
Epoch 3, cost is  41.4319
Epoch 4, cost is  42.0325
Training took 0.106630 minutes
Weight histogram
[1887  128 2702 1397 1790 1552 2466 2248 3219  836] [-0.41314214 -0.37213349 -0.33112484 -0.29011619 -0.24910754 -0.20809889
 -0.16709024 -0.12608158 -0.08507293 -0.04406428 -0.00305563]
[1477 2096 2044 1864 1946 1845 1769 1728 1703 1753] [-0.41314214 -0.37213349 -0.33112484 -0.29011619 -0.24910754 -0.20809889
 -0.16709024 -0.12608158 -0.08507293 -0.04406428 -0.00305563]
-89.9827
89.8952
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  25.7388
Epoch 1, cost is  24.5956
Epoch 2, cost is  24.4528
Epoch 3, cost is  24.5873
Epoch 4, cost is  24.9847
Training took 0.071183 minutes
Weight histogram
[1823 1327 2145 2071 2313 2524 3598 1878 1031 1540] [-0.48172882 -0.4338615  -0.38599418 -0.33812686 -0.29025954 -0.24239223
 -0.19452491 -0.14665759 -0.09879027 -0.05092295 -0.00305563]
[2564 3204 2109 1786 2000 1818 1728 1693 1671 1677] [-0.48172882 -0.4338615  -0.38599418 -0.33812686 -0.29025954 -0.24239223
 -0.19452491 -0.14665759 -0.09879027 -0.05092295 -0.00305563]
-43.0541
46.4556
fine tuning ...
Epoch 0
Fine tuning took 0.051428 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050603 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050691 minutes
{0: [0.17118226600985223, 0.18472906403940886, 0.18226600985221675, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75862068965517238, 0.72660098522167482, 0.73152709359605916, 0.74137931034482762], 5: [0.070197044334975367, 0.088669950738916259, 0.086206896551724144, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.383833 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  43.5071
Epoch 1, cost is  41.4275
Epoch 2, cost is  41.1436
Epoch 3, cost is  41.4319
Epoch 4, cost is  42.0325
Training took 0.106530 minutes
Weight histogram
[1887  128 2702 1397 1790 1552 2466 2248 3219  836] [-0.41314214 -0.37213349 -0.33112484 -0.29011619 -0.24910754 -0.20809889
 -0.16709024 -0.12608158 -0.08507293 -0.04406428 -0.00305563]
[1477 2096 2044 1864 1946 1845 1769 1728 1703 1753] [-0.41314214 -0.37213349 -0.33112484 -0.29011619 -0.24910754 -0.20809889
 -0.16709024 -0.12608158 -0.08507293 -0.04406428 -0.00305563]
-89.9827
89.8952
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  9.29943
Epoch 1, cost is  8.73799
Epoch 2, cost is  8.58607
Epoch 3, cost is  8.5257
Epoch 4, cost is  8.53113
Training took 0.086420 minutes
Weight histogram
[2382 1645  659 3951 2022 2903 3352  898 2169  269] [-0.29232776 -0.26340055 -0.23447334 -0.20554612 -0.17661891 -0.1476917
 -0.11876448 -0.08983727 -0.06091006 -0.03198284 -0.00305563]
[1374 2981 3004 1946 1934 1961 1818 1683 1698 1851] [-0.29232776 -0.26340055 -0.23447334 -0.20554612 -0.17661891 -0.1476917
 -0.11876448 -0.08983727 -0.06091006 -0.03198284 -0.00305563]
-23.7028
23.567
fine tuning ...
Epoch 0
Fine tuning took 0.054206 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.054004 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.052997 minutes
{0: [0.27832512315270935, 0.25, 0.27955665024630544, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.64039408866995073, 0.60960591133004927, 0.63669950738916259], 5: [0.087438423645320201, 0.10960591133004927, 0.11083743842364532, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380887 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  43.5071
Epoch 1, cost is  41.4275
Epoch 2, cost is  41.1436
Epoch 3, cost is  41.4319
Epoch 4, cost is  42.0325
Training took 0.107013 minutes
Weight histogram
[1887  128 2702 1397 1790 1552 2466 2248 3219  836] [-0.41314214 -0.37213349 -0.33112484 -0.29011619 -0.24910754 -0.20809889
 -0.16709024 -0.12608158 -0.08507293 -0.04406428 -0.00305563]
[1477 2096 2044 1864 1946 1845 1769 1728 1703 1753] [-0.41314214 -0.37213349 -0.33112484 -0.29011619 -0.24910754 -0.20809889
 -0.16709024 -0.12608158 -0.08507293 -0.04406428 -0.00305563]
-89.9827
89.8952
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.8459
Epoch 1, cost is  3.47526
Epoch 2, cost is  3.36785
Epoch 3, cost is  3.3294
Epoch 4, cost is  3.30442
Training took 0.109203 minutes
Weight histogram
[2950 3628 2462 2264 3204 2106  919 2035  619   63] [-0.18473278 -0.16656506 -0.14839735 -0.13022963 -0.11206192 -0.0938942
 -0.07572649 -0.05755877 -0.03939106 -0.02122334 -0.00305563]
[ 694 1676 2299 2454 2592 2687 1948 2237 1856 1807] [-0.18473278 -0.16656506 -0.14839735 -0.13022963 -0.11206192 -0.0938942
 -0.07572649 -0.05755877 -0.03939106 -0.02122334 -0.00305563]
-15.6184
15.4728
fine tuning ...
Epoch 0
Fine tuning took 0.056667 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056551 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.056571 minutes
{0: [0.27339901477832512, 0.27955665024630544, 0.26231527093596058, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.61822660098522164, 0.60837438423645318, 0.61945812807881773], 5: [0.094827586206896547, 0.10221674876847291, 0.12931034482758622, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382376 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  22.7688
Epoch 1, cost is  21.49
Epoch 2, cost is  21.1368
Epoch 3, cost is  21.0472
Epoch 4, cost is  21.1234
Training took 0.150839 minutes
Weight histogram
[2075 1894 1936 1488 1460 1784 2008 1866 2595 1119] [-0.44230983 -0.39845432 -0.35459881 -0.3107433  -0.26688779 -0.22303228
 -0.17917678 -0.13532127 -0.09146576 -0.04761025 -0.00375474]
[1309 1953 1896 1903 1921 1868 1879 1841 1728 1927] [-0.44230983 -0.39845432 -0.35459881 -0.3107433  -0.26688779 -0.22303228
 -0.17917678 -0.13532127 -0.09146576 -0.04761025 -0.00375474]
-52.3946
51.8927
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  27.2885
Epoch 1, cost is  25.9344
Epoch 2, cost is  25.6574
Epoch 3, cost is  25.8073
Epoch 4, cost is  26.1222
Training took 0.085472 minutes
Weight histogram
[1608 1261 2329 1775 1941 2263 2027 2802 2056 2188] [-1.04310536 -0.9391703  -0.83523524 -0.73130018 -0.62736512 -0.52343005
 -0.41949499 -0.31555993 -0.21162487 -0.1076898  -0.00375474]
[3678 2329 1998 1828 1949 1785 1792 1665 1566 1660] [-1.04310536 -0.9391703  -0.83523524 -0.73130018 -0.62736512 -0.52343005
 -0.41949499 -0.31555993 -0.21162487 -0.1076898  -0.00375474]
-61.4488
67.0702
fine tuning ...
Epoch 0
Fine tuning took 0.058024 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057895 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057879 minutes
{0: [0.21921182266009853, 0.2413793103448276, 0.2229064039408867, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63793103448275867, 0.61330049261083741, 0.62931034482758619, 0.60591133004926112], 5: [0.14285714285714285, 0.14532019704433496, 0.14778325123152711, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380912 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  22.7688
Epoch 1, cost is  21.49
Epoch 2, cost is  21.1368
Epoch 3, cost is  21.0472
Epoch 4, cost is  21.1234
Training took 0.147756 minutes
Weight histogram
[2075 1894 1936 1488 1460 1784 2008 1866 2595 1119] [-0.44230983 -0.39845432 -0.35459881 -0.3107433  -0.26688779 -0.22303228
 -0.17917678 -0.13532127 -0.09146576 -0.04761025 -0.00375474]
[1309 1953 1896 1903 1921 1868 1879 1841 1728 1927] [-0.44230983 -0.39845432 -0.35459881 -0.3107433  -0.26688779 -0.22303228
 -0.17917678 -0.13532127 -0.09146576 -0.04761025 -0.00375474]
-52.3946
51.8927
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.4217
Epoch 1, cost is  9.81376
Epoch 2, cost is  9.66113
Epoch 3, cost is  9.63524
Epoch 4, cost is  9.68116
Training took 0.110672 minutes
Weight histogram
[2234 1764 1886 1754 2019 2111 2316 3125 1247 1794] [-0.56326848 -0.50731711 -0.45136573 -0.39541436 -0.33946299 -0.28351161
 -0.22756024 -0.17160886 -0.11565749 -0.05970612 -0.00375474]
[2268 3377 2063 1909 1898 1773 1725 1718 1689 1830] [-0.56326848 -0.50731711 -0.45136573 -0.39541436 -0.33946299 -0.28351161
 -0.22756024 -0.17160886 -0.11565749 -0.05970612 -0.00375474]
-31.8295
37.0401
fine tuning ...
Epoch 0
Fine tuning took 0.061362 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.061462 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062031 minutes
{0: [0.39039408866995073, 0.3251231527093596, 0.29802955665024633, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.51354679802955661, 0.56403940886699511, 0.57512315270935965], 5: [0.1268472906403941, 0.16133004926108374, 0.13793103448275862, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382170 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  22.7688
Epoch 1, cost is  21.49
Epoch 2, cost is  21.1368
Epoch 3, cost is  21.0472
Epoch 4, cost is  21.1234
Training took 0.148193 minutes
Weight histogram
[2075 1894 1936 1488 1460 1784 2008 1866 2595 1119] [-0.44230983 -0.39845432 -0.35459881 -0.3107433  -0.26688779 -0.22303228
 -0.17917678 -0.13532127 -0.09146576 -0.04761025 -0.00375474]
[1309 1953 1896 1903 1921 1868 1879 1841 1728 1927] [-0.44230983 -0.39845432 -0.35459881 -0.3107433  -0.26688779 -0.22303228
 -0.17917678 -0.13532127 -0.09146576 -0.04761025 -0.00375474]
-52.3946
51.8927
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.40268
Epoch 1, cost is  3.98571
Epoch 2, cost is  3.84618
Epoch 3, cost is  3.76963
Epoch 4, cost is  3.73661
Training took 0.154059 minutes
Weight histogram
[2255 2224 2569 2239 2140 2511 2867 1210 1896  339] [-0.29564661 -0.26645742 -0.23726823 -0.20807905 -0.17888986 -0.14970068
 -0.12051149 -0.0913223  -0.06213312 -0.03294393 -0.00375474]
[1152 2320 2553 2485 1929 2051 2019 2063 1771 1907] [-0.29564661 -0.26645742 -0.23726823 -0.20807905 -0.17888986 -0.14970068
 -0.12051149 -0.0913223  -0.06213312 -0.03294393 -0.00375474]
-17.0825
20.9166
fine tuning ...
Epoch 0
Fine tuning took 0.066757 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.066219 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067091 minutes
{0: [0.26600985221674878, 0.24630541871921183, 0.25, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.60344827586206895, 0.6219211822660099, 0.61206896551724133], 5: [0.12315270935960591, 0.15024630541871922, 0.12807881773399016, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381413 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.1237
Epoch 1, cost is  12.012
Epoch 2, cost is  11.5507
Epoch 3, cost is  11.3022
Epoch 4, cost is  11.1816
Training took 0.203111 minutes
Weight histogram
[2039 1984 2036 2011 1835 1540 1828 1895 2688  369] [-0.30752501 -0.27711689 -0.24670877 -0.21630065 -0.18589253 -0.15548441
 -0.12507629 -0.09466817 -0.06426005 -0.03385193 -0.00344381]
[1154 1597 1666 1903 1869 1870 1953 2185 1857 2171] [-0.30752501 -0.27711689 -0.24670877 -0.21630065 -0.18589253 -0.15548441
 -0.12507629 -0.09466817 -0.06426005 -0.03385193 -0.00344381]
-43.8696
40.0979
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  25.9448
Epoch 1, cost is  24.3976
Epoch 2, cost is  24.0736
Epoch 3, cost is  24.1563
Epoch 4, cost is  24.4489
Training took 0.108320 minutes
Weight histogram
[1875 1612 1991 1424 1960 2163 2704 4071  385 2065] [-0.93039006 -0.83769543 -0.74500081 -0.65230618 -0.55961156 -0.46691693
 -0.37422231 -0.28152768 -0.18883306 -0.09613843 -0.00344381]
[4192 2314 2011 1805 1865 1684 1662 1578 1559 1580] [-0.93039006 -0.83769543 -0.74500081 -0.65230618 -0.55961156 -0.46691693
 -0.37422231 -0.28152768 -0.18883306 -0.09613843 -0.00344381]
-68.4187
77.5682
fine tuning ...
Epoch 0
Fine tuning took 0.067366 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067190 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067149 minutes
{0: [0.29064039408866993, 0.30418719211822659, 0.29064039408866993, 0.32019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.53940886699507384, 0.56280788177339902, 0.52339901477832518], 5: [0.13423645320197045, 0.15640394088669951, 0.14655172413793102, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381193 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.1237
Epoch 1, cost is  12.012
Epoch 2, cost is  11.5507
Epoch 3, cost is  11.3022
Epoch 4, cost is  11.1816
Training took 0.204579 minutes
Weight histogram
[2039 1984 2036 2011 1835 1540 1828 1895 2688  369] [-0.30752501 -0.27711689 -0.24670877 -0.21630065 -0.18589253 -0.15548441
 -0.12507629 -0.09466817 -0.06426005 -0.03385193 -0.00344381]
[1154 1597 1666 1903 1869 1870 1953 2185 1857 2171] [-0.30752501 -0.27711689 -0.24670877 -0.21630065 -0.18589253 -0.15548441
 -0.12507629 -0.09466817 -0.06426005 -0.03385193 -0.00344381]
-43.8696
40.0979
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  11.8753
Epoch 1, cost is  11.2013
Epoch 2, cost is  11.0882
Epoch 3, cost is  11.1275
Epoch 4, cost is  11.2532
Training took 0.150127 minutes
Weight histogram
[1977 1576 1936 1711 1730 2196 2486 4145  440 2053] [-0.56866312 -0.51214119 -0.45561926 -0.39909733 -0.34257539 -0.28605346
 -0.22953153 -0.1730096  -0.11648767 -0.05996574 -0.00344381]
[3533 2990 2006 1874 1850 1574 1641 1588 1579 1615] [-0.56866312 -0.51214119 -0.45561926 -0.39909733 -0.34257539 -0.28605346
 -0.22953153 -0.1730096  -0.11648767 -0.05996574 -0.00344381]
-41.7283
39.1845
fine tuning ...
Epoch 0
Fine tuning took 0.071432 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070120 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.072188 minutes
{0: [0.39408866995073893, 0.41009852216748771, 0.41871921182266009, 0.41625615763546797], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.46674876847290642, 0.43965517241379309, 0.41256157635467983, 0.44458128078817732], 5: [0.13916256157635468, 0.15024630541871922, 0.16871921182266009, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.382516 minutes
Weight histogram
[  41  329 1288 2864 2947 3546 2891 2473 1627  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 162  264  424  616  728  573 1219 3108 5450 5681] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
0.983091
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.1237
Epoch 1, cost is  12.012
Epoch 2, cost is  11.5507
Epoch 3, cost is  11.3022
Epoch 4, cost is  11.1816
Training took 0.207927 minutes
Weight histogram
[2039 1984 2036 2011 1835 1540 1828 1895 2688  369] [-0.30752501 -0.27711689 -0.24670877 -0.21630065 -0.18589253 -0.15548441
 -0.12507629 -0.09466817 -0.06426005 -0.03385193 -0.00344381]
[1154 1597 1666 1903 1869 1870 1953 2185 1857 2171] [-0.30752501 -0.27711689 -0.24670877 -0.21630065 -0.18589253 -0.15548441
 -0.12507629 -0.09466817 -0.06426005 -0.03385193 -0.00344381]
-43.8696
40.0979
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.14201
Epoch 1, cost is  4.67102
Epoch 2, cost is  4.53018
Epoch 3, cost is  4.4777
Epoch 4, cost is  4.47941
Training took 0.212783 minutes
Weight histogram
[2029 1805 2162 1709 2526 2699 4319  797 1773  431] [-0.31517017 -0.28399753 -0.2528249  -0.22165226 -0.19047962 -0.15930699
 -0.12813435 -0.09696171 -0.06578908 -0.03461644 -0.00344381]
[2026 3117 2640 1984 1890 1737 1820 1722 1600 1714] [-0.31517017 -0.28399753 -0.2528249  -0.22165226 -0.19047962 -0.15930699
 -0.12813435 -0.09696171 -0.06578908 -0.03461644 -0.00344381]
-28.0458
28.958
fine tuning ...
Epoch 0
Fine tuning took 0.076499 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.076851 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.077528 minutes
{0: [0.37931034482758619, 0.38669950738916259, 0.40517241379310343, 0.40886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.46182266009852219, 0.44334975369458129, 0.43842364532019706, 0.45443349753694579], 5: [0.15886699507389163, 0.16995073891625614, 0.15640394088669951, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380967 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  48.0182
Epoch 1, cost is  45.8293
Epoch 2, cost is  45.3849
Epoch 3, cost is  45.4893
Epoch 4, cost is  45.9642
Training took 0.107919 minutes
Weight histogram
[1411 2315  313 2894 1578 1588 2393 3091 3239 1428] [-0.47643259 -0.4290949  -0.3817572  -0.3344195  -0.28708181 -0.23974411
 -0.19240641 -0.14506872 -0.09773102 -0.05039333 -0.00305563]
[1728 2308 2293 2071 2186 1985 1960 1909 1964 1846] [-0.47643259 -0.4290949  -0.3817572  -0.3344195  -0.28708181 -0.23974411
 -0.19240641 -0.14506872 -0.09773102 -0.05039333 -0.00305563]
-110.618
99.2474
training layer 2, rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  28.7533
Epoch 1, cost is  27.5471
Epoch 2, cost is  27.5178
Epoch 3, cost is  27.7554
Epoch 4, cost is  28.0876
Training took 0.070727 minutes
Weight histogram
[1847 2133 1686 2831 2005 2756 3726 2552  749 1990] [-0.52634937 -0.47401999 -0.42169062 -0.36936124 -0.31703187 -0.2647025
 -0.21237312 -0.16004375 -0.10771438 -0.055385   -0.00305563]
[3050 3280 2267 2057 2186 1950 1914 1881 1887 1803] [-0.52634937 -0.47401999 -0.42169062 -0.36936124 -0.31703187 -0.2647025
 -0.21237312 -0.16004375 -0.10771438 -0.055385   -0.00305563]
-49.4286
54.1911
fine tuning ...
Epoch 0
Fine tuning took 0.051804 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050949 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.050569 minutes
{0: [0.18719211822660098, 0.18719211822660098, 0.19334975369458129, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76354679802955661, 0.76231527093596063, 0.74137931034482762, 0.75492610837438423], 5: [0.049261083743842367, 0.050492610837438424, 0.065270935960591137, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.379157 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  48.0182
Epoch 1, cost is  45.8293
Epoch 2, cost is  45.3849
Epoch 3, cost is  45.4893
Epoch 4, cost is  45.9642
Training took 0.106347 minutes
Weight histogram
[1411 2315  313 2894 1578 1588 2393 3091 3239 1428] [-0.47643259 -0.4290949  -0.3817572  -0.3344195  -0.28708181 -0.23974411
 -0.19240641 -0.14506872 -0.09773102 -0.05039333 -0.00305563]
[1728 2308 2293 2071 2186 1985 1960 1909 1964 1846] [-0.47643259 -0.4290949  -0.3817572  -0.3344195  -0.28708181 -0.23974411
 -0.19240641 -0.14506872 -0.09773102 -0.05039333 -0.00305563]
-110.618
99.2474
training layer 2, rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  10.7175
Epoch 1, cost is  10.1493
Epoch 2, cost is  10.0145
Epoch 3, cost is  9.96314
Epoch 4, cost is  10.0094
Training took 0.085041 minutes
Weight histogram
[2240 3364  468 2994 3035 2716 3831 1122 2151  354] [-0.31276309 -0.28179235 -0.2508216  -0.21985086 -0.18888011 -0.15790936
 -0.12693862 -0.09596787 -0.06499712 -0.03402638 -0.00305563]
[1671 3425 2967 2116 2236 2127 1888 1898 2067 1880] [-0.31276309 -0.28179235 -0.2508216  -0.21985086 -0.18888011 -0.15790936
 -0.12693862 -0.09596787 -0.06499712 -0.03402638 -0.00305563]
-27.517
28.6041
fine tuning ...
Epoch 0
Fine tuning took 0.054601 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055876 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.055634 minutes
{0: [0.24876847290640394, 0.24261083743842365, 0.26108374384236455, 0.26970443349753692], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57019704433497542, 0.63669950738916259, 0.58374384236453203, 0.58990147783251234], 5: [0.18103448275862069, 0.1206896551724138, 0.15517241379310345, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.398314 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  48.0182
Epoch 1, cost is  45.8293
Epoch 2, cost is  45.3849
Epoch 3, cost is  45.4893
Epoch 4, cost is  45.9642
Training took 0.106444 minutes
Weight histogram
[1411 2315  313 2894 1578 1588 2393 3091 3239 1428] [-0.47643259 -0.4290949  -0.3817572  -0.3344195  -0.28708181 -0.23974411
 -0.19240641 -0.14506872 -0.09773102 -0.05039333 -0.00305563]
[1728 2308 2293 2071 2186 1985 1960 1909 1964 1846] [-0.47643259 -0.4290949  -0.3817572  -0.3344195  -0.28708181 -0.23974411
 -0.19240641 -0.14506872 -0.09773102 -0.05039333 -0.00305563]
-110.618
99.2474
training layer 2, rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_100-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  3.37499
Epoch 1, cost is  2.96903
Epoch 2, cost is  2.81951
Epoch 3, cost is  2.73039
Epoch 4, cost is  2.70569
Training took 0.108087 minutes
Weight histogram
[1995 1343 4618 2929 2755 3347 2159 1787 1246   96] [-0.21681295 -0.19543722 -0.17406149 -0.15268576 -0.13131002 -0.10993429
 -0.08855856 -0.06718283 -0.04580709 -0.02443136 -0.00305563]
[ 788 2024 2491 2713 2837 2505 2356 2091 1983 2487] [-0.21681295 -0.19543722 -0.17406149 -0.15268576 -0.13131002 -0.10993429
 -0.08855856 -0.06718283 -0.04580709 -0.02443136 -0.00305563]
-15.879
15.9205
fine tuning ...
Epoch 0
Fine tuning took 0.057541 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.057276 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.058500 minutes
{0: [0.24261083743842365, 0.27832512315270935, 0.28448275862068967, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.60960591133004927, 0.58374384236453203, 0.56896551724137934], 5: [0.12561576354679804, 0.11206896551724138, 0.13177339901477833, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.413958 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  24.5685
Epoch 1, cost is  23.1903
Epoch 2, cost is  22.7367
Epoch 3, cost is  22.5758
Epoch 4, cost is  22.6053
Training took 0.149286 minutes
Weight histogram
[1975 2140 1935 2030 1987 1990 2108 2017 2709 1359] [-0.4961307  -0.44689311 -0.39765551 -0.34841792 -0.29918032 -0.24994272
 -0.20070513 -0.15146753 -0.10222993 -0.05299234 -0.00375474]
[1496 2165 2071 2139 2101 2058 2102 1902 2128 2088] [-0.4961307  -0.44689311 -0.39765551 -0.34841792 -0.29918032 -0.24994272
 -0.20070513 -0.15146753 -0.10222993 -0.05299234 -0.00375474]
-60.1864
59.7386
training layer 2, rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  31.6489
Epoch 1, cost is  30.3083
Epoch 2, cost is  30.2491
Epoch 3, cost is  30.5395
Epoch 4, cost is  30.9617
Training took 0.082179 minutes
Weight histogram
[2260 1637 1657 2467 1999 2499 2127 2633 2731 2265] [-1.14101636 -1.0272902  -0.91356404 -0.79983788 -0.68611172 -0.57238555
 -0.45865939 -0.34493323 -0.23120707 -0.1174809  -0.00375474]
[4005 2587 2190 2118 2116 2018 1921 1788 1850 1682] [-1.14101636 -1.0272902  -0.91356404 -0.79983788 -0.68611172 -0.57238555
 -0.45865939 -0.34493323 -0.23120707 -0.1174809  -0.00375474]
-72.357
79.5148
fine tuning ...
Epoch 0
Fine tuning took 0.059052 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.058850 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.064277 minutes
{0: [0.24507389162561577, 0.33004926108374383, 0.27463054187192121, 0.30049261083743845], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.57512315270935965, 0.62684729064039413, 0.60467980295566504], 5: [0.1145320197044335, 0.094827586206896547, 0.098522167487684734, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.408048 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  24.5685
Epoch 1, cost is  23.1903
Epoch 2, cost is  22.7367
Epoch 3, cost is  22.5758
Epoch 4, cost is  22.6053
Training took 0.148494 minutes
Weight histogram
[1975 2140 1935 2030 1987 1990 2108 2017 2709 1359] [-0.4961307  -0.44689311 -0.39765551 -0.34841792 -0.29918032 -0.24994272
 -0.20070513 -0.15146753 -0.10222993 -0.05299234 -0.00375474]
[1496 2165 2071 2139 2101 2058 2102 1902 2128 2088] [-0.4961307  -0.44689311 -0.39765551 -0.34841792 -0.29918032 -0.24994272
 -0.20070513 -0.15146753 -0.10222993 -0.05299234 -0.00375474]
-60.1864
59.7386
training layer 2, rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  12.6818
Epoch 1, cost is  11.9237
Epoch 2, cost is  11.7265
Epoch 3, cost is  11.6814
Epoch 4, cost is  11.7263
Training took 0.108950 minutes
Weight histogram
[1981 2425 2029 2043 2119 2343 2300 3472 1401 2162] [-0.62522477 -0.56307777 -0.50093076 -0.43878376 -0.37663676 -0.31448976
 -0.25234275 -0.19019575 -0.12804875 -0.06590174 -0.00375474]
[2698 3494 2242 2118 2080 1924 1947 1895 2037 1840] [-0.62522477 -0.56307777 -0.50093076 -0.43878376 -0.37663676 -0.31448976
 -0.25234275 -0.19019575 -0.12804875 -0.06590174 -0.00375474]
-35.5824
40.3768
fine tuning ...
Epoch 0
Fine tuning took 0.061357 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062654 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.062068 minutes
{0: [0.21551724137931033, 0.21305418719211822, 0.21182266009852216, 0.28817733990147781], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.60960591133004927, 0.62561576354679804, 0.53694581280788178], 5: [0.16009852216748768, 0.17733990147783252, 0.1625615763546798, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.411700 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  24.5685
Epoch 1, cost is  23.1903
Epoch 2, cost is  22.7367
Epoch 3, cost is  22.5758
Epoch 4, cost is  22.6053
Training took 0.163859 minutes
Weight histogram
[1975 2140 1935 2030 1987 1990 2108 2017 2709 1359] [-0.4961307  -0.44689311 -0.39765551 -0.34841792 -0.29918032 -0.24994272
 -0.20070513 -0.15146753 -0.10222993 -0.05299234 -0.00375474]
[1496 2165 2071 2139 2101 2058 2102 1902 2128 2088] [-0.4961307  -0.44689311 -0.39765551 -0.34841792 -0.29918032 -0.24994272
 -0.20070513 -0.15146753 -0.10222993 -0.05299234 -0.00375474]
-60.1864
59.7386
training layer 2, rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_250-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  4.57836
Epoch 1, cost is  4.13744
Epoch 2, cost is  3.97085
Epoch 3, cost is  3.8845
Epoch 4, cost is  3.84233
Training took 0.168549 minutes
Weight histogram
[2407 2450 2550 2592 2551 2528 2809 1814 2122  452] [-0.32151604 -0.28973991 -0.25796378 -0.22618765 -0.19441152 -0.16263539
 -0.13085926 -0.09908313 -0.067307   -0.03553087 -0.00375474]
[1347 2613 2926 2390 2203 2212 2325 1978 2080 2201] [-0.32151604 -0.28973991 -0.25796378 -0.22618765 -0.19441152 -0.16263539
 -0.13085926 -0.09908313 -0.067307   -0.03553087 -0.00375474]
-18.5633
24.8812
fine tuning ...
Epoch 0
Fine tuning took 0.067993 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067717 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.067321 minutes
{0: [0.25615763546798032, 0.23152709359605911, 0.26477832512315269, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.6428571428571429, 0.58251231527093594, 0.59113300492610843], 5: [0.13300492610837439, 0.12561576354679804, 0.15270935960591134, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.381987 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  14.335
Epoch 1, cost is  13.0687
Epoch 2, cost is  12.5737
Epoch 3, cost is  12.3131
Epoch 4, cost is  12.1629
Training took 0.203723 minutes
Weight histogram
[2012 2127 2113 2200 2023 1953 2080 2165 2880  697] [-0.34445384 -0.31035284 -0.27625183 -0.24215083 -0.20804983 -0.17394882
 -0.13984782 -0.10574682 -0.07164581 -0.03754481 -0.00344381]
[1292 1753 1921 2066 2048 2090 2286 2131 2291 2372] [-0.34445384 -0.31035284 -0.27625183 -0.24215083 -0.20804983 -0.17394882
 -0.13984782 -0.10574682 -0.07164581 -0.03754481 -0.00344381]
-46.7501
43.069
training layer 2, rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  29.6864
Epoch 1, cost is  28.386
Epoch 2, cost is  28.1694
Epoch 3, cost is  28.3152
Epoch 4, cost is  28.6563
Training took 0.107511 minutes
Weight histogram
[1874 1768 1917 2229 1630 2344 2882 4778  765 2088] [-1.0590812  -0.95351746 -0.84795372 -0.74238998 -0.63682624 -0.5312625
 -0.42569876 -0.32013502 -0.21457128 -0.10900754 -0.00344381]
[4531 2539 2176 2105 1989 1885 1799 1774 1789 1688] [-1.0590812  -0.95351746 -0.84795372 -0.74238998 -0.63682624 -0.5312625
 -0.42569876 -0.32013502 -0.21457128 -0.10900754 -0.00344381]
-79.2082
88.6843
fine tuning ...
Epoch 0
Fine tuning took 0.065412 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.065998 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.066122 minutes
{0: [0.43719211822660098, 0.34729064039408869, 0.38423645320197042, 0.34852216748768472], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.38793103448275862, 0.47167487684729065, 0.44211822660098521, 0.45197044334975367], 5: [0.1748768472906404, 0.18103448275862069, 0.17364532019704434, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.383888 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  14.335
Epoch 1, cost is  13.0687
Epoch 2, cost is  12.5737
Epoch 3, cost is  12.3131
Epoch 4, cost is  12.1629
Training took 0.206443 minutes
Weight histogram
[2012 2127 2113 2200 2023 1953 2080 2165 2880  697] [-0.34445384 -0.31035284 -0.27625183 -0.24215083 -0.20804983 -0.17394882
 -0.13984782 -0.10574682 -0.07164581 -0.03754481 -0.00344381]
[1292 1753 1921 2066 2048 2090 2286 2131 2291 2372] [-0.34445384 -0.31035284 -0.27625183 -0.24215083 -0.20804983 -0.17394882
 -0.13984782 -0.10574682 -0.07164581 -0.03754481 -0.00344381]
-46.7501
43.069
training layer 2, rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-250_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  13.8946
Epoch 1, cost is  13.0616
Epoch 2, cost is  12.8312
Epoch 3, cost is  12.8197
Epoch 4, cost is  12.8968
Training took 0.147806 minutes
Weight histogram
[1942 2070 1810 2199 1763 2384 2652 4707  679 2069] [-0.63809729 -0.57463194 -0.51116659 -0.44770124 -0.38423589 -0.32077055
 -0.2573052  -0.19383985 -0.1303745  -0.06690915 -0.00344381]
[4126 2937 2206 2130 1891 1832 1810 1789 1815 1739] [-0.63809729 -0.57463194 -0.51116659 -0.44770124 -0.38423589 -0.32077055
 -0.2573052  -0.19383985 -0.1303745  -0.06690915 -0.00344381]
-45.2984
45.8243
fine tuning ...
Epoch 0
Fine tuning took 0.071304 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.069715 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.070664 minutes
{0: [0.44211822660098521, 0.42857142857142855, 0.39408866995073893, 0.41009852216748771], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.39039408866995073, 0.40763546798029554, 0.41871921182266009, 0.43472906403940886], 5: [0.16748768472906403, 0.16379310344827586, 0.18719211822660098, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}
training layer 0, rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
... loaded trained layer rbm_1250-500_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.380723 minutes
Weight histogram
[  41  329 1288 2864 3003 4195 3887 2780 1644  219] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
[ 165  277  450  657  676  618 1632 3005 6670 6100] [ -3.47374647e-04  -2.24119995e-04  -1.00865343e-04   2.23893090e-05
   1.45643961e-04   2.68898613e-04   3.92153265e-04   5.15407918e-04
   6.38662570e-04   7.61917222e-04   8.85171874e-04]
-1.20566
1.178
training layer 1, rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_classical0.5_wd0.0001_dropout0.50
Epoch 0, cost is  14.335
Epoch 1, cost is  13.0687
Epoch 2, cost is  12.5737
Epoch 3, cost is  12.3131
Epoch 4, cost is  12.1629
Training took 0.202667 minutes
Weight histogram
[2012 2127 2113 2200 2023 1953 2080 2165 2880  697] [-0.34445384 -0.31035284 -0.27625183 -0.24215083 -0.20804983 -0.17394882
 -0.13984782 -0.10574682 -0.07164581 -0.03754481 -0.00344381]
[1292 1753 1921 2066 2048 2090 2286 2131 2291 2372] [-0.34445384 -0.31035284 -0.27625183 -0.24215083 -0.20804983 -0.17394882
 -0.13984782 -0.10574682 -0.07164581 -0.03754481 -0.00344381]
-46.7501
43.069
training layer 2, rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
... loaded trained layer rbm_500-500_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_dropout0.50
Epoch 0, cost is  5.80706
Epoch 1, cost is  5.2705
Epoch 2, cost is  5.10007
Epoch 3, cost is  5.01856
Epoch 4, cost is  4.99993
Training took 0.211243 minutes
Weight histogram
[2299 2187 2095 2182 2263 2631 4455 1878 1587  698] [-0.3442564  -0.31017514 -0.27609388 -0.24201262 -0.20793136 -0.1738501
 -0.13976884 -0.10568758 -0.07160632 -0.03752506 -0.00344381]
[2419 3500 2608 2171 2015 1968 2005 1773 1928 1888] [-0.3442564  -0.31017514 -0.27609388 -0.24201262 -0.20793136 -0.1738501
 -0.13976884 -0.10568758 -0.07160632 -0.03752506 -0.00344381]
-32.1624
33.587
fine tuning ...
Epoch 0
Fine tuning took 0.076770 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.076841 minutes
fine tuning ...
Epoch 0
Fine tuning took 0.076304 minutes
{0: [0.44334975369458129, 0.40147783251231528, 0.44704433497536944, 0.40270935960591131], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.40886699507389163, 0.4039408866995074, 0.4211822660098522], 5: [0.14655172413793102, 0.18965517241379309, 0.14901477832512317, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}
