Using gpu device 0: GRID K2
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/local/lib/python2.7/dist-packages/theano/scan_module/scan_perform_ext.py:85: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentADBN2
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.329909 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.89236
Epoch 1, cost is  4.01946
Epoch 2, cost is  3.17876
Epoch 3, cost is  2.74113
Epoch 4, cost is  2.46135
Training took 0.256090 minutes
Weight histogram
[465 365 285 235 148 185 111  70 141  20] [-0.04600819 -0.04141874 -0.03682928 -0.03223982 -0.02765037 -0.02306091
 -0.01847146 -0.013882   -0.00929254 -0.00470309 -0.00011363]
[249 130 124 150 174 197 213 241 260 287] [-0.04600819 -0.04141874 -0.03682928 -0.03223982 -0.02765037 -0.02306091
 -0.01847146 -0.013882   -0.00929254 -0.00470309 -0.00011363]
-1.11958
1.33242
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.326311 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.35296
Epoch 1, cost is  4.79574
Epoch 2, cost is  3.98046
Epoch 3, cost is  3.5747
Epoch 4, cost is  3.35849
Training took 0.213447 minutes
Weight histogram
[361 263 244 691 719 511 361 352 458  90] [-0.06899464 -0.06210654 -0.05521844 -0.04833034 -0.04144224 -0.03455414
 -0.02766604 -0.02077794 -0.01388983 -0.00700173 -0.00011363]
[630 336 399 464 552 633 276 225 253 282] [-0.06899464 -0.06210654 -0.05521844 -0.04833034 -0.04144224 -0.03455414
 -0.02766604 -0.02077794 -0.01388983 -0.00700173 -0.00011363]
-1.54294
1.65656
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.5127
Epoch 1, cost is  5.49518
Epoch 2, cost is  4.71205
Epoch 3, cost is  4.23112
Epoch 4, cost is  3.96301
Training took 0.228203 minutes
Weight histogram
[357 257 219 241 163 152 160 100  87 289] [-0.08295341 -0.07468473 -0.06641606 -0.05814738 -0.04987871 -0.04161003
 -0.03334136 -0.02507268 -0.016804   -0.00853533 -0.00026665]
[295 180 159 168 169 181 192 207 231 243] [-0.08295341 -0.07468473 -0.06641606 -0.05814738 -0.04987871 -0.04161003
 -0.03334136 -0.02507268 -0.016804   -0.00853533 -0.00026665]
-1.40156
2.36592
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045340 minutes
Epoch 0
Fine tuning took 0.041857 minutes
Epoch 0
Fine tuning took 0.041656 minutes
{'zero': {0: [0.21551724137931033, 0.1354679802955665, 0.11945812807881774, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61576354679802958, 0.71798029556650245, 0.7857142857142857, 0.74384236453201968], 5: [0.16871921182266009, 0.14655172413793102, 0.094827586206896547, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21551724137931033, 0.18103448275862069, 0.11699507389162561, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61576354679802958, 0.7142857142857143, 0.76108374384236455, 0.75615763546798032], 5: [0.16871921182266009, 0.10467980295566502, 0.12192118226600986, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21551724137931033, 0.16133004926108374, 0.14655172413793102, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61576354679802958, 0.7142857142857143, 0.75246305418719217, 0.76724137931034486], 5: [0.16871921182266009, 0.12438423645320197, 0.10098522167487685, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21551724137931033, 0.17364532019704434, 0.12438423645320197, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61576354679802958, 0.69704433497536944, 0.77093596059113301, 0.73768472906403937], 5: [0.16871921182266009, 0.12931034482758622, 0.10467980295566502, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.330652 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.89236
Epoch 1, cost is  4.01946
Epoch 2, cost is  3.17876
Epoch 3, cost is  2.74113
Epoch 4, cost is  2.46135
Training took 0.257818 minutes
Weight histogram
[465 365 285 235 148 185 111  70 141  20] [-0.04600819 -0.04141874 -0.03682928 -0.03223982 -0.02765037 -0.02306091
 -0.01847146 -0.013882   -0.00929254 -0.00470309 -0.00011363]
[249 130 124 150 174 197 213 241 260 287] [-0.04600819 -0.04141874 -0.03682928 -0.03223982 -0.02765037 -0.02306091
 -0.01847146 -0.013882   -0.00929254 -0.00470309 -0.00011363]
-1.11958
1.33242
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.322957 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.35296
Epoch 1, cost is  4.79574
Epoch 2, cost is  3.98046
Epoch 3, cost is  3.5747
Epoch 4, cost is  3.35849
Training took 0.214682 minutes
Weight histogram
[361 263 244 691 719 511 361 352 458  90] [-0.06899464 -0.06210654 -0.05521844 -0.04833034 -0.04144224 -0.03455414
 -0.02766604 -0.02077794 -0.01388983 -0.00700173 -0.00011363]
[630 336 399 464 552 633 276 225 253 282] [-0.06899464 -0.06210654 -0.05521844 -0.04833034 -0.04144224 -0.03455414
 -0.02766604 -0.02077794 -0.01388983 -0.00700173 -0.00011363]
-1.54294
1.65656
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.4054
Epoch 1, cost is  5.30793
Epoch 2, cost is  4.22474
Epoch 3, cost is  3.55888
Epoch 4, cost is  3.11473
Training took 0.273637 minutes
Weight histogram
[389 305 229 213 191 174 136  93 271  24] [-0.05099323 -0.04592504 -0.04085686 -0.03578868 -0.03072049 -0.02565231
 -0.02058413 -0.01551594 -0.01044776 -0.00537957 -0.00031139]
[303 208 162 155 159 180 185 209 222 242] [-0.05099323 -0.04592504 -0.04085686 -0.03578868 -0.03072049 -0.02565231
 -0.02058413 -0.01551594 -0.01044776 -0.00537957 -0.00031139]
-1.1998
1.50283
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043053 minutes
Epoch 0
Fine tuning took 0.043226 minutes
Epoch 0
Fine tuning took 0.043560 minutes
{'zero': {0: [0.13669950738916256, 0.16625615763546797, 0.16995073891625614, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72167487684729059, 0.66995073891625612, 0.7142857142857143, 0.65886699507389157], 5: [0.14162561576354679, 0.16379310344827586, 0.11576354679802955, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13669950738916256, 0.17857142857142858, 0.16748768472906403, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72167487684729059, 0.67733990147783252, 0.71798029556650245, 0.6785714285714286], 5: [0.14162561576354679, 0.14408866995073891, 0.1145320197044335, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13669950738916256, 0.15886699507389163, 0.18349753694581281, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72167487684729059, 0.66502463054187189, 0.70197044334975367, 0.7142857142857143], 5: [0.14162561576354679, 0.17610837438423646, 0.1145320197044335, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13669950738916256, 0.15886699507389163, 0.18349753694581281, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72167487684729059, 0.6576354679802956, 0.70073891625615758, 0.70320197044334976], 5: [0.14162561576354679, 0.18349753694581281, 0.11576354679802955, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.318390 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  5.89236
Epoch 1, cost is  4.01946
Epoch 2, cost is  3.17876
Epoch 3, cost is  2.74113
Epoch 4, cost is  2.46135
Training took 0.247756 minutes
Weight histogram
[465 365 285 235 148 185 111  70 141  20] [-0.04600819 -0.04141874 -0.03682928 -0.03223982 -0.02765037 -0.02306091
 -0.01847146 -0.013882   -0.00929254 -0.00470309 -0.00011363]
[249 130 124 150 174 197 213 241 260 287] [-0.04600819 -0.04141874 -0.03682928 -0.03223982 -0.02765037 -0.02306091
 -0.01847146 -0.013882   -0.00929254 -0.00470309 -0.00011363]
-1.11958
1.33242
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.314071 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.35296
Epoch 1, cost is  4.79574
Epoch 2, cost is  3.98046
Epoch 3, cost is  3.5747
Epoch 4, cost is  3.35849
Training took 0.213176 minutes
Weight histogram
[361 263 244 691 719 511 361 352 458  90] [-0.06899464 -0.06210654 -0.05521844 -0.04833034 -0.04144224 -0.03455414
 -0.02766604 -0.02077794 -0.01388983 -0.00700173 -0.00011363]
[630 336 399 464 552 633 276 225 253 282] [-0.06899464 -0.06210654 -0.05521844 -0.04833034 -0.04144224 -0.03455414
 -0.02766604 -0.02077794 -0.01388983 -0.00700173 -0.00011363]
-1.54294
1.65656
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  6.27377
Epoch 1, cost is  5.16977
Epoch 2, cost is  3.87731
Epoch 3, cost is  3.11312
Epoch 4, cost is  2.61622
Training took 0.356570 minutes
Weight histogram
[450 360 231 228 201 164 120 219  41  11] [-0.03414995 -0.03076266 -0.02737537 -0.02398808 -0.02060079 -0.0172135
 -0.01382622 -0.01043893 -0.00705164 -0.00366435 -0.00027706]
[333 219 151 138 153 172 188 196 224 251] [-0.03414995 -0.03076266 -0.02737537 -0.02398808 -0.02060079 -0.0172135
 -0.01382622 -0.01043893 -0.00705164 -0.00366435 -0.00027706]
-0.860391
1.31404
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046284 minutes
Epoch 0
Fine tuning took 0.046545 minutes
Epoch 0
Fine tuning took 0.049590 minutes
{'zero': {0: [0.14901477832512317, 0.20689655172413793, 0.18719211822660098, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.57635467980295563, 0.60837438423645318, 0.61330049261083741], 5: [0.2105911330049261, 0.21674876847290642, 0.20443349753694581, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14901477832512317, 0.20689655172413793, 0.18965517241379309, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.59359605911330049, 0.64039408866995073, 0.65394088669950734], 5: [0.2105911330049261, 0.19950738916256158, 0.16995073891625614, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14901477832512317, 0.19458128078817735, 0.18349753694581281, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.6145320197044335, 0.60960591133004927, 0.71059113300492616], 5: [0.2105911330049261, 0.19088669950738915, 0.20689655172413793, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14901477832512317, 0.20689655172413793, 0.17364532019704434, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.6071428571428571, 0.63793103448275867, 0.67241379310344829], 5: [0.2105911330049261, 0.18596059113300492, 0.18842364532019704, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.307647 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.44551
Epoch 1, cost is  5.32153
Epoch 2, cost is  4.29552
Epoch 3, cost is  3.70433
Epoch 4, cost is  3.30172
Training took 0.247421 minutes
Weight histogram
[419 280 268 204 243 179 109 237  61  25] [ -3.37964557e-02  -3.04198745e-02  -2.70432934e-02  -2.36667122e-02
  -2.02901310e-02  -1.69135499e-02  -1.35369687e-02  -1.01603875e-02
  -6.78380634e-03  -3.40722517e-03  -3.06439943e-05]
[364 192 146 144 154 174 188 210 215 238] [ -3.37964557e-02  -3.04198745e-02  -2.70432934e-02  -2.36667122e-02
  -2.02901310e-02  -1.69135499e-02  -1.35369687e-02  -1.01603875e-02
  -6.78380634e-03  -3.40722517e-03  -3.06439943e-05]
-0.857679
1.00306
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.307737 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.73051
Epoch 1, cost is  5.95557
Epoch 2, cost is  5.05338
Epoch 3, cost is  4.48227
Epoch 4, cost is  4.08348
Training took 0.213728 minutes
Weight histogram
[265 239 587 552 451 475 458 300 649  74] [ -4.39459980e-02  -3.95544626e-02  -3.51629272e-02  -3.07713918e-02
  -2.63798564e-02  -2.19883210e-02  -1.75967856e-02  -1.32052502e-02
  -8.81371479e-03  -4.42217939e-03  -3.06439943e-05]
[942 414 380 414 470 523 298 184 207 218] [ -4.39459980e-02  -3.95544626e-02  -3.51629272e-02  -3.07713918e-02
  -2.63798564e-02  -2.19883210e-02  -1.75967856e-02  -1.32052502e-02
  -8.81371479e-03  -4.42217939e-03  -3.06439943e-05]
-0.857679
1.10385
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.69416
Epoch 1, cost is  6.17446
Epoch 2, cost is  5.5933
Epoch 3, cost is  5.12847
Epoch 4, cost is  4.75558
Training took 0.228021 minutes
Weight histogram
[281 210 223 164 194 130 126 108 539  50] [-0.0558529  -0.05028093 -0.04470896 -0.039137   -0.03356503 -0.02799306
 -0.0224211  -0.01684913 -0.01127716 -0.00570519 -0.00013323]
[410 149 172 204 177 165 189 178 183 198] [-0.0558529  -0.05028093 -0.04470896 -0.039137   -0.03356503 -0.02799306
 -0.0224211  -0.01684913 -0.01127716 -0.00570519 -0.00013323]
-0.857919
0.828722
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045004 minutes
Epoch 0
Fine tuning took 0.041922 minutes
Epoch 0
Fine tuning took 0.042029 minutes
{'zero': {0: [0.2105911330049261, 0.12438423645320197, 0.22660098522167488, 0.21182266009852216], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61945812807881773, 0.6280788177339901, 0.59852216748768472, 0.57635467980295563], 5: [0.16995073891625614, 0.24753694581280788, 0.1748768472906404, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2105911330049261, 0.14778325123152711, 0.26477832512315269, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61945812807881773, 0.62068965517241381, 0.55665024630541871, 0.61083743842364535], 5: [0.16995073891625614, 0.23152709359605911, 0.17857142857142858, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2105911330049261, 0.14532019704433496, 0.21182266009852216, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61945812807881773, 0.62315270935960587, 0.6071428571428571, 0.52955665024630538], 5: [0.16995073891625614, 0.23152709359605911, 0.18103448275862069, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2105911330049261, 0.12931034482758622, 0.24014778325123154, 0.21182266009852216], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61945812807881773, 0.62684729064039413, 0.58497536945812811, 0.56527093596059108], 5: [0.16995073891625614, 0.24384236453201971, 0.1748768472906404, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.324542 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.44551
Epoch 1, cost is  5.32153
Epoch 2, cost is  4.29552
Epoch 3, cost is  3.70433
Epoch 4, cost is  3.30172
Training took 0.253937 minutes
Weight histogram
[419 280 268 204 243 179 109 237  61  25] [ -3.37964557e-02  -3.04198745e-02  -2.70432934e-02  -2.36667122e-02
  -2.02901310e-02  -1.69135499e-02  -1.35369687e-02  -1.01603875e-02
  -6.78380634e-03  -3.40722517e-03  -3.06439943e-05]
[364 192 146 144 154 174 188 210 215 238] [ -3.37964557e-02  -3.04198745e-02  -2.70432934e-02  -2.36667122e-02
  -2.02901310e-02  -1.69135499e-02  -1.35369687e-02  -1.01603875e-02
  -6.78380634e-03  -3.40722517e-03  -3.06439943e-05]
-0.857679
1.00306
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.320753 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.73051
Epoch 1, cost is  5.95557
Epoch 2, cost is  5.05338
Epoch 3, cost is  4.48227
Epoch 4, cost is  4.08348
Training took 0.213983 minutes
Weight histogram
[265 239 587 552 451 475 458 300 649  74] [ -4.39459980e-02  -3.95544626e-02  -3.51629272e-02  -3.07713918e-02
  -2.63798564e-02  -2.19883210e-02  -1.75967856e-02  -1.32052502e-02
  -8.81371479e-03  -4.42217939e-03  -3.06439943e-05]
[942 414 380 414 470 523 298 184 207 218] [ -4.39459980e-02  -3.95544626e-02  -3.51629272e-02  -3.07713918e-02
  -2.63798564e-02  -2.19883210e-02  -1.75967856e-02  -1.32052502e-02
  -8.81371479e-03  -4.42217939e-03  -3.06439943e-05]
-0.857679
1.10385
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.57563
Epoch 1, cost is  6.04294
Epoch 2, cost is  5.46637
Epoch 3, cost is  4.85704
Epoch 4, cost is  4.28955
Training took 0.279715 minutes
Weight histogram
[291 228 290 211 186 130 134 441  92  22] [-0.03642388 -0.0327998  -0.02917571 -0.02555162 -0.02192753 -0.01830344
 -0.01467935 -0.01105526 -0.00743117 -0.00380709 -0.000183  ]
[385 183 235 206 185 163 161 155 168 184] [-0.03642388 -0.0327998  -0.02917571 -0.02555162 -0.02192753 -0.01830344
 -0.01467935 -0.01105526 -0.00743117 -0.00380709 -0.000183  ]
-0.823236
0.799811
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043494 minutes
Epoch 0
Fine tuning took 0.044844 minutes
Epoch 0
Fine tuning took 0.044446 minutes
{'zero': {0: [0.19458128078817735, 0.14039408866995073, 0.20320197044334976, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.58497536945812811, 0.58620689655172409, 0.57019704433497542], 5: [0.16502463054187191, 0.27463054187192121, 0.2105911330049261, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19458128078817735, 0.15147783251231528, 0.2019704433497537, 0.24876847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.58004926108374388, 0.60837438423645318, 0.57389162561576357], 5: [0.16502463054187191, 0.26847290640394089, 0.18965517241379309, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19458128078817735, 0.14778325123152711, 0.18719211822660098, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.58374384236453203, 0.6071428571428571, 0.53201970443349755], 5: [0.16502463054187191, 0.26847290640394089, 0.20566502463054187, 0.20320197044334976], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19458128078817735, 0.1206896551724138, 0.19950738916256158, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64039408866995073, 0.60467980295566504, 0.60344827586206895, 0.58497536945812811], 5: [0.16502463054187191, 0.27463054187192121, 0.19704433497536947, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.326641 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.44551
Epoch 1, cost is  5.32153
Epoch 2, cost is  4.29552
Epoch 3, cost is  3.70433
Epoch 4, cost is  3.30172
Training took 0.246683 minutes
Weight histogram
[419 280 268 204 243 179 109 237  61  25] [ -3.37964557e-02  -3.04198745e-02  -2.70432934e-02  -2.36667122e-02
  -2.02901310e-02  -1.69135499e-02  -1.35369687e-02  -1.01603875e-02
  -6.78380634e-03  -3.40722517e-03  -3.06439943e-05]
[364 192 146 144 154 174 188 210 215 238] [ -3.37964557e-02  -3.04198745e-02  -2.70432934e-02  -2.36667122e-02
  -2.02901310e-02  -1.69135499e-02  -1.35369687e-02  -1.01603875e-02
  -6.78380634e-03  -3.40722517e-03  -3.06439943e-05]
-0.857679
1.00306
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.324877 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.73051
Epoch 1, cost is  5.95557
Epoch 2, cost is  5.05338
Epoch 3, cost is  4.48227
Epoch 4, cost is  4.08348
Training took 0.215225 minutes
Weight histogram
[265 239 587 552 451 475 458 300 649  74] [ -4.39459980e-02  -3.95544626e-02  -3.51629272e-02  -3.07713918e-02
  -2.63798564e-02  -2.19883210e-02  -1.75967856e-02  -1.32052502e-02
  -8.81371479e-03  -4.42217939e-03  -3.06439943e-05]
[942 414 380 414 470 523 298 184 207 218] [ -4.39459980e-02  -3.95544626e-02  -3.51629272e-02  -3.07713918e-02
  -2.63798564e-02  -2.19883210e-02  -1.75967856e-02  -1.32052502e-02
  -8.81371479e-03  -4.42217939e-03  -3.06439943e-05]
-0.857679
1.10385
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  6.40393
Epoch 1, cost is  5.89868
Epoch 2, cost is  5.37975
Epoch 3, cost is  4.66958
Epoch 4, cost is  4.0155
Training took 0.364274 minutes
Weight histogram
[356 325 313 218 157 176 327 108  31  14] [-0.02634971 -0.0237295  -0.02110929 -0.01848909 -0.01586888 -0.01324867
 -0.01062846 -0.00800825 -0.00538804 -0.00276783 -0.00014763]
[368 229 275 214 169 146 140 153 161 170] [-0.02634971 -0.0237295  -0.02110929 -0.01848909 -0.01586888 -0.01324867
 -0.01062846 -0.00800825 -0.00538804 -0.00276783 -0.00014763]
-0.511141
0.788044
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049863 minutes
Epoch 0
Fine tuning took 0.047007 minutes
Epoch 0
Fine tuning took 0.047022 minutes
{'zero': {0: [0.18842364532019704, 0.18349753694581281, 0.28448275862068967, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66256157635467983, 0.57512315270935965, 0.54926108374384242, 0.56034482758620685], 5: [0.14901477832512317, 0.2413793103448276, 0.16625615763546797, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18842364532019704, 0.18842364532019704, 0.24261083743842365, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66256157635467983, 0.59852216748768472, 0.56773399014778325, 0.57512315270935965], 5: [0.14901477832512317, 0.21305418719211822, 0.18965517241379309, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18842364532019704, 0.18965517241379309, 0.2105911330049261, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66256157635467983, 0.57758620689655171, 0.60098522167487689, 0.55418719211822665], 5: [0.14901477832512317, 0.23275862068965517, 0.18842364532019704, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18842364532019704, 0.17857142857142858, 0.21305418719211822, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66256157635467983, 0.5923645320197044, 0.59975369458128081, 0.58004926108374388], 5: [0.14901477832512317, 0.22906403940886699, 0.18719211822660098, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.338486 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.80903
Epoch 1, cost is  6.66108
Epoch 2, cost is  6.46261
Epoch 3, cost is  6.20627
Epoch 4, cost is  5.99305
Training took 0.249241 minutes
Weight histogram
[204 189 164 290 753 166 100  70  51  38] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
[736 236 152 134 125 116 131 134 129 132] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
-0.244204
0.282905
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.325586 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.87917
Epoch 1, cost is  6.82136
Epoch 2, cost is  6.76169
Epoch 3, cost is  6.66867
Epoch 4, cost is  6.50723
Training took 0.214107 minutes
Weight histogram
[ 204  189  164  538 2112  327  200  139  100   77] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
[1658  526  333  266  229  210  218  214  201  195] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
-0.256337
0.320322
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.77998
Epoch 1, cost is  6.60609
Epoch 2, cost is  6.46297
Epoch 3, cost is  6.32273
Epoch 4, cost is  6.16275
Training took 0.224175 minutes
Weight histogram
[1497  174  105   73   51   38   29   24   18   16] [ -1.02523947e-02  -9.22908606e-03  -8.20577741e-03  -7.18246876e-03
  -6.15916010e-03  -5.13585145e-03  -4.11254280e-03  -3.08923415e-03
  -2.06592549e-03  -1.04261684e-03  -1.93081887e-05]
[601 272 212 178 156 140 128 120 111 107] [ -1.02523947e-02  -9.22908606e-03  -8.20577741e-03  -7.18246876e-03
  -6.15916010e-03  -5.13585145e-03  -4.11254280e-03  -3.08923415e-03
  -2.06592549e-03  -1.04261684e-03  -1.93081887e-05]
-0.0724766
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041799 minutes
Epoch 0
Fine tuning took 0.041870 minutes
Epoch 0
Fine tuning took 0.042680 minutes
{'zero': {0: [0.21305418719211822, 0.30295566502463056, 0.25, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51724137931034486, 0.5357142857142857, 0.53940886699507384, 0.51354679802955661], 5: [0.26970443349753692, 0.16133004926108374, 0.2105911330049261, 0.22536945812807882], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21305418719211822, 0.2536945812807882, 0.2376847290640394, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51724137931034486, 0.5923645320197044, 0.52339901477832518, 0.53201970443349755], 5: [0.26970443349753692, 0.1539408866995074, 0.23891625615763548, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21305418719211822, 0.27832512315270935, 0.21428571428571427, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51724137931034486, 0.57635467980295563, 0.56896551724137934, 0.54926108374384242], 5: [0.26970443349753692, 0.14532019704433496, 0.21674876847290642, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21305418719211822, 0.2857142857142857, 0.26108374384236455, 0.25], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51724137931034486, 0.56034482758620685, 0.54556650246305416, 0.51847290640394084], 5: [0.26970443349753692, 0.1539408866995074, 0.19334975369458129, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.322620 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.80903
Epoch 1, cost is  6.66108
Epoch 2, cost is  6.46261
Epoch 3, cost is  6.20627
Epoch 4, cost is  5.99305
Training took 0.246214 minutes
Weight histogram
[204 189 164 290 753 166 100  70  51  38] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
[736 236 152 134 125 116 131 134 129 132] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
-0.244204
0.282905
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.327149 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.87917
Epoch 1, cost is  6.82136
Epoch 2, cost is  6.76169
Epoch 3, cost is  6.66867
Epoch 4, cost is  6.50723
Training took 0.213667 minutes
Weight histogram
[ 204  189  164  538 2112  327  200  139  100   77] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
[1658  526  333  266  229  210  218  214  201  195] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
-0.256337
0.320322
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.63927
Epoch 1, cost is  6.3831
Epoch 2, cost is  6.1998
Epoch 3, cost is  6.02651
Epoch 4, cost is  5.84597
Training took 0.268459 minutes
Weight histogram
[432 861 308 147  94  62  44  33  25  19] [ -1.24916639e-02  -1.12495187e-02  -1.00073734e-02  -8.76522819e-03
  -7.52308295e-03  -6.28093771e-03  -5.03879247e-03  -3.79664723e-03
  -2.55450199e-03  -1.31235675e-03  -7.02115067e-05]
[534 250 204 173 156 151 140 142 139 136] [ -1.24916639e-02  -1.12495187e-02  -1.00073734e-02  -8.76522819e-03
  -7.52308295e-03  -6.28093771e-03  -5.03879247e-03  -3.79664723e-03
  -2.55450199e-03  -1.31235675e-03  -7.02115067e-05]
-0.0707698
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043638 minutes
Epoch 0
Fine tuning took 0.043876 minutes
Epoch 0
Fine tuning took 0.043945 minutes
{'zero': {0: [0.21551724137931033, 0.3251231527093596, 0.22044334975369459, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5073891625615764, 0.50123152709359609, 0.5788177339901478, 0.50862068965517238], 5: [0.27709359605911332, 0.17364532019704434, 0.20073891625615764, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21551724137931033, 0.26600985221674878, 0.24753694581280788, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5073891625615764, 0.58004926108374388, 0.51108374384236455, 0.5073891625615764], 5: [0.27709359605911332, 0.1539408866995074, 0.2413793103448276, 0.25], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21551724137931033, 0.29310344827586204, 0.2376847290640394, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5073891625615764, 0.54926108374384242, 0.53817733990147787, 0.56403940886699511], 5: [0.27709359605911332, 0.15763546798029557, 0.22413793103448276, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21551724137931033, 0.27955665024630544, 0.23522167487684728, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5073891625615764, 0.54802955665024633, 0.58004926108374388, 0.54064039408866993], 5: [0.27709359605911332, 0.17241379310344829, 0.18472906403940886, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.327680 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  6.80903
Epoch 1, cost is  6.66108
Epoch 2, cost is  6.46261
Epoch 3, cost is  6.20627
Epoch 4, cost is  5.99305
Training took 0.256066 minutes
Weight histogram
[204 189 164 290 753 166 100  70  51  38] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
[736 236 152 134 125 116 131 134 129 132] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
-0.244204
0.282905
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.330502 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.87917
Epoch 1, cost is  6.82136
Epoch 2, cost is  6.76169
Epoch 3, cost is  6.66867
Epoch 4, cost is  6.50723
Training took 0.216375 minutes
Weight histogram
[ 204  189  164  538 2112  327  200  139  100   77] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
[1658  526  333  266  229  210  218  214  201  195] [ -1.33731412e-02  -1.20322524e-02  -1.06913636e-02  -9.35047482e-03
  -8.00958603e-03  -6.66869725e-03  -5.32780846e-03  -3.98691968e-03
  -2.64603089e-03  -1.30514210e-03   3.57466815e-05]
-0.256337
0.320322
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  6.41798
Epoch 1, cost is  6.04002
Epoch 2, cost is  5.81206
Epoch 3, cost is  5.61778
Epoch 4, cost is  5.43094
Training took 0.364736 minutes
Weight histogram
[399 517 488 238 141  89  59  41  31  22] [ -1.41741699e-02  -1.27605367e-02  -1.13469036e-02  -9.93327046e-03
  -8.51963732e-03  -7.10600419e-03  -5.69237105e-03  -4.27873792e-03
  -2.86510478e-03  -1.45147165e-03  -3.78385121e-05]
[464 226 188 171 155 158 156 163 172 172] [ -1.41741699e-02  -1.27605367e-02  -1.13469036e-02  -9.93327046e-03
  -8.51963732e-03  -7.10600419e-03  -5.69237105e-03  -4.27873792e-03
  -2.86510478e-03  -1.45147165e-03  -3.78385121e-05]
-0.0690757
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047138 minutes
Epoch 0
Fine tuning took 0.046940 minutes
Epoch 0
Fine tuning took 0.050492 minutes
{'zero': {0: [0.24014778325123154, 0.32142857142857145, 0.26600985221674878, 0.24630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49014778325123154, 0.51970443349753692, 0.53694581280788178, 0.53325123152709364], 5: [0.26970443349753692, 0.15886699507389163, 0.19704433497536947, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24014778325123154, 0.2857142857142857, 0.26354679802955666, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49014778325123154, 0.57266009852216748, 0.51477832512315269, 0.52093596059113301], 5: [0.26970443349753692, 0.14162561576354679, 0.22167487684729065, 0.24876847290640394], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24014778325123154, 0.29310344827586204, 0.25246305418719212, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49014778325123154, 0.54064039408866993, 0.54064039408866993, 0.5357142857142857], 5: [0.26970443349753692, 0.16625615763546797, 0.20689655172413793, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24014778325123154, 0.27339901477832512, 0.26477832512315269, 0.23645320197044334], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49014778325123154, 0.53325123152709364, 0.5431034482758621, 0.5431034482758621], 5: [0.26970443349753692, 0.19334975369458129, 0.19211822660098521, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.311954 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.80585
Epoch 1, cost is  2.18939
Epoch 2, cost is  1.91508
Epoch 3, cost is  1.85109
Epoch 4, cost is  1.824
Training took 0.248248 minutes
Weight histogram
[422 391 364 268 219 138  86  61  38  38] [-0.09432351 -0.08496891 -0.07561431 -0.06625972 -0.05690512 -0.04755052
 -0.03819593 -0.02884133 -0.01948673 -0.01013214 -0.00077754]
[ 93  94 122 158 186 217 251 272 305 327] [-0.09432351 -0.08496891 -0.07561431 -0.06625972 -0.05690512 -0.04755052
 -0.03819593 -0.02884133 -0.01948673 -0.01013214 -0.00077754]
-3.03548
3.43254
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.330205 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.5113
Epoch 1, cost is  3.1789
Epoch 2, cost is  3.06221
Epoch 3, cost is  3.10225
Epoch 4, cost is  3.27834
Training took 0.215808 minutes
Weight histogram
[387 324 320 379 919 733 443 263 146 136] [-0.1533521  -0.13809464 -0.12283719 -0.10757973 -0.09232227 -0.07706482
 -0.06180736 -0.04654991 -0.03129245 -0.016035   -0.00077754]
[273 331 468 599 715 600 247 261 271 285] [-0.1533521  -0.13809464 -0.12283719 -0.10757973 -0.09232227 -0.07706482
 -0.06180736 -0.04654991 -0.03129245 -0.016035   -0.00077754]
-4.64845
5.35019
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  5.08771
Epoch 1, cost is  3.69935
Epoch 2, cost is  3.49263
Epoch 3, cost is  3.56516
Epoch 4, cost is  3.67052
Training took 0.229041 minutes
Weight histogram
[388 316 383 240 226 158 104  78  59  73] [-0.17713617 -0.15955673 -0.1419773  -0.12439787 -0.10681843 -0.089239
 -0.07165957 -0.05408013 -0.0365007  -0.01892127 -0.00134183]
[129 119 140 166 190 219 240 259 275 288] [-0.17713617 -0.15955673 -0.1419773  -0.12439787 -0.10681843 -0.089239
 -0.07165957 -0.05408013 -0.0365007  -0.01892127 -0.00134183]
-5.09286
6.97239
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045443 minutes
Epoch 0
Fine tuning took 0.045703 minutes
Epoch 0
Fine tuning took 0.042082 minutes
{'zero': {0: [0.2536945812807882, 0.14655172413793102, 0.17610837438423646, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.72783251231527091, 0.64408866995073888, 0.65394088669950734], 5: [0.088669950738916259, 0.12561576354679804, 0.17980295566502463, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2536945812807882, 0.11945812807881774, 0.1625615763546798, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.7931034482758621, 0.76231527093596063, 0.73152709359605916], 5: [0.088669950738916259, 0.087438423645320201, 0.075123152709359611, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2536945812807882, 0.11576354679802955, 0.14901477832512317, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.7857142857142857, 0.76847290640394084, 0.74876847290640391], 5: [0.088669950738916259, 0.098522167487684734, 0.082512315270935957, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2536945812807882, 0.10467980295566502, 0.15270935960591134, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.81527093596059108, 0.76354679802955661, 0.75123152709359609], 5: [0.088669950738916259, 0.080049261083743842, 0.083743842364532015, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.314120 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.80585
Epoch 1, cost is  2.18939
Epoch 2, cost is  1.91508
Epoch 3, cost is  1.85109
Epoch 4, cost is  1.824
Training took 0.238695 minutes
Weight histogram
[422 391 364 268 219 138  86  61  38  38] [-0.09432351 -0.08496891 -0.07561431 -0.06625972 -0.05690512 -0.04755052
 -0.03819593 -0.02884133 -0.01948673 -0.01013214 -0.00077754]
[ 93  94 122 158 186 217 251 272 305 327] [-0.09432351 -0.08496891 -0.07561431 -0.06625972 -0.05690512 -0.04755052
 -0.03819593 -0.02884133 -0.01948673 -0.01013214 -0.00077754]
-3.03548
3.43254
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.324614 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.5113
Epoch 1, cost is  3.1789
Epoch 2, cost is  3.06221
Epoch 3, cost is  3.10225
Epoch 4, cost is  3.27834
Training took 0.209595 minutes
Weight histogram
[387 324 320 379 919 733 443 263 146 136] [-0.1533521  -0.13809464 -0.12283719 -0.10757973 -0.09232227 -0.07706482
 -0.06180736 -0.04654991 -0.03129245 -0.016035   -0.00077754]
[273 331 468 599 715 600 247 261 271 285] [-0.1533521  -0.13809464 -0.12283719 -0.10757973 -0.09232227 -0.07706482
 -0.06180736 -0.04654991 -0.03129245 -0.016035   -0.00077754]
-4.64845
5.35019
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  4.66511
Epoch 1, cost is  2.69784
Epoch 2, cost is  2.33813
Epoch 3, cost is  2.24926
Epoch 4, cost is  2.26594
Training took 0.275400 minutes
Weight histogram
[406 344 318 254 208 182 108  79  63  63] [-0.11230296 -0.10120899 -0.09011502 -0.07902105 -0.06792708 -0.05683311
 -0.04573914 -0.03464517 -0.0235512  -0.01245723 -0.00136326]
[136 109 129 149 178 213 248 262 295 306] [-0.11230296 -0.10120899 -0.09011502 -0.07902105 -0.06792708 -0.05683311
 -0.04573914 -0.03464517 -0.0235512  -0.01245723 -0.00136326]
-4.44434
6.04168
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042901 minutes
Epoch 0
Fine tuning took 0.042763 minutes
Epoch 0
Fine tuning took 0.044689 minutes
{'zero': {0: [0.1206896551724138, 0.16871921182266009, 0.13054187192118227, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.65640394088669951, 0.66995073891625612, 0.63793103448275867], 5: [0.1145320197044335, 0.1748768472906404, 0.19950738916256158, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1206896551724138, 0.088669950738916259, 0.097290640394088676, 0.073891625615763554], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.78940886699507384, 0.80541871921182262, 0.81896551724137934], 5: [0.1145320197044335, 0.12192118226600986, 0.097290640394088676, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1206896551724138, 0.071428571428571425, 0.097290640394088676, 0.089901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.81157635467980294, 0.81650246305418717, 0.7857142857142857], 5: [0.1145320197044335, 0.11699507389162561, 0.086206896551724144, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1206896551724138, 0.068965517241379309, 0.092364532019704432, 0.083743842364532015], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.84113300492610843, 0.81034482758620685, 0.79679802955665024], 5: [0.1145320197044335, 0.089901477832512317, 0.097290640394088676, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.324264 minutes
Weight histogram
[ 57 176 262 290 348 297 342 178  54  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[ 79  73  90 113 133 160 220 276 371 510] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.656221
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  3.80585
Epoch 1, cost is  2.18939
Epoch 2, cost is  1.91508
Epoch 3, cost is  1.85109
Epoch 4, cost is  1.824
Training took 0.247586 minutes
Weight histogram
[422 391 364 268 219 138  86  61  38  38] [-0.09432351 -0.08496891 -0.07561431 -0.06625972 -0.05690512 -0.04755052
 -0.03819593 -0.02884133 -0.01948673 -0.01013214 -0.00077754]
[ 93  94 122 158 186 217 251 272 305 327] [-0.09432351 -0.08496891 -0.07561431 -0.06625972 -0.05690512 -0.04755052
 -0.03819593 -0.02884133 -0.01948673 -0.01013214 -0.00077754]
-3.03548
3.43254
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.320193 minutes
Weight histogram
[104 357 582 718 761 684 504 232  87  21] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
[164 150 197 237 282 328 477 562 845 808] [ -7.24884303e-05   7.52069420e-05   2.22902314e-04   3.70597687e-04
   5.18293059e-04   6.65988431e-04   8.13683803e-04   9.61379176e-04
   1.10907455e-03   1.25676992e-03   1.40446529e-03]
-0.833419
0.690193
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.5113
Epoch 1, cost is  3.1789
Epoch 2, cost is  3.06221
Epoch 3, cost is  3.10225
Epoch 4, cost is  3.27834
Training took 0.215913 minutes
Weight histogram
[387 324 320 379 919 733 443 263 146 136] [-0.1533521  -0.13809464 -0.12283719 -0.10757973 -0.09232227 -0.07706482
 -0.06180736 -0.04654991 -0.03129245 -0.016035   -0.00077754]
[273 331 468 599 715 600 247 261 271 285] [-0.1533521  -0.13809464 -0.12283719 -0.10757973 -0.09232227 -0.07706482
 -0.06180736 -0.04654991 -0.03129245 -0.016035   -0.00077754]
-4.64845
5.35019
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(500,)
Epoch 0, cost is  4.39226
Epoch 1, cost is  2.15057
Epoch 2, cost is  1.67701
Epoch 3, cost is  1.52819
Epoch 4, cost is  1.45271
Training took 0.359541 minutes
Weight histogram
[434 375 340 229 225 149  99  88  57  29] [-0.07301652 -0.06584702 -0.05867751 -0.05150801 -0.0443385  -0.037169
 -0.02999949 -0.02282999 -0.01566048 -0.00849098 -0.00132147]
[135  98 112 135 168 205 247 283 302 340] [-0.07301652 -0.06584702 -0.05867751 -0.05150801 -0.0443385  -0.037169
 -0.02999949 -0.02282999 -0.01566048 -0.00849098 -0.00132147]
-3.66463
3.72153
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045649 minutes
Epoch 0
Fine tuning took 0.046329 minutes
Epoch 0
Fine tuning took 0.046227 minutes
{'zero': {0: [0.10098522167487685, 0.17241379310344829, 0.14655172413793102, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80049261083743839, 0.65024630541871919, 0.70812807881773399, 0.69458128078817738], 5: [0.098522167487684734, 0.17733990147783252, 0.14532019704433496, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10098522167487685, 0.083743842364532015, 0.073891625615763554, 0.093596059113300489], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80049261083743839, 0.81650246305418717, 0.85837438423645318, 0.85344827586206895], 5: [0.098522167487684734, 0.099753694581280791, 0.067733990147783252, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10098522167487685, 0.076354679802955669, 0.088669950738916259, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80049261083743839, 0.80418719211822665, 0.84605911330049266, 0.8214285714285714], 5: [0.098522167487684734, 0.11945812807881774, 0.065270935960591137, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10098522167487685, 0.14162561576354679, 0.084975369458128072, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.80049261083743839, 0.77216748768472909, 0.84605911330049266, 0.78940886699507384], 5: [0.098522167487684734, 0.086206896551724144, 0.068965517241379309, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270936 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.3883
Epoch 1, cost is  2.22093
Epoch 2, cost is  2.10141
Epoch 3, cost is  2.01143
Epoch 4, cost is  1.93931
Training took 0.229022 minutes
Weight histogram
[961 864 586 476 398 212 215 136 169  33] [-0.06049447 -0.05445638 -0.0484183  -0.04238022 -0.03634213 -0.03030405
 -0.02426597 -0.01822788 -0.0121898  -0.00615172 -0.00011363]
[321 191 247 294 337 405 497 547 587 624] [-0.06049447 -0.05445638 -0.0484183  -0.04238022 -0.03634213 -0.03030405
 -0.02426597 -0.01822788 -0.0121898  -0.00615172 -0.00011363]
-1.7036
1.92108
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270931 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.32071
Epoch 1, cost is  3.20884
Epoch 2, cost is  3.13032
Epoch 3, cost is  3.07037
Epoch 4, cost is  3.0341
Training took 0.201966 minutes
Weight histogram
[ 861  709  640  395  310 1005  832  523  398  402] [-0.09493853 -0.08545604 -0.07597355 -0.06649106 -0.05700857 -0.04752608
 -0.03804359 -0.0285611  -0.01907861 -0.00959612 -0.00011363]
[827 610 801 914 347 410 491 532 556 587] [-0.09493853 -0.08545604 -0.07597355 -0.06649106 -0.05700857 -0.04752608
 -0.03804359 -0.0285611  -0.01907861 -0.00959612 -0.00011363]
-2.40017
2.54794
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.53848
Epoch 1, cost is  5.46044
Epoch 2, cost is  4.62866
Epoch 3, cost is  4.1891
Epoch 4, cost is  3.94357
Training took 0.233613 minutes
Weight histogram
[692 516 437 480 324 297 297 241 184 582] [-0.08295341 -0.07468395 -0.06641449 -0.05814503 -0.04987557 -0.04160611
 -0.03333665 -0.0250672  -0.01679774 -0.00852828 -0.00025882]
[603 374 302 332 346 362 408 442 483 398] [-0.08295341 -0.07468395 -0.06641449 -0.05814503 -0.04987557 -0.04160611
 -0.03333665 -0.0250672  -0.01679774 -0.00852828 -0.00025882]
-1.6243
2.38194
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041480 minutes
Epoch 0
Fine tuning took 0.041272 minutes
Epoch 0
Fine tuning took 0.041052 minutes
{'zero': {0: [0.23275862068965517, 0.2857142857142857, 0.18103448275862069, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.60344827586206895, 0.68103448275862066, 0.71674876847290636], 5: [0.14655172413793102, 0.11083743842364532, 0.13793103448275862, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23275862068965517, 0.29064039408866993, 0.17980295566502463, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.56650246305418717, 0.68596059113300489, 0.72044334975369462], 5: [0.14655172413793102, 0.14285714285714285, 0.13423645320197045, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23275862068965517, 0.27216748768472904, 0.17364532019704434, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.6280788177339901, 0.69088669950738912, 0.72044334975369462], 5: [0.14655172413793102, 0.099753694581280791, 0.1354679802955665, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23275862068965517, 0.25862068965517243, 0.17241379310344829, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.63793103448275867, 0.6785714285714286, 0.69950738916256161], 5: [0.14655172413793102, 0.10344827586206896, 0.14901477832512317, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271416 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.3883
Epoch 1, cost is  2.22093
Epoch 2, cost is  2.10141
Epoch 3, cost is  2.01143
Epoch 4, cost is  1.93931
Training took 0.227751 minutes
Weight histogram
[961 864 586 476 398 212 215 136 169  33] [-0.06049447 -0.05445638 -0.0484183  -0.04238022 -0.03634213 -0.03030405
 -0.02426597 -0.01822788 -0.0121898  -0.00615172 -0.00011363]
[321 191 247 294 337 405 497 547 587 624] [-0.06049447 -0.05445638 -0.0484183  -0.04238022 -0.03634213 -0.03030405
 -0.02426597 -0.01822788 -0.0121898  -0.00615172 -0.00011363]
-1.7036
1.92108
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265108 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.32071
Epoch 1, cost is  3.20884
Epoch 2, cost is  3.13032
Epoch 3, cost is  3.07037
Epoch 4, cost is  3.0341
Training took 0.198548 minutes
Weight histogram
[ 861  709  640  395  310 1005  832  523  398  402] [-0.09493853 -0.08545604 -0.07597355 -0.06649106 -0.05700857 -0.04752608
 -0.03804359 -0.0285611  -0.01907861 -0.00959612 -0.00011363]
[827 610 801 914 347 410 491 532 556 587] [-0.09493853 -0.08545604 -0.07597355 -0.06649106 -0.05700857 -0.04752608
 -0.03804359 -0.0285611  -0.01907861 -0.00959612 -0.00011363]
-2.40017
2.54794
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.43928
Epoch 1, cost is  5.2665
Epoch 2, cost is  4.16507
Epoch 3, cost is  3.50086
Epoch 4, cost is  3.05535
Training took 0.277003 minutes
Weight histogram
[697 663 450 428 374 321 308 202 557  50] [-0.05099323 -0.04592456 -0.0408559  -0.03578724 -0.03071857 -0.02564991
 -0.02058125 -0.01551258 -0.01044392 -0.00537525 -0.00030659]
[640 399 312 333 334 366 398 436 467 365] [-0.05099323 -0.04592456 -0.0408559  -0.03578724 -0.03071857 -0.02564991
 -0.02058125 -0.01551258 -0.01044392 -0.00537525 -0.00030659]
-1.1998
1.50283
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042675 minutes
Epoch 0
Fine tuning took 0.043427 minutes
Epoch 0
Fine tuning took 0.044418 minutes
{'zero': {0: [0.15024630541871922, 0.11945812807881774, 0.14532019704433496, 0.12807881773399016], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.76477832512315269, 0.74137931034482762, 0.72167487684729059], 5: [0.088669950738916259, 0.11576354679802955, 0.11330049261083744, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15024630541871922, 0.14285714285714285, 0.14655172413793102, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.75492610837438423, 0.74014778325123154, 0.70197044334975367], 5: [0.088669950738916259, 0.10221674876847291, 0.11330049261083744, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15024630541871922, 0.1354679802955665, 0.13669950738916256, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.74384236453201968, 0.74753694581280783, 0.70443349753694584], 5: [0.088669950738916259, 0.1206896551724138, 0.11576354679802955, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15024630541871922, 0.14408866995073891, 0.15270935960591134, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76108374384236455, 0.75369458128078815, 0.72167487684729059, 0.69581280788177335], 5: [0.088669950738916259, 0.10221674876847291, 0.12561576354679804, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.259805 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.3883
Epoch 1, cost is  2.22093
Epoch 2, cost is  2.10141
Epoch 3, cost is  2.01143
Epoch 4, cost is  1.93931
Training took 0.230213 minutes
Weight histogram
[961 864 586 476 398 212 215 136 169  33] [-0.06049447 -0.05445638 -0.0484183  -0.04238022 -0.03634213 -0.03030405
 -0.02426597 -0.01822788 -0.0121898  -0.00615172 -0.00011363]
[321 191 247 294 337 405 497 547 587 624] [-0.06049447 -0.05445638 -0.0484183  -0.04238022 -0.03634213 -0.03030405
 -0.02426597 -0.01822788 -0.0121898  -0.00615172 -0.00011363]
-1.7036
1.92108
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268985 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.32071
Epoch 1, cost is  3.20884
Epoch 2, cost is  3.13032
Epoch 3, cost is  3.07037
Epoch 4, cost is  3.0341
Training took 0.198032 minutes
Weight histogram
[ 861  709  640  395  310 1005  832  523  398  402] [-0.09493853 -0.08545604 -0.07597355 -0.06649106 -0.05700857 -0.04752608
 -0.03804359 -0.0285611  -0.01907861 -0.00959612 -0.00011363]
[827 610 801 914 347 410 491 532 556 587] [-0.09493853 -0.08545604 -0.07597355 -0.06649106 -0.05700857 -0.04752608
 -0.03804359 -0.0285611  -0.01907861 -0.00959612 -0.00011363]
-2.40017
2.54794
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.31651
Epoch 1, cost is  5.14355
Epoch 2, cost is  3.86309
Epoch 3, cost is  3.07421
Epoch 4, cost is  2.53996
Training took 0.361444 minutes
Weight histogram
[817 709 476 475 389 360 263 449  89  23] [-0.03414995 -0.03076191 -0.02737388 -0.02398584 -0.0205978  -0.01720977
 -0.01382173 -0.0104337  -0.00704566 -0.00365762 -0.00026959]
[697 415 307 296 323 364 385 413 471 379] [-0.03414995 -0.03076191 -0.02737388 -0.02398584 -0.0205978  -0.01720977
 -0.01382173 -0.0104337  -0.00704566 -0.00365762 -0.00026959]
-0.8878
1.31404
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046646 minutes
Epoch 0
Fine tuning took 0.046356 minutes
Epoch 0
Fine tuning took 0.046055 minutes
{'zero': {0: [0.16625615763546797, 0.098522167487684734, 0.13054187192118227, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72290640394088668, 0.75985221674876846, 0.71551724137931039, 0.68349753694581283], 5: [0.11083743842364532, 0.14162561576354679, 0.1539408866995074, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16625615763546797, 0.099753694581280791, 0.13669950738916256, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72290640394088668, 0.74384236453201968, 0.72290640394088668, 0.69704433497536944], 5: [0.11083743842364532, 0.15640394088669951, 0.14039408866995073, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16625615763546797, 0.088669950738916259, 0.15763546798029557, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72290640394088668, 0.74014778325123154, 0.72783251231527091, 0.69827586206896552], 5: [0.11083743842364532, 0.17118226600985223, 0.1145320197044335, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16625615763546797, 0.099753694581280791, 0.14778325123152711, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72290640394088668, 0.75246305418719217, 0.72290640394088668, 0.69827586206896552], 5: [0.11083743842364532, 0.14778325123152711, 0.12931034482758622, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274143 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.10001
Epoch 1, cost is  2.87516
Epoch 2, cost is  2.70318
Epoch 3, cost is  2.56329
Epoch 4, cost is  2.45061
Training took 0.227879 minutes
Weight histogram
[930 802 525 459 305 341 254 142 254  38] [ -4.54053916e-02  -4.08679168e-02  -3.63304421e-02  -3.17929673e-02
  -2.72554926e-02  -2.27180178e-02  -1.81805430e-02  -1.36430683e-02
  -9.10559352e-03  -4.56811876e-03  -3.06439943e-05]
[492 251 244 293 338 373 459 492 540 568] [ -4.54053916e-02  -4.08679168e-02  -3.63304421e-02  -3.17929673e-02
  -2.72554926e-02  -2.27180178e-02  -1.81805430e-02  -1.36430683e-02
  -9.10559352e-03  -4.56811876e-03  -3.06439943e-05]
-1.18042
1.33782
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.259168 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.88279
Epoch 1, cost is  3.68995
Epoch 2, cost is  3.55068
Epoch 3, cost is  3.43976
Epoch 4, cost is  3.35613
Training took 0.198980 minutes
Weight histogram
[671 627 483 417 373 961 737 729 896 181] [ -6.88312277e-02  -6.19511693e-02  -5.50711110e-02  -4.81910526e-02
  -4.13109942e-02  -3.44309359e-02  -2.75508775e-02  -2.06708191e-02
  -1.37907607e-02  -6.91070237e-03  -3.06439943e-05]
[1243  663  782  726  330  369  418  472  517  555] [ -6.88312277e-02  -6.19511693e-02  -5.50711110e-02  -4.81910526e-02
  -4.13109942e-02  -3.44309359e-02  -2.75508775e-02  -2.06708191e-02
  -1.37907607e-02  -6.91070237e-03  -3.06439943e-05]
-1.4286
1.65791
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.72842
Epoch 1, cost is  6.30776
Epoch 2, cost is  5.71378
Epoch 3, cost is  5.14049
Epoch 4, cost is  4.75874
Training took 0.230447 minutes
Weight histogram
[ 400  423  419  352  356  330  285  228 1150  107] [-0.0558529  -0.05028005 -0.0447072  -0.03913435 -0.0335615  -0.02798865
 -0.0224158  -0.01684295 -0.0112701  -0.00569725 -0.0001244 ]
[892 347 405 380 339 343 370 363 389 222] [-0.0558529  -0.05028005 -0.0447072  -0.03913435 -0.0335615  -0.02798865
 -0.0224158  -0.01684295 -0.0112701  -0.00569725 -0.0001244 ]
-0.857919
1.1037
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041758 minutes
Epoch 0
Fine tuning took 0.041165 minutes
Epoch 0
Fine tuning took 0.041739 minutes
{'zero': {0: [0.29064039408866993, 0.27955665024630544, 0.16871921182266009, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.58620689655172409, 0.69581280788177335, 0.72167487684729059], 5: [0.17241379310344829, 0.13423645320197045, 0.1354679802955665, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29064039408866993, 0.23645320197044334, 0.12315270935960591, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.62315270935960587, 0.70073891625615758, 0.71182266009852213], 5: [0.17241379310344829, 0.14039408866995073, 0.17610837438423646, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29064039408866993, 0.23399014778325122, 0.12192118226600986, 0.1145320197044335], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.61576354679802958, 0.7068965517241379, 0.74507389162561577], 5: [0.17241379310344829, 0.15024630541871922, 0.17118226600985223, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29064039408866993, 0.24261083743842365, 0.13916256157635468, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53694581280788178, 0.6071428571428571, 0.66871921182266014, 0.72660098522167482], 5: [0.17241379310344829, 0.15024630541871922, 0.19211822660098521, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.284432 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.10001
Epoch 1, cost is  2.87516
Epoch 2, cost is  2.70318
Epoch 3, cost is  2.56329
Epoch 4, cost is  2.45061
Training took 0.231180 minutes
Weight histogram
[930 802 525 459 305 341 254 142 254  38] [ -4.54053916e-02  -4.08679168e-02  -3.63304421e-02  -3.17929673e-02
  -2.72554926e-02  -2.27180178e-02  -1.81805430e-02  -1.36430683e-02
  -9.10559352e-03  -4.56811876e-03  -3.06439943e-05]
[492 251 244 293 338 373 459 492 540 568] [ -4.54053916e-02  -4.08679168e-02  -3.63304421e-02  -3.17929673e-02
  -2.72554926e-02  -2.27180178e-02  -1.81805430e-02  -1.36430683e-02
  -9.10559352e-03  -4.56811876e-03  -3.06439943e-05]
-1.18042
1.33782
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268243 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.88279
Epoch 1, cost is  3.68995
Epoch 2, cost is  3.55068
Epoch 3, cost is  3.43976
Epoch 4, cost is  3.35613
Training took 0.202773 minutes
Weight histogram
[671 627 483 417 373 961 737 729 896 181] [ -6.88312277e-02  -6.19511693e-02  -5.50711110e-02  -4.81910526e-02
  -4.13109942e-02  -3.44309359e-02  -2.75508775e-02  -2.06708191e-02
  -1.37907607e-02  -6.91070237e-03  -3.06439943e-05]
[1243  663  782  726  330  369  418  472  517  555] [ -6.88312277e-02  -6.19511693e-02  -5.50711110e-02  -4.81910526e-02
  -4.13109942e-02  -3.44309359e-02  -2.75508775e-02  -2.06708191e-02
  -1.37907607e-02  -6.91070237e-03  -3.06439943e-05]
-1.4286
1.65791
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.62951
Epoch 1, cost is  6.19568
Epoch 2, cost is  5.59161
Epoch 3, cost is  4.91214
Epoch 4, cost is  4.36335
Training took 0.271934 minutes
Weight histogram
[349 518 532 423 413 333 282 944 209  47] [-0.03642388 -0.03279881 -0.02917374 -0.02554867 -0.0219236  -0.01829853
 -0.01467346 -0.01104839 -0.00742332 -0.00379825 -0.00017318]
[829 422 480 385 323 319 334 334 351 273] [-0.03642388 -0.03279881 -0.02917374 -0.02554867 -0.0219236  -0.01829853
 -0.01467346 -0.01104839 -0.00742332 -0.00379825 -0.00017318]
-0.823236
0.912906
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043254 minutes
Epoch 0
Fine tuning took 0.043217 minutes
Epoch 0
Fine tuning took 0.042817 minutes
{'zero': {0: [0.24507389162561577, 0.19827586206896552, 0.17733990147783252, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.64901477832512311, 0.68965517241379315, 0.69704433497536944], 5: [0.13054187192118227, 0.15270935960591134, 0.13300492610837439, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24507389162561577, 0.19088669950738915, 0.19704433497536947, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.65270935960591137, 0.66995073891625612, 0.68719211822660098], 5: [0.13054187192118227, 0.15640394088669951, 0.13300492610837439, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24507389162561577, 0.20320197044334976, 0.19458128078817735, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.66625615763546797, 0.68349753694581283, 0.71059113300492616], 5: [0.13054187192118227, 0.13054187192118227, 0.12192118226600986, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24507389162561577, 0.19088669950738915, 0.18719211822660098, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62438423645320196, 0.65024630541871919, 0.68349753694581283, 0.70320197044334976], 5: [0.13054187192118227, 0.15886699507389163, 0.12931034482758622, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.260863 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.10001
Epoch 1, cost is  2.87516
Epoch 2, cost is  2.70318
Epoch 3, cost is  2.56329
Epoch 4, cost is  2.45061
Training took 0.223841 minutes
Weight histogram
[930 802 525 459 305 341 254 142 254  38] [ -4.54053916e-02  -4.08679168e-02  -3.63304421e-02  -3.17929673e-02
  -2.72554926e-02  -2.27180178e-02  -1.81805430e-02  -1.36430683e-02
  -9.10559352e-03  -4.56811876e-03  -3.06439943e-05]
[492 251 244 293 338 373 459 492 540 568] [ -4.54053916e-02  -4.08679168e-02  -3.63304421e-02  -3.17929673e-02
  -2.72554926e-02  -2.27180178e-02  -1.81805430e-02  -1.36430683e-02
  -9.10559352e-03  -4.56811876e-03  -3.06439943e-05]
-1.18042
1.33782
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.263916 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.88279
Epoch 1, cost is  3.68995
Epoch 2, cost is  3.55068
Epoch 3, cost is  3.43976
Epoch 4, cost is  3.35613
Training took 0.204801 minutes
Weight histogram
[671 627 483 417 373 961 737 729 896 181] [ -6.88312277e-02  -6.19511693e-02  -5.50711110e-02  -4.81910526e-02
  -4.13109942e-02  -3.44309359e-02  -2.75508775e-02  -2.06708191e-02
  -1.37907607e-02  -6.91070237e-03  -3.06439943e-05]
[1243  663  782  726  330  369  418  472  517  555] [ -6.88312277e-02  -6.19511693e-02  -5.50711110e-02  -4.81910526e-02
  -4.13109942e-02  -3.44309359e-02  -2.75508775e-02  -2.06708191e-02
  -1.37907607e-02  -6.91070237e-03  -3.06439943e-05]
-1.4286
1.65791
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.4834
Epoch 1, cost is  6.06737
Epoch 2, cost is  5.52284
Epoch 3, cost is  4.74099
Epoch 4, cost is  4.08816
Training took 0.366639 minutes
Weight histogram
[434 650 635 499 402 368 697 271  65  29] [-0.02634971 -0.02372879 -0.02110787 -0.01848695 -0.01586603 -0.01324511
 -0.0106242  -0.00800328 -0.00538236 -0.00276144 -0.00014052]
[805 528 544 374 313 303 298 318 330 237] [-0.02634971 -0.02372879 -0.02110787 -0.01848695 -0.01586603 -0.01324511
 -0.0106242  -0.00800328 -0.00538236 -0.00276144 -0.00014052]
-0.553165
0.788044
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049552 minutes
Epoch 0
Fine tuning took 0.046518 minutes
Epoch 0
Fine tuning took 0.046661 minutes
{'zero': {0: [0.18472906403940886, 0.16502463054187191, 0.17857142857142858, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.67364532019704437, 0.68965517241379315, 0.67241379310344829], 5: [0.14285714285714285, 0.16133004926108374, 0.13177339901477833, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18472906403940886, 0.16995073891625614, 0.1539408866995074, 0.21182266009852216], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.68103448275862066, 0.72536945812807885, 0.66256157635467983], 5: [0.14285714285714285, 0.14901477832512317, 0.1206896551724138, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18472906403940886, 0.15640394088669951, 0.13916256157635468, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.68965517241379315, 0.69827586206896552, 0.63423645320197042], 5: [0.14285714285714285, 0.1539408866995074, 0.1625615763546798, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18472906403940886, 0.16009852216748768, 0.1625615763546798, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.68596059113300489, 0.69334975369458129, 0.6219211822660099], 5: [0.14285714285714285, 0.1539408866995074, 0.14408866995073891, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.276532 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.82171
Epoch 1, cost is  5.60754
Epoch 2, cost is  5.35453
Epoch 3, cost is  5.10758
Epoch 4, cost is  4.88075
Training took 0.225572 minutes
Weight histogram
[776 678 501 307 265 599 610 160  94  60] [ -1.96242202e-02  -1.76582235e-02  -1.56922268e-02  -1.37262301e-02
  -1.17602335e-02  -9.79423676e-03  -7.82824007e-03  -5.86224339e-03
  -3.89624670e-03  -1.93025001e-03   3.57466815e-05]
[1130  382  398  402  353  300  281  270  262  272] [ -1.96242202e-02  -1.76582235e-02  -1.56922268e-02  -1.37262301e-02
  -1.17602335e-02  -9.79423676e-03  -7.82824007e-03  -5.86224339e-03
  -3.89624670e-03  -1.93025001e-03   3.57466815e-05]
-0.462366
0.537804
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.262387 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.32833
Epoch 1, cost is  6.15045
Epoch 2, cost is  5.96048
Epoch 3, cost is  5.75649
Epoch 4, cost is  5.56003
Training took 0.200183 minutes
Weight histogram
[ 375  448  574  573  560 2587  460  239  155  104] [ -1.75135601e-02  -1.57586294e-02  -1.40036988e-02  -1.22487681e-02
  -1.04938374e-02  -8.73890672e-03  -6.98397604e-03  -5.22904536e-03
  -3.47411468e-03  -1.71918400e-03   3.57466815e-05]
[2835  904  524  298  292  269  247  240  232  234] [ -1.75135601e-02  -1.57586294e-02  -1.40036988e-02  -1.22487681e-02
  -1.04938374e-02  -8.73890672e-03  -6.98397604e-03  -5.22904536e-03
  -3.47411468e-03  -1.71918400e-03   3.57466815e-05]
-0.481675
0.617711
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.81207
Epoch 1, cost is  6.66933
Epoch 2, cost is  6.54198
Epoch 3, cost is  6.40526
Epoch 4, cost is  6.23721
Training took 0.229295 minutes
Weight histogram
[1660 1169  618  196  124   89   68   52   40   34] [ -1.02815339e-02  -9.25484447e-03  -8.22815500e-03  -7.20146553e-03
  -6.17477606e-03  -5.14808659e-03  -4.12139712e-03  -3.09470765e-03
  -2.06801818e-03  -1.04132872e-03  -1.46392467e-05]
[1263  578  451  374  327  302  274  232  126  123] [ -1.02815339e-02  -9.25484447e-03  -8.22815500e-03  -7.20146553e-03
  -6.17477606e-03  -5.14808659e-03  -4.12139712e-03  -3.09470765e-03
  -2.06801818e-03  -1.04132872e-03  -1.46392467e-05]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041652 minutes
Epoch 0
Fine tuning took 0.041551 minutes
Epoch 0
Fine tuning took 0.045382 minutes
{'zero': {0: [0.26354679802955666, 0.24384236453201971, 0.2019704433497537, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48645320197044334, 0.58374384236453203, 0.56034482758620685, 0.53078817733990147], 5: [0.25, 0.17241379310344829, 0.2376847290640394, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26354679802955666, 0.24630541871921183, 0.19950738916256158, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48645320197044334, 0.60221674876847286, 0.59482758620689657, 0.5357142857142857], 5: [0.25, 0.15147783251231528, 0.20566502463054187, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26354679802955666, 0.25123152709359609, 0.20812807881773399, 0.25615763546798032], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48645320197044334, 0.58743842364532017, 0.56403940886699511, 0.55911330049261088], 5: [0.25, 0.16133004926108374, 0.22783251231527094, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26354679802955666, 0.24384236453201971, 0.21182266009852216, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48645320197044334, 0.60591133004926112, 0.55418719211822665, 0.55665024630541871], 5: [0.25, 0.15024630541871922, 0.23399014778325122, 0.20320197044334976], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.283157 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.82171
Epoch 1, cost is  5.60754
Epoch 2, cost is  5.35453
Epoch 3, cost is  5.10758
Epoch 4, cost is  4.88075
Training took 0.230168 minutes
Weight histogram
[776 678 501 307 265 599 610 160  94  60] [ -1.96242202e-02  -1.76582235e-02  -1.56922268e-02  -1.37262301e-02
  -1.17602335e-02  -9.79423676e-03  -7.82824007e-03  -5.86224339e-03
  -3.89624670e-03  -1.93025001e-03   3.57466815e-05]
[1130  382  398  402  353  300  281  270  262  272] [ -1.96242202e-02  -1.76582235e-02  -1.56922268e-02  -1.37262301e-02
  -1.17602335e-02  -9.79423676e-03  -7.82824007e-03  -5.86224339e-03
  -3.89624670e-03  -1.93025001e-03   3.57466815e-05]
-0.462366
0.537804
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.283161 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.32833
Epoch 1, cost is  6.15045
Epoch 2, cost is  5.96048
Epoch 3, cost is  5.75649
Epoch 4, cost is  5.56003
Training took 0.203818 minutes
Weight histogram
[ 375  448  574  573  560 2587  460  239  155  104] [ -1.75135601e-02  -1.57586294e-02  -1.40036988e-02  -1.22487681e-02
  -1.04938374e-02  -8.73890672e-03  -6.98397604e-03  -5.22904536e-03
  -3.47411468e-03  -1.71918400e-03   3.57466815e-05]
[2835  904  524  298  292  269  247  240  232  234] [ -1.75135601e-02  -1.57586294e-02  -1.40036988e-02  -1.22487681e-02
  -1.04938374e-02  -8.73890672e-03  -6.98397604e-03  -5.22904536e-03
  -3.47411468e-03  -1.71918400e-03   3.57466815e-05]
-0.481675
0.617711
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.70121
Epoch 1, cost is  6.48942
Epoch 2, cost is  6.32212
Epoch 3, cost is  6.14988
Epoch 4, cost is  5.97215
Training took 0.276167 minutes
Weight histogram
[ 623 1142  993  616  254  151  101   74   55   41] [ -1.24916639e-02  -1.12490776e-02  -1.00064913e-02  -8.76390506e-03
  -7.52131877e-03  -6.27873248e-03  -5.03614620e-03  -3.79355991e-03
  -2.55097362e-03  -1.30838733e-03  -6.58010467e-05]
[1096  521  415  367  338  322  317  314  196  164] [ -1.24916639e-02  -1.12490776e-02  -1.00064913e-02  -8.76390506e-03
  -7.52131877e-03  -6.27873248e-03  -5.03614620e-03  -3.79355991e-03
  -2.55097362e-03  -1.30838733e-03  -6.58010467e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043638 minutes
Epoch 0
Fine tuning took 0.043520 minutes
Epoch 0
Fine tuning took 0.043293 minutes
{'zero': {0: [0.25492610837438423, 0.28201970443349755, 0.23152709359605911, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48399014778325122, 0.55295566502463056, 0.54064039408866993, 0.50985221674876846], 5: [0.26108374384236455, 0.16502463054187191, 0.22783251231527094, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25492610837438423, 0.26477832512315269, 0.21551724137931033, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48399014778325122, 0.57266009852216748, 0.58004926108374388, 0.51108374384236455], 5: [0.26108374384236455, 0.1625615763546798, 0.20443349753694581, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25492610837438423, 0.27586206896551724, 0.2229064039408867, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48399014778325122, 0.54926108374384242, 0.56527093596059108, 0.54556650246305416], 5: [0.26108374384236455, 0.1748768472906404, 0.21182266009852216, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25492610837438423, 0.27339901477832512, 0.2229064039408867, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48399014778325122, 0.5714285714285714, 0.53694581280788178, 0.52955665024630538], 5: [0.26108374384236455, 0.15517241379310345, 0.24014778325123154, 0.22783251231527094], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266222 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.82171
Epoch 1, cost is  5.60754
Epoch 2, cost is  5.35453
Epoch 3, cost is  5.10758
Epoch 4, cost is  4.88075
Training took 0.227427 minutes
Weight histogram
[776 678 501 307 265 599 610 160  94  60] [ -1.96242202e-02  -1.76582235e-02  -1.56922268e-02  -1.37262301e-02
  -1.17602335e-02  -9.79423676e-03  -7.82824007e-03  -5.86224339e-03
  -3.89624670e-03  -1.93025001e-03   3.57466815e-05]
[1130  382  398  402  353  300  281  270  262  272] [ -1.96242202e-02  -1.76582235e-02  -1.56922268e-02  -1.37262301e-02
  -1.17602335e-02  -9.79423676e-03  -7.82824007e-03  -5.86224339e-03
  -3.89624670e-03  -1.93025001e-03   3.57466815e-05]
-0.462366
0.537804
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270653 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.32833
Epoch 1, cost is  6.15045
Epoch 2, cost is  5.96048
Epoch 3, cost is  5.75649
Epoch 4, cost is  5.56003
Training took 0.205000 minutes
Weight histogram
[ 375  448  574  573  560 2587  460  239  155  104] [ -1.75135601e-02  -1.57586294e-02  -1.40036988e-02  -1.22487681e-02
  -1.04938374e-02  -8.73890672e-03  -6.98397604e-03  -5.22904536e-03
  -3.47411468e-03  -1.71918400e-03   3.57466815e-05]
[2835  904  524  298  292  269  247  240  232  234] [ -1.75135601e-02  -1.57586294e-02  -1.40036988e-02  -1.22487681e-02
  -1.04938374e-02  -8.73890672e-03  -6.98397604e-03  -5.22904536e-03
  -3.47411468e-03  -1.71918400e-03   3.57466815e-05]
-0.481675
0.617711
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.52722
Epoch 1, cost is  6.21729
Epoch 2, cost is  6.01048
Epoch 3, cost is  5.82861
Epoch 4, cost is  5.65913
Training took 0.363138 minutes
Weight histogram
[557 837 896 779 409 224 140  94  66  48] [ -1.41741699e-02  -1.27601098e-02  -1.13460497e-02  -9.93198963e-03
  -8.51792956e-03  -7.10386948e-03  -5.68980941e-03  -4.27574933e-03
  -2.86168926e-03  -1.44762918e-03  -3.35691075e-05]
[937 470 391 356 346 344 361 375 259 211] [ -1.41741699e-02  -1.27601098e-02  -1.13460497e-02  -9.93198963e-03
  -8.51792956e-03  -7.10386948e-03  -5.68980941e-03  -4.27574933e-03
  -2.86168926e-03  -1.44762918e-03  -3.35691075e-05]
-0.0718176
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049532 minutes
Epoch 0
Fine tuning took 0.046951 minutes
Epoch 0
Fine tuning took 0.046661 minutes
{'zero': {0: [0.25985221674876846, 0.25123152709359609, 0.24630541871921183, 0.27463054187192121], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47536945812807879, 0.57019704433497542, 0.51970443349753692, 0.50862068965517238], 5: [0.26477832512315269, 0.17857142857142858, 0.23399014778325122, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25985221674876846, 0.24261083743842365, 0.22413793103448276, 0.28078817733990147], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47536945812807879, 0.5923645320197044, 0.5431034482758621, 0.50369458128078815], 5: [0.26477832512315269, 0.16502463054187191, 0.23275862068965517, 0.21551724137931033], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25985221674876846, 0.27586206896551724, 0.24507389162561577, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47536945812807879, 0.52463054187192115, 0.54064039408866993, 0.55049261083743839], 5: [0.26477832512315269, 0.19950738916256158, 0.21428571428571427, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25985221674876846, 0.2536945812807882, 0.23891625615763548, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47536945812807879, 0.56896551724137934, 0.55049261083743839, 0.50492610837438423], 5: [0.26477832512315269, 0.17733990147783252, 0.2105911330049261, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269702 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.91748
Epoch 1, cost is  1.82028
Epoch 2, cost is  1.80899
Epoch 3, cost is  1.82903
Epoch 4, cost is  1.84571
Training took 0.228387 minutes
Weight histogram
[896 861 645 489 473 269 196 107  67  47] [-0.12333165 -0.11107624 -0.09882083 -0.08656542 -0.07431001 -0.0620546
 -0.04979918 -0.03754377 -0.02528836 -0.01303295 -0.00077754]
[134 174 247 310 382 442 539 600 600 622] [-0.12333165 -0.11107624 -0.09882083 -0.08656542 -0.07431001 -0.0620546
 -0.04979918 -0.03754377 -0.02528836 -0.01303295 -0.00077754]
-4.81205
7.21779
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.255886 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4485
Epoch 1, cost is  3.50119
Epoch 2, cost is  3.63152
Epoch 3, cost is  3.80107
Epoch 4, cost is  3.97065
Training took 0.200710 minutes
Weight histogram
[ 602  626  683  549  458  655 1223  748  337  194] [-0.22508428 -0.2026536  -0.18022293 -0.15779225 -0.13536158 -0.11293091
 -0.09050023 -0.06806956 -0.04563889 -0.02320821 -0.00077754]
[ 471  763 1095  796  427  458  521  515  510  519] [-0.22508428 -0.2026536  -0.18022293 -0.15779225 -0.13536158 -0.11293091
 -0.09050023 -0.06806956 -0.04563889 -0.02320821 -0.00077754]
-8.37384
7.19576
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.09414
Epoch 1, cost is  3.74647
Epoch 2, cost is  3.5716
Epoch 3, cost is  3.6634
Epoch 4, cost is  3.81712
Training took 0.231562 minutes
Weight histogram
[702 611 714 563 494 325 217 159 118 147] [-0.17713617 -0.15955292 -0.14196967 -0.12438642 -0.10680317 -0.08921992
 -0.07163667 -0.05405341 -0.03647016 -0.01888691 -0.00130366]
[258 237 286 340 390 441 487 524 552 535] [-0.17713617 -0.15955292 -0.14196967 -0.12438642 -0.10680317 -0.08921992
 -0.07163667 -0.05405341 -0.03647016 -0.01888691 -0.00130366]
-5.72433
6.97239
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042433 minutes
Epoch 0
Fine tuning took 0.042459 minutes
Epoch 0
Fine tuning took 0.041801 minutes
{'zero': {0: [0.15640394088669951, 0.19088669950738915, 0.16748768472906403, 0.21182266009852216], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.73029556650246308, 0.70935960591133007, 0.70812807881773399], 5: [0.1354679802955665, 0.078817733990147784, 0.12315270935960591, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15640394088669951, 0.18103448275862069, 0.15886699507389163, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.69704433497536944, 0.6785714285714286, 0.65270935960591137], 5: [0.1354679802955665, 0.12192118226600986, 0.1625615763546798, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15640394088669951, 0.15763546798029557, 0.17980295566502463, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.7068965517241379, 0.69334975369458129, 0.69704433497536944], 5: [0.1354679802955665, 0.1354679802955665, 0.1268472906403941, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15640394088669951, 0.19211822660098521, 0.15024630541871922, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.67733990147783252, 0.66379310344827591, 0.62438423645320196], 5: [0.1354679802955665, 0.13054187192118227, 0.18596059113300492, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267510 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.91748
Epoch 1, cost is  1.82028
Epoch 2, cost is  1.80899
Epoch 3, cost is  1.82903
Epoch 4, cost is  1.84571
Training took 0.225483 minutes
Weight histogram
[896 861 645 489 473 269 196 107  67  47] [-0.12333165 -0.11107624 -0.09882083 -0.08656542 -0.07431001 -0.0620546
 -0.04979918 -0.03754377 -0.02528836 -0.01303295 -0.00077754]
[134 174 247 310 382 442 539 600 600 622] [-0.12333165 -0.11107624 -0.09882083 -0.08656542 -0.07431001 -0.0620546
 -0.04979918 -0.03754377 -0.02528836 -0.01303295 -0.00077754]
-4.81205
7.21779
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264653 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4485
Epoch 1, cost is  3.50119
Epoch 2, cost is  3.63152
Epoch 3, cost is  3.80107
Epoch 4, cost is  3.97065
Training took 0.203215 minutes
Weight histogram
[ 602  626  683  549  458  655 1223  748  337  194] [-0.22508428 -0.2026536  -0.18022293 -0.15779225 -0.13536158 -0.11293091
 -0.09050023 -0.06806956 -0.04563889 -0.02320821 -0.00077754]
[ 471  763 1095  796  427  458  521  515  510  519] [-0.22508428 -0.2026536  -0.18022293 -0.15779225 -0.13536158 -0.11293091
 -0.09050023 -0.06806956 -0.04563889 -0.02320821 -0.00077754]
-8.37384
7.19576
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.7064
Epoch 1, cost is  2.77808
Epoch 2, cost is  2.41027
Epoch 3, cost is  2.32283
Epoch 4, cost is  2.33507
Training took 0.266271 minutes
Weight histogram
[689 746 631 532 442 365 225 168 125 127] [-0.11230296 -0.10120534 -0.09010773 -0.07901011 -0.06791249 -0.05681488
 -0.04571726 -0.03461964 -0.02352202 -0.01242441 -0.00132679]
[273 222 265 305 363 444 492 535 583 568] [-0.11230296 -0.10120534 -0.09010773 -0.07901011 -0.06791249 -0.05681488
 -0.04571726 -0.03461964 -0.02352202 -0.01242441 -0.00132679]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043122 minutes
Epoch 0
Fine tuning took 0.043177 minutes
Epoch 0
Fine tuning took 0.044857 minutes
{'zero': {0: [0.10837438423645321, 0.14532019704433496, 0.20689655172413793, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76970443349753692, 0.74630541871921185, 0.66502463054187189, 0.75615763546798032], 5: [0.12192118226600986, 0.10837438423645321, 0.12807881773399016, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10837438423645321, 0.1354679802955665, 0.12807881773399016, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76970443349753692, 0.76108374384236455, 0.75123152709359609, 0.76847290640394084], 5: [0.12192118226600986, 0.10344827586206896, 0.1206896551724138, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10837438423645321, 0.17241379310344829, 0.16995073891625614, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76970443349753692, 0.7068965517241379, 0.72536945812807885, 0.71921182266009853], 5: [0.12192118226600986, 0.1206896551724138, 0.10467980295566502, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10837438423645321, 0.13300492610837439, 0.13423645320197045, 0.10591133004926108], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76970443349753692, 0.75369458128078815, 0.74014778325123154, 0.75123152709359609], 5: [0.12192118226600986, 0.11330049261083744, 0.12561576354679804, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.263516 minutes
Weight histogram
[ 115  350  417  447  464  213  403 1168  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 101  102  138  174  227  346  435  676  706 1145] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.16306
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.91748
Epoch 1, cost is  1.82028
Epoch 2, cost is  1.80899
Epoch 3, cost is  1.82903
Epoch 4, cost is  1.84571
Training took 0.223988 minutes
Weight histogram
[896 861 645 489 473 269 196 107  67  47] [-0.12333165 -0.11107624 -0.09882083 -0.08656542 -0.07431001 -0.0620546
 -0.04979918 -0.03754377 -0.02528836 -0.01303295 -0.00077754]
[134 174 247 310 382 442 539 600 600 622] [-0.12333165 -0.11107624 -0.09882083 -0.08656542 -0.07431001 -0.0620546
 -0.04979918 -0.03754377 -0.02528836 -0.01303295 -0.00077754]
-4.81205
7.21779
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.256855 minutes
Weight histogram
[ 111  384  620  769  783  866  911 1135  443   53] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
[ 202  206  276  364  447  636  949 1048  742 1205] [ -7.24884303e-05   8.04801835e-05   2.33448797e-04   3.86417411e-04
   5.39386025e-04   6.92354639e-04   8.45323253e-04   9.98291867e-04
   1.15126048e-03   1.30422909e-03   1.45719771e-03]
-0.838392
0.820803
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.4485
Epoch 1, cost is  3.50119
Epoch 2, cost is  3.63152
Epoch 3, cost is  3.80107
Epoch 4, cost is  3.97065
Training took 0.203192 minutes
Weight histogram
[ 602  626  683  549  458  655 1223  748  337  194] [-0.22508428 -0.2026536  -0.18022293 -0.15779225 -0.13536158 -0.11293091
 -0.09050023 -0.06806956 -0.04563889 -0.02320821 -0.00077754]
[ 471  763 1095  796  427  458  521  515  510  519] [-0.22508428 -0.2026536  -0.18022293 -0.15779225 -0.13536158 -0.11293091
 -0.09050023 -0.06806956 -0.04563889 -0.02320821 -0.00077754]
-8.37384
7.19576
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.44604
Epoch 1, cost is  2.22844
Epoch 2, cost is  1.73345
Epoch 3, cost is  1.56258
Epoch 4, cost is  1.48102
Training took 0.357017 minutes
Weight histogram
[826 722 693 487 438 323 206 177 118  60] [-0.07301652 -0.06584312 -0.05866972 -0.05149632 -0.04432292 -0.03714952
 -0.02997612 -0.02280271 -0.01562931 -0.00845591 -0.00128251]
[274 203 234 286 359 431 504 569 619 571] [-0.07301652 -0.06584312 -0.05866972 -0.05149632 -0.04432292 -0.03714952
 -0.02997612 -0.02280271 -0.01562931 -0.00845591 -0.00128251]
-3.66463
3.82097
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046928 minutes
Epoch 0
Fine tuning took 0.046508 minutes
Epoch 0
Fine tuning took 0.046489 minutes
{'zero': {0: [0.12192118226600986, 0.1625615763546798, 0.18596059113300492, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76231527093596063, 0.7426108374384236, 0.70320197044334976, 0.74137931034482762], 5: [0.11576354679802955, 0.094827586206896547, 0.11083743842364532, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12192118226600986, 0.13669950738916256, 0.18719211822660098, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76231527093596063, 0.77093596059113301, 0.65886699507389157, 0.73768472906403937], 5: [0.11576354679802955, 0.092364532019704432, 0.1539408866995074, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12192118226600986, 0.14778325123152711, 0.20073891625615764, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76231527093596063, 0.73399014778325122, 0.6576354679802956, 0.75], 5: [0.11576354679802955, 0.11822660098522167, 0.14162561576354679, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12192118226600986, 0.16871921182266009, 0.22044334975369459, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76231527093596063, 0.76847290640394084, 0.62561576354679804, 0.71551724137931039], 5: [0.11576354679802955, 0.062807881773399021, 0.1539408866995074, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.283119 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.97572
Epoch 1, cost is  1.87971
Epoch 2, cost is  1.81999
Epoch 3, cost is  1.78254
Epoch 4, cost is  1.74335
Training took 0.242700 minutes
Weight histogram
[1356  861 1182  837  630  465  249  249  191   55] [-0.07385938 -0.0664848  -0.05911023 -0.05173565 -0.04436108 -0.03698651
 -0.02961193 -0.02223736 -0.01486278 -0.00748821 -0.00011363]
[377 270 365 444 539 709 747 825 875 924] [-0.07385938 -0.0664848  -0.05911023 -0.05173565 -0.04436108 -0.03698651
 -0.02961193 -0.02223736 -0.01486278 -0.00748821 -0.00011363]
-2.20964
2.33442
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.275216 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.13698
Epoch 1, cost is  3.06046
Epoch 2, cost is  3.0387
Epoch 3, cost is  3.02619
Epoch 4, cost is  3.02334
Training took 0.202241 minutes
Weight histogram
[1291  954 1012  784  522  433 1320  739  575  470] [ -1.15290329e-01  -1.03772659e-01  -9.22549896e-02  -8.07373199e-02
  -6.92196503e-02  -5.77019806e-02  -4.61843110e-02  -3.46666413e-02
  -2.31489717e-02  -1.16313020e-02  -1.13632348e-04]
[ 996  913 1221  454  581  679  726  808  860  862] [ -1.15290329e-01  -1.03772659e-01  -9.22549896e-02  -8.07373199e-02
  -6.92196503e-02  -5.77019806e-02  -4.61843110e-02  -3.46666413e-02
  -2.31489717e-02  -1.16313020e-02  -1.13632348e-04]
-3.07498
3.31744
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55156
Epoch 1, cost is  5.42079
Epoch 2, cost is  4.58622
Epoch 3, cost is  4.17168
Epoch 4, cost is  3.93864
Training took 0.230556 minutes
Weight histogram
[1038  767  667  716  472  432  425  388  283  887] [-0.08295341 -0.07468248 -0.06641156 -0.05814064 -0.04986971 -0.04159879
 -0.03332786 -0.02505694 -0.01678601 -0.00851509 -0.00024416]
[917 551 442 498 517 555 613 683 748 551] [-0.08295341 -0.07468248 -0.06641156 -0.05814064 -0.04986971 -0.04159879
 -0.03332786 -0.02505694 -0.01678601 -0.00851509 -0.00024416]
-1.6243
2.5708
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041614 minutes
Epoch 0
Fine tuning took 0.041750 minutes
Epoch 0
Fine tuning took 0.041866 minutes
{'zero': {0: [0.24261083743842365, 0.25492610837438423, 0.20320197044334976, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.66871921182266014, 0.69827586206896552, 0.75], 5: [0.05295566502463054, 0.076354679802955669, 0.098522167487684734, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24261083743842365, 0.24630541871921183, 0.20812807881773399, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.68349753694581283, 0.71305418719211822, 0.72413793103448276], 5: [0.05295566502463054, 0.070197044334975367, 0.078817733990147784, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24261083743842365, 0.22660098522167488, 0.19704433497536947, 0.22044334975369459], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.69211822660098521, 0.72783251231527091, 0.69211822660098521], 5: [0.05295566502463054, 0.081280788177339899, 0.075123152709359611, 0.087438423645320201], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24261083743842365, 0.24261083743842365, 0.20812807881773399, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.68226600985221675, 0.72660098522167482, 0.73768472906403937], 5: [0.05295566502463054, 0.075123152709359611, 0.065270935960591137, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.286482 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.97572
Epoch 1, cost is  1.87971
Epoch 2, cost is  1.81999
Epoch 3, cost is  1.78254
Epoch 4, cost is  1.74335
Training took 0.229550 minutes
Weight histogram
[1356  861 1182  837  630  465  249  249  191   55] [-0.07385938 -0.0664848  -0.05911023 -0.05173565 -0.04436108 -0.03698651
 -0.02961193 -0.02223736 -0.01486278 -0.00748821 -0.00011363]
[377 270 365 444 539 709 747 825 875 924] [-0.07385938 -0.0664848  -0.05911023 -0.05173565 -0.04436108 -0.03698651
 -0.02961193 -0.02223736 -0.01486278 -0.00748821 -0.00011363]
-2.20964
2.33442
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270512 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.13698
Epoch 1, cost is  3.06046
Epoch 2, cost is  3.0387
Epoch 3, cost is  3.02619
Epoch 4, cost is  3.02334
Training took 0.202113 minutes
Weight histogram
[1291  954 1012  784  522  433 1320  739  575  470] [ -1.15290329e-01  -1.03772659e-01  -9.22549896e-02  -8.07373199e-02
  -6.92196503e-02  -5.77019806e-02  -4.61843110e-02  -3.46666413e-02
  -2.31489717e-02  -1.16313020e-02  -1.13632348e-04]
[ 996  913 1221  454  581  679  726  808  860  862] [ -1.15290329e-01  -1.03772659e-01  -9.22549896e-02  -8.07373199e-02
  -6.92196503e-02  -5.77019806e-02  -4.61843110e-02  -3.46666413e-02
  -2.31489717e-02  -1.16313020e-02  -1.13632348e-04]
-3.07498
3.31744
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.45521
Epoch 1, cost is  5.25366
Epoch 2, cost is  4.14181
Epoch 3, cost is  3.50296
Epoch 4, cost is  3.06596
Training took 0.266241 minutes
Weight histogram
[974 996 709 641 566 458 469 334 851  77] [-0.05099323 -0.04592384 -0.04085445 -0.03578507 -0.03071568 -0.02564629
 -0.02057691 -0.01550752 -0.01043814 -0.00536875 -0.00029936]
[974 579 466 503 496 557 613 657 698 532] [-0.05099323 -0.04592384 -0.04085445 -0.03578507 -0.03071568 -0.02564629
 -0.02057691 -0.01550752 -0.01043814 -0.00536875 -0.00029936]
-1.1998
1.51512
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046328 minutes
Epoch 0
Fine tuning took 0.046627 minutes
Epoch 0
Fine tuning took 0.042889 minutes
{'zero': {0: [0.19458128078817735, 0.20320197044334976, 0.16871921182266009, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.65394088669950734, 0.69581280788177335, 0.72290640394088668], 5: [0.080049261083743842, 0.14285714285714285, 0.1354679802955665, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19458128078817735, 0.19334975369458129, 0.17857142857142858, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.6785714285714286, 0.69950738916256161, 0.73645320197044339], 5: [0.080049261083743842, 0.12807881773399016, 0.12192118226600986, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19458128078817735, 0.19581280788177341, 0.19334975369458129, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.68472906403940892, 0.70935960591133007, 0.7142857142857143], 5: [0.080049261083743842, 0.11945812807881774, 0.097290640394088676, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19458128078817735, 0.20566502463054187, 0.18226600985221675, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.69334975369458129, 0.7068965517241379, 0.76108374384236455], 5: [0.080049261083743842, 0.10098522167487685, 0.11083743842364532, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266842 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.97572
Epoch 1, cost is  1.87971
Epoch 2, cost is  1.81999
Epoch 3, cost is  1.78254
Epoch 4, cost is  1.74335
Training took 0.224115 minutes
Weight histogram
[1356  861 1182  837  630  465  249  249  191   55] [-0.07385938 -0.0664848  -0.05911023 -0.05173565 -0.04436108 -0.03698651
 -0.02961193 -0.02223736 -0.01486278 -0.00748821 -0.00011363]
[377 270 365 444 539 709 747 825 875 924] [-0.07385938 -0.0664848  -0.05911023 -0.05173565 -0.04436108 -0.03698651
 -0.02961193 -0.02223736 -0.01486278 -0.00748821 -0.00011363]
-2.20964
2.33442
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265519 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.13698
Epoch 1, cost is  3.06046
Epoch 2, cost is  3.0387
Epoch 3, cost is  3.02619
Epoch 4, cost is  3.02334
Training took 0.202208 minutes
Weight histogram
[1291  954 1012  784  522  433 1320  739  575  470] [ -1.15290329e-01  -1.03772659e-01  -9.22549896e-02  -8.07373199e-02
  -6.92196503e-02  -5.77019806e-02  -4.61843110e-02  -3.46666413e-02
  -2.31489717e-02  -1.16313020e-02  -1.13632348e-04]
[ 996  913 1221  454  581  679  726  808  860  862] [ -1.15290329e-01  -1.03772659e-01  -9.22549896e-02  -8.07373199e-02
  -6.92196503e-02  -5.77019806e-02  -4.61843110e-02  -3.46666413e-02
  -2.31489717e-02  -1.16313020e-02  -1.13632348e-04]
-3.07498
3.31744
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.33577
Epoch 1, cost is  5.1419
Epoch 2, cost is  3.88693
Epoch 3, cost is  3.0856
Epoch 4, cost is  2.54962
Training took 0.357805 minutes
Weight histogram
[1102 1086  762  711  574  561  416  686  142   35] [-0.03414995 -0.03076108 -0.02737222 -0.02398336 -0.02059449 -0.01720563
 -0.01381677 -0.0104279  -0.00703904 -0.00365017 -0.00026131]
[1066  593  460  450  492  551  579  619  704  561] [-0.03414995 -0.03076108 -0.02737222 -0.02398336 -0.02059449 -0.01720563
 -0.01381677 -0.0104279  -0.00703904 -0.00365017 -0.00026131]
-0.8878
1.31404
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046481 minutes
Epoch 0
Fine tuning took 0.046445 minutes
Epoch 0
Fine tuning took 0.045716 minutes
{'zero': {0: [0.1625615763546798, 0.2229064039408867, 0.15270935960591134, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.52216748768472909, 0.58990147783251234, 0.57512315270935965], 5: [0.16502463054187191, 0.25492610837438423, 0.25738916256157635, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1625615763546798, 0.21551724137931033, 0.15886699507389163, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.55295566502463056, 0.58743842364532017, 0.59605911330049266], 5: [0.16502463054187191, 0.23152709359605911, 0.2536945812807882, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1625615763546798, 0.21674876847290642, 0.16379310344827586, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.55049261083743839, 0.63177339901477836, 0.58128078817733986], 5: [0.16502463054187191, 0.23275862068965517, 0.20443349753694581, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1625615763546798, 0.19088669950738915, 0.15024630541871922, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67241379310344829, 0.59975369458128081, 0.61822660098522164, 0.58497536945812811], 5: [0.16502463054187191, 0.20935960591133004, 0.23152709359605911, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269529 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42467
Epoch 1, cost is  2.31705
Epoch 2, cost is  2.23302
Epoch 3, cost is  2.16346
Epoch 4, cost is  2.09841
Training took 0.230995 minutes
Weight histogram
[1304  895 1092  828  565  392  395  259  290   55] [ -5.56764156e-02  -5.01118384e-02  -4.45472612e-02  -3.89826841e-02
  -3.34181069e-02  -2.78535298e-02  -2.22889526e-02  -1.67243755e-02
  -1.11597983e-02  -5.59522115e-03  -3.06439943e-05]
[579 315 370 442 543 634 704 776 835 877] [ -5.56764156e-02  -5.01118384e-02  -4.45472612e-02  -3.89826841e-02
  -3.34181069e-02  -2.78535298e-02  -2.22889526e-02  -1.67243755e-02
  -1.11597983e-02  -5.59522115e-03  -3.06439943e-05]
-1.42898
1.58959
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.262954 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.3575
Epoch 1, cost is  3.26545
Epoch 2, cost is  3.2032
Epoch 3, cost is  3.15583
Epoch 4, cost is  3.11513
Training took 0.202376 minutes
Weight histogram
[1291  918  806  629  533  491 1140  895  702  695] [ -8.40674415e-02  -7.56637618e-02  -6.72600820e-02  -5.88564023e-02
  -5.04527225e-02  -4.20490428e-02  -3.36453630e-02  -2.52416833e-02
  -1.68380035e-02  -8.43432375e-03  -3.06439943e-05]
[1448  919 1026  430  506  604  674  773  852  868] [ -8.40674415e-02  -7.56637618e-02  -6.72600820e-02  -5.88564023e-02
  -5.04527225e-02  -4.20490428e-02  -3.36453630e-02  -2.52416833e-02
  -1.68380035e-02  -8.43432375e-03  -3.06439943e-05]
-1.86991
2.29969
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.7402
Epoch 1, cost is  6.34357
Epoch 2, cost is  5.73792
Epoch 3, cost is  5.16798
Epoch 4, cost is  4.79275
Training took 0.225042 minutes
Weight histogram
[ 477  669  601  534  498  516  466  368 1779  167] [-0.0558529  -0.05027937 -0.04470583 -0.0391323  -0.03355877 -0.02798524
 -0.02241171 -0.01683817 -0.01126464 -0.00569111 -0.00011758]
[1369  542  600  553  483  497  542  541  573  375] [-0.0558529  -0.05027937 -0.04470583 -0.0391323  -0.03355877 -0.02798524
 -0.02241171 -0.01683817 -0.01126464 -0.00569111 -0.00011758]
-0.921519
1.1068
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044817 minutes
Epoch 0
Fine tuning took 0.041323 minutes
Epoch 0
Fine tuning took 0.041419 minutes
{'zero': {0: [0.13054187192118227, 0.22536945812807882, 0.15763546798029557, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65147783251231528, 0.59852216748768472, 0.71798029556650245, 0.6428571428571429], 5: [0.21798029556650247, 0.17610837438423646, 0.12438423645320197, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13054187192118227, 0.20935960591133004, 0.13916256157635468, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65147783251231528, 0.60344827586206895, 0.75, 0.68349753694581283], 5: [0.21798029556650247, 0.18719211822660098, 0.11083743842364532, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13054187192118227, 0.19334975369458129, 0.1354679802955665, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65147783251231528, 0.60837438423645318, 0.75123152709359609, 0.64408866995073888], 5: [0.21798029556650247, 0.19827586206896552, 0.11330049261083744, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13054187192118227, 0.18719211822660098, 0.14655172413793102, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65147783251231528, 0.63300492610837433, 0.72413793103448276, 0.66625615763546797], 5: [0.21798029556650247, 0.17980295566502463, 0.12931034482758622, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272356 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42467
Epoch 1, cost is  2.31705
Epoch 2, cost is  2.23302
Epoch 3, cost is  2.16346
Epoch 4, cost is  2.09841
Training took 0.233363 minutes
Weight histogram
[1304  895 1092  828  565  392  395  259  290   55] [ -5.56764156e-02  -5.01118384e-02  -4.45472612e-02  -3.89826841e-02
  -3.34181069e-02  -2.78535298e-02  -2.22889526e-02  -1.67243755e-02
  -1.11597983e-02  -5.59522115e-03  -3.06439943e-05]
[579 315 370 442 543 634 704 776 835 877] [ -5.56764156e-02  -5.01118384e-02  -4.45472612e-02  -3.89826841e-02
  -3.34181069e-02  -2.78535298e-02  -2.22889526e-02  -1.67243755e-02
  -1.11597983e-02  -5.59522115e-03  -3.06439943e-05]
-1.42898
1.58959
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270557 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.3575
Epoch 1, cost is  3.26545
Epoch 2, cost is  3.2032
Epoch 3, cost is  3.15583
Epoch 4, cost is  3.11513
Training took 0.197806 minutes
Weight histogram
[1291  918  806  629  533  491 1140  895  702  695] [ -8.40674415e-02  -7.56637618e-02  -6.72600820e-02  -5.88564023e-02
  -5.04527225e-02  -4.20490428e-02  -3.36453630e-02  -2.52416833e-02
  -1.68380035e-02  -8.43432375e-03  -3.06439943e-05]
[1448  919 1026  430  506  604  674  773  852  868] [ -8.40674415e-02  -7.56637618e-02  -6.72600820e-02  -5.88564023e-02
  -5.04527225e-02  -4.20490428e-02  -3.36453630e-02  -2.52416833e-02
  -1.68380035e-02  -8.43432375e-03  -3.06439943e-05]
-1.86991
2.29969
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.64676
Epoch 1, cost is  6.23847
Epoch 2, cost is  5.61737
Epoch 3, cost is  4.94481
Epoch 4, cost is  4.38157
Training took 0.286076 minutes
Weight histogram
[ 354  827  767  626  621  559  450 1444  354   73] [-0.03642388 -0.03279803 -0.02917218 -0.02554633 -0.02192047 -0.01829462
 -0.01466877 -0.01104292 -0.00741706 -0.00379121 -0.00016536]
[1287  665  706  552  467  477  517  508  525  371] [-0.03642388 -0.03279803 -0.02917218 -0.02554633 -0.02192047 -0.01829462
 -0.01466877 -0.01104292 -0.00741706 -0.00379121 -0.00016536]
-0.823236
0.921125
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042934 minutes
Epoch 0
Fine tuning took 0.043252 minutes
Epoch 0
Fine tuning took 0.043116 minutes
{'zero': {0: [0.15886699507389163, 0.21921182266009853, 0.17610837438423646, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.61330049261083741, 0.67610837438423643, 0.66379310344827591], 5: [0.14285714285714285, 0.16748768472906403, 0.14778325123152711, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15886699507389163, 0.18103448275862069, 0.15886699507389163, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.6280788177339901, 0.69704433497536944, 0.65394088669950734], 5: [0.14285714285714285, 0.19088669950738915, 0.14408866995073891, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15886699507389163, 0.18596059113300492, 0.16871921182266009, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.63669950738916259, 0.68965517241379315, 0.66748768472906406], 5: [0.14285714285714285, 0.17733990147783252, 0.14162561576354679, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15886699507389163, 0.18965517241379309, 0.16379310344827586, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.65517241379310343, 0.68965517241379315, 0.66995073891625612], 5: [0.14285714285714285, 0.15517241379310345, 0.14655172413793102, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269802 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.42467
Epoch 1, cost is  2.31705
Epoch 2, cost is  2.23302
Epoch 3, cost is  2.16346
Epoch 4, cost is  2.09841
Training took 0.224301 minutes
Weight histogram
[1304  895 1092  828  565  392  395  259  290   55] [ -5.56764156e-02  -5.01118384e-02  -4.45472612e-02  -3.89826841e-02
  -3.34181069e-02  -2.78535298e-02  -2.22889526e-02  -1.67243755e-02
  -1.11597983e-02  -5.59522115e-03  -3.06439943e-05]
[579 315 370 442 543 634 704 776 835 877] [ -5.56764156e-02  -5.01118384e-02  -4.45472612e-02  -3.89826841e-02
  -3.34181069e-02  -2.78535298e-02  -2.22889526e-02  -1.67243755e-02
  -1.11597983e-02  -5.59522115e-03  -3.06439943e-05]
-1.42898
1.58959
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267121 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.3575
Epoch 1, cost is  3.26545
Epoch 2, cost is  3.2032
Epoch 3, cost is  3.15583
Epoch 4, cost is  3.11513
Training took 0.202731 minutes
Weight histogram
[1291  918  806  629  533  491 1140  895  702  695] [ -8.40674415e-02  -7.56637618e-02  -6.72600820e-02  -5.88564023e-02
  -5.04527225e-02  -4.20490428e-02  -3.36453630e-02  -2.52416833e-02
  -1.68380035e-02  -8.43432375e-03  -3.06439943e-05]
[1448  919 1026  430  506  604  674  773  852  868] [ -8.40674415e-02  -7.56637618e-02  -6.72600820e-02  -5.88564023e-02
  -5.04527225e-02  -4.20490428e-02  -3.36453630e-02  -2.52416833e-02
  -1.68380035e-02  -8.43432375e-03  -3.06439943e-05]
-1.86991
2.29969
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.50936
Epoch 1, cost is  6.11759
Epoch 2, cost is  5.54921
Epoch 3, cost is  4.76754
Epoch 4, cost is  4.1303
Training took 0.356495 minutes
Weight histogram
[ 434  996  948  736  710  573 1057  474  102   45] [-0.02634971 -0.02372817 -0.02110662 -0.01848508 -0.01586353 -0.01324199
 -0.01062044 -0.0079989  -0.00537735 -0.00275581 -0.00013426]
[1247  837  782  524  464  459  455  472  489  346] [-0.02634971 -0.02372817 -0.02110662 -0.01848508 -0.01586353 -0.01324199
 -0.01062044 -0.0079989  -0.00537735 -0.00275581 -0.00013426]
-0.571883
0.823935
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046161 minutes
Epoch 0
Fine tuning took 0.046428 minutes
Epoch 0
Fine tuning took 0.046574 minutes
{'zero': {0: [0.15270935960591134, 0.24630541871921183, 0.23152709359605911, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.56527093596059108, 0.63793103448275867, 0.67980295566502458], 5: [0.14655172413793102, 0.18842364532019704, 0.13054187192118227, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15270935960591134, 0.2413793103448276, 0.23152709359605911, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.58374384236453203, 0.61699507389162567, 0.65886699507389157], 5: [0.14655172413793102, 0.1748768472906404, 0.15147783251231528, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15270935960591134, 0.20320197044334976, 0.20812807881773399, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.59729064039408863, 0.66748768472906406, 0.66009852216748766], 5: [0.14655172413793102, 0.19950738916256158, 0.12438423645320197, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15270935960591134, 0.21428571428571427, 0.19827586206896552, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.60467980295566504, 0.68226600985221675, 0.68472906403940892], 5: [0.14655172413793102, 0.18103448275862069, 0.11945812807881774, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.273676 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.67231
Epoch 1, cost is  4.48977
Epoch 2, cost is  4.33702
Epoch 3, cost is  4.20954
Epoch 4, cost is  4.08142
Training took 0.224588 minutes
Weight histogram
[840 726 980 927 626 376 619 732 162  87] [ -2.62033865e-02  -2.35794732e-02  -2.09555599e-02  -1.83316465e-02
  -1.57077332e-02  -1.30838199e-02  -1.04599066e-02  -7.83599327e-03
  -5.21207995e-03  -2.58816664e-03   3.57466815e-05]
[1390  653  616  485  453  440  446  495  535  562] [ -2.62033865e-02  -2.35794732e-02  -2.09555599e-02  -1.83316465e-02
  -1.57077332e-02  -1.30838199e-02  -1.04599066e-02  -7.83599327e-03
  -5.21207995e-03  -2.58816664e-03   3.57466815e-05]
-0.644436
0.735366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264002 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34676
Epoch 1, cost is  5.18558
Epoch 2, cost is  5.04484
Epoch 3, cost is  4.91851
Epoch 4, cost is  4.806
Training took 0.200449 minutes
Weight histogram
[ 500  585  493  462  715  915  909 2914  408  199] [ -2.90271491e-02  -2.61208595e-02  -2.32145699e-02  -2.03082804e-02
  -1.74019908e-02  -1.44957012e-02  -1.15894116e-02  -8.68312205e-03
  -5.77683247e-03  -2.87054289e-03   3.57466815e-05]
[3514  895  504  447  412  422  447  469  478  512] [ -2.90271491e-02  -2.61208595e-02  -2.32145699e-02  -2.03082804e-02
  -1.74019908e-02  -1.44957012e-02  -1.15894116e-02  -8.68312205e-03
  -5.77683247e-03  -2.87054289e-03   3.57466815e-05]
-0.610958
0.829228
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.83404
Epoch 1, cost is  6.72467
Epoch 2, cost is  6.63242
Epoch 3, cost is  6.54193
Epoch 4, cost is  6.43746
Training took 0.228273 minutes
Weight histogram
[1660 1773 1627  343  213  148  110   84   64   53] [ -1.02815339e-02  -9.25451763e-03  -8.22750132e-03  -7.20048501e-03
  -6.17346870e-03  -5.14645240e-03  -4.11943609e-03  -3.09241978e-03
  -2.06540348e-03  -1.03838717e-03  -1.13708611e-05]
[2035  932  723  606  523  487  288  232  126  123] [ -1.02815339e-02  -9.25451763e-03  -8.22750132e-03  -7.20048501e-03
  -6.17346870e-03  -5.14645240e-03  -4.11943609e-03  -3.09241978e-03
  -2.06540348e-03  -1.03838717e-03  -1.13708611e-05]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041638 minutes
Epoch 0
Fine tuning took 0.041996 minutes
Epoch 0
Fine tuning took 0.041672 minutes
{'zero': {0: [0.32019704433497537, 0.24630541871921183, 0.2105911330049261, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37931034482758619, 0.41133004926108374, 0.48522167487684731, 0.52216748768472909], 5: [0.30049261083743845, 0.34236453201970446, 0.30418719211822659, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.32019704433497537, 0.24876847290640394, 0.20320197044334976, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37931034482758619, 0.42980295566502463, 0.51231527093596063, 0.57389162561576357], 5: [0.30049261083743845, 0.32142857142857145, 0.28448275862068967, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.32019704433497537, 0.25123152709359609, 0.2376847290640394, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37931034482758619, 0.39285714285714285, 0.5, 0.58620689655172409], 5: [0.30049261083743845, 0.35591133004926107, 0.26231527093596058, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.32019704433497537, 0.25246305418719212, 0.23029556650246305, 0.23645320197044334], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.37931034482758619, 0.41009852216748771, 0.51231527093596063, 0.56773399014778325], 5: [0.30049261083743845, 0.33743842364532017, 0.25738916256157635, 0.19581280788177341], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272657 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.67231
Epoch 1, cost is  4.48977
Epoch 2, cost is  4.33702
Epoch 3, cost is  4.20954
Epoch 4, cost is  4.08142
Training took 0.229218 minutes
Weight histogram
[840 726 980 927 626 376 619 732 162  87] [ -2.62033865e-02  -2.35794732e-02  -2.09555599e-02  -1.83316465e-02
  -1.57077332e-02  -1.30838199e-02  -1.04599066e-02  -7.83599327e-03
  -5.21207995e-03  -2.58816664e-03   3.57466815e-05]
[1390  653  616  485  453  440  446  495  535  562] [ -2.62033865e-02  -2.35794732e-02  -2.09555599e-02  -1.83316465e-02
  -1.57077332e-02  -1.30838199e-02  -1.04599066e-02  -7.83599327e-03
  -5.21207995e-03  -2.58816664e-03   3.57466815e-05]
-0.644436
0.735366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269708 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34676
Epoch 1, cost is  5.18558
Epoch 2, cost is  5.04484
Epoch 3, cost is  4.91851
Epoch 4, cost is  4.806
Training took 0.203790 minutes
Weight histogram
[ 500  585  493  462  715  915  909 2914  408  199] [ -2.90271491e-02  -2.61208595e-02  -2.32145699e-02  -2.03082804e-02
  -1.74019908e-02  -1.44957012e-02  -1.15894116e-02  -8.68312205e-03
  -5.77683247e-03  -2.87054289e-03   3.57466815e-05]
[3514  895  504  447  412  422  447  469  478  512] [ -2.90271491e-02  -2.61208595e-02  -2.32145699e-02  -2.03082804e-02
  -1.74019908e-02  -1.44957012e-02  -1.15894116e-02  -8.68312205e-03
  -5.77683247e-03  -2.87054289e-03   3.57466815e-05]
-0.610958
0.829228
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74554
Epoch 1, cost is  6.5898
Epoch 2, cost is  6.47595
Epoch 3, cost is  6.36357
Epoch 4, cost is  6.244
Training took 0.273175 minutes
Weight histogram
[ 623 1143 1746 1417  449  258  168  118   88   65] [ -1.24916639e-02  -1.12487631e-02  -1.00058622e-02  -8.76296133e-03
  -7.52006046e-03  -6.27715960e-03  -5.03425873e-03  -3.79135787e-03
  -2.54845701e-03  -1.30555614e-03  -6.26552792e-05]
[1769  841  679  596  546  522  448  314  196  164] [ -1.24916639e-02  -1.12487631e-02  -1.00058622e-02  -8.76296133e-03
  -7.52006046e-03  -6.27715960e-03  -5.03425873e-03  -3.79135787e-03
  -2.54845701e-03  -1.30555614e-03  -6.26552792e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043381 minutes
Epoch 0
Fine tuning took 0.043217 minutes
Epoch 0
Fine tuning took 0.043308 minutes
{'zero': {0: [0.33497536945812806, 0.2413793103448276, 0.17857142857142858, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.34729064039408869, 0.38300492610837439, 0.58990147783251234, 0.57635467980295563], 5: [0.31773399014778325, 0.37561576354679804, 0.23152709359605911, 0.18596059113300492], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33497536945812806, 0.22660098522167488, 0.17980295566502463, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.34729064039408869, 0.3854679802955665, 0.57266009852216748, 0.57512315270935965], 5: [0.31773399014778325, 0.38793103448275862, 0.24753694581280788, 0.21674876847290642], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33497536945812806, 0.21305418719211822, 0.2019704433497537, 0.24630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.34729064039408869, 0.38054187192118227, 0.53201970443349755, 0.5714285714285714], 5: [0.31773399014778325, 0.40640394088669951, 0.26600985221674878, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33497536945812806, 0.23645320197044334, 0.18719211822660098, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.34729064039408869, 0.35714285714285715, 0.55665024630541871, 0.56527093596059108], 5: [0.31773399014778325, 0.40640394088669951, 0.25615763546798032, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.276267 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.67231
Epoch 1, cost is  4.48977
Epoch 2, cost is  4.33702
Epoch 3, cost is  4.20954
Epoch 4, cost is  4.08142
Training took 0.225960 minutes
Weight histogram
[840 726 980 927 626 376 619 732 162  87] [ -2.62033865e-02  -2.35794732e-02  -2.09555599e-02  -1.83316465e-02
  -1.57077332e-02  -1.30838199e-02  -1.04599066e-02  -7.83599327e-03
  -5.21207995e-03  -2.58816664e-03   3.57466815e-05]
[1390  653  616  485  453  440  446  495  535  562] [ -2.62033865e-02  -2.35794732e-02  -2.09555599e-02  -1.83316465e-02
  -1.57077332e-02  -1.30838199e-02  -1.04599066e-02  -7.83599327e-03
  -5.21207995e-03  -2.58816664e-03   3.57466815e-05]
-0.644436
0.735366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.286432 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  5.34676
Epoch 1, cost is  5.18558
Epoch 2, cost is  5.04484
Epoch 3, cost is  4.91851
Epoch 4, cost is  4.806
Training took 0.203164 minutes
Weight histogram
[ 500  585  493  462  715  915  909 2914  408  199] [ -2.90271491e-02  -2.61208595e-02  -2.32145699e-02  -2.03082804e-02
  -1.74019908e-02  -1.44957012e-02  -1.15894116e-02  -8.68312205e-03
  -5.77683247e-03  -2.87054289e-03   3.57466815e-05]
[3514  895  504  447  412  422  447  469  478  512] [ -2.90271491e-02  -2.61208595e-02  -2.32145699e-02  -2.03082804e-02
  -1.74019908e-02  -1.44957012e-02  -1.15894116e-02  -8.68312205e-03
  -5.77683247e-03  -2.87054289e-03   3.57466815e-05]
-0.610958
0.829228
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.60635
Epoch 1, cost is  6.3816
Epoch 2, cost is  6.24284
Epoch 3, cost is  6.11901
Epoch 4, cost is  5.99488
Training took 0.377140 minutes
Weight histogram
[ 558  837 1337 1600  785  391  233  153  105   76] [ -1.41741699e-02  -1.27597859e-02  -1.13454020e-02  -9.93101806e-03
  -8.51663413e-03  -7.10225020e-03  -5.68786627e-03  -4.27348234e-03
  -2.85909840e-03  -1.44471447e-03  -3.03305387e-05]
[1513  767  641  579  572  550  582  401  259  211] [ -1.41741699e-02  -1.27597859e-02  -1.13454020e-02  -9.93101806e-03
  -8.51663413e-03  -7.10225020e-03  -5.68786627e-03  -4.27348234e-03
  -2.85909840e-03  -1.44471447e-03  -3.03305387e-05]
-0.0718176
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046401 minutes
Epoch 0
Fine tuning took 0.046665 minutes
Epoch 0
Fine tuning took 0.046583 minutes
{'zero': {0: [0.32266009852216748, 0.30788177339901479, 0.1625615763546798, 0.30049261083743845], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.38300492610837439, 0.42733990147783252, 0.52586206896551724, 0.51600985221674878], 5: [0.29433497536945813, 0.26477832512315269, 0.31157635467980294, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.32266009852216748, 0.34236453201970446, 0.16748768472906403, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.38300492610837439, 0.3682266009852217, 0.51231527093596063, 0.52832512315270941], 5: [0.29433497536945813, 0.2894088669950739, 0.32019704433497537, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.32266009852216748, 0.31773399014778325, 0.16625615763546797, 0.25862068965517243], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.38300492610837439, 0.37192118226600984, 0.5357142857142857, 0.56034482758620685], 5: [0.29433497536945813, 0.31034482758620691, 0.29802955665024633, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.32266009852216748, 0.30541871921182268, 0.15270935960591134, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.38300492610837439, 0.37192118226600984, 0.5357142857142857, 0.52586206896551724], 5: [0.29433497536945813, 0.32266009852216748, 0.31157635467980294, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.283420 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.13185
Epoch 1, cost is  2.01998
Epoch 2, cost is  2.01266
Epoch 3, cost is  2.03386
Epoch 4, cost is  2.0689
Training took 0.228777 minutes
Weight histogram
[1089  906  994 1039  711  591  384  202   99   60] [-0.15684576 -0.14123894 -0.12563212 -0.1100253  -0.09441847 -0.07881165
 -0.06320483 -0.04759801 -0.03199118 -0.01638436 -0.00077754]
[184 273 394 505 619 801 792 829 835 843] [-0.15684576 -0.14123894 -0.12563212 -0.1100253  -0.09441847 -0.07881165
 -0.06320483 -0.04759801 -0.03199118 -0.01638436 -0.00077754]
-6.9177
11.2516
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269629 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.64329
Epoch 1, cost is  4.58778
Epoch 2, cost is  4.70173
Epoch 3, cost is  4.85421
Epoch 4, cost is  5.03215
Training took 0.201620 minutes
Weight histogram
[ 757  791  681  872  919  666  668 1690  769  287] [-0.31132713 -0.28027217 -0.24921721 -0.21816225 -0.18710729 -0.15605233
 -0.12499738 -0.09394242 -0.06288746 -0.0318325  -0.00077754]
[ 719 1318 1145  602  697  705  709  759  729  717] [-0.31132713 -0.28027217 -0.24921721 -0.21816225 -0.18710729 -0.15605233
 -0.12499738 -0.09394242 -0.06288746 -0.0318325  -0.00077754]
-10.972
9.95922
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.11223
Epoch 1, cost is  3.80654
Epoch 2, cost is  3.64492
Epoch 3, cost is  3.74262
Epoch 4, cost is  3.90991
Training took 0.230397 minutes
Weight histogram
[ 991  894 1112  866  748  502  325  238  178  221] [-0.17713617 -0.15955115 -0.14196613 -0.12438112 -0.1067961  -0.08921108
 -0.07162606 -0.05404105 -0.03645603 -0.01887101 -0.00128599]
[382 354 428 514 585 657 729 783 831 812] [-0.17713617 -0.15955115 -0.14196613 -0.12438112 -0.1067961  -0.08921108
 -0.07162606 -0.05404105 -0.03645603 -0.01887101 -0.00128599]
-6.86173
6.97239
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042806 minutes
Epoch 0
Fine tuning took 0.042920 minutes
Epoch 0
Fine tuning took 0.042831 minutes
{'zero': {0: [0.20073891625615764, 0.28448275862068967, 0.22783251231527094, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.56773399014778325, 0.56403940886699511, 0.59482758620689657], 5: [0.16625615763546797, 0.14778325123152711, 0.20812807881773399, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20073891625615764, 0.16625615763546797, 0.18596059113300492, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.69827586206896552, 0.66256157635467983, 0.70073891625615758], 5: [0.16625615763546797, 0.1354679802955665, 0.15147783251231528, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20073891625615764, 0.19088669950738915, 0.22044334975369459, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.65394088669950734, 0.62561576354679804, 0.64408866995073888], 5: [0.16625615763546797, 0.15517241379310345, 0.1539408866995074, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20073891625615764, 0.17241379310344829, 0.17857142857142858, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.66871921182266014, 0.67733990147783252, 0.64901477832512311], 5: [0.16625615763546797, 0.15886699507389163, 0.14408866995073891, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267992 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.13185
Epoch 1, cost is  2.01998
Epoch 2, cost is  2.01266
Epoch 3, cost is  2.03386
Epoch 4, cost is  2.0689
Training took 0.225190 minutes
Weight histogram
[1089  906  994 1039  711  591  384  202   99   60] [-0.15684576 -0.14123894 -0.12563212 -0.1100253  -0.09441847 -0.07881165
 -0.06320483 -0.04759801 -0.03199118 -0.01638436 -0.00077754]
[184 273 394 505 619 801 792 829 835 843] [-0.15684576 -0.14123894 -0.12563212 -0.1100253  -0.09441847 -0.07881165
 -0.06320483 -0.04759801 -0.03199118 -0.01638436 -0.00077754]
-6.9177
11.2516
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.259335 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.64329
Epoch 1, cost is  4.58778
Epoch 2, cost is  4.70173
Epoch 3, cost is  4.85421
Epoch 4, cost is  5.03215
Training took 0.201229 minutes
Weight histogram
[ 757  791  681  872  919  666  668 1690  769  287] [-0.31132713 -0.28027217 -0.24921721 -0.21816225 -0.18710729 -0.15605233
 -0.12499738 -0.09394242 -0.06288746 -0.0318325  -0.00077754]
[ 719 1318 1145  602  697  705  709  759  729  717] [-0.31132713 -0.28027217 -0.24921721 -0.21816225 -0.18710729 -0.15605233
 -0.12499738 -0.09394242 -0.06288746 -0.0318325  -0.00077754]
-10.972
9.95922
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.74192
Epoch 1, cost is  2.84245
Epoch 2, cost is  2.46766
Epoch 3, cost is  2.37669
Epoch 4, cost is  2.41323
Training took 0.276306 minutes
Weight histogram
[ 987 1094  947  820  698  541  353  257  187  191] [-0.11230296 -0.10120447 -0.09010597 -0.07900748 -0.06790899 -0.0568105
 -0.04571201 -0.03461351 -0.02351502 -0.01241653 -0.00131804]
[411 338 408 461 549 681 731 814 869 813] [-0.11230296 -0.10120447 -0.09010597 -0.07900748 -0.06790899 -0.0568105
 -0.04571201 -0.03461351 -0.02351502 -0.01241653 -0.00131804]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044876 minutes
Epoch 0
Fine tuning took 0.044642 minutes
Epoch 0
Fine tuning took 0.047702 minutes
{'zero': {0: [0.20812807881773399, 0.32389162561576357, 0.27216748768472904, 0.30541871921182268], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.52709359605911332, 0.56280788177339902, 0.6071428571428571], 5: [0.13054187192118227, 0.14901477832512317, 0.16502463054187191, 0.087438423645320201], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20812807881773399, 0.17118226600985223, 0.20443349753694581, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.68472906403940892, 0.6280788177339901, 0.66871921182266014], 5: [0.13054187192118227, 0.14408866995073891, 0.16748768472906403, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20812807881773399, 0.20443349753694581, 0.21551724137931033, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.65640394088669951, 0.62931034482758619, 0.65024630541871919], 5: [0.13054187192118227, 0.13916256157635468, 0.15517241379310345, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20812807881773399, 0.19458128078817735, 0.23645320197044334, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.61576354679802958, 0.53325123152709364, 0.62068965517241381], 5: [0.13054187192118227, 0.18965517241379309, 0.23029556650246305, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265929 minutes
Weight histogram
[ 115  350  417  447  562 1003 1379 1329  446   27] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 112  114  180  210  284  409  775  740 1413 1838] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.4457
0.888366
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.13185
Epoch 1, cost is  2.01998
Epoch 2, cost is  2.01266
Epoch 3, cost is  2.03386
Epoch 4, cost is  2.0689
Training took 0.228643 minutes
Weight histogram
[1089  906  994 1039  711  591  384  202   99   60] [-0.15684576 -0.14123894 -0.12563212 -0.1100253  -0.09441847 -0.07881165
 -0.06320483 -0.04759801 -0.03199118 -0.01638436 -0.00077754]
[184 273 394 505 619 801 792 829 835 843] [-0.15684576 -0.14123894 -0.12563212 -0.1100253  -0.09441847 -0.07881165
 -0.06320483 -0.04759801 -0.03199118 -0.01638436 -0.00077754]
-6.9177
11.2516
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274350 minutes
Weight histogram
[ 127  441  710  862  840 1030 1807 1685  521   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 222  232  346  399  604  818 1241  831 1160 2247] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.948083
0.869496
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.64329
Epoch 1, cost is  4.58778
Epoch 2, cost is  4.70173
Epoch 3, cost is  4.85421
Epoch 4, cost is  5.03215
Training took 0.200759 minutes
Weight histogram
[ 757  791  681  872  919  666  668 1690  769  287] [-0.31132713 -0.28027217 -0.24921721 -0.21816225 -0.18710729 -0.15605233
 -0.12499738 -0.09394242 -0.06288746 -0.0318325  -0.00077754]
[ 719 1318 1145  602  697  705  709  759  729  717] [-0.31132713 -0.28027217 -0.24921721 -0.21816225 -0.18710729 -0.15605233
 -0.12499738 -0.09394242 -0.06288746 -0.0318325  -0.00077754]
-10.972
9.95922
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.48783
Epoch 1, cost is  2.29682
Epoch 2, cost is  1.78547
Epoch 3, cost is  1.61044
Epoch 4, cost is  1.52431
Training took 0.347330 minutes
Weight histogram
[1163 1111 1024  756  660  504  319  268  178   92] [-0.07301652 -0.06584196 -0.05866739 -0.05149282 -0.04431825 -0.03714368
 -0.02996911 -0.02279455 -0.01561998 -0.00844541 -0.00127084]
[415 309 358 443 549 656 757 849 926 813] [-0.07301652 -0.06584196 -0.05866739 -0.05149282 -0.04431825 -0.03714368
 -0.02996911 -0.02279455 -0.01561998 -0.00844541 -0.00127084]
-3.76965
3.9492
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047348 minutes
Epoch 0
Fine tuning took 0.047108 minutes
Epoch 0
Fine tuning took 0.046916 minutes
{'zero': {0: [0.16871921182266009, 0.30295566502463056, 0.22783251231527094, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.61699507389162567, 0.65147783251231528, 0.75123152709359609], 5: [0.097290640394088676, 0.080049261083743842, 0.1206896551724138, 0.054187192118226604], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16871921182266009, 0.12192118226600986, 0.11576354679802955, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.7857142857142857, 0.75, 0.75369458128078815], 5: [0.097290640394088676, 0.092364532019704432, 0.13423645320197045, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16871921182266009, 0.18103448275862069, 0.14778325123152711, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.70320197044334976, 0.69827586206896552, 0.68719211822660098], 5: [0.097290640394088676, 0.11576354679802955, 0.1539408866995074, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16871921182266009, 0.11083743842364532, 0.11330049261083744, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73399014778325122, 0.78448275862068961, 0.78940886699507384, 0.67733990147783252], 5: [0.097290640394088676, 0.10467980295566502, 0.097290640394088676, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264496 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.8682
Epoch 1, cost is  1.77395
Epoch 2, cost is  1.73424
Epoch 3, cost is  1.70786
Epoch 4, cost is  1.68551
Training took 0.233416 minutes
Weight histogram
[1889 1415 1003 1312  901  617  368  297  169  129] [-0.08346639 -0.07513111 -0.06679584 -0.05846056 -0.05012529 -0.04179001
 -0.03345473 -0.02511946 -0.01678418 -0.00844891 -0.00011363]
[ 425  357  481  612  812  899 1002 1071 1202 1239] [-0.08346639 -0.07513111 -0.06679584 -0.05846056 -0.05012529 -0.04179001
 -0.03345473 -0.02511946 -0.01678418 -0.00844891 -0.00011363]
-2.72672
3.43923
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269176 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.22957
Epoch 1, cost is  3.18507
Epoch 2, cost is  3.18504
Epoch 3, cost is  3.19566
Epoch 4, cost is  3.21623
Training took 0.203815 minutes
Weight histogram
[1388 1492 1233 1173  907  580 1001 1114  700  537] [ -1.34411305e-01  -1.20981538e-01  -1.07551771e-01  -9.41220033e-02
  -8.06922361e-02  -6.72624688e-02  -5.38327015e-02  -4.04029342e-02
  -2.69731669e-02  -1.35433996e-02  -1.13632348e-04]
[1183 1284  983  690  842  923 1061 1059 1031 1069] [ -1.34411305e-01  -1.20981538e-01  -1.07551771e-01  -9.41220033e-02
  -8.06922361e-02  -6.72624688e-02  -5.38327015e-02  -4.04029342e-02
  -2.69731669e-02  -1.35433996e-02  -1.13632348e-04]
-3.83804
4.5183
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55561
Epoch 1, cost is  5.39245
Epoch 2, cost is  4.56223
Epoch 3, cost is  4.15667
Epoch 4, cost is  3.91533
Training took 0.230653 minutes
Weight histogram
[1374 1010  915  921  644  573  546  538  390 1189] [-0.08295341 -0.07468203 -0.06641064 -0.05813926 -0.04986788 -0.0415965
 -0.03332511 -0.02505373 -0.01678235 -0.00851097 -0.00023958]
[1239  724  586  662  692  750  846  926 1019  656] [-0.08295341 -0.07468203 -0.06641064 -0.05813926 -0.04986788 -0.0415965
 -0.03332511 -0.02505373 -0.01678235 -0.00851097 -0.00023958]
-1.6243
2.5996
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041696 minutes
Epoch 0
Fine tuning took 0.041852 minutes
Epoch 0
Fine tuning took 0.042034 minutes
{'zero': {0: [0.25246305418719212, 0.37931034482758619, 0.27832512315270935, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58374384236453203, 0.50862068965517238, 0.61699507389162567, 0.60837438423645318], 5: [0.16379310344827586, 0.11206896551724138, 0.10467980295566502, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25246305418719212, 0.33128078817733991, 0.27463054187192121, 0.28325123152709358], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58374384236453203, 0.54802955665024633, 0.63423645320197042, 0.62068965517241381], 5: [0.16379310344827586, 0.1206896551724138, 0.091133004926108374, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25246305418719212, 0.3251231527093596, 0.27093596059113301, 0.26847290640394089], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58374384236453203, 0.56650246305418717, 0.6280788177339901, 0.61206896551724133], 5: [0.16379310344827586, 0.10837438423645321, 0.10098522167487685, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25246305418719212, 0.30665024630541871, 0.27586206896551724, 0.29802955665024633], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58374384236453203, 0.57266009852216748, 0.63054187192118227, 0.59852216748768472], 5: [0.16379310344827586, 0.1206896551724138, 0.093596059113300489, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270383 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.8682
Epoch 1, cost is  1.77395
Epoch 2, cost is  1.73424
Epoch 3, cost is  1.70786
Epoch 4, cost is  1.68551
Training took 0.224603 minutes
Weight histogram
[1889 1415 1003 1312  901  617  368  297  169  129] [-0.08346639 -0.07513111 -0.06679584 -0.05846056 -0.05012529 -0.04179001
 -0.03345473 -0.02511946 -0.01678418 -0.00844891 -0.00011363]
[ 425  357  481  612  812  899 1002 1071 1202 1239] [-0.08346639 -0.07513111 -0.06679584 -0.05846056 -0.05012529 -0.04179001
 -0.03345473 -0.02511946 -0.01678418 -0.00844891 -0.00011363]
-2.72672
3.43923
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267758 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.22957
Epoch 1, cost is  3.18507
Epoch 2, cost is  3.18504
Epoch 3, cost is  3.19566
Epoch 4, cost is  3.21623
Training took 0.203010 minutes
Weight histogram
[1388 1492 1233 1173  907  580 1001 1114  700  537] [ -1.34411305e-01  -1.20981538e-01  -1.07551771e-01  -9.41220033e-02
  -8.06922361e-02  -6.72624688e-02  -5.38327015e-02  -4.04029342e-02
  -2.69731669e-02  -1.35433996e-02  -1.13632348e-04]
[1183 1284  983  690  842  923 1061 1059 1031 1069] [ -1.34411305e-01  -1.20981538e-01  -1.07551771e-01  -9.41220033e-02
  -8.06922361e-02  -6.72624688e-02  -5.38327015e-02  -4.04029342e-02
  -2.69731669e-02  -1.35433996e-02  -1.13632348e-04]
-3.83804
4.5183
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.45986
Epoch 1, cost is  5.23385
Epoch 2, cost is  4.10478
Epoch 3, cost is  3.47409
Epoch 4, cost is  3.04871
Training took 0.275954 minutes
Weight histogram
[1203 1354  970  868  746  602  633  473 1147  104] [-0.05099323 -0.04592326 -0.04085329 -0.03578333 -0.03071336 -0.02564339
 -0.02057342 -0.01550346 -0.01043349 -0.00536352 -0.00029356]
[1310  760  623  671  670  748  824  887  944  663] [-0.05099323 -0.04592326 -0.04085329 -0.03578333 -0.03071336 -0.02564339
 -0.02057342 -0.01550346 -0.01043349 -0.00536352 -0.00029356]
-1.1998
1.51512
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043070 minutes
Epoch 0
Fine tuning took 0.042977 minutes
Epoch 0
Fine tuning took 0.043021 minutes
{'zero': {0: [0.24384236453201971, 0.26724137931034481, 0.25123152709359609, 0.27955665024630544], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.56527093596059108, 0.58990147783251234, 0.56650246305418717], 5: [0.17857142857142858, 0.16748768472906403, 0.15886699507389163, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24384236453201971, 0.25862068965517243, 0.23152709359605911, 0.27709359605911332], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.57389162561576357, 0.59359605911330049, 0.58374384236453203], 5: [0.17857142857142858, 0.16748768472906403, 0.1748768472906404, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24384236453201971, 0.25, 0.23891625615763548, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.58866995073891626, 0.59113300492610843, 0.58004926108374388], 5: [0.17857142857142858, 0.16133004926108374, 0.16995073891625614, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24384236453201971, 0.25615763546798032, 0.24384236453201971, 0.29064039408866993], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.56280788177339902, 0.58251231527093594, 0.59359605911330049], 5: [0.17857142857142858, 0.18103448275862069, 0.17364532019704434, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.275590 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.8682
Epoch 1, cost is  1.77395
Epoch 2, cost is  1.73424
Epoch 3, cost is  1.70786
Epoch 4, cost is  1.68551
Training took 0.227885 minutes
Weight histogram
[1889 1415 1003 1312  901  617  368  297  169  129] [-0.08346639 -0.07513111 -0.06679584 -0.05846056 -0.05012529 -0.04179001
 -0.03345473 -0.02511946 -0.01678418 -0.00844891 -0.00011363]
[ 425  357  481  612  812  899 1002 1071 1202 1239] [-0.08346639 -0.07513111 -0.06679584 -0.05846056 -0.05012529 -0.04179001
 -0.03345473 -0.02511946 -0.01678418 -0.00844891 -0.00011363]
-2.72672
3.43923
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.258185 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.22957
Epoch 1, cost is  3.18507
Epoch 2, cost is  3.18504
Epoch 3, cost is  3.19566
Epoch 4, cost is  3.21623
Training took 0.206014 minutes
Weight histogram
[1388 1492 1233 1173  907  580 1001 1114  700  537] [ -1.34411305e-01  -1.20981538e-01  -1.07551771e-01  -9.41220033e-02
  -8.06922361e-02  -6.72624688e-02  -5.38327015e-02  -4.04029342e-02
  -2.69731669e-02  -1.35433996e-02  -1.13632348e-04]
[1183 1284  983  690  842  923 1061 1059 1031 1069] [ -1.34411305e-01  -1.20981538e-01  -1.07551771e-01  -9.41220033e-02
  -8.06922361e-02  -6.72624688e-02  -5.38327015e-02  -4.04029342e-02
  -2.69731669e-02  -1.35433996e-02  -1.13632348e-04]
-3.83804
4.5183
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.34344
Epoch 1, cost is  5.13634
Epoch 2, cost is  3.87363
Epoch 3, cost is  3.08122
Epoch 4, cost is  2.56229
Training took 0.352153 minutes
Weight histogram
[1354 1443 1071  957  770  758  576  922  201   48] [-0.03414995 -0.0307603  -0.02737066 -0.02398101 -0.02059137 -0.01720172
 -0.01381208 -0.01042243 -0.00703279 -0.00364314 -0.0002535 ]
[1434  772  612  602  661  730  776  824  940  749] [-0.03414995 -0.0307603  -0.02737066 -0.02398101 -0.02059137 -0.01720172
 -0.01381208 -0.01042243 -0.00703279 -0.00364314 -0.0002535 ]
-0.8878
1.40444
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046378 minutes
Epoch 0
Fine tuning took 0.046215 minutes
Epoch 0
Fine tuning took 0.046161 minutes
{'zero': {0: [0.29433497536945813, 0.27955665024630544, 0.23152709359605911, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51847290640394084, 0.43842364532019706, 0.46305418719211822, 0.49876847290640391], 5: [0.18719211822660098, 0.28201970443349755, 0.30541871921182268, 0.24753694581280788], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29433497536945813, 0.23399014778325122, 0.27339901477832512, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51847290640394084, 0.49753694581280788, 0.44950738916256155, 0.55788177339901479], 5: [0.18719211822660098, 0.26847290640394089, 0.27709359605911332, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29433497536945813, 0.27216748768472904, 0.26724137931034481, 0.22660098522167488], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51847290640394084, 0.49137931034482757, 0.48768472906403942, 0.54556650246305416], 5: [0.18719211822660098, 0.23645320197044334, 0.24507389162561577, 0.22783251231527094], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29433497536945813, 0.24630541871921183, 0.27709359605911332, 0.23891625615763548], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51847290640394084, 0.46921182266009853, 0.46182266009852219, 0.55295566502463056], 5: [0.18719211822660098, 0.28448275862068967, 0.26108374384236455, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265933 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.15628
Epoch 1, cost is  2.06948
Epoch 2, cost is  2.01255
Epoch 3, cost is  1.97364
Epoch 4, cost is  1.93322
Training took 0.221954 minutes
Weight histogram
[1904 1392 1005 1205  851  555  454  348  314   72] [ -6.29209131e-02  -5.66318862e-02  -5.03428593e-02  -4.40538324e-02
  -3.77648055e-02  -3.14757785e-02  -2.51867516e-02  -1.88977247e-02
  -1.26086978e-02  -6.31967090e-03  -3.06439943e-05]
[ 644  387  488  610  759  847  965 1025 1168 1207] [ -6.29209131e-02  -5.66318862e-02  -5.03428593e-02  -4.40538324e-02
  -3.77648055e-02  -3.14757785e-02  -2.51867516e-02  -1.88977247e-02
  -1.26086978e-02  -6.31967090e-03  -3.06439943e-05]
-1.74248
1.81264
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274325 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.21222
Epoch 1, cost is  3.15941
Epoch 2, cost is  3.12566
Epoch 3, cost is  3.10172
Epoch 4, cost is  3.08229
Training took 0.204520 minutes
Weight histogram
[1500 1595 1120  886  728  591 1004 1082  836  783] [ -9.65888575e-02  -8.69330361e-02  -7.72772148e-02  -6.76213934e-02
  -5.79655721e-02  -4.83097507e-02  -3.86539294e-02  -2.89981080e-02
  -1.93422867e-02  -9.68646534e-03  -3.06439943e-05]
[1642 1243  803  614  743  858 1030 1060 1045 1087] [ -9.65888575e-02  -8.69330361e-02  -7.72772148e-02  -6.76213934e-02
  -5.79655721e-02  -4.83097507e-02  -3.86539294e-02  -2.89981080e-02
  -1.93422867e-02  -9.68646534e-03  -3.06439943e-05]
-2.34666
2.75263
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.7433
Epoch 1, cost is  6.36186
Epoch 2, cost is  5.71108
Epoch 3, cost is  5.12467
Epoch 4, cost is  4.71676
Training took 0.228333 minutes
Weight histogram
[ 515  921  774  739  636  693  648  532 2413  229] [-0.0558529  -0.05027908 -0.04470527 -0.03913146 -0.03355764 -0.02798383
 -0.02241001 -0.0168362  -0.01126239 -0.00568857 -0.00011476]
[1873  764  839  732  620  701  748  760  689  374] [-0.0558529  -0.05027908 -0.04470527 -0.03913146 -0.03355764 -0.02798383
 -0.02241001 -0.0168362  -0.01126239 -0.00568857 -0.00011476]
-0.921519
1.1068
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041696 minutes
Epoch 0
Fine tuning took 0.041937 minutes
Epoch 0
Fine tuning took 0.041600 minutes
{'zero': {0: [0.2536945812807882, 0.22167487684729065, 0.29064039408866993, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.66379310344827591, 0.60098522167487689, 0.6576354679802956], 5: [0.096059113300492605, 0.1145320197044335, 0.10837438423645321, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2536945812807882, 0.19334975369458129, 0.30788177339901479, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.70197044334975367, 0.57019704433497542, 0.66625615763546797], 5: [0.096059113300492605, 0.10467980295566502, 0.12192118226600986, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2536945812807882, 0.20443349753694581, 0.28817733990147781, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.68719211822660098, 0.59359605911330049, 0.69704433497536944], 5: [0.096059113300492605, 0.10837438423645321, 0.11822660098522167, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2536945812807882, 0.21798029556650247, 0.31034482758620691, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65024630541871919, 0.69088669950738912, 0.58004926108374388, 0.68472906403940892], 5: [0.096059113300492605, 0.091133004926108374, 0.10960591133004927, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.273095 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.15628
Epoch 1, cost is  2.06948
Epoch 2, cost is  2.01255
Epoch 3, cost is  1.97364
Epoch 4, cost is  1.93322
Training took 0.228295 minutes
Weight histogram
[1904 1392 1005 1205  851  555  454  348  314   72] [ -6.29209131e-02  -5.66318862e-02  -5.03428593e-02  -4.40538324e-02
  -3.77648055e-02  -3.14757785e-02  -2.51867516e-02  -1.88977247e-02
  -1.26086978e-02  -6.31967090e-03  -3.06439943e-05]
[ 644  387  488  610  759  847  965 1025 1168 1207] [ -6.29209131e-02  -5.66318862e-02  -5.03428593e-02  -4.40538324e-02
  -3.77648055e-02  -3.14757785e-02  -2.51867516e-02  -1.88977247e-02
  -1.26086978e-02  -6.31967090e-03  -3.06439943e-05]
-1.74248
1.81264
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265861 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.21222
Epoch 1, cost is  3.15941
Epoch 2, cost is  3.12566
Epoch 3, cost is  3.10172
Epoch 4, cost is  3.08229
Training took 0.200295 minutes
Weight histogram
[1500 1595 1120  886  728  591 1004 1082  836  783] [ -9.65888575e-02  -8.69330361e-02  -7.72772148e-02  -6.76213934e-02
  -5.79655721e-02  -4.83097507e-02  -3.86539294e-02  -2.89981080e-02
  -1.93422867e-02  -9.68646534e-03  -3.06439943e-05]
[1642 1243  803  614  743  858 1030 1060 1045 1087] [ -9.65888575e-02  -8.69330361e-02  -7.72772148e-02  -6.76213934e-02
  -5.79655721e-02  -4.83097507e-02  -3.86539294e-02  -2.89981080e-02
  -1.93422867e-02  -9.68646534e-03  -3.06439943e-05]
-2.34666
2.75263
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.65023
Epoch 1, cost is  6.25745
Epoch 2, cost is  5.59418
Epoch 3, cost is  4.90076
Epoch 4, cost is  4.31008
Training took 0.268054 minutes
Weight histogram
[ 354 1071 1033  837  826  789  633 1827  631   99] [-0.03642388 -0.03279803 -0.02917218 -0.02554633 -0.02192047 -0.01829462
 -0.01466877 -0.01104292 -0.00741706 -0.00379121 -0.00016536]
[1755  943  929  712  626  666  706  696  701  366] [-0.03642388 -0.03279803 -0.02917218 -0.02554633 -0.02192047 -0.01829462
 -0.01466877 -0.01104292 -0.00741706 -0.00379121 -0.00016536]
-0.823236
0.921125
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046508 minutes
Epoch 0
Fine tuning took 0.046460 minutes
Epoch 0
Fine tuning took 0.043090 minutes
{'zero': {0: [0.24630541871921183, 0.22167487684729065, 0.29679802955665024, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.64408866995073888, 0.58866995073891626, 0.6428571428571429], 5: [0.11083743842364532, 0.13423645320197045, 0.1145320197044335, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24630541871921183, 0.22413793103448276, 0.2857142857142857, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.63423645320197042, 0.59975369458128081, 0.68472906403940892], 5: [0.11083743842364532, 0.14162561576354679, 0.1145320197044335, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24630541871921183, 0.24261083743842365, 0.28448275862068967, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.6280788177339901, 0.6145320197044335, 0.66379310344827591], 5: [0.11083743842364532, 0.12931034482758622, 0.10098522167487685, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24630541871921183, 0.21428571428571427, 0.29310344827586204, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.65024630541871919, 0.56650246305418717, 0.63669950738916259], 5: [0.11083743842364532, 0.1354679802955665, 0.14039408866995073, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269163 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.15628
Epoch 1, cost is  2.06948
Epoch 2, cost is  2.01255
Epoch 3, cost is  1.97364
Epoch 4, cost is  1.93322
Training took 0.228029 minutes
Weight histogram
[1904 1392 1005 1205  851  555  454  348  314   72] [ -6.29209131e-02  -5.66318862e-02  -5.03428593e-02  -4.40538324e-02
  -3.77648055e-02  -3.14757785e-02  -2.51867516e-02  -1.88977247e-02
  -1.26086978e-02  -6.31967090e-03  -3.06439943e-05]
[ 644  387  488  610  759  847  965 1025 1168 1207] [ -6.29209131e-02  -5.66318862e-02  -5.03428593e-02  -4.40538324e-02
  -3.77648055e-02  -3.14757785e-02  -2.51867516e-02  -1.88977247e-02
  -1.26086978e-02  -6.31967090e-03  -3.06439943e-05]
-1.74248
1.81264
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267535 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.21222
Epoch 1, cost is  3.15941
Epoch 2, cost is  3.12566
Epoch 3, cost is  3.10172
Epoch 4, cost is  3.08229
Training took 0.202571 minutes
Weight histogram
[1500 1595 1120  886  728  591 1004 1082  836  783] [ -9.65888575e-02  -8.69330361e-02  -7.72772148e-02  -6.76213934e-02
  -5.79655721e-02  -4.83097507e-02  -3.86539294e-02  -2.89981080e-02
  -1.93422867e-02  -9.68646534e-03  -3.06439943e-05]
[1642 1243  803  614  743  858 1030 1060 1045 1087] [ -9.65888575e-02  -8.69330361e-02  -7.72772148e-02  -6.76213934e-02
  -5.79655721e-02  -4.83097507e-02  -3.86539294e-02  -2.89981080e-02
  -1.93422867e-02  -9.68646534e-03  -3.06439943e-05]
-2.34666
2.75263
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.51545
Epoch 1, cost is  6.13952
Epoch 2, cost is  5.53499
Epoch 3, cost is  4.7517
Epoch 4, cost is  4.08842
Training took 0.359032 minutes
Weight histogram
[ 434 1275 1272 1009  998  807 1391  712  141   61] [-0.02634971 -0.02372784 -0.02110597 -0.0184841  -0.01586222 -0.01324035
 -0.01061848 -0.00799661 -0.00537474 -0.00275287 -0.00013099]
[1700 1177 1009  672  630  619  618  644  658  373] [-0.02634971 -0.02372784 -0.02110597 -0.0184841  -0.01586222 -0.01324035
 -0.01061848 -0.00799661 -0.00537474 -0.00275287 -0.00013099]
-0.592333
0.823935
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046521 minutes
Epoch 0
Fine tuning took 0.046726 minutes
Epoch 0
Fine tuning took 0.046700 minutes
{'zero': {0: [0.24507389162561577, 0.21551724137931033, 0.26477832512315269, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68349753694581283, 0.63916256157635465, 0.59605911330049266, 0.71921182266009853], 5: [0.071428571428571425, 0.14532019704433496, 0.13916256157635468, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24507389162561577, 0.22167487684729065, 0.25123152709359609, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68349753694581283, 0.63300492610837433, 0.59729064039408863, 0.74507389162561577], 5: [0.071428571428571425, 0.14532019704433496, 0.15147783251231528, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24507389162561577, 0.15763546798029557, 0.25862068965517243, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68349753694581283, 0.68349753694581283, 0.62315270935960587, 0.70197044334975367], 5: [0.071428571428571425, 0.15886699507389163, 0.11822660098522167, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24507389162561577, 0.20073891625615764, 0.26724137931034481, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68349753694581283, 0.66625615763546797, 0.59852216748768472, 0.72660098522167482], 5: [0.071428571428571425, 0.13300492610837439, 0.13423645320197045, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270359 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.98085
Epoch 1, cost is  3.86526
Epoch 2, cost is  3.76682
Epoch 3, cost is  3.67523
Epoch 4, cost is  3.59266
Training took 0.223785 minutes
Weight histogram
[1427  926  954 1002 1198  693  426 1112  251  111] [ -3.14163193e-02  -2.82711127e-02  -2.51259061e-02  -2.19806995e-02
  -1.88354929e-02  -1.56902863e-02  -1.25450797e-02  -9.39987312e-03
  -6.25466652e-03  -3.10945992e-03   3.57466815e-05]
[1577  860  675  585  577  628  689  788  853  868] [ -3.14163193e-02  -2.82711127e-02  -2.51259061e-02  -2.19806995e-02
  -1.88354929e-02  -1.56902863e-02  -1.25450797e-02  -9.39987312e-03
  -6.25466652e-03  -3.10945992e-03   3.57466815e-05]
-0.730563
0.871818
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270275 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.78308
Epoch 1, cost is  4.68036
Epoch 2, cost is  4.58865
Epoch 3, cost is  4.50298
Epoch 4, cost is  4.42558
Training took 0.204588 minutes
Weight histogram
[ 885  808  803  716  654  775 1146 2563 1488  287] [ -3.76028642e-02  -3.38390031e-02  -3.00751420e-02  -2.63112809e-02
  -2.25474199e-02  -1.87835588e-02  -1.50196977e-02  -1.12558366e-02
  -7.49197550e-03  -3.72811441e-03   3.57466815e-05]
[4074  729  623  565  619  648  685  682  742  758] [ -3.76028642e-02  -3.38390031e-02  -3.00751420e-02  -2.63112809e-02
  -2.25474199e-02  -1.87835588e-02  -1.50196977e-02  -1.12558366e-02
  -7.49197550e-03  -3.72811441e-03   3.57466815e-05]
-0.70885
0.99214
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.84484
Epoch 1, cost is  6.74881
Epoch 2, cost is  6.66936
Epoch 3, cost is  6.59282
Epoch 4, cost is  6.50813
Training took 0.229361 minutes
Weight histogram
[1660 1774 3187  516  314  213  155  118   90   73] [ -1.02815339e-02  -9.25442986e-03  -8.22732578e-03  -7.20022171e-03
  -6.17311763e-03  -5.14601355e-03  -4.11890948e-03  -3.09180540e-03
  -2.06470133e-03  -1.03759725e-03  -1.04931751e-05]
[2897 1332 1033  858  724  487  288  232  126  123] [ -1.02815339e-02  -9.25442986e-03  -8.22732578e-03  -7.20022171e-03
  -6.17311763e-03  -5.14601355e-03  -4.11890948e-03  -3.09180540e-03
  -2.06470133e-03  -1.03759725e-03  -1.04931751e-05]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041693 minutes
Epoch 0
Fine tuning took 0.041896 minutes
Epoch 0
Fine tuning took 0.041663 minutes
{'zero': {0: [0.20320197044334976, 0.21305418719211822, 0.23275862068965517, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.63793103448275867, 0.5714285714285714, 0.57635467980295563], 5: [0.26970443349753692, 0.14901477832512317, 0.19581280788177341, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20320197044334976, 0.22783251231527094, 0.24630541871921183, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.64162561576354682, 0.55541871921182262, 0.51724137931034486], 5: [0.26970443349753692, 0.13054187192118227, 0.19827586206896552, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20320197044334976, 0.19211822660098521, 0.22413793103448276, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.6280788177339901, 0.5714285714285714, 0.51970443349753692], 5: [0.26970443349753692, 0.17980295566502463, 0.20443349753694581, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20320197044334976, 0.20320197044334976, 0.23152709359605911, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52709359605911332, 0.62684729064039413, 0.56034482758620685, 0.54187192118226601], 5: [0.26970443349753692, 0.16995073891625614, 0.20812807881773399, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.281912 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.98085
Epoch 1, cost is  3.86526
Epoch 2, cost is  3.76682
Epoch 3, cost is  3.67523
Epoch 4, cost is  3.59266
Training took 0.231924 minutes
Weight histogram
[1427  926  954 1002 1198  693  426 1112  251  111] [ -3.14163193e-02  -2.82711127e-02  -2.51259061e-02  -2.19806995e-02
  -1.88354929e-02  -1.56902863e-02  -1.25450797e-02  -9.39987312e-03
  -6.25466652e-03  -3.10945992e-03   3.57466815e-05]
[1577  860  675  585  577  628  689  788  853  868] [ -3.14163193e-02  -2.82711127e-02  -2.51259061e-02  -2.19806995e-02
  -1.88354929e-02  -1.56902863e-02  -1.25450797e-02  -9.39987312e-03
  -6.25466652e-03  -3.10945992e-03   3.57466815e-05]
-0.730563
0.871818
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267218 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.78308
Epoch 1, cost is  4.68036
Epoch 2, cost is  4.58865
Epoch 3, cost is  4.50298
Epoch 4, cost is  4.42558
Training took 0.204156 minutes
Weight histogram
[ 885  808  803  716  654  775 1146 2563 1488  287] [ -3.76028642e-02  -3.38390031e-02  -3.00751420e-02  -2.63112809e-02
  -2.25474199e-02  -1.87835588e-02  -1.50196977e-02  -1.12558366e-02
  -7.49197550e-03  -3.72811441e-03   3.57466815e-05]
[4074  729  623  565  619  648  685  682  742  758] [ -3.76028642e-02  -3.38390031e-02  -3.00751420e-02  -2.63112809e-02
  -2.25474199e-02  -1.87835588e-02  -1.50196977e-02  -1.12558366e-02
  -7.49197550e-03  -3.72811441e-03   3.57466815e-05]
-0.70885
0.99214
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.76602
Epoch 1, cost is  6.63204
Epoch 2, cost is  6.53598
Epoch 3, cost is  6.44265
Epoch 4, cost is  6.34753
Training took 0.271306 minutes
Weight histogram
[ 623 1143 2051 2600  683  379  240  168  122   91] [ -1.24916639e-02  -1.12486657e-02  -1.00056674e-02  -8.76266915e-03
  -7.51967090e-03  -6.27667264e-03  -5.03367439e-03  -3.79067613e-03
  -2.54767787e-03  -1.30467962e-03  -6.16813631e-05]
[2528 1209  968  858  783  632  448  314  196  164] [ -1.24916639e-02  -1.12486657e-02  -1.00056674e-02  -8.76266915e-03
  -7.51967090e-03  -6.27667264e-03  -5.03367439e-03  -3.79067613e-03
  -2.54767787e-03  -1.30467962e-03  -6.16813631e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042984 minutes
Epoch 0
Fine tuning took 0.047131 minutes
Epoch 0
Fine tuning took 0.043309 minutes
{'zero': {0: [0.19334975369458129, 0.18226600985221675, 0.19334975369458129, 0.31527093596059114], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5357142857142857, 0.68596059113300489, 0.6145320197044335, 0.49137931034482757], 5: [0.27093596059113301, 0.13177339901477833, 0.19211822660098521, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19334975369458129, 0.20566502463054187, 0.21674876847290642, 0.36945812807881773], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5357142857142857, 0.66009852216748766, 0.5788177339901478, 0.47167487684729065], 5: [0.27093596059113301, 0.13423645320197045, 0.20443349753694581, 0.15886699507389163], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19334975369458129, 0.16009852216748768, 0.18472906403940886, 0.30418719211822659], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5357142857142857, 0.69458128078817738, 0.6354679802955665, 0.52339901477832518], 5: [0.27093596059113301, 0.14532019704433496, 0.17980295566502463, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19334975369458129, 0.19211822660098521, 0.17980295566502463, 0.32142857142857145], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5357142857142857, 0.66009852216748766, 0.6219211822660099, 0.47660098522167488], 5: [0.27093596059113301, 0.14778325123152711, 0.19827586206896552, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.285772 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.98085
Epoch 1, cost is  3.86526
Epoch 2, cost is  3.76682
Epoch 3, cost is  3.67523
Epoch 4, cost is  3.59266
Training took 0.228443 minutes
Weight histogram
[1427  926  954 1002 1198  693  426 1112  251  111] [ -3.14163193e-02  -2.82711127e-02  -2.51259061e-02  -2.19806995e-02
  -1.88354929e-02  -1.56902863e-02  -1.25450797e-02  -9.39987312e-03
  -6.25466652e-03  -3.10945992e-03   3.57466815e-05]
[1577  860  675  585  577  628  689  788  853  868] [ -3.14163193e-02  -2.82711127e-02  -2.51259061e-02  -2.19806995e-02
  -1.88354929e-02  -1.56902863e-02  -1.25450797e-02  -9.39987312e-03
  -6.25466652e-03  -3.10945992e-03   3.57466815e-05]
-0.730563
0.871818
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.282911 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.78308
Epoch 1, cost is  4.68036
Epoch 2, cost is  4.58865
Epoch 3, cost is  4.50298
Epoch 4, cost is  4.42558
Training took 0.204975 minutes
Weight histogram
[ 885  808  803  716  654  775 1146 2563 1488  287] [ -3.76028642e-02  -3.38390031e-02  -3.00751420e-02  -2.63112809e-02
  -2.25474199e-02  -1.87835588e-02  -1.50196977e-02  -1.12558366e-02
  -7.49197550e-03  -3.72811441e-03   3.57466815e-05]
[4074  729  623  565  619  648  685  682  742  758] [ -3.76028642e-02  -3.38390031e-02  -3.00751420e-02  -2.63112809e-02
  -2.25474199e-02  -1.87835588e-02  -1.50196977e-02  -1.12558366e-02
  -7.49197550e-03  -3.72811441e-03   3.57466815e-05]
-0.70885
0.99214
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.6431
Epoch 1, cost is  6.45068
Epoch 2, cost is  6.33582
Epoch 3, cost is  6.23326
Epoch 4, cost is  6.13005
Training took 0.365917 minutes
Weight histogram
[ 558  837 1425 2608 1283  581  338  217  148  105] [ -1.41741699e-02  -1.27596913e-02  -1.13452126e-02  -9.93073404e-03
  -8.51625543e-03  -7.10177683e-03  -5.68729822e-03  -4.27281961e-03
  -2.85834100e-03  -1.44386240e-03  -2.93837893e-05]
[2172 1100  926  845  819  785  582  401  259  211] [ -1.41741699e-02  -1.27596913e-02  -1.13452126e-02  -9.93073404e-03
  -8.51625543e-03  -7.10177683e-03  -5.68729822e-03  -4.27281961e-03
  -2.85834100e-03  -1.44386240e-03  -2.93837893e-05]
-0.0721031
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046438 minutes
Epoch 0
Fine tuning took 0.046491 minutes
Epoch 0
Fine tuning took 0.046672 minutes
{'zero': {0: [0.21428571428571427, 0.17980295566502463, 0.22167487684729065, 0.33128078817733991], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.64778325123152714, 0.56280788177339902, 0.49014778325123154], 5: [0.26970443349753692, 0.17241379310344829, 0.21551724137931033, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21428571428571427, 0.20443349753694581, 0.22536945812807882, 0.34852216748768472], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.65517241379310343, 0.54556650246305416, 0.48768472906403942], 5: [0.26970443349753692, 0.14039408866995073, 0.22906403940886699, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21428571428571427, 0.20320197044334976, 0.24261083743842365, 0.3460591133004926], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.62068965517241381, 0.54433497536945807, 0.47413793103448276], 5: [0.26970443349753692, 0.17610837438423646, 0.21305418719211822, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21428571428571427, 0.17733990147783252, 0.25123152709359609, 0.35467980295566504], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.65394088669950734, 0.53201970443349755, 0.48275862068965519], 5: [0.26970443349753692, 0.16871921182266009, 0.21674876847290642, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270534 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.51025
Epoch 1, cost is  2.35876
Epoch 2, cost is  2.34268
Epoch 3, cost is  2.35302
Epoch 4, cost is  2.38165
Training took 0.227045 minutes
Weight histogram
[1284 1038 1274 1037 1380  851  642  367  150   77] [-0.19009151 -0.17116011 -0.15222871 -0.13329732 -0.11436592 -0.09543452
 -0.07650313 -0.05757173 -0.03864033 -0.01970894 -0.00077754]
[ 235  385  571  732  962  988 1026 1031 1098 1072] [-0.19009151 -0.17116011 -0.15222871 -0.13329732 -0.11436592 -0.09543452
 -0.07650313 -0.05757173 -0.03864033 -0.01970894 -0.00077754]
-9.16755
13.1418
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269612 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.08099
Epoch 1, cost is  5.9888
Epoch 2, cost is  6.10653
Epoch 3, cost is  6.27653
Epoch 4, cost is  6.47875
Training took 0.206261 minutes
Weight histogram
[1027 1011  980  952  924 1160  855 1422 1408  386] [-0.38793665 -0.34922074 -0.31050483 -0.27178892 -0.23307301 -0.1943571
 -0.15564118 -0.11692527 -0.07820936 -0.03949345 -0.00077754]
[1033 1916  757  890  913  958  944  929  903  882] [-0.38793665 -0.34922074 -0.31050483 -0.27178892 -0.23307301 -0.1943571
 -0.15564118 -0.11692527 -0.07820936 -0.03949345 -0.00077754]
-13.6869
13.9001
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.10753
Epoch 1, cost is  3.80818
Epoch 2, cost is  3.66826
Epoch 3, cost is  3.77737
Epoch 4, cost is  3.97372
Training took 0.232453 minutes
Weight histogram
[1204 1259 1456 1194 1010  685  440  318  238  296] [-0.17713617 -0.15954908 -0.14196199 -0.12437491 -0.10678782 -0.08920073
 -0.07161364 -0.05402656 -0.03643947 -0.01885238 -0.0012653 ]
[ 505  468  573  685  778  878  974 1043 1113 1083] [-0.17713617 -0.15954908 -0.14196199 -0.12437491 -0.10678782 -0.08920073
 -0.07161364 -0.05402656 -0.03643947 -0.01885238 -0.0012653 ]
-6.86173
6.97239
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042719 minutes
Epoch 0
Fine tuning took 0.042479 minutes
Epoch 0
Fine tuning took 0.042626 minutes
{'zero': {0: [0.43349753694581283, 0.19211822660098521, 0.2019704433497537, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45443349753694579, 0.65024630541871919, 0.6354679802955665, 0.69334975369458129], 5: [0.11206896551724138, 0.15763546798029557, 0.1625615763546798, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.43349753694581283, 0.29679802955665024, 0.23152709359605911, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45443349753694579, 0.58004926108374388, 0.57758620689655171, 0.64039408866995073], 5: [0.11206896551724138, 0.12315270935960591, 0.19088669950738915, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.43349753694581283, 0.25862068965517243, 0.2229064039408867, 0.21182266009852216], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45443349753694579, 0.63054187192118227, 0.6071428571428571, 0.65517241379310343], 5: [0.11206896551724138, 0.11083743842364532, 0.16995073891625614, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.43349753694581283, 0.31034482758620691, 0.24261083743842365, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45443349753694579, 0.52955665024630538, 0.53940886699507384, 0.64162561576354682], 5: [0.11206896551724138, 0.16009852216748768, 0.21798029556650247, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.285103 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.51025
Epoch 1, cost is  2.35876
Epoch 2, cost is  2.34268
Epoch 3, cost is  2.35302
Epoch 4, cost is  2.38165
Training took 0.228364 minutes
Weight histogram
[1284 1038 1274 1037 1380  851  642  367  150   77] [-0.19009151 -0.17116011 -0.15222871 -0.13329732 -0.11436592 -0.09543452
 -0.07650313 -0.05757173 -0.03864033 -0.01970894 -0.00077754]
[ 235  385  571  732  962  988 1026 1031 1098 1072] [-0.19009151 -0.17116011 -0.15222871 -0.13329732 -0.11436592 -0.09543452
 -0.07650313 -0.05757173 -0.03864033 -0.01970894 -0.00077754]
-9.16755
13.1418
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268526 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.08099
Epoch 1, cost is  5.9888
Epoch 2, cost is  6.10653
Epoch 3, cost is  6.27653
Epoch 4, cost is  6.47875
Training took 0.208210 minutes
Weight histogram
[1027 1011  980  952  924 1160  855 1422 1408  386] [-0.38793665 -0.34922074 -0.31050483 -0.27178892 -0.23307301 -0.1943571
 -0.15564118 -0.11692527 -0.07820936 -0.03949345 -0.00077754]
[1033 1916  757  890  913  958  944  929  903  882] [-0.38793665 -0.34922074 -0.31050483 -0.27178892 -0.23307301 -0.1943571
 -0.15564118 -0.11692527 -0.07820936 -0.03949345 -0.00077754]
-13.6869
13.9001
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.7316
Epoch 1, cost is  2.84768
Epoch 2, cost is  2.48433
Epoch 3, cost is  2.38132
Epoch 4, cost is  2.40444
Training took 0.279424 minutes
Weight histogram
[1227 1461 1267 1133  953  725  483  346  251  254] [-0.11230296 -0.10120251 -0.09010206 -0.07900161 -0.06790116 -0.05680071
 -0.04570026 -0.03459981 -0.02349936 -0.01239891 -0.00129846]
[ 549  455  546  624  734  923  973 1089 1153 1054] [-0.11230296 -0.10120251 -0.09010206 -0.07900161 -0.06790116 -0.05680071
 -0.04570026 -0.03459981 -0.02349936 -0.01239891 -0.00129846]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047648 minutes
Epoch 0
Fine tuning took 0.044161 minutes
Epoch 0
Fine tuning took 0.043875 minutes
{'zero': {0: [0.16009852216748768, 0.10098522167487685, 0.13669950738916256, 0.23399014778325122], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.75492610837438423, 0.61083743842364535, 0.65024630541871919], 5: [0.11330049261083744, 0.14408866995073891, 0.25246305418719212, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16009852216748768, 0.18719211822660098, 0.15640394088669951, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.70443349753694584, 0.68103448275862066, 0.64039408866995073], 5: [0.11330049261083744, 0.10837438423645321, 0.1625615763546798, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16009852216748768, 0.18472906403940886, 0.20689655172413793, 0.25123152709359609], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.71182266009852213, 0.65147783251231528, 0.63177339901477836], 5: [0.11330049261083744, 0.10344827586206896, 0.14162561576354679, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16009852216748768, 0.14408866995073891, 0.18965517241379309, 0.30172413793103448], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72660098522167482, 0.7068965517241379, 0.66379310344827591, 0.58866995073891626], 5: [0.11330049261083744, 0.14901477832512317, 0.14655172413793102, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267664 minutes
Weight histogram
[ 115  350  417  447  562 1197 1998 2072  865   77] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
[ 119  134  188  247  370  591  770  975 1945 2761] [ -7.24884303e-05   1.36926772e-04   3.46341974e-04   5.55757176e-04
   7.65172378e-04   9.74587580e-04   1.18400278e-03   1.39341798e-03
   1.60283319e-03   1.81224839e-03   2.02166359e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.51025
Epoch 1, cost is  2.35876
Epoch 2, cost is  2.34268
Epoch 3, cost is  2.35302
Epoch 4, cost is  2.38165
Training took 0.220705 minutes
Weight histogram
[1284 1038 1274 1037 1380  851  642  367  150   77] [-0.19009151 -0.17116011 -0.15222871 -0.13329732 -0.11436592 -0.09543452
 -0.07650313 -0.05757173 -0.03864033 -0.01970894 -0.00077754]
[ 235  385  571  732  962  988 1026 1031 1098 1072] [-0.19009151 -0.17116011 -0.15222871 -0.13329732 -0.11436592 -0.09543452
 -0.07650313 -0.05757173 -0.03864033 -0.01970894 -0.00077754]
-9.16755
13.1418
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269104 minutes
Weight histogram
[ 127  442  794 1099 1711 1646 1979 1722  528   77] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
[ 234  266  374  452  699 1062 1136  994 1945 2963] [ -7.24884303e-05   9.20005623e-05   2.56489555e-04   4.20978547e-04
   5.85467540e-04   7.49956533e-04   9.14445525e-04   1.07893452e-03
   1.24342351e-03   1.40791250e-03   1.57240150e-03]
-0.966533
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  6.08099
Epoch 1, cost is  5.9888
Epoch 2, cost is  6.10653
Epoch 3, cost is  6.27653
Epoch 4, cost is  6.47875
Training took 0.203790 minutes
Weight histogram
[1027 1011  980  952  924 1160  855 1422 1408  386] [-0.38793665 -0.34922074 -0.31050483 -0.27178892 -0.23307301 -0.1943571
 -0.15564118 -0.11692527 -0.07820936 -0.03949345 -0.00077754]
[1033 1916  757  890  913  958  944  929  903  882] [-0.38793665 -0.34922074 -0.31050483 -0.27178892 -0.23307301 -0.1943571
 -0.15564118 -0.11692527 -0.07820936 -0.03949345 -0.00077754]
-13.6869
13.9001
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.4911
Epoch 1, cost is  2.31779
Epoch 2, cost is  1.80078
Epoch 3, cost is  1.61332
Epoch 4, cost is  1.52315
Training took 0.355426 minutes
Weight histogram
[1476 1512 1354 1036  863  707  431  357  240  124] [-0.07301652 -0.06584164 -0.05866675 -0.05149186 -0.04431698 -0.03714209
 -0.0299672  -0.02279231 -0.01561743 -0.00844254 -0.00126765]
[ 554  415  480  598  737  880 1004 1125 1224 1083] [-0.07301652 -0.06584164 -0.05866675 -0.05149186 -0.04431698 -0.03714209
 -0.0299672  -0.02279231 -0.01561743 -0.00844254 -0.00126765]
-3.86554
4.26369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047042 minutes
Epoch 0
Fine tuning took 0.047029 minutes
Epoch 0
Fine tuning took 0.047065 minutes
{'zero': {0: [0.16748768472906403, 0.043103448275862072, 0.12192118226600986, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.83497536945812811, 0.70197044334975367, 0.83866995073891626], 5: [0.10221674876847291, 0.12192118226600986, 0.17610837438423646, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16748768472906403, 0.088669950738916259, 0.10098522167487685, 0.084975369458128072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.80788177339901479, 0.81403940886699511, 0.86206896551724133], 5: [0.10221674876847291, 0.10344827586206896, 0.084975369458128072, 0.05295566502463054], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16748768472906403, 0.12931034482758622, 0.1145320197044335, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.78448275862068961, 0.77709359605911332, 0.81280788177339902], 5: [0.10221674876847291, 0.086206896551724144, 0.10837438423645321, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16748768472906403, 0.093596059113300489, 0.093596059113300489, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.72290640394088668, 0.84975369458128081, 0.84975369458128081], 5: [0.10221674876847291, 0.18349753694581281, 0.056650246305418719, 0.059113300492610835], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.261551 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83053
Epoch 1, cost is  1.74522
Epoch 2, cost is  1.71345
Epoch 3, cost is  1.69748
Epoch 4, cost is  1.68015
Training took 0.230664 minutes
Weight histogram
[1988 1908 1578 1225 1282  887  566  336  187  168] [-0.0940351  -0.08464295 -0.0752508  -0.06585866 -0.05646651 -0.04707436
 -0.03768222 -0.02829007 -0.01889793 -0.00950578 -0.00011363]
[ 469  436  600  823  989 1137 1218 1379 1481 1593] [-0.0940351  -0.08464295 -0.0752508  -0.06585866 -0.05646651 -0.04707436
 -0.03768222 -0.02829007 -0.01889793 -0.00950578 -0.00011363]
-3.08141
4.23851
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270034 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.36998
Epoch 1, cost is  3.27385
Epoch 2, cost is  3.24277
Epoch 3, cost is  3.2326
Epoch 4, cost is  3.24748
Training took 0.202218 minutes
Weight histogram
[1926 1199 1758 1408 1265  909  580 1608  880  617] [ -1.53868824e-01  -1.38493305e-01  -1.23117786e-01  -1.07742267e-01
  -9.23667475e-02  -7.69912283e-02  -6.16157091e-02  -4.62401899e-02
  -3.08646707e-02  -1.54891515e-02  -1.13632348e-04]
[1336 1592  789  889 1025 1171 1215 1180 1360 1593] [ -1.53868824e-01  -1.38493305e-01  -1.23117786e-01  -1.07742267e-01
  -9.23667475e-02  -7.69912283e-02  -6.16157091e-02  -4.62401899e-02
  -3.08646707e-02  -1.54891515e-02  -1.13632348e-04]
-4.34311
5.00329
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55007
Epoch 1, cost is  5.36566
Epoch 2, cost is  4.55512
Epoch 3, cost is  4.15764
Epoch 4, cost is  3.92061
Training took 0.231846 minutes
Weight histogram
[1710 1259 1156 1138  814  707  665  691  500 1485] [-0.08295341 -0.07468185 -0.06641029 -0.05813873 -0.04986717 -0.04159561
 -0.03332405 -0.02505249 -0.01678093 -0.00850937 -0.00023781]
[1541  890  716  810  856  933 1046 1155 1267  911] [-0.08295341 -0.07468185 -0.06641029 -0.05813873 -0.04986717 -0.04159561
 -0.03332405 -0.02505249 -0.01678093 -0.00850937 -0.00023781]
-1.6243
2.70963
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041667 minutes
Epoch 0
Fine tuning took 0.041855 minutes
Epoch 0
Fine tuning took 0.041466 minutes
{'zero': {0: [0.25985221674876846, 0.30911330049261082, 0.28448275862068967, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.54556650246305416, 0.56157635467980294, 0.57512315270935965], 5: [0.17241379310344829, 0.14532019704433496, 0.1539408866995074, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25985221674876846, 0.29679802955665024, 0.26970443349753692, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.5431034482758621, 0.56403940886699511, 0.63177339901477836], 5: [0.17241379310344829, 0.16009852216748768, 0.16625615763546797, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25985221674876846, 0.26354679802955666, 0.28448275862068967, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.57635467980295563, 0.55788177339901479, 0.61576354679802958], 5: [0.17241379310344829, 0.16009852216748768, 0.15763546798029557, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25985221674876846, 0.32142857142857145, 0.26354679802955666, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56773399014778325, 0.52709359605911332, 0.55049261083743839, 0.61083743842364535], 5: [0.17241379310344829, 0.15147783251231528, 0.18596059113300492, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274061 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83053
Epoch 1, cost is  1.74522
Epoch 2, cost is  1.71345
Epoch 3, cost is  1.69748
Epoch 4, cost is  1.68015
Training took 0.229873 minutes
Weight histogram
[1988 1908 1578 1225 1282  887  566  336  187  168] [-0.0940351  -0.08464295 -0.0752508  -0.06585866 -0.05646651 -0.04707436
 -0.03768222 -0.02829007 -0.01889793 -0.00950578 -0.00011363]
[ 469  436  600  823  989 1137 1218 1379 1481 1593] [-0.0940351  -0.08464295 -0.0752508  -0.06585866 -0.05646651 -0.04707436
 -0.03768222 -0.02829007 -0.01889793 -0.00950578 -0.00011363]
-3.08141
4.23851
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.276034 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.36998
Epoch 1, cost is  3.27385
Epoch 2, cost is  3.24277
Epoch 3, cost is  3.2326
Epoch 4, cost is  3.24748
Training took 0.210706 minutes
Weight histogram
[1926 1199 1758 1408 1265  909  580 1608  880  617] [ -1.53868824e-01  -1.38493305e-01  -1.23117786e-01  -1.07742267e-01
  -9.23667475e-02  -7.69912283e-02  -6.16157091e-02  -4.62401899e-02
  -3.08646707e-02  -1.54891515e-02  -1.13632348e-04]
[1336 1592  789  889 1025 1171 1215 1180 1360 1593] [ -1.53868824e-01  -1.38493305e-01  -1.23117786e-01  -1.07742267e-01
  -9.23667475e-02  -7.69912283e-02  -6.16157091e-02  -4.62401899e-02
  -3.08646707e-02  -1.54891515e-02  -1.13632348e-04]
-4.34311
5.00329
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.45521
Epoch 1, cost is  5.20723
Epoch 2, cost is  4.09702
Epoch 3, cost is  3.47662
Epoch 4, cost is  3.05617
Training took 0.291250 minutes
Weight histogram
[1444 1707 1238 1093  922  745  790  614 1440  132] [-0.05099323 -0.04592303 -0.04085284 -0.03578265 -0.03071245 -0.02564226
 -0.02057207 -0.01550187 -0.01043168 -0.00536149 -0.0002913 ]
[1634  932  767  833  830  928 1023 1101 1172  905] [-0.05099323 -0.04592303 -0.04085284 -0.03578265 -0.03071245 -0.02564226
 -0.02057207 -0.01550187 -0.01043168 -0.00536149 -0.0002913 ]
-1.1998
1.51512
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046278 minutes
Epoch 0
Fine tuning took 0.043082 minutes
Epoch 0
Fine tuning took 0.043143 minutes
{'zero': {0: [0.1539408866995074, 0.18103448275862069, 0.17857142857142858, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.66748768472906406, 0.63916256157635465, 0.68472906403940892], 5: [0.16871921182266009, 0.15147783251231528, 0.18226600985221675, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1539408866995074, 0.19334975369458129, 0.14655172413793102, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.64039408866995073, 0.6711822660098522, 0.62561576354679804], 5: [0.16871921182266009, 0.16625615763546797, 0.18226600985221675, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1539408866995074, 0.20935960591133004, 0.1625615763546798, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.66379310344827591, 0.65640394088669951, 0.66009852216748766], 5: [0.16871921182266009, 0.1268472906403941, 0.18103448275862069, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1539408866995074, 0.18965517241379309, 0.16995073891625614, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.64655172413793105, 0.65886699507389157, 0.68349753694581283], 5: [0.16871921182266009, 0.16379310344827586, 0.17118226600985223, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274526 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.83053
Epoch 1, cost is  1.74522
Epoch 2, cost is  1.71345
Epoch 3, cost is  1.69748
Epoch 4, cost is  1.68015
Training took 0.222574 minutes
Weight histogram
[1988 1908 1578 1225 1282  887  566  336  187  168] [-0.0940351  -0.08464295 -0.0752508  -0.06585866 -0.05646651 -0.04707436
 -0.03768222 -0.02829007 -0.01889793 -0.00950578 -0.00011363]
[ 469  436  600  823  989 1137 1218 1379 1481 1593] [-0.0940351  -0.08464295 -0.0752508  -0.06585866 -0.05646651 -0.04707436
 -0.03768222 -0.02829007 -0.01889793 -0.00950578 -0.00011363]
-3.08141
4.23851
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265626 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.36998
Epoch 1, cost is  3.27385
Epoch 2, cost is  3.24277
Epoch 3, cost is  3.2326
Epoch 4, cost is  3.24748
Training took 0.201612 minutes
Weight histogram
[1926 1199 1758 1408 1265  909  580 1608  880  617] [ -1.53868824e-01  -1.38493305e-01  -1.23117786e-01  -1.07742267e-01
  -9.23667475e-02  -7.69912283e-02  -6.16157091e-02  -4.62401899e-02
  -3.08646707e-02  -1.54891515e-02  -1.13632348e-04]
[1336 1592  789  889 1025 1171 1215 1180 1360 1593] [ -1.53868824e-01  -1.38493305e-01  -1.23117786e-01  -1.07742267e-01
  -9.23667475e-02  -7.69912283e-02  -6.16157091e-02  -4.62401899e-02
  -3.08646707e-02  -1.54891515e-02  -1.13632348e-04]
-4.34311
5.00329
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.34081
Epoch 1, cost is  5.10994
Epoch 2, cost is  3.87576
Epoch 3, cost is  3.08665
Epoch 4, cost is  2.56275
Training took 0.363230 minutes
Weight histogram
[1620 1805 1366 1208  956  945  745 1158  261   61] [-0.03414995 -0.03076006 -0.02737018 -0.02398029 -0.02059041 -0.01720052
 -0.01381064 -0.01042075 -0.00703087 -0.00364098 -0.0002511 ]
[1785  950  760  751  824  911  968 1016 1166  994] [-0.03414995 -0.03076006 -0.02737018 -0.02398029 -0.02059041 -0.01720052
 -0.01381064 -0.01042075 -0.00703087 -0.00364098 -0.0002511 ]
-0.890843
1.40444
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046576 minutes
Epoch 0
Fine tuning took 0.046037 minutes
Epoch 0
Fine tuning took 0.046610 minutes
{'zero': {0: [0.12438423645320197, 0.1354679802955665, 0.17980295566502463, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.67487684729064035, 0.62438423645320196, 0.64778325123152714], 5: [0.19827586206896552, 0.18965517241379309, 0.19581280788177341, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12438423645320197, 0.17241379310344829, 0.20935960591133004, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.68103448275862066, 0.64655172413793105, 0.68349753694581283], 5: [0.19827586206896552, 0.14655172413793102, 0.14408866995073891, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12438423645320197, 0.15886699507389163, 0.18965517241379309, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.67241379310344829, 0.6071428571428571, 0.65147783251231528], 5: [0.19827586206896552, 0.16871921182266009, 0.20320197044334976, 0.19334975369458129], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12438423645320197, 0.18472906403940886, 0.21182266009852216, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.66748768472906406, 0.59729064039408863, 0.70073891625615758], 5: [0.19827586206896552, 0.14778325123152711, 0.19088669950738915, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269724 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.97419
Epoch 1, cost is  1.90293
Epoch 2, cost is  1.86045
Epoch 3, cost is  1.83262
Epoch 4, cost is  1.80329
Training took 0.229177 minutes
Weight histogram
[2018 1998 1514 1201 1249  736  510  451  353   95] [ -6.99489638e-02  -6.29571318e-02  -5.59652999e-02  -4.89734679e-02
  -4.19816359e-02  -3.49898039e-02  -2.79979719e-02  -2.10061399e-02
  -1.40143080e-02  -7.02247598e-03  -3.06439943e-05]
[ 698  467  605  797  947 1068 1177 1351 1445 1570] [ -6.99489638e-02  -6.29571318e-02  -5.59652999e-02  -4.89734679e-02
  -4.19816359e-02  -3.49898039e-02  -2.79979719e-02  -2.10061399e-02
  -1.40143080e-02  -7.02247598e-03  -3.06439943e-05]
-2.00772
2.13746
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266439 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.1127
Epoch 1, cost is  3.04847
Epoch 2, cost is  3.01861
Epoch 3, cost is  2.99809
Epoch 4, cost is  2.98917
Training took 0.205855 minutes
Weight histogram
[1947 1436 1755 1215 1027  712  754 1348 1082  874] [ -1.09827854e-01  -9.88481328e-02  -8.78684118e-02  -7.68886908e-02
  -6.59089698e-02  -5.49292489e-02  -4.39495279e-02  -3.29698069e-02
  -2.19900859e-02  -1.10103650e-02  -3.06439943e-05]
[1791 1513  626  769  937 1124 1209 1196 1369 1616] [ -1.09827854e-01  -9.88481328e-02  -8.78684118e-02  -7.68886908e-02
  -6.59089698e-02  -5.49292489e-02  -4.39495279e-02  -3.29698069e-02
  -2.19900859e-02  -1.10103650e-02  -3.06439943e-05]
-2.74271
3.01996
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74372
Epoch 1, cost is  6.35822
Epoch 2, cost is  5.66049
Epoch 3, cost is  5.06869
Epoch 4, cost is  4.65974
Training took 0.230227 minutes
Weight histogram
[ 587 1146  942  942  776  853  835  704 3048  292] [-0.0558529  -0.05027907 -0.04470525 -0.03913143 -0.03355761 -0.02798378
 -0.02240996 -0.01683614 -0.01126231 -0.00568849 -0.00011467]
[2371  991 1054  862  786  890  949  962  835  425] [-0.0558529  -0.05027907 -0.04470525 -0.03913143 -0.03355761 -0.02798378
 -0.02240996 -0.01683614 -0.01126231 -0.00568849 -0.00011467]
-0.921519
1.11944
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041492 minutes
Epoch 0
Fine tuning took 0.041567 minutes
Epoch 0
Fine tuning took 0.042285 minutes
{'zero': {0: [0.25862068965517243, 0.26724137931034481, 0.18842364532019704, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.56527093596059108, 0.6145320197044335, 0.72167487684729059], 5: [0.13177339901477833, 0.16748768472906403, 0.19704433497536947, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25862068965517243, 0.23399014778325122, 0.2105911330049261, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.58990147783251234, 0.55418719211822665, 0.69211822660098521], 5: [0.13177339901477833, 0.17610837438423646, 0.23522167487684728, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25862068965517243, 0.26724137931034481, 0.20812807881773399, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.55541871921182262, 0.55172413793103448, 0.7573891625615764], 5: [0.13177339901477833, 0.17733990147783252, 0.24014778325123154, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25862068965517243, 0.2857142857142857, 0.21921182266009853, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.60960591133004927, 0.54187192118226601, 0.56527093596059108, 0.75123152709359609], 5: [0.13177339901477833, 0.17241379310344829, 0.21551724137931033, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.283250 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.97419
Epoch 1, cost is  1.90293
Epoch 2, cost is  1.86045
Epoch 3, cost is  1.83262
Epoch 4, cost is  1.80329
Training took 0.228292 minutes
Weight histogram
[2018 1998 1514 1201 1249  736  510  451  353   95] [ -6.99489638e-02  -6.29571318e-02  -5.59652999e-02  -4.89734679e-02
  -4.19816359e-02  -3.49898039e-02  -2.79979719e-02  -2.10061399e-02
  -1.40143080e-02  -7.02247598e-03  -3.06439943e-05]
[ 698  467  605  797  947 1068 1177 1351 1445 1570] [ -6.99489638e-02  -6.29571318e-02  -5.59652999e-02  -4.89734679e-02
  -4.19816359e-02  -3.49898039e-02  -2.79979719e-02  -2.10061399e-02
  -1.40143080e-02  -7.02247598e-03  -3.06439943e-05]
-2.00772
2.13746
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.260823 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.1127
Epoch 1, cost is  3.04847
Epoch 2, cost is  3.01861
Epoch 3, cost is  2.99809
Epoch 4, cost is  2.98917
Training took 0.199452 minutes
Weight histogram
[1947 1436 1755 1215 1027  712  754 1348 1082  874] [ -1.09827854e-01  -9.88481328e-02  -8.78684118e-02  -7.68886908e-02
  -6.59089698e-02  -5.49292489e-02  -4.39495279e-02  -3.29698069e-02
  -2.19900859e-02  -1.10103650e-02  -3.06439943e-05]
[1791 1513  626  769  937 1124 1209 1196 1369 1616] [ -1.09827854e-01  -9.88481328e-02  -8.78684118e-02  -7.68886908e-02
  -6.59089698e-02  -5.49292489e-02  -4.39495279e-02  -3.29698069e-02
  -2.19900859e-02  -1.10103650e-02  -3.06439943e-05]
-2.74271
3.01996
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.65062
Epoch 1, cost is  6.25743
Epoch 2, cost is  5.54576
Epoch 3, cost is  4.85487
Epoch 4, cost is  4.27115
Training took 0.274029 minutes
Weight histogram
[ 354 1346 1270 1053 1024 1003  827 2162  960  126] [-0.03642388 -0.03279803 -0.02917218 -0.02554633 -0.02192047 -0.01829462
 -0.01466877 -0.01104292 -0.00741706 -0.00379121 -0.00016536]
[2206 1211 1136  847  792  841  885  872  858  477] [-0.03642388 -0.03279803 -0.02917218 -0.02554633 -0.02192047 -0.01829462
 -0.01466877 -0.01104292 -0.00741706 -0.00379121 -0.00016536]
-0.823236
0.921125
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043073 minutes
Epoch 0
Fine tuning took 0.043442 minutes
Epoch 0
Fine tuning took 0.042851 minutes
{'zero': {0: [0.20935960591133004, 0.2536945812807882, 0.27586206896551724, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.6071428571428571, 0.59359605911330049, 0.66995073891625612], 5: [0.12931034482758622, 0.13916256157635468, 0.13054187192118227, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20935960591133004, 0.25123152709359609, 0.27339901477832512, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.60837438423645318, 0.58251231527093594, 0.68719211822660098], 5: [0.12931034482758622, 0.14039408866995073, 0.14408866995073891, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20935960591133004, 0.23399014778325122, 0.26724137931034481, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.59359605911330049, 0.60467980295566504, 0.68103448275862066], 5: [0.12931034482758622, 0.17241379310344829, 0.12807881773399016, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20935960591133004, 0.23399014778325122, 0.29556650246305421, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.61206896551724133, 0.60344827586206895, 0.70073891625615758], 5: [0.12931034482758622, 0.1539408866995074, 0.10098522167487685, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.275170 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.97419
Epoch 1, cost is  1.90293
Epoch 2, cost is  1.86045
Epoch 3, cost is  1.83262
Epoch 4, cost is  1.80329
Training took 0.235666 minutes
Weight histogram
[2018 1998 1514 1201 1249  736  510  451  353   95] [ -6.99489638e-02  -6.29571318e-02  -5.59652999e-02  -4.89734679e-02
  -4.19816359e-02  -3.49898039e-02  -2.79979719e-02  -2.10061399e-02
  -1.40143080e-02  -7.02247598e-03  -3.06439943e-05]
[ 698  467  605  797  947 1068 1177 1351 1445 1570] [ -6.99489638e-02  -6.29571318e-02  -5.59652999e-02  -4.89734679e-02
  -4.19816359e-02  -3.49898039e-02  -2.79979719e-02  -2.10061399e-02
  -1.40143080e-02  -7.02247598e-03  -3.06439943e-05]
-2.00772
2.13746
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269619 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.1127
Epoch 1, cost is  3.04847
Epoch 2, cost is  3.01861
Epoch 3, cost is  2.99809
Epoch 4, cost is  2.98917
Training took 0.200789 minutes
Weight histogram
[1947 1436 1755 1215 1027  712  754 1348 1082  874] [ -1.09827854e-01  -9.88481328e-02  -8.78684118e-02  -7.68886908e-02
  -6.59089698e-02  -5.49292489e-02  -4.39495279e-02  -3.29698069e-02
  -2.19900859e-02  -1.10103650e-02  -3.06439943e-05]
[1791 1513  626  769  937 1124 1209 1196 1369 1616] [ -1.09827854e-01  -9.88481328e-02  -8.78684118e-02  -7.68886908e-02
  -6.59089698e-02  -5.49292489e-02  -4.39495279e-02  -3.29698069e-02
  -2.19900859e-02  -1.10103650e-02  -3.06439943e-05]
-2.74271
3.01996
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.51661
Epoch 1, cost is  6.14236
Epoch 2, cost is  5.49418
Epoch 3, cost is  4.71704
Epoch 4, cost is  4.07998
Training took 0.357984 minutes
Weight histogram
[ 434 1566 1574 1276 1282 1055 1716  964  181   77] [-0.02634971 -0.02372784 -0.02110597 -0.0184841  -0.01586222 -0.01324035
 -0.01061848 -0.00799661 -0.00537474 -0.00275287 -0.00013099]
[2132 1499 1218  815  784  776  775  798  816  512] [-0.02634971 -0.02372784 -0.02110597 -0.0184841  -0.01586222 -0.01324035
 -0.01061848 -0.00799661 -0.00537474 -0.00275287 -0.00013099]
-0.592333
0.829825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.050212 minutes
Epoch 0
Fine tuning took 0.049773 minutes
Epoch 0
Fine tuning took 0.046475 minutes
{'zero': {0: [0.16995073891625614, 0.2536945812807882, 0.26231527093596058, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.59852216748768472, 0.59975369458128081, 0.69581280788177335], 5: [0.11330049261083744, 0.14778325123152711, 0.13793103448275862, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16995073891625614, 0.26231527093596058, 0.28201970443349755, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.60591133004926112, 0.56896551724137934, 0.65024630541871919], 5: [0.11330049261083744, 0.13177339901477833, 0.14901477832512317, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16995073891625614, 0.22783251231527094, 0.29187192118226601, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.63054187192118227, 0.58004926108374388, 0.6785714285714286], 5: [0.11330049261083744, 0.14162561576354679, 0.12807881773399016, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16995073891625614, 0.24876847290640394, 0.25985221674876846, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.61206896551724133, 0.57512315270935965, 0.65640394088669951], 5: [0.11330049261083744, 0.13916256157635468, 0.16502463054187191, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.262776 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.55323
Epoch 1, cost is  3.47232
Epoch 2, cost is  3.40275
Epoch 3, cost is  3.33936
Epoch 4, cost is  3.2834
Training took 0.227319 minutes
Weight histogram
[1785 1534 1081 1038 1325 1153  573 1113  389  134] [ -3.58366780e-02  -3.22494355e-02  -2.86621931e-02  -2.50749506e-02
  -2.14877081e-02  -1.79004657e-02  -1.43132232e-02  -1.07259807e-02
  -7.13873825e-03  -3.55149579e-03   3.57466815e-05]
[1764  976  741  697  757  841 1004 1040 1108 1197] [ -3.58366780e-02  -3.22494355e-02  -2.86621931e-02  -2.50749506e-02
  -2.14877081e-02  -1.79004657e-02  -1.43132232e-02  -1.07259807e-02
  -7.13873825e-03  -3.55149579e-03   3.57466815e-05]
-0.814971
0.998837
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271378 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.33243
Epoch 1, cost is  4.24725
Epoch 2, cost is  4.17293
Epoch 3, cost is  4.11242
Epoch 4, cost is  4.05225
Training took 0.197879 minutes
Weight histogram
[1271  825 1122 1033  880  800  974 1518 3325  402] [ -4.65206653e-02  -4.18650241e-02  -3.72093829e-02  -3.25537417e-02
  -2.78981005e-02  -2.32424593e-02  -1.85868181e-02  -1.39311769e-02
  -9.27553571e-03  -4.61989452e-03   3.57466815e-05]
[4254  854  715  743  799  845  869  931 1054 1086] [ -4.65206653e-02  -4.18650241e-02  -3.72093829e-02  -3.25537417e-02
  -2.78981005e-02  -2.32424593e-02  -1.85868181e-02  -1.39311769e-02
  -9.27553571e-03  -4.61989452e-03   3.57466815e-05]
-0.821147
1.15518
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.84784
Epoch 1, cost is  6.75567
Epoch 2, cost is  6.67959
Epoch 3, cost is  6.60755
Epoch 4, cost is  6.52687
Training took 0.230534 minutes
Weight histogram
[1660 1774 4727  702  417  281  201  153  117   93] [ -1.02815339e-02  -9.25434904e-03  -8.22716415e-03  -7.19997926e-03
  -6.17279438e-03  -5.14560949e-03  -4.11842460e-03  -3.09123971e-03
  -2.06405482e-03  -1.03686993e-03  -9.68503628e-06]
[3793 1747 1353 1123  853  487  288  232  126  123] [ -1.02815339e-02  -9.25434904e-03  -8.22716415e-03  -7.19997926e-03
  -6.17279438e-03  -5.14560949e-03  -4.11842460e-03  -3.09123971e-03
  -2.06405482e-03  -1.03686993e-03  -9.68503628e-06]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044980 minutes
Epoch 0
Fine tuning took 0.045368 minutes
Epoch 0
Fine tuning took 0.041585 minutes
{'zero': {0: [0.19827586206896552, 0.29679802955665024, 0.27709359605911332, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.45443349753694579, 0.47536945812807879, 0.66009852216748766], 5: [0.089901477832512317, 0.24876847290640394, 0.24753694581280788, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19827586206896552, 0.31896551724137934, 0.26847290640394089, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.44704433497536944, 0.48768472906403942, 0.66748768472906406], 5: [0.089901477832512317, 0.23399014778325122, 0.24384236453201971, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19827586206896552, 0.28694581280788178, 0.27832512315270935, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.44334975369458129, 0.48275862068965519, 0.66502463054187189], 5: [0.089901477832512317, 0.26970443349753692, 0.23891625615763548, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19827586206896552, 0.30295566502463056, 0.26970443349753692, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71182266009852213, 0.42733990147783252, 0.48768472906403942, 0.66256157635467983], 5: [0.089901477832512317, 0.26970443349753692, 0.24261083743842365, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.289336 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.55323
Epoch 1, cost is  3.47232
Epoch 2, cost is  3.40275
Epoch 3, cost is  3.33936
Epoch 4, cost is  3.2834
Training took 0.232137 minutes
Weight histogram
[1785 1534 1081 1038 1325 1153  573 1113  389  134] [ -3.58366780e-02  -3.22494355e-02  -2.86621931e-02  -2.50749506e-02
  -2.14877081e-02  -1.79004657e-02  -1.43132232e-02  -1.07259807e-02
  -7.13873825e-03  -3.55149579e-03   3.57466815e-05]
[1764  976  741  697  757  841 1004 1040 1108 1197] [ -3.58366780e-02  -3.22494355e-02  -2.86621931e-02  -2.50749506e-02
  -2.14877081e-02  -1.79004657e-02  -1.43132232e-02  -1.07259807e-02
  -7.13873825e-03  -3.55149579e-03   3.57466815e-05]
-0.814971
0.998837
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268266 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.33243
Epoch 1, cost is  4.24725
Epoch 2, cost is  4.17293
Epoch 3, cost is  4.11242
Epoch 4, cost is  4.05225
Training took 0.201515 minutes
Weight histogram
[1271  825 1122 1033  880  800  974 1518 3325  402] [ -4.65206653e-02  -4.18650241e-02  -3.72093829e-02  -3.25537417e-02
  -2.78981005e-02  -2.32424593e-02  -1.85868181e-02  -1.39311769e-02
  -9.27553571e-03  -4.61989452e-03   3.57466815e-05]
[4254  854  715  743  799  845  869  931 1054 1086] [ -4.65206653e-02  -4.18650241e-02  -3.72093829e-02  -3.25537417e-02
  -2.78981005e-02  -2.32424593e-02  -1.85868181e-02  -1.39311769e-02
  -9.27553571e-03  -4.61989452e-03   3.57466815e-05]
-0.821147
1.15518
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.77224
Epoch 1, cost is  6.64453
Epoch 2, cost is  6.55342
Epoch 3, cost is  6.46579
Epoch 4, cost is  6.37593
Training took 0.283984 minutes
Weight histogram
[ 623 1143 2181 3928  938  505  314  218  158  117] [ -1.24916639e-02  -1.12486657e-02  -1.00056674e-02  -8.76266915e-03
  -7.51967090e-03  -6.27667264e-03  -5.03367439e-03  -3.79067613e-03
  -2.54767787e-03  -1.30467962e-03  -6.16813631e-05]
[3325 1590 1274 1121 1033  660  448  314  196  164] [ -1.24916639e-02  -1.12486657e-02  -1.00056674e-02  -8.76266915e-03
  -7.51967090e-03  -6.27667264e-03  -5.03367439e-03  -3.79067613e-03
  -2.54767787e-03  -1.30467962e-03  -6.16813631e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042846 minutes
Epoch 0
Fine tuning took 0.043263 minutes
Epoch 0
Fine tuning took 0.043125 minutes
{'zero': {0: [0.18103448275862069, 0.26108374384236455, 0.39655172413793105, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.43226600985221675, 0.37684729064039407, 0.64532019704433496], 5: [0.059113300492610835, 0.30665024630541871, 0.22660098522167488, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18103448275862069, 0.2857142857142857, 0.41379310344827586, 0.10467980295566502], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.4039408866995074, 0.3460591133004926, 0.66871921182266014], 5: [0.059113300492610835, 0.31034482758620691, 0.24014778325123154, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18103448275862069, 0.26847290640394089, 0.41871921182266009, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.43472906403940886, 0.36699507389162561, 0.6576354679802956], 5: [0.059113300492610835, 0.29679802955665024, 0.21428571428571427, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18103448275862069, 0.2857142857142857, 0.40517241379310343, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75985221674876846, 0.41256157635467983, 0.39162561576354682, 0.6428571428571429], 5: [0.059113300492610835, 0.30172413793103448, 0.20320197044334976, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270435 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.55323
Epoch 1, cost is  3.47232
Epoch 2, cost is  3.40275
Epoch 3, cost is  3.33936
Epoch 4, cost is  3.2834
Training took 0.223656 minutes
Weight histogram
[1785 1534 1081 1038 1325 1153  573 1113  389  134] [ -3.58366780e-02  -3.22494355e-02  -2.86621931e-02  -2.50749506e-02
  -2.14877081e-02  -1.79004657e-02  -1.43132232e-02  -1.07259807e-02
  -7.13873825e-03  -3.55149579e-03   3.57466815e-05]
[1764  976  741  697  757  841 1004 1040 1108 1197] [ -3.58366780e-02  -3.22494355e-02  -2.86621931e-02  -2.50749506e-02
  -2.14877081e-02  -1.79004657e-02  -1.43132232e-02  -1.07259807e-02
  -7.13873825e-03  -3.55149579e-03   3.57466815e-05]
-0.814971
0.998837
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271694 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.33243
Epoch 1, cost is  4.24725
Epoch 2, cost is  4.17293
Epoch 3, cost is  4.11242
Epoch 4, cost is  4.05225
Training took 0.201772 minutes
Weight histogram
[1271  825 1122 1033  880  800  974 1518 3325  402] [ -4.65206653e-02  -4.18650241e-02  -3.72093829e-02  -3.25537417e-02
  -2.78981005e-02  -2.32424593e-02  -1.85868181e-02  -1.39311769e-02
  -9.27553571e-03  -4.61989452e-03   3.57466815e-05]
[4254  854  715  743  799  845  869  931 1054 1086] [ -4.65206653e-02  -4.18650241e-02  -3.72093829e-02  -3.25537417e-02
  -2.78981005e-02  -2.32424593e-02  -1.85868181e-02  -1.39311769e-02
  -9.27553571e-03  -4.61989452e-03   3.57466815e-05]
-0.821147
1.15518
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.65401
Epoch 1, cost is  6.47157
Epoch 2, cost is  6.36342
Epoch 3, cost is  6.26646
Epoch 4, cost is  6.17006
Training took 0.368910 minutes
Weight histogram
[ 558  837 1426 3617 1851  780  447  282  192  135] [ -1.41741699e-02  -1.27595962e-02  -1.13450226e-02  -9.93044891e-03
  -8.51587526e-03  -7.10130161e-03  -5.68672796e-03  -4.27215431e-03
  -2.85758066e-03  -1.44300701e-03  -2.84333619e-05]
[2862 1449 1224 1125 1081  931  582  401  259  211] [ -1.41741699e-02  -1.27595962e-02  -1.13450226e-02  -9.93044891e-03
  -8.51587526e-03  -7.10130161e-03  -5.68672796e-03  -4.27215431e-03
  -2.85758066e-03  -1.44300701e-03  -2.84333619e-05]
-0.0721031
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049471 minutes
Epoch 0
Fine tuning took 0.046887 minutes
Epoch 0
Fine tuning took 0.046332 minutes
{'zero': {0: [0.18103448275862069, 0.25, 0.36206896551724138, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.4211822660098522, 0.45443349753694579, 0.6576354679802956], 5: [0.088669950738916259, 0.3288177339901478, 0.18349753694581281, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18103448275862069, 0.27586206896551724, 0.35960591133004927, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.39285714285714285, 0.48152709359605911, 0.66502463054187189], 5: [0.088669950738916259, 0.33128078817733991, 0.15886699507389163, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18103448275862069, 0.23275862068965517, 0.33743842364532017, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.43472906403940886, 0.46921182266009853, 0.67980295566502458], 5: [0.088669950738916259, 0.33251231527093594, 0.19334975369458129, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18103448275862069, 0.25492610837438423, 0.35837438423645318, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73029556650246308, 0.39039408866995073, 0.45566502463054187, 0.6576354679802956], 5: [0.088669950738916259, 0.35467980295566504, 0.18596059113300492, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.262605 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.77342
Epoch 1, cost is  2.58751
Epoch 2, cost is  2.54935
Epoch 3, cost is  2.54106
Epoch 4, cost is  2.55223
Training took 0.227255 minutes
Weight histogram
[1639 1267 1360 1406 1347 1313  917  561  227   88] [-0.22065639 -0.19866851 -0.17668062 -0.15469274 -0.13270485 -0.11071697
 -0.08872908 -0.0667412  -0.04475331 -0.02276543 -0.00077754]
[ 286  505  739 1028 1137 1192 1207 1275 1339 1417] [-0.22065639 -0.19866851 -0.17668062 -0.15469274 -0.13270485 -0.11071697
 -0.08872908 -0.0667412  -0.04475331 -0.02276543 -0.00077754]
-9.26229
15.1713
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270395 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.36257
Epoch 1, cost is  7.06396
Epoch 2, cost is  7.10525
Epoch 3, cost is  7.20438
Epoch 4, cost is  7.33847
Training took 0.204894 minutes
Weight histogram
[1430  918 1195 1240 1182 1156 1305  975 2189  560] [-0.46813473 -0.42139901 -0.37466329 -0.32792757 -0.28119185 -0.23445614
 -0.18772042 -0.1409847  -0.09424898 -0.04751326 -0.00077754]
[1346 1877  968 1096 1117 1126 1096 1062 1218 1244] [-0.46813473 -0.42139901 -0.37466329 -0.32792757 -0.28119185 -0.23445614
 -0.18772042 -0.1409847  -0.09424898 -0.04751326 -0.00077754]
-17.2157
16.6659
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.11225
Epoch 1, cost is  3.8169
Epoch 2, cost is  3.66003
Epoch 3, cost is  3.74229
Epoch 4, cost is  3.93459
Training took 0.221625 minutes
Weight histogram
[1399 1634 1804 1531 1263  861  566  398  298  371] [-0.17713617 -0.15954908 -0.14196199 -0.12437491 -0.10678782 -0.08920073
 -0.07161364 -0.05402656 -0.03643947 -0.01885238 -0.0012653 ]
[ 627  583  719  858  975 1101 1224 1306 1391 1341] [-0.17713617 -0.15954908 -0.14196199 -0.12437491 -0.10678782 -0.08920073
 -0.07161364 -0.05402656 -0.03643947 -0.01885238 -0.0012653 ]
-6.86173
6.97239
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043348 minutes
Epoch 0
Fine tuning took 0.042924 minutes
Epoch 0
Fine tuning took 0.042797 minutes
{'zero': {0: [0.37315270935960593, 0.44334975369458129, 0.29802955665024633, 0.34975369458128081], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.40270935960591131, 0.58128078817733986, 0.48522167487684731], 5: [0.11083743842364532, 0.1539408866995074, 0.1206896551724138, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.37315270935960593, 0.39408866995073893, 0.34482758620689657, 0.28201970443349755], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.51970443349753692, 0.55295566502463056, 0.55172413793103448], 5: [0.11083743842364532, 0.086206896551724144, 0.10221674876847291, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.37315270935960593, 0.39162561576354682, 0.32142857142857145, 0.32266009852216748], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.47044334975369456, 0.57389162561576357, 0.50369458128078815], 5: [0.11083743842364532, 0.13793103448275862, 0.10467980295566502, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.37315270935960593, 0.38669950738916259, 0.35221674876847292, 0.31403940886699505], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.52586206896551724, 0.54187192118226601, 0.53817733990147787], 5: [0.11083743842364532, 0.087438423645320201, 0.10591133004926108, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266352 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.77342
Epoch 1, cost is  2.58751
Epoch 2, cost is  2.54935
Epoch 3, cost is  2.54106
Epoch 4, cost is  2.55223
Training took 0.231868 minutes
Weight histogram
[1639 1267 1360 1406 1347 1313  917  561  227   88] [-0.22065639 -0.19866851 -0.17668062 -0.15469274 -0.13270485 -0.11071697
 -0.08872908 -0.0667412  -0.04475331 -0.02276543 -0.00077754]
[ 286  505  739 1028 1137 1192 1207 1275 1339 1417] [-0.22065639 -0.19866851 -0.17668062 -0.15469274 -0.13270485 -0.11071697
 -0.08872908 -0.0667412  -0.04475331 -0.02276543 -0.00077754]
-9.26229
15.1713
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.261045 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.36257
Epoch 1, cost is  7.06396
Epoch 2, cost is  7.10525
Epoch 3, cost is  7.20438
Epoch 4, cost is  7.33847
Training took 0.196202 minutes
Weight histogram
[1430  918 1195 1240 1182 1156 1305  975 2189  560] [-0.46813473 -0.42139901 -0.37466329 -0.32792757 -0.28119185 -0.23445614
 -0.18772042 -0.1409847  -0.09424898 -0.04751326 -0.00077754]
[1346 1877  968 1096 1117 1126 1096 1062 1218 1244] [-0.46813473 -0.42139901 -0.37466329 -0.32792757 -0.28119185 -0.23445614
 -0.18772042 -0.1409847  -0.09424898 -0.04751326 -0.00077754]
-17.2157
16.6659
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.74692
Epoch 1, cost is  2.88569
Epoch 2, cost is  2.51096
Epoch 3, cost is  2.41804
Epoch 4, cost is  2.42966
Training took 0.268586 minutes
Weight histogram
[1462 1851 1593 1423 1210  905  614  435  315  317] [-0.11230296 -0.1011992  -0.09009543 -0.07899167 -0.06788791 -0.05678415
 -0.04568038 -0.03457662 -0.02347286 -0.01236909 -0.00126533]
[ 685  568  684  784  919 1149 1212 1352 1429 1343] [-0.11230296 -0.1011992  -0.09009543 -0.07899167 -0.06788791 -0.05678415
 -0.04568038 -0.03457662 -0.02347286 -0.01236909 -0.00126533]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044269 minutes
Epoch 0
Fine tuning took 0.047448 minutes
Epoch 0
Fine tuning took 0.044335 minutes
{'zero': {0: [0.16748768472906403, 0.24261083743842365, 0.22906403940886699, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.5357142857142857, 0.64162561576354682, 0.51970443349753692], 5: [0.12315270935960591, 0.22167487684729065, 0.12931034482758622, 0.29064039408866993], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16748768472906403, 0.20566502463054187, 0.2105911330049261, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.61083743842364535, 0.66256157635467983, 0.6354679802955665], 5: [0.12315270935960591, 0.18349753694581281, 0.1268472906403941, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16748768472906403, 0.23522167487684728, 0.23152709359605911, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.57019704433497542, 0.62684729064039413, 0.59482758620689657], 5: [0.12315270935960591, 0.19458128078817735, 0.14162561576354679, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16748768472906403, 0.19950738916256158, 0.17980295566502463, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70935960591133007, 0.6145320197044335, 0.7068965517241379, 0.61330049261083741], 5: [0.12315270935960591, 0.18596059113300492, 0.11330049261083744, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.277489 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2538 1264  500   47] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 119  134  191  248  382  580  778  997 1925 4771] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.77342
Epoch 1, cost is  2.58751
Epoch 2, cost is  2.54935
Epoch 3, cost is  2.54106
Epoch 4, cost is  2.55223
Training took 0.229906 minutes
Weight histogram
[1639 1267 1360 1406 1347 1313  917  561  227   88] [-0.22065639 -0.19866851 -0.17668062 -0.15469274 -0.13270485 -0.11071697
 -0.08872908 -0.0667412  -0.04475331 -0.02276543 -0.00077754]
[ 286  505  739 1028 1137 1192 1207 1275 1339 1417] [-0.22065639 -0.19866851 -0.17668062 -0.15469274 -0.13270485 -0.11071697
 -0.08872908 -0.0667412  -0.04475331 -0.02276543 -0.00077754]
-9.26229
15.1713
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265784 minutes
Weight histogram
[ 130  452  808 1122 1728 1809 2939 2297  722  143] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
[ 237  273  382  459  738 1129 1103 1041 2054 4734] [ -7.24884303e-05   9.32572191e-05   2.59002869e-04   4.24748518e-04
   5.90494167e-04   7.56239817e-04   9.21985466e-04   1.08773112e-03
   1.25347677e-03   1.41922241e-03   1.58496806e-03]
-1.12418
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  7.36257
Epoch 1, cost is  7.06396
Epoch 2, cost is  7.10525
Epoch 3, cost is  7.20438
Epoch 4, cost is  7.33847
Training took 0.204128 minutes
Weight histogram
[1430  918 1195 1240 1182 1156 1305  975 2189  560] [-0.46813473 -0.42139901 -0.37466329 -0.32792757 -0.28119185 -0.23445614
 -0.18772042 -0.1409847  -0.09424898 -0.04751326 -0.00077754]
[1346 1877  968 1096 1117 1126 1096 1062 1218 1244] [-0.46813473 -0.42139901 -0.37466329 -0.32792757 -0.28119185 -0.23445614
 -0.18772042 -0.1409847  -0.09424898 -0.04751326 -0.00077754]
-17.2157
16.6659
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.51459
Epoch 1, cost is  2.34887
Epoch 2, cost is  1.8234
Epoch 3, cost is  1.64599
Epoch 4, cost is  1.55098
Training took 0.365925 minutes
Weight histogram
[1790 1898 1687 1316 1086  901  543  446  303  155] [-0.07301652 -0.06584055 -0.05866458 -0.05148861 -0.04431264 -0.03713667
 -0.02996069 -0.02278472 -0.01560875 -0.00843278 -0.00125681]
[ 692  522  606  753  920 1098 1248 1397 1516 1373] [-0.07301652 -0.06584055 -0.05866458 -0.05148861 -0.04431264 -0.03713667
 -0.02996069 -0.02278472 -0.01560875 -0.00843278 -0.00125681]
-3.98509
4.26369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047152 minutes
Epoch 0
Fine tuning took 0.047880 minutes
Epoch 0
Fine tuning took 0.047602 minutes
{'zero': {0: [0.18719211822660098, 0.20073891625615764, 0.15640394088669951, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64655172413793105, 0.52339901477832518, 0.71305418719211822, 0.70073891625615758], 5: [0.16625615763546797, 0.27586206896551724, 0.13054187192118227, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18719211822660098, 0.24753694581280788, 0.16133004926108374, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64655172413793105, 0.62438423645320196, 0.63793103448275867, 0.63177339901477836], 5: [0.16625615763546797, 0.12807881773399016, 0.20073891625615764, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18719211822660098, 0.2376847290640394, 0.17241379310344829, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64655172413793105, 0.58004926108374388, 0.65270935960591137, 0.62561576354679804], 5: [0.16625615763546797, 0.18226600985221675, 0.1748768472906404, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18719211822660098, 0.23522167487684728, 0.1354679802955665, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64655172413793105, 0.64901477832512311, 0.58251231527093594, 0.6354679802955665], 5: [0.16625615763546797, 0.11576354679802955, 0.28201970443349755, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268006 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.87313
Epoch 1, cost is  1.7951
Epoch 2, cost is  1.7656
Epoch 3, cost is  1.74977
Epoch 4, cost is  1.74312
Training took 0.230538 minutes
Weight histogram
[2508 1885 2020 1650 1467 1078  725  385  255  177] [-0.10241844 -0.09218796 -0.08195748 -0.071727   -0.06149652 -0.05126603
 -0.04103555 -0.03080507 -0.02057459 -0.01034411 -0.00011363]
[ 509  534  727 1033 1189 1344 1519 1644 1802 1849] [-0.10241844 -0.09218796 -0.08195748 -0.071727   -0.06149652 -0.05126603
 -0.04103555 -0.03080507 -0.02057459 -0.01034411 -0.00011363]
-3.25168
5.07894
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269170 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.57087
Epoch 1, cost is  3.45567
Epoch 2, cost is  3.42897
Epoch 3, cost is  3.4318
Epoch 4, cost is  3.43362
Training took 0.199751 minutes
Weight histogram
[1993 2008 1522 1984 1441 1195  745 1524 1061  702] [ -1.71071529e-01  -1.53975740e-01  -1.36879950e-01  -1.19784160e-01
  -1.02688371e-01  -8.55925809e-02  -6.84967912e-02  -5.14010015e-02
  -3.43052118e-02  -1.72094221e-02  -1.13632348e-04]
[1504 1708  828 1092 1256 1376 1341 1542 1795 1733] [ -1.71071529e-01  -1.53975740e-01  -1.36879950e-01  -1.19784160e-01
  -1.02688371e-01  -8.55925809e-02  -6.84967912e-02  -5.14010015e-02
  -3.43052118e-02  -1.72094221e-02  -1.13632348e-04]
-5.16664
5.74029
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55675
Epoch 1, cost is  5.38394
Epoch 2, cost is  4.56144
Epoch 3, cost is  4.16444
Epoch 4, cost is  3.92869
Training took 0.230197 minutes
Weight histogram
[2028 1500 1404 1353  985  857  786  843  610 1784] [-0.08295341 -0.0746815  -0.06640959 -0.05813768 -0.04986577 -0.04159386
 -0.03332195 -0.02505005 -0.01677814 -0.00850623 -0.00023432]
[1845 1063  846  958 1022 1118 1245 1387 1515 1151] [-0.08295341 -0.0746815  -0.06640959 -0.05813768 -0.04986577 -0.04159386
 -0.03332195 -0.02505005 -0.01677814 -0.00850623 -0.00023432]
-1.6243
2.76155
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041731 minutes
Epoch 0
Fine tuning took 0.041952 minutes
Epoch 0
Fine tuning took 0.041776 minutes
{'zero': {0: [0.25246305418719212, 0.24753694581280788, 0.24507389162561577, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59359605911330049, 0.62684729064039413, 0.66748768472906406, 0.7426108374384236], 5: [0.1539408866995074, 0.12561576354679804, 0.087438423645320201, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25246305418719212, 0.2376847290640394, 0.21305418719211822, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59359605911330049, 0.63793103448275867, 0.69950738916256161, 0.75369458128078815], 5: [0.1539408866995074, 0.12438423645320197, 0.087438423645320201, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25246305418719212, 0.23522167487684728, 0.22660098522167488, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59359605911330049, 0.65024630541871919, 0.68472906403940892, 0.7426108374384236], 5: [0.1539408866995074, 0.1145320197044335, 0.088669950738916259, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25246305418719212, 0.23399014778325122, 0.2229064039408867, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59359605911330049, 0.63423645320197042, 0.69334975369458129, 0.72167487684729059], 5: [0.1539408866995074, 0.13177339901477833, 0.083743842364532015, 0.076354679802955669], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274291 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.87313
Epoch 1, cost is  1.7951
Epoch 2, cost is  1.7656
Epoch 3, cost is  1.74977
Epoch 4, cost is  1.74312
Training took 0.231399 minutes
Weight histogram
[2508 1885 2020 1650 1467 1078  725  385  255  177] [-0.10241844 -0.09218796 -0.08195748 -0.071727   -0.06149652 -0.05126603
 -0.04103555 -0.03080507 -0.02057459 -0.01034411 -0.00011363]
[ 509  534  727 1033 1189 1344 1519 1644 1802 1849] [-0.10241844 -0.09218796 -0.08195748 -0.071727   -0.06149652 -0.05126603
 -0.04103555 -0.03080507 -0.02057459 -0.01034411 -0.00011363]
-3.25168
5.07894
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.277301 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.57087
Epoch 1, cost is  3.45567
Epoch 2, cost is  3.42897
Epoch 3, cost is  3.4318
Epoch 4, cost is  3.43362
Training took 0.207823 minutes
Weight histogram
[1993 2008 1522 1984 1441 1195  745 1524 1061  702] [ -1.71071529e-01  -1.53975740e-01  -1.36879950e-01  -1.19784160e-01
  -1.02688371e-01  -8.55925809e-02  -6.84967912e-02  -5.14010015e-02
  -3.43052118e-02  -1.72094221e-02  -1.13632348e-04]
[1504 1708  828 1092 1256 1376 1341 1542 1795 1733] [ -1.71071529e-01  -1.53975740e-01  -1.36879950e-01  -1.19784160e-01
  -1.02688371e-01  -8.55925809e-02  -6.84967912e-02  -5.14010015e-02
  -3.43052118e-02  -1.72094221e-02  -1.13632348e-04]
-5.16664
5.74029
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46276
Epoch 1, cost is  5.2289
Epoch 2, cost is  4.10392
Epoch 3, cost is  3.48517
Epoch 4, cost is  3.06762
Training took 0.285659 minutes
Weight histogram
[1641 2085 1508 1322 1100  890  951  760 1733  160] [-0.05099323 -0.04592256 -0.0408519  -0.03578123 -0.03071057 -0.0256399
 -0.02056924 -0.01549857 -0.01042791 -0.00535724 -0.00028658]
[1960 1106  914  992  990 1109 1225 1316 1399 1139] [-0.05099323 -0.04592256 -0.0408519  -0.03578123 -0.03071057 -0.0256399
 -0.02056924 -0.01549857 -0.01042791 -0.00535724 -0.00028658]
-1.1998
1.51512
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043545 minutes
Epoch 0
Fine tuning took 0.043776 minutes
Epoch 0
Fine tuning took 0.043144 minutes
{'zero': {0: [0.19334975369458129, 0.15886699507389163, 0.16625615763546797, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.67733990147783252, 0.70443349753694584, 0.6354679802955665], 5: [0.13300492610837439, 0.16379310344827586, 0.12931034482758622, 0.21798029556650247], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19334975369458129, 0.17118226600985223, 0.1539408866995074, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.71674876847290636, 0.76847290640394084, 0.72290640394088668], 5: [0.13300492610837439, 0.11206896551724138, 0.077586206896551727, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19334975369458129, 0.17364532019704434, 0.15270935960591134, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.69458128078817738, 0.73891625615763545, 0.69088669950738912], 5: [0.13300492610837439, 0.13177339901477833, 0.10837438423645321, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19334975369458129, 0.15886699507389163, 0.16625615763546797, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67364532019704437, 0.70443349753694584, 0.73768472906403937, 0.7142857142857143], 5: [0.13300492610837439, 0.13669950738916256, 0.096059113300492605, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.253329 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.87313
Epoch 1, cost is  1.7951
Epoch 2, cost is  1.7656
Epoch 3, cost is  1.74977
Epoch 4, cost is  1.74312
Training took 0.220035 minutes
Weight histogram
[2508 1885 2020 1650 1467 1078  725  385  255  177] [-0.10241844 -0.09218796 -0.08195748 -0.071727   -0.06149652 -0.05126603
 -0.04103555 -0.03080507 -0.02057459 -0.01034411 -0.00011363]
[ 509  534  727 1033 1189 1344 1519 1644 1802 1849] [-0.10241844 -0.09218796 -0.08195748 -0.071727   -0.06149652 -0.05126603
 -0.04103555 -0.03080507 -0.02057459 -0.01034411 -0.00011363]
-3.25168
5.07894
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267599 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.57087
Epoch 1, cost is  3.45567
Epoch 2, cost is  3.42897
Epoch 3, cost is  3.4318
Epoch 4, cost is  3.43362
Training took 0.201776 minutes
Weight histogram
[1993 2008 1522 1984 1441 1195  745 1524 1061  702] [ -1.71071529e-01  -1.53975740e-01  -1.36879950e-01  -1.19784160e-01
  -1.02688371e-01  -8.55925809e-02  -6.84967912e-02  -5.14010015e-02
  -3.43052118e-02  -1.72094221e-02  -1.13632348e-04]
[1504 1708  828 1092 1256 1376 1341 1542 1795 1733] [ -1.71071529e-01  -1.53975740e-01  -1.36879950e-01  -1.19784160e-01
  -1.02688371e-01  -8.55925809e-02  -6.84967912e-02  -5.14010015e-02
  -3.43052118e-02  -1.72094221e-02  -1.13632348e-04]
-5.16664
5.74029
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.34963
Epoch 1, cost is  5.13217
Epoch 2, cost is  3.88365
Epoch 3, cost is  3.09535
Epoch 4, cost is  2.58307
Training took 0.357270 minutes
Weight histogram
[1820 2178 1700 1449 1166 1134  912 1394  323   74] [-0.03414995 -0.03075964 -0.02736933 -0.02397902 -0.02058872 -0.01719841
 -0.0138081  -0.01041779 -0.00702749 -0.00363718 -0.00024687]
[2145 1124  908  903  988 1090 1160 1213 1393 1226] [-0.03414995 -0.03075964 -0.02736933 -0.02397902 -0.02058872 -0.01719841
 -0.0138081  -0.01041779 -0.00702749 -0.00363718 -0.00024687]
-0.953796
1.40444
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046387 minutes
Epoch 0
Fine tuning took 0.046854 minutes
Epoch 0
Fine tuning took 0.046930 minutes
{'zero': {0: [0.18349753694581281, 0.1748768472906404, 0.14655172413793102, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.5431034482758621, 0.69211822660098521, 0.57019704433497542], 5: [0.11822660098522167, 0.28201970443349755, 0.16133004926108374, 0.26231527093596058], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18349753694581281, 0.19950738916256158, 0.13300492610837439, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.57512315270935965, 0.72536945812807885, 0.66133004926108374], 5: [0.11822660098522167, 0.22536945812807882, 0.14162561576354679, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18349753694581281, 0.16625615763546797, 0.14778325123152711, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.59113300492610843, 0.70073891625615758, 0.66748768472906406], 5: [0.11822660098522167, 0.24261083743842365, 0.15147783251231528, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18349753694581281, 0.18349753694581281, 0.14532019704433496, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69827586206896552, 0.59482758620689657, 0.71674876847290636, 0.69581280788177335], 5: [0.11822660098522167, 0.22167487684729065, 0.13793103448275862, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271192 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.92124
Epoch 1, cost is  1.85845
Epoch 2, cost is  1.82588
Epoch 3, cost is  1.79891
Epoch 4, cost is  1.77737
Training took 0.222826 minutes
Weight histogram
[2641 2082 1939 1412 1464 1005  574  532  374  127] [ -7.60276616e-02  -6.84279598e-02  -6.08282580e-02  -5.32285563e-02
  -4.56288545e-02  -3.80291528e-02  -3.04294510e-02  -2.28297493e-02
  -1.52300475e-02  -7.63034575e-03  -3.06439943e-05]
[ 752  544  735  979 1139 1285 1477 1606 1780 1853] [ -7.60276616e-02  -6.84279598e-02  -6.08282580e-02  -5.32285563e-02
  -4.56288545e-02  -3.80291528e-02  -3.04294510e-02  -2.28297493e-02
  -1.52300475e-02  -7.63034575e-03  -3.06439943e-05]
-2.20903
2.39702
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.263187 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.18249
Epoch 1, cost is  3.09669
Epoch 2, cost is  3.07872
Epoch 3, cost is  3.06107
Epoch 4, cost is  3.05708
Training took 0.199619 minutes
Weight histogram
[2712 1602 1868 1726 1115  896  702 1439 1196  919] [ -1.18810810e-01  -1.06932794e-01  -9.50547769e-02  -8.31767603e-02
  -7.12987437e-02  -5.94207271e-02  -4.75427105e-02  -3.56646938e-02
  -2.37866772e-02  -1.19086606e-02  -3.06439943e-05]
[1960 1501  743  949 1175 1353 1347 1529 1824 1794] [ -1.18810810e-01  -1.06932794e-01  -9.50547769e-02  -8.31767603e-02
  -7.12987437e-02  -5.94207271e-02  -4.75427105e-02  -3.56646938e-02
  -2.37866772e-02  -1.19086606e-02  -3.06439943e-05]
-3.1403
3.64896
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74414
Epoch 1, cost is  6.35563
Epoch 2, cost is  5.65301
Epoch 3, cost is  5.05177
Epoch 4, cost is  4.63769
Training took 0.222183 minutes
Weight histogram
[ 615 1391 1122 1154  918 1011 1028  878 3677  356] [-0.0558529  -0.05027884 -0.04470479 -0.03913073 -0.03355668 -0.02798262
 -0.02240857 -0.01683451 -0.01126046 -0.0056864  -0.00011235]
[2855 1226 1258 1012  946 1087 1145 1146  996  479] [-0.0558529  -0.05027884 -0.04470479 -0.03913073 -0.03355668 -0.02798262
 -0.02240857 -0.01683451 -0.01126046 -0.0056864  -0.00011235]
-0.921519
1.26551
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041501 minutes
Epoch 0
Fine tuning took 0.041698 minutes
Epoch 0
Fine tuning took 0.041256 minutes
{'zero': {0: [0.2413793103448276, 0.23152709359605911, 0.26600985221674878, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.43472906403940886, 0.47906403940886699, 0.58497536945812811, 0.6071428571428571], 5: [0.32389162561576357, 0.2894088669950739, 0.14901477832512317, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2413793103448276, 0.21798029556650247, 0.25492610837438423, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.43472906403940886, 0.48768472906403942, 0.6145320197044335, 0.58620689655172409], 5: [0.32389162561576357, 0.29433497536945813, 0.13054187192118227, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2413793103448276, 0.24261083743842365, 0.27093596059113301, 0.24630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.43472906403940886, 0.46921182266009853, 0.59113300492610843, 0.57635467980295563], 5: [0.32389162561576357, 0.28817733990147781, 0.13793103448275862, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2413793103448276, 0.23522167487684728, 0.23275862068965517, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.43472906403940886, 0.48522167487684731, 0.62068965517241381, 0.60098522167487689], 5: [0.32389162561576357, 0.27955665024630544, 0.14655172413793102, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.288999 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.92124
Epoch 1, cost is  1.85845
Epoch 2, cost is  1.82588
Epoch 3, cost is  1.79891
Epoch 4, cost is  1.77737
Training took 0.228360 minutes
Weight histogram
[2641 2082 1939 1412 1464 1005  574  532  374  127] [ -7.60276616e-02  -6.84279598e-02  -6.08282580e-02  -5.32285563e-02
  -4.56288545e-02  -3.80291528e-02  -3.04294510e-02  -2.28297493e-02
  -1.52300475e-02  -7.63034575e-03  -3.06439943e-05]
[ 752  544  735  979 1139 1285 1477 1606 1780 1853] [ -7.60276616e-02  -6.84279598e-02  -6.08282580e-02  -5.32285563e-02
  -4.56288545e-02  -3.80291528e-02  -3.04294510e-02  -2.28297493e-02
  -1.52300475e-02  -7.63034575e-03  -3.06439943e-05]
-2.20903
2.39702
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272527 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.18249
Epoch 1, cost is  3.09669
Epoch 2, cost is  3.07872
Epoch 3, cost is  3.06107
Epoch 4, cost is  3.05708
Training took 0.201752 minutes
Weight histogram
[2712 1602 1868 1726 1115  896  702 1439 1196  919] [ -1.18810810e-01  -1.06932794e-01  -9.50547769e-02  -8.31767603e-02
  -7.12987437e-02  -5.94207271e-02  -4.75427105e-02  -3.56646938e-02
  -2.37866772e-02  -1.19086606e-02  -3.06439943e-05]
[1960 1501  743  949 1175 1353 1347 1529 1824 1794] [ -1.18810810e-01  -1.06932794e-01  -9.50547769e-02  -8.31767603e-02
  -7.12987437e-02  -5.94207271e-02  -4.75427105e-02  -3.56646938e-02
  -2.37866772e-02  -1.19086606e-02  -3.06439943e-05]
-3.1403
3.64896
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.65089
Epoch 1, cost is  6.25805
Epoch 2, cost is  5.55361
Epoch 3, cost is  4.83657
Epoch 4, cost is  4.23383
Training took 0.276808 minutes
Weight histogram
[ 354 1568 1531 1288 1212 1244 1020 2462 1318  153] [-0.03642388 -0.03279803 -0.02917218 -0.02554633 -0.02192047 -0.01829462
 -0.01466877 -0.01104292 -0.00741706 -0.00379121 -0.00016536]
[2660 1500 1339 1000  963 1020 1070 1053 1012  533] [-0.03642388 -0.03279803 -0.02917218 -0.02554633 -0.02192047 -0.01829462
 -0.01466877 -0.01104292 -0.00741706 -0.00379121 -0.00016536]
-0.823236
0.921125
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042826 minutes
Epoch 0
Fine tuning took 0.042848 minutes
Epoch 0
Fine tuning took 0.043257 minutes
{'zero': {0: [0.24876847290640394, 0.22536945812807882, 0.20935960591133004, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.56157635467980294, 0.56157635467980294, 0.63793103448275867], 5: [0.2413793103448276, 0.21305418719211822, 0.22906403940886699, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24876847290640394, 0.21551724137931033, 0.21182266009852216, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.58251231527093594, 0.55541871921182262, 0.63054187192118227], 5: [0.2413793103448276, 0.2019704433497537, 0.23275862068965517, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24876847290640394, 0.21428571428571427, 0.17980295566502463, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.56650246305418717, 0.5788177339901478, 0.61330049261083741], 5: [0.2413793103448276, 0.21921182266009853, 0.2413793103448276, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24876847290640394, 0.22783251231527094, 0.21305418719211822, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.56034482758620685, 0.56034482758620685, 0.63177339901477836], 5: [0.2413793103448276, 0.21182266009852216, 0.22660098522167488, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.260830 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.92124
Epoch 1, cost is  1.85845
Epoch 2, cost is  1.82588
Epoch 3, cost is  1.79891
Epoch 4, cost is  1.77737
Training took 0.227084 minutes
Weight histogram
[2641 2082 1939 1412 1464 1005  574  532  374  127] [ -7.60276616e-02  -6.84279598e-02  -6.08282580e-02  -5.32285563e-02
  -4.56288545e-02  -3.80291528e-02  -3.04294510e-02  -2.28297493e-02
  -1.52300475e-02  -7.63034575e-03  -3.06439943e-05]
[ 752  544  735  979 1139 1285 1477 1606 1780 1853] [ -7.60276616e-02  -6.84279598e-02  -6.08282580e-02  -5.32285563e-02
  -4.56288545e-02  -3.80291528e-02  -3.04294510e-02  -2.28297493e-02
  -1.52300475e-02  -7.63034575e-03  -3.06439943e-05]
-2.20903
2.39702
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.258581 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.18249
Epoch 1, cost is  3.09669
Epoch 2, cost is  3.07872
Epoch 3, cost is  3.06107
Epoch 4, cost is  3.05708
Training took 0.205229 minutes
Weight histogram
[2712 1602 1868 1726 1115  896  702 1439 1196  919] [ -1.18810810e-01  -1.06932794e-01  -9.50547769e-02  -8.31767603e-02
  -7.12987437e-02  -5.94207271e-02  -4.75427105e-02  -3.56646938e-02
  -2.37866772e-02  -1.19086606e-02  -3.06439943e-05]
[1960 1501  743  949 1175 1353 1347 1529 1824 1794] [ -1.18810810e-01  -1.06932794e-01  -9.50547769e-02  -8.31767603e-02
  -7.12987437e-02  -5.94207271e-02  -4.75427105e-02  -3.56646938e-02
  -2.37866772e-02  -1.19086606e-02  -3.06439943e-05]
-3.1403
3.64896
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.51715
Epoch 1, cost is  6.14698
Epoch 2, cost is  5.50579
Epoch 3, cost is  4.69933
Epoch 4, cost is  4.05936
Training took 0.354481 minutes
Weight histogram
[ 434 1789 1901 1573 1584 1301 2021 1233  221   93] [-0.02634971 -0.0237277  -0.02110569 -0.01848368 -0.01586167 -0.01323966
 -0.01061765 -0.00799564 -0.00537363 -0.00275162 -0.00012961]
[2563 1835 1438  959  947  940  928  965  973  602] [-0.02634971 -0.0237277  -0.02110569 -0.01848368 -0.01586167 -0.01323966
 -0.01061765 -0.00799564 -0.00537363 -0.00275162 -0.00012961]
-0.59627
0.829825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047061 minutes
Epoch 0
Fine tuning took 0.046610 minutes
Epoch 0
Fine tuning took 0.046548 minutes
{'zero': {0: [0.28817733990147781, 0.22783251231527094, 0.20566502463054187, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51231527093596063, 0.55541871921182262, 0.60960591133004927, 0.6354679802955665], 5: [0.19950738916256158, 0.21674876847290642, 0.18472906403940886, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28817733990147781, 0.18965517241379309, 0.21305418719211822, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51231527093596063, 0.60591133004926112, 0.57266009852216748, 0.63423645320197042], 5: [0.19950738916256158, 0.20443349753694581, 0.21428571428571427, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28817733990147781, 0.22536945812807882, 0.18103448275862069, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51231527093596063, 0.56773399014778325, 0.63669950738916259, 0.62438423645320196], 5: [0.19950738916256158, 0.20689655172413793, 0.18226600985221675, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28817733990147781, 0.20689655172413793, 0.18842364532019704, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51231527093596063, 0.55418719211822665, 0.6071428571428571, 0.6354679802955665], 5: [0.19950738916256158, 0.23891625615763548, 0.20443349753694581, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.256947 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.27181
Epoch 1, cost is  3.20555
Epoch 2, cost is  3.14984
Epoch 3, cost is  3.10207
Epoch 4, cost is  3.05831
Training took 0.225595 minutes
Weight histogram
[2178 1836 1668 1167 1163 1518  824  698  943  155] [ -3.95467803e-02  -3.55885276e-02  -3.16302749e-02  -2.76720222e-02
  -2.37137695e-02  -1.97555168e-02  -1.57972641e-02  -1.18390114e-02
  -7.88075871e-03  -3.92250602e-03   3.57466815e-05]
[1929 1061  818  839  952 1136 1212 1295 1423 1485] [ -3.95467803e-02  -3.55885276e-02  -3.16302749e-02  -2.76720222e-02
  -2.37137695e-02  -1.97555168e-02  -1.57972641e-02  -1.18390114e-02
  -7.88075871e-03  -3.92250602e-03   3.57466815e-05]
-0.897784
1.10236
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272933 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.05443
Epoch 1, cost is  3.99162
Epoch 2, cost is  3.93324
Epoch 3, cost is  3.89014
Epoch 4, cost is  3.84577
Training took 0.200181 minutes
Weight histogram
[1608 1644  966 1207 1122  977  948 1596 3609  498] [ -5.26216999e-02  -4.73559552e-02  -4.20902106e-02  -3.68244659e-02
  -3.15587212e-02  -2.62929766e-02  -2.10272319e-02  -1.57614873e-02
  -1.04957426e-02  -5.22999797e-03   3.57466815e-05]
[4412  954  837  920  993 1012 1126 1247 1295 1379] [ -5.26216999e-02  -4.73559552e-02  -4.20902106e-02  -3.68244659e-02
  -3.15587212e-02  -2.62929766e-02  -2.10272319e-02  -1.57614873e-02
  -1.04957426e-02  -5.22999797e-03   3.57466815e-05]
-0.953116
1.24265
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.85255
Epoch 1, cost is  6.76597
Epoch 2, cost is  6.69457
Epoch 3, cost is  6.62717
Epoch 4, cost is  6.55214
Training took 0.227006 minutes
Weight histogram
[1660 1783 6224  906  531  350  249  189  144  114] [ -1.02815339e-02  -9.25416925e-03  -8.22680456e-03  -7.19943987e-03
  -6.17207518e-03  -5.14471049e-03  -4.11734580e-03  -3.08998111e-03
  -2.06261642e-03  -1.03525173e-03  -7.88704347e-06]
[4724 2173 1680 1397  920  487  288  232  126  123] [ -1.02815339e-02  -9.25416925e-03  -8.22680456e-03  -7.19943987e-03
  -6.17207518e-03  -5.14471049e-03  -4.11734580e-03  -3.08998111e-03
  -2.06261642e-03  -1.03525173e-03  -7.88704347e-06]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041203 minutes
Epoch 0
Fine tuning took 0.041130 minutes
Epoch 0
Fine tuning took 0.041007 minutes
{'zero': {0: [0.36699507389162561, 0.22536945812807882, 0.16995073891625614, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.53940886699507384, 0.70812807881773399, 0.65024630541871919], 5: [0.10714285714285714, 0.23522167487684728, 0.12192118226600986, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.36699507389162561, 0.16995073891625614, 0.17857142857142858, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.56773399014778325, 0.68719211822660098, 0.6280788177339901], 5: [0.10714285714285714, 0.26231527093596058, 0.13423645320197045, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.36699507389162561, 0.1625615763546798, 0.15270935960591134, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.58866995073891626, 0.71674876847290636, 0.63669950738916259], 5: [0.10714285714285714, 0.24876847290640394, 0.13054187192118227, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.36699507389162561, 0.19581280788177341, 0.16748768472906403, 0.19458128078817735], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52586206896551724, 0.55049261083743839, 0.71921182266009853, 0.64408866995073888], 5: [0.10714285714285714, 0.2536945812807882, 0.11330049261083744, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.260901 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.27181
Epoch 1, cost is  3.20555
Epoch 2, cost is  3.14984
Epoch 3, cost is  3.10207
Epoch 4, cost is  3.05831
Training took 0.226487 minutes
Weight histogram
[2178 1836 1668 1167 1163 1518  824  698  943  155] [ -3.95467803e-02  -3.55885276e-02  -3.16302749e-02  -2.76720222e-02
  -2.37137695e-02  -1.97555168e-02  -1.57972641e-02  -1.18390114e-02
  -7.88075871e-03  -3.92250602e-03   3.57466815e-05]
[1929 1061  818  839  952 1136 1212 1295 1423 1485] [ -3.95467803e-02  -3.55885276e-02  -3.16302749e-02  -2.76720222e-02
  -2.37137695e-02  -1.97555168e-02  -1.57972641e-02  -1.18390114e-02
  -7.88075871e-03  -3.92250602e-03   3.57466815e-05]
-0.897784
1.10236
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.260918 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.05443
Epoch 1, cost is  3.99162
Epoch 2, cost is  3.93324
Epoch 3, cost is  3.89014
Epoch 4, cost is  3.84577
Training took 0.199972 minutes
Weight histogram
[1608 1644  966 1207 1122  977  948 1596 3609  498] [ -5.26216999e-02  -4.73559552e-02  -4.20902106e-02  -3.68244659e-02
  -3.15587212e-02  -2.62929766e-02  -2.10272319e-02  -1.57614873e-02
  -1.04957426e-02  -5.22999797e-03   3.57466815e-05]
[4412  954  837  920  993 1012 1126 1247 1295 1379] [ -5.26216999e-02  -4.73559552e-02  -4.20902106e-02  -3.68244659e-02
  -3.15587212e-02  -2.62929766e-02  -2.10272319e-02  -1.57614873e-02
  -1.04957426e-02  -5.22999797e-03   3.57466815e-05]
-0.953116
1.24265
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78112
Epoch 1, cost is  6.66114
Epoch 2, cost is  6.57646
Epoch 3, cost is  6.49445
Epoch 4, cost is  6.41061
Training took 0.264816 minutes
Weight histogram
[ 623 1144 2183 5336 1223  639  394  271  193  144] [ -1.24916639e-02  -1.12485048e-02  -1.00053457e-02  -8.76218659e-03
  -7.51902749e-03  -6.27586838e-03  -5.03270927e-03  -3.78955016e-03
  -2.54639105e-03  -1.30323195e-03  -6.00728381e-05]
[4145 1986 1590 1394 1253  660  448  314  196  164] [ -1.24916639e-02  -1.12485048e-02  -1.00053457e-02  -8.76218659e-03
  -7.51902749e-03  -6.27586838e-03  -5.03270927e-03  -3.78955016e-03
  -2.54639105e-03  -1.30323195e-03  -6.00728381e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043660 minutes
Epoch 0
Fine tuning took 0.043098 minutes
Epoch 0
Fine tuning took 0.046537 minutes
{'zero': {0: [0.3682266009852217, 0.2376847290640394, 0.16748768472906403, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.49753694581280788, 0.7068965517241379, 0.53940886699507384], 5: [0.077586206896551727, 0.26477832512315269, 0.12561576354679804, 0.26354679802955666], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3682266009852217, 0.22906403940886699, 0.15270935960591134, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.5357142857142857, 0.74137931034482762, 0.52339901477832518], 5: [0.077586206896551727, 0.23522167487684728, 0.10591133004926108, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3682266009852217, 0.23645320197044334, 0.16871921182266009, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.52955665024630538, 0.70320197044334976, 0.51477832512315269], 5: [0.077586206896551727, 0.23399014778325122, 0.12807881773399016, 0.29310344827586204], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3682266009852217, 0.25985221674876846, 0.16379310344827586, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55418719211822665, 0.50123152709359609, 0.71798029556650245, 0.50492610837438423], 5: [0.077586206896551727, 0.23891625615763548, 0.11822660098522167, 0.29926108374384236], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269497 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.27181
Epoch 1, cost is  3.20555
Epoch 2, cost is  3.14984
Epoch 3, cost is  3.10207
Epoch 4, cost is  3.05831
Training took 0.226878 minutes
Weight histogram
[2178 1836 1668 1167 1163 1518  824  698  943  155] [ -3.95467803e-02  -3.55885276e-02  -3.16302749e-02  -2.76720222e-02
  -2.37137695e-02  -1.97555168e-02  -1.57972641e-02  -1.18390114e-02
  -7.88075871e-03  -3.92250602e-03   3.57466815e-05]
[1929 1061  818  839  952 1136 1212 1295 1423 1485] [ -3.95467803e-02  -3.55885276e-02  -3.16302749e-02  -2.76720222e-02
  -2.37137695e-02  -1.97555168e-02  -1.57972641e-02  -1.18390114e-02
  -7.88075871e-03  -3.92250602e-03   3.57466815e-05]
-0.897784
1.10236
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264065 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.05443
Epoch 1, cost is  3.99162
Epoch 2, cost is  3.93324
Epoch 3, cost is  3.89014
Epoch 4, cost is  3.84577
Training took 0.200519 minutes
Weight histogram
[1608 1644  966 1207 1122  977  948 1596 3609  498] [ -5.26216999e-02  -4.73559552e-02  -4.20902106e-02  -3.68244659e-02
  -3.15587212e-02  -2.62929766e-02  -2.10272319e-02  -1.57614873e-02
  -1.04957426e-02  -5.22999797e-03   3.57466815e-05]
[4412  954  837  920  993 1012 1126 1247 1295 1379] [ -5.26216999e-02  -4.73559552e-02  -4.20902106e-02  -3.68244659e-02
  -3.15587212e-02  -2.62929766e-02  -2.10272319e-02  -1.57614873e-02
  -1.04957426e-02  -5.22999797e-03   3.57466815e-05]
-0.953116
1.24265
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.66944
Epoch 1, cost is  6.4984
Epoch 2, cost is  6.39722
Epoch 3, cost is  6.30729
Epoch 4, cost is  6.21682
Training took 0.363677 minutes
Weight histogram
[ 558  838 1426 4501 2517  994  564  349  237  166] [ -1.41741699e-02  -1.27594414e-02  -1.13447129e-02  -9.92998444e-03
  -8.51525597e-03  -7.10052750e-03  -5.68579903e-03  -4.27107055e-03
  -2.85634208e-03  -1.44161361e-03  -2.68851381e-05]
[3578 1817 1530 1410 1354 1008  582  401  259  211] [ -1.41741699e-02  -1.27594414e-02  -1.13447129e-02  -9.92998444e-03
  -8.51525597e-03  -7.10052750e-03  -5.68579903e-03  -4.27107055e-03
  -2.85634208e-03  -1.44161361e-03  -2.68851381e-05]
-0.0721031
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049706 minutes
Epoch 0
Fine tuning took 0.045966 minutes
Epoch 0
Fine tuning took 0.046283 minutes
{'zero': {0: [0.33620689655172414, 0.27339901477832512, 0.20689655172413793, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.47783251231527096, 0.71305418719211822, 0.4963054187192118], 5: [0.092364532019704432, 0.24876847290640394, 0.080049261083743842, 0.27832512315270935], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33620689655172414, 0.30541871921182268, 0.21428571428571427, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.45443349753694579, 0.7068965517241379, 0.45935960591133007], 5: [0.092364532019704432, 0.24014778325123154, 0.078817733990147784, 0.29310344827586204], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33620689655172414, 0.2536945812807882, 0.21674876847290642, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.47783251231527096, 0.69334975369458129, 0.4963054187192118], 5: [0.092364532019704432, 0.26847290640394089, 0.089901477832512317, 0.27955665024630544], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33620689655172414, 0.28448275862068967, 0.24261083743842365, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5714285714285714, 0.46551724137931033, 0.67980295566502458, 0.45073891625615764], 5: [0.092364532019704432, 0.25, 0.077586206896551727, 0.30911330049261082], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272786 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.11932
Epoch 1, cost is  2.91906
Epoch 2, cost is  2.87427
Epoch 3, cost is  2.86689
Epoch 4, cost is  2.87787
Training took 0.230557 minutes
Weight histogram
[1854 1577 1496 1467 1598 1606 1318  770  348  116] [-0.25271615 -0.22752229 -0.20232843 -0.17713457 -0.15194071 -0.12674685
 -0.10155299 -0.07635912 -0.05116526 -0.0259714  -0.00077754]
[ 342  641  942 1290 1346 1375 1458 1553 1631 1572] [-0.25271615 -0.22752229 -0.20232843 -0.17713457 -0.15194071 -0.12674685
 -0.10155299 -0.07635912 -0.05116526 -0.0259714  -0.00077754]
-11.9047
17.2366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272817 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.5468
Epoch 1, cost is  8.1472
Epoch 2, cost is  8.15256
Epoch 3, cost is  8.2328
Epoch 4, cost is  8.35684
Training took 0.202210 minutes
Weight histogram
[1555 1313 1252 1407 1429 1334 1500 1334 2241  810] [-0.5484913  -0.49371992 -0.43894855 -0.38417717 -0.3294058  -0.27463442
 -0.21986304 -0.16509167 -0.11032029 -0.05554892 -0.00077754]
[1694 1819 1205 1259 1331 1277 1225 1449 1474 1442] [-0.5484913  -0.49371992 -0.43894855 -0.38417717 -0.3294058  -0.27463442
 -0.21986304 -0.16509167 -0.11032029 -0.05554892 -0.00077754]
-18.7384
20.4333
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.13244
Epoch 1, cost is  3.84292
Epoch 2, cost is  3.67181
Epoch 3, cost is  3.78349
Epoch 4, cost is  3.93302
Training took 0.236793 minutes
Weight histogram
[1580 2013 2161 1838 1544 1038  693  479  359  445] [-0.17713617 -0.15954824 -0.14196031 -0.12437237 -0.10678444 -0.08919651
 -0.07160858 -0.05402065 -0.03643272 -0.01884479 -0.00125685]
[ 750  701  864 1028 1169 1320 1464 1568 1664 1622] [-0.17713617 -0.15954824 -0.14196031 -0.12437237 -0.10678444 -0.08919651
 -0.07160858 -0.05402065 -0.03643272 -0.01884479 -0.00125685]
-6.86173
7.17654
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042583 minutes
Epoch 0
Fine tuning took 0.042638 minutes
Epoch 0
Fine tuning took 0.046000 minutes
{'zero': {0: [0.26600985221674878, 0.1539408866995074, 0.19088669950738915, 0.27216748768472904], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50615763546798032, 0.56650246305418717, 0.5788177339901478, 0.55911330049261088], 5: [0.22783251231527094, 0.27955665024630544, 0.23029556650246305, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26600985221674878, 0.16009852216748768, 0.2229064039408867, 0.25], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50615763546798032, 0.58251231527093594, 0.58743842364532017, 0.55541871921182262], 5: [0.22783251231527094, 0.25738916256157635, 0.18965517241379309, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26600985221674878, 0.18719211822660098, 0.20566502463054187, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50615763546798032, 0.5431034482758621, 0.57389162561576357, 0.5431034482758621], 5: [0.22783251231527094, 0.26970443349753692, 0.22044334975369459, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26600985221674878, 0.16871921182266009, 0.21674876847290642, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50615763546798032, 0.59113300492610843, 0.6071428571428571, 0.56650246305418717], 5: [0.22783251231527094, 0.24014778325123154, 0.17610837438423646, 0.17241379310344829], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268738 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.11932
Epoch 1, cost is  2.91906
Epoch 2, cost is  2.87427
Epoch 3, cost is  2.86689
Epoch 4, cost is  2.87787
Training took 0.225856 minutes
Weight histogram
[1854 1577 1496 1467 1598 1606 1318  770  348  116] [-0.25271615 -0.22752229 -0.20232843 -0.17713457 -0.15194071 -0.12674685
 -0.10155299 -0.07635912 -0.05116526 -0.0259714  -0.00077754]
[ 342  641  942 1290 1346 1375 1458 1553 1631 1572] [-0.25271615 -0.22752229 -0.20232843 -0.17713457 -0.15194071 -0.12674685
 -0.10155299 -0.07635912 -0.05116526 -0.0259714  -0.00077754]
-11.9047
17.2366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269819 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.5468
Epoch 1, cost is  8.1472
Epoch 2, cost is  8.15256
Epoch 3, cost is  8.2328
Epoch 4, cost is  8.35684
Training took 0.204294 minutes
Weight histogram
[1555 1313 1252 1407 1429 1334 1500 1334 2241  810] [-0.5484913  -0.49371992 -0.43894855 -0.38417717 -0.3294058  -0.27463442
 -0.21986304 -0.16509167 -0.11032029 -0.05554892 -0.00077754]
[1694 1819 1205 1259 1331 1277 1225 1449 1474 1442] [-0.5484913  -0.49371992 -0.43894855 -0.38417717 -0.3294058  -0.27463442
 -0.21986304 -0.16509167 -0.11032029 -0.05554892 -0.00077754]
-18.7384
20.4333
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.76693
Epoch 1, cost is  2.90558
Epoch 2, cost is  2.51561
Epoch 3, cost is  2.42133
Epoch 4, cost is  2.4387
Training took 0.276809 minutes
Weight histogram
[1664 2243 1936 1705 1483 1088  748  524  379  380] [-0.11230296 -0.10119887 -0.09009478 -0.0789907  -0.06788661 -0.05678252
 -0.04567843 -0.03457435 -0.02347026 -0.01236617 -0.00126208]
[ 821  683  822  945 1105 1374 1454 1617 1702 1627] [-0.11230296 -0.10119887 -0.09009478 -0.0789907  -0.06788661 -0.05678252
 -0.04567843 -0.03457435 -0.02347026 -0.01236617 -0.00126208]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048182 minutes
Epoch 0
Fine tuning took 0.044516 minutes
Epoch 0
Fine tuning took 0.043682 minutes
{'zero': {0: [0.15640394088669951, 0.33374384236453203, 0.16009852216748768, 0.32758620689655171], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.58497536945812811, 0.66009852216748766, 0.53940886699507384], 5: [0.14039408866995073, 0.081280788177339899, 0.17980295566502463, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15640394088669951, 0.19334975369458129, 0.13300492610837439, 0.13793103448275862], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.62684729064039413, 0.55172413793103448, 0.62438423645320196], 5: [0.14039408866995073, 0.17980295566502463, 0.31527093596059114, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15640394088669951, 0.20935960591133004, 0.1539408866995074, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.63177339901477836, 0.5923645320197044, 0.60960591133004927], 5: [0.14039408866995073, 0.15886699507389163, 0.2536945812807882, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15640394088669951, 0.16502463054187191, 0.12192118226600986, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70320197044334976, 0.65394088669950734, 0.51354679802955661, 0.63300492610837433], 5: [0.14039408866995073, 0.18103448275862069, 0.3645320197044335, 0.25985221674876846], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268884 minutes
Weight histogram
[ 173  446  567  640 1438 2512 2770 2411 1139   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 122  142  203  254  405  628  831 1176 2644 5745] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.11932
Epoch 1, cost is  2.91906
Epoch 2, cost is  2.87427
Epoch 3, cost is  2.86689
Epoch 4, cost is  2.87787
Training took 0.229444 minutes
Weight histogram
[1854 1577 1496 1467 1598 1606 1318  770  348  116] [-0.25271615 -0.22752229 -0.20232843 -0.17713457 -0.15194071 -0.12674685
 -0.10155299 -0.07635912 -0.05116526 -0.0259714  -0.00077754]
[ 342  641  942 1290 1346 1375 1458 1553 1631 1572] [-0.25271615 -0.22752229 -0.20232843 -0.17713457 -0.15194071 -0.12674685
 -0.10155299 -0.07635912 -0.05116526 -0.0259714  -0.00077754]
-11.9047
17.2366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264781 minutes
Weight histogram
[ 179  574  983 1541 1983 2767 3037 2149  836  126] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 243  283  398  506  770 1318  974 1220 2856 5607] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  8.5468
Epoch 1, cost is  8.1472
Epoch 2, cost is  8.15256
Epoch 3, cost is  8.2328
Epoch 4, cost is  8.35684
Training took 0.198054 minutes
Weight histogram
[1555 1313 1252 1407 1429 1334 1500 1334 2241  810] [-0.5484913  -0.49371992 -0.43894855 -0.38417717 -0.3294058  -0.27463442
 -0.21986304 -0.16509167 -0.11032029 -0.05554892 -0.00077754]
[1694 1819 1205 1259 1331 1277 1225 1449 1474 1442] [-0.5484913  -0.49371992 -0.43894855 -0.38417717 -0.3294058  -0.27463442
 -0.21986304 -0.16509167 -0.11032029 -0.05554892 -0.00077754]
-18.7384
20.4333
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.52773
Epoch 1, cost is  2.36772
Epoch 2, cost is  1.8416
Epoch 3, cost is  1.66182
Epoch 4, cost is  1.56747
Training took 0.368354 minutes
Weight histogram
[2088 2314 1991 1607 1311 1091  659  538  364  187] [-0.07301652 -0.06583861 -0.0586607  -0.05148278 -0.04430487 -0.03712696
 -0.02994905 -0.02277113 -0.01559322 -0.00841531 -0.00123739]
[ 829  629  729  905 1101 1313 1493 1665 1806 1680] [-0.07301652 -0.06583861 -0.0586607  -0.05148278 -0.04430487 -0.03712696
 -0.02994905 -0.02277113 -0.01559322 -0.00841531 -0.00123739]
-3.98509
4.26369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046816 minutes
Epoch 0
Fine tuning took 0.047503 minutes
Epoch 0
Fine tuning took 0.047173 minutes
{'zero': {0: [0.12561576354679804, 0.16009852216748768, 0.093596059113300489, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.79187192118226601, 0.80665024630541871, 0.75985221674876846], 5: [0.10960591133004927, 0.048029556650246302, 0.099753694581280791, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12561576354679804, 0.091133004926108374, 0.082512315270935957, 0.18226600985221675], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.79556650246305416, 0.73645320197044339, 0.69704433497536944], 5: [0.10960591133004927, 0.11330049261083744, 0.18103448275862069, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12561576354679804, 0.10591133004926108, 0.11330049261083744, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.74137931034482762, 0.71921182266009853, 0.67487684729064035], 5: [0.10960591133004927, 0.15270935960591134, 0.16748768472906403, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12561576354679804, 0.10467980295566502, 0.092364532019704432, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76477832512315269, 0.81034482758620685, 0.73768472906403937, 0.71059113300492616], 5: [0.10960591133004927, 0.084975369458128072, 0.16995073891625614, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.273179 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.96615
Epoch 1, cost is  1.87087
Epoch 2, cost is  1.83003
Epoch 3, cost is  1.81097
Epoch 4, cost is  1.79321
Training took 0.234576 minutes
Weight histogram
[1968 2515 2202 2225 1622 1599  990  526  334  194] [ -1.15361430e-01  -1.03836650e-01  -9.23118703e-02  -8.07870905e-02
  -6.92623108e-02  -5.77375310e-02  -4.62127513e-02  -3.46879716e-02
  -2.31631918e-02  -1.16384121e-02  -1.13632348e-04]
[ 556  617  871 1222 1408 1570 1750 1962 2044 2175] [ -1.15361430e-01  -1.03836650e-01  -9.23118703e-02  -8.07870905e-02
  -6.92623108e-02  -5.77375310e-02  -4.62127513e-02  -3.46879716e-02
  -2.31631918e-02  -1.16384121e-02  -1.13632348e-04]
-3.56243
5.11192
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.259834 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.61705
Epoch 1, cost is  3.50233
Epoch 2, cost is  3.47567
Epoch 3, cost is  3.47496
Epoch 4, cost is  3.47207
Training took 0.200763 minutes
Weight histogram
[2007 2241 1873 2048 1936 1531 1037 1355 1360  812] [ -1.91154480e-01  -1.72050395e-01  -1.52946310e-01  -1.33842226e-01
  -1.14738141e-01  -9.56340562e-02  -7.65299714e-02  -5.74258866e-02
  -3.83218019e-02  -1.92177171e-02  -1.13632348e-04]
[1676 1695 1010 1273 1485 1504 1545 2031 1919 2062] [ -1.91154480e-01  -1.72050395e-01  -1.52946310e-01  -1.33842226e-01
  -1.14738141e-01  -9.56340562e-02  -7.65299714e-02  -5.74258866e-02
  -3.83218019e-02  -1.92177171e-02  -1.13632348e-04]
-5.43093
6.26193
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.55946
Epoch 1, cost is  5.4076
Epoch 2, cost is  4.59804
Epoch 3, cost is  4.19851
Epoch 4, cost is  3.95621
Training took 0.234494 minutes
Weight histogram
[2310 1768 1662 1574 1153  998  902 1008  714 2086] [-0.08295341 -0.07468115 -0.0664089  -0.05813664 -0.04986438 -0.04159213
 -0.03331987 -0.02504761 -0.01677536 -0.0085031  -0.00023084]
[2150 1234  975 1107 1185 1305 1444 1617 1759 1399] [-0.08295341 -0.07468115 -0.0664089  -0.05813664 -0.04986438 -0.04159213
 -0.03331987 -0.02504761 -0.01677536 -0.0085031  -0.00023084]
-1.6243
2.76155
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041785 minutes
Epoch 0
Fine tuning took 0.041768 minutes
Epoch 0
Fine tuning took 0.041570 minutes
{'zero': {0: [0.22536945812807882, 0.17364532019704434, 0.23522167487684728, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.67980295566502458, 0.63423645320197042, 0.71551724137931039], 5: [0.14408866995073891, 0.14655172413793102, 0.13054187192118227, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22536945812807882, 0.17241379310344829, 0.21921182266009853, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.68349753694581283, 0.65517241379310343, 0.70320197044334976], 5: [0.14408866995073891, 0.14408866995073891, 0.12561576354679804, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22536945812807882, 0.19334975369458129, 0.22783251231527094, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.65886699507389157, 0.66625615763546797, 0.70073891625615758], 5: [0.14408866995073891, 0.14778325123152711, 0.10591133004926108, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22536945812807882, 0.1748768472906404, 0.2413793103448276, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63054187192118227, 0.7068965517241379, 0.65024630541871919, 0.68472906403940892], 5: [0.14408866995073891, 0.11822660098522167, 0.10837438423645321, 0.11206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.280601 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.96615
Epoch 1, cost is  1.87087
Epoch 2, cost is  1.83003
Epoch 3, cost is  1.81097
Epoch 4, cost is  1.79321
Training took 0.229717 minutes
Weight histogram
[1968 2515 2202 2225 1622 1599  990  526  334  194] [ -1.15361430e-01  -1.03836650e-01  -9.23118703e-02  -8.07870905e-02
  -6.92623108e-02  -5.77375310e-02  -4.62127513e-02  -3.46879716e-02
  -2.31631918e-02  -1.16384121e-02  -1.13632348e-04]
[ 556  617  871 1222 1408 1570 1750 1962 2044 2175] [ -1.15361430e-01  -1.03836650e-01  -9.23118703e-02  -8.07870905e-02
  -6.92623108e-02  -5.77375310e-02  -4.62127513e-02  -3.46879716e-02
  -2.31631918e-02  -1.16384121e-02  -1.13632348e-04]
-3.56243
5.11192
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271735 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.61705
Epoch 1, cost is  3.50233
Epoch 2, cost is  3.47567
Epoch 3, cost is  3.47496
Epoch 4, cost is  3.47207
Training took 0.200556 minutes
Weight histogram
[2007 2241 1873 2048 1936 1531 1037 1355 1360  812] [ -1.91154480e-01  -1.72050395e-01  -1.52946310e-01  -1.33842226e-01
  -1.14738141e-01  -9.56340562e-02  -7.65299714e-02  -5.74258866e-02
  -3.83218019e-02  -1.92177171e-02  -1.13632348e-04]
[1676 1695 1010 1273 1485 1504 1545 2031 1919 2062] [ -1.91154480e-01  -1.72050395e-01  -1.52946310e-01  -1.33842226e-01
  -1.14738141e-01  -9.56340562e-02  -7.65299714e-02  -5.74258866e-02
  -3.83218019e-02  -1.92177171e-02  -1.13632348e-04]
-5.43093
6.26193
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46748
Epoch 1, cost is  5.24743
Epoch 2, cost is  4.13118
Epoch 3, cost is  3.50585
Epoch 4, cost is  3.0917
Training took 0.274707 minutes
Weight histogram
[1836 2448 1789 1549 1286 1030 1118  904 2028  187] [-0.05099323 -0.04592217 -0.04085112 -0.03578007 -0.03070902 -0.02563796
 -0.02056691 -0.01549586 -0.01042481 -0.00535376 -0.0002827 ]
[2286 1281 1061 1149 1152 1285 1424 1528 1625 1384] [-0.05099323 -0.04592217 -0.04085112 -0.03578007 -0.03070902 -0.02563796
 -0.02056691 -0.01549586 -0.01042481 -0.00535376 -0.0002827 ]
-1.2098
1.51827
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044234 minutes
Epoch 0
Fine tuning took 0.043375 minutes
Epoch 0
Fine tuning took 0.043126 minutes
{'zero': {0: [0.20566502463054187, 0.21182266009852216, 0.24384236453201971, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.68596059113300489, 0.65024630541871919, 0.6785714285714286], 5: [0.077586206896551727, 0.10221674876847291, 0.10591133004926108, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20566502463054187, 0.20566502463054187, 0.2229064039408867, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.6785714285714286, 0.66748768472906406, 0.73152709359605916], 5: [0.077586206896551727, 0.11576354679802955, 0.10960591133004927, 0.082512315270935957], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20566502463054187, 0.19950738916256158, 0.22413793103448276, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.69458128078817738, 0.65640394088669951, 0.70566502463054193], 5: [0.077586206896551727, 0.10591133004926108, 0.11945812807881774, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20566502463054187, 0.21798029556650247, 0.20812807881773399, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71674876847290636, 0.66748768472906406, 0.69088669950738912, 0.70566502463054193], 5: [0.077586206896551727, 0.1145320197044335, 0.10098522167487685, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271326 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.96615
Epoch 1, cost is  1.87087
Epoch 2, cost is  1.83003
Epoch 3, cost is  1.81097
Epoch 4, cost is  1.79321
Training took 0.222192 minutes
Weight histogram
[1968 2515 2202 2225 1622 1599  990  526  334  194] [ -1.15361430e-01  -1.03836650e-01  -9.23118703e-02  -8.07870905e-02
  -6.92623108e-02  -5.77375310e-02  -4.62127513e-02  -3.46879716e-02
  -2.31631918e-02  -1.16384121e-02  -1.13632348e-04]
[ 556  617  871 1222 1408 1570 1750 1962 2044 2175] [ -1.15361430e-01  -1.03836650e-01  -9.23118703e-02  -8.07870905e-02
  -6.92623108e-02  -5.77375310e-02  -4.62127513e-02  -3.46879716e-02
  -2.31631918e-02  -1.16384121e-02  -1.13632348e-04]
-3.56243
5.11192
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266407 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.61705
Epoch 1, cost is  3.50233
Epoch 2, cost is  3.47567
Epoch 3, cost is  3.47496
Epoch 4, cost is  3.47207
Training took 0.202647 minutes
Weight histogram
[2007 2241 1873 2048 1936 1531 1037 1355 1360  812] [ -1.91154480e-01  -1.72050395e-01  -1.52946310e-01  -1.33842226e-01
  -1.14738141e-01  -9.56340562e-02  -7.65299714e-02  -5.74258866e-02
  -3.83218019e-02  -1.92177171e-02  -1.13632348e-04]
[1676 1695 1010 1273 1485 1504 1545 2031 1919 2062] [ -1.91154480e-01  -1.72050395e-01  -1.52946310e-01  -1.33842226e-01
  -1.14738141e-01  -9.56340562e-02  -7.65299714e-02  -5.74258866e-02
  -3.83218019e-02  -1.92177171e-02  -1.13632348e-04]
-5.43093
6.26193
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.35411
Epoch 1, cost is  5.15109
Epoch 2, cost is  3.92245
Epoch 3, cost is  3.12579
Epoch 4, cost is  2.6103
Training took 0.358147 minutes
Weight histogram
[1993 2566 2035 1696 1379 1324 1078 1631  386   87] [-0.03414995 -0.03075964 -0.02736933 -0.02397902 -0.02058872 -0.01719841
 -0.0138081  -0.01041779 -0.00702749 -0.00363718 -0.00024687]
[2501 1303 1057 1052 1151 1269 1347 1409 1616 1470] [-0.03414995 -0.03075964 -0.02736933 -0.02397902 -0.02058872 -0.01719841
 -0.0138081  -0.01041779 -0.00702749 -0.00363718 -0.00024687]
-0.957712
1.40444
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046792 minutes
Epoch 0
Fine tuning took 0.046700 minutes
Epoch 0
Fine tuning took 0.046798 minutes
{'zero': {0: [0.3682266009852217, 0.27216748768472904, 0.33374384236453203, 0.35344827586206895], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.52216748768472909, 0.50246305418719217, 0.53817733990147787], 5: [0.11576354679802955, 0.20566502463054187, 0.16379310344827586, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3682266009852217, 0.2376847290640394, 0.25123152709359609, 0.29064039408866993], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.63054187192118227, 0.60344827586206895, 0.60960591133004927], 5: [0.11576354679802955, 0.13177339901477833, 0.14532019704433496, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3682266009852217, 0.22536945812807882, 0.29556650246305421, 0.27832512315270935], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.60221674876847286, 0.5788177339901478, 0.6071428571428571], 5: [0.11576354679802955, 0.17241379310344829, 0.12561576354679804, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3682266009852217, 0.24014778325123154, 0.26231527093596058, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.60960591133004927, 0.57266009852216748, 0.64901477832512311], 5: [0.11576354679802955, 0.15024630541871922, 0.16502463054187191, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265248 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.90185
Epoch 1, cost is  1.82991
Epoch 2, cost is  1.79615
Epoch 3, cost is  1.77405
Epoch 4, cost is  1.75635
Training took 0.227363 minutes
Weight histogram
[2015 2852 2470 1922 1497 1375  838  589  351  266] [ -8.45727921e-02  -7.61185772e-02  -6.76643624e-02  -5.92101476e-02
  -5.07559328e-02  -4.23017180e-02  -3.38475032e-02  -2.53932884e-02
  -1.69390736e-02  -8.48485880e-03  -3.06439943e-05]
[ 798  628  885 1127 1329 1514 1700 1907 2040 2247] [ -8.45727921e-02  -7.61185772e-02  -6.76643624e-02  -5.92101476e-02
  -5.07559328e-02  -4.23017180e-02  -3.38475032e-02  -2.53932884e-02
  -1.69390736e-02  -8.48485880e-03  -3.06439943e-05]
-2.39805
2.66878
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269943 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.09656
Epoch 1, cost is  3.00736
Epoch 2, cost is  2.98179
Epoch 3, cost is  2.9662
Epoch 4, cost is  2.95513
Training took 0.199672 minutes
Weight histogram
[2020 3068 1620 2174 1516 1191  823 1372 1392 1024] [ -1.32013649e-01  -1.18815348e-01  -1.05617048e-01  -9.24187473e-02
  -7.92204468e-02  -6.60221464e-02  -5.28238459e-02  -3.96255454e-02
  -2.64272449e-02  -1.32289445e-02  -3.06439943e-05]
[2109 1488  874 1131 1407 1503 1506 2023 1984 2175] [ -1.32013649e-01  -1.18815348e-01  -1.05617048e-01  -9.24187473e-02
  -7.92204468e-02  -6.60221464e-02  -5.28238459e-02  -3.96255454e-02
  -2.64272449e-02  -1.32289445e-02  -3.06439943e-05]
-3.5358
3.89294
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74484
Epoch 1, cost is  6.35345
Epoch 2, cost is  5.66434
Epoch 3, cost is  5.07117
Epoch 4, cost is  4.67515
Training took 0.235493 minutes
Weight histogram
[ 651 1632 1307 1360 1057 1179 1217 1049 4302  421] [-0.0558529  -0.05027876 -0.04470462 -0.03913048 -0.03355634 -0.0279822
 -0.02240806 -0.01683392 -0.01125978 -0.00568564 -0.0001115 ]
[3312 1444 1448 1153 1094 1248 1321 1319 1172  664] [-0.0558529  -0.05027876 -0.04470462 -0.03913048 -0.03355634 -0.0279822
 -0.02240806 -0.01683392 -0.01125978 -0.00568564 -0.0001115 ]
-0.925005
1.26551
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041677 minutes
Epoch 0
Fine tuning took 0.041559 minutes
Epoch 0
Fine tuning took 0.041794 minutes
{'zero': {0: [0.22660098522167488, 0.20443349753694581, 0.20689655172413793, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.68965517241379315, 0.69211822660098521, 0.68965517241379315], 5: [0.088669950738916259, 0.10591133004926108, 0.10098522167487685, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22660098522167488, 0.18596059113300492, 0.22413793103448276, 0.24630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.71798029556650245, 0.68472906403940892, 0.67487684729064035], 5: [0.088669950738916259, 0.096059113300492605, 0.091133004926108374, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22660098522167488, 0.18103448275862069, 0.2413793103448276, 0.22783251231527094], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.72906403940886699, 0.64655172413793105, 0.67733990147783252], 5: [0.088669950738916259, 0.089901477832512317, 0.11206896551724138, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22660098522167488, 0.17241379310344829, 0.21798029556650247, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68472906403940892, 0.73152709359605916, 0.68103448275862066, 0.70073891625615758], 5: [0.088669950738916259, 0.096059113300492605, 0.10098522167487685, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271208 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.90185
Epoch 1, cost is  1.82991
Epoch 2, cost is  1.79615
Epoch 3, cost is  1.77405
Epoch 4, cost is  1.75635
Training took 0.230600 minutes
Weight histogram
[2015 2852 2470 1922 1497 1375  838  589  351  266] [ -8.45727921e-02  -7.61185772e-02  -6.76643624e-02  -5.92101476e-02
  -5.07559328e-02  -4.23017180e-02  -3.38475032e-02  -2.53932884e-02
  -1.69390736e-02  -8.48485880e-03  -3.06439943e-05]
[ 798  628  885 1127 1329 1514 1700 1907 2040 2247] [ -8.45727921e-02  -7.61185772e-02  -6.76643624e-02  -5.92101476e-02
  -5.07559328e-02  -4.23017180e-02  -3.38475032e-02  -2.53932884e-02
  -1.69390736e-02  -8.48485880e-03  -3.06439943e-05]
-2.39805
2.66878
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.282153 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.09656
Epoch 1, cost is  3.00736
Epoch 2, cost is  2.98179
Epoch 3, cost is  2.9662
Epoch 4, cost is  2.95513
Training took 0.202064 minutes
Weight histogram
[2020 3068 1620 2174 1516 1191  823 1372 1392 1024] [ -1.32013649e-01  -1.18815348e-01  -1.05617048e-01  -9.24187473e-02
  -7.92204468e-02  -6.60221464e-02  -5.28238459e-02  -3.96255454e-02
  -2.64272449e-02  -1.32289445e-02  -3.06439943e-05]
[2109 1488  874 1131 1407 1503 1506 2023 1984 2175] [ -1.32013649e-01  -1.18815348e-01  -1.05617048e-01  -9.24187473e-02
  -7.92204468e-02  -6.60221464e-02  -5.28238459e-02  -3.96255454e-02
  -2.64272449e-02  -1.32289445e-02  -3.06439943e-05]
-3.5358
3.89294
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.6517
Epoch 1, cost is  6.25804
Epoch 2, cost is  5.56528
Epoch 3, cost is  4.85886
Epoch 4, cost is  4.26864
Training took 0.273569 minutes
Weight histogram
[ 354 1776 1806 1522 1417 1476 1214 2763 1667  180] [-0.03642388 -0.03279739 -0.0291709  -0.02554441 -0.02191792 -0.01829142
 -0.01466493 -0.01103844 -0.00741195 -0.00378545 -0.00015896]
[3083 1769 1545 1147 1115 1179 1240 1220 1176  701] [-0.03642388 -0.03279739 -0.0291709  -0.02554441 -0.02191792 -0.01829142
 -0.01466493 -0.01103844 -0.00741195 -0.00378545 -0.00015896]
-0.823236
0.921125
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046600 minutes
Epoch 0
Fine tuning took 0.043580 minutes
Epoch 0
Fine tuning took 0.044256 minutes
{'zero': {0: [0.16133004926108374, 0.2105911330049261, 0.23522167487684728, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.66379310344827591, 0.66871921182266014, 0.69458128078817738], 5: [0.093596059113300489, 0.12561576354679804, 0.096059113300492605, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16133004926108374, 0.25123152709359609, 0.24014778325123154, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.65270935960591137, 0.65394088669950734, 0.72783251231527091], 5: [0.093596059113300489, 0.096059113300492605, 0.10591133004926108, 0.076354679802955669], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16133004926108374, 0.21921182266009853, 0.2894088669950739, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.66256157635467983, 0.6280788177339901, 0.72290640394088668], 5: [0.093596059113300489, 0.11822660098522167, 0.082512315270935957, 0.075123152709359611], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16133004926108374, 0.22906403940886699, 0.24630541871921183, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.66256157635467983, 0.67980295566502458, 0.7068965517241379], 5: [0.093596059113300489, 0.10837438423645321, 0.073891625615763554, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.283100 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.90185
Epoch 1, cost is  1.82991
Epoch 2, cost is  1.79615
Epoch 3, cost is  1.77405
Epoch 4, cost is  1.75635
Training took 0.225260 minutes
Weight histogram
[2015 2852 2470 1922 1497 1375  838  589  351  266] [ -8.45727921e-02  -7.61185772e-02  -6.76643624e-02  -5.92101476e-02
  -5.07559328e-02  -4.23017180e-02  -3.38475032e-02  -2.53932884e-02
  -1.69390736e-02  -8.48485880e-03  -3.06439943e-05]
[ 798  628  885 1127 1329 1514 1700 1907 2040 2247] [ -8.45727921e-02  -7.61185772e-02  -6.76643624e-02  -5.92101476e-02
  -5.07559328e-02  -4.23017180e-02  -3.38475032e-02  -2.53932884e-02
  -1.69390736e-02  -8.48485880e-03  -3.06439943e-05]
-2.39805
2.66878
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270349 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.09656
Epoch 1, cost is  3.00736
Epoch 2, cost is  2.98179
Epoch 3, cost is  2.9662
Epoch 4, cost is  2.95513
Training took 0.205800 minutes
Weight histogram
[2020 3068 1620 2174 1516 1191  823 1372 1392 1024] [ -1.32013649e-01  -1.18815348e-01  -1.05617048e-01  -9.24187473e-02
  -7.92204468e-02  -6.60221464e-02  -5.28238459e-02  -3.96255454e-02
  -2.64272449e-02  -1.32289445e-02  -3.06439943e-05]
[2109 1488  874 1131 1407 1503 1506 2023 1984 2175] [ -1.32013649e-01  -1.18815348e-01  -1.05617048e-01  -9.24187473e-02
  -7.92204468e-02  -6.60221464e-02  -5.28238459e-02  -3.96255454e-02
  -2.64272449e-02  -1.32289445e-02  -3.06439943e-05]
-3.5358
3.89294
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.51857
Epoch 1, cost is  6.14729
Epoch 2, cost is  5.51584
Epoch 3, cost is  4.72943
Epoch 4, cost is  4.09763
Training took 0.359116 minutes
Weight histogram
[ 434 2012 2236 1861 1893 1544 2319 1504  262  110] [-0.02634971 -0.02372743 -0.02110514 -0.01848285 -0.01586056 -0.01323828
 -0.01061599 -0.0079937  -0.00537142 -0.00274913 -0.00012684]
[2971 2159 1657 1103 1100 1092 1077 1123 1132  761] [-0.02634971 -0.02372743 -0.02110514 -0.01848285 -0.01586056 -0.01323828
 -0.01061599 -0.0079937  -0.00537142 -0.00274913 -0.00012684]
-0.620613
0.829825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049698 minutes
Epoch 0
Fine tuning took 0.046381 minutes
Epoch 0
Fine tuning took 0.049800 minutes
{'zero': {0: [0.15024630541871922, 0.22536945812807882, 0.1748768472906404, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76600985221674878, 0.66379310344827591, 0.73645320197044339, 0.69704433497536944], 5: [0.083743842364532015, 0.11083743842364532, 0.088669950738916259, 0.072660098522167482], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15024630541871922, 0.23275862068965517, 0.21551724137931033, 0.24507389162561577], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76600985221674878, 0.66009852216748766, 0.68842364532019706, 0.65394088669950734], 5: [0.083743842364532015, 0.10714285714285714, 0.096059113300492605, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15024630541871922, 0.22413793103448276, 0.2019704433497537, 0.23152709359605911], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76600985221674878, 0.64901477832512311, 0.70073891625615758, 0.67364532019704437], 5: [0.083743842364532015, 0.1268472906403941, 0.097290640394088676, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15024630541871922, 0.2413793103448276, 0.18842364532019704, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.76600985221674878, 0.66256157635467983, 0.70935960591133007, 0.68226600985221675], 5: [0.083743842364532015, 0.096059113300492605, 0.10221674876847291, 0.080049261083743842], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.280541 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.04663
Epoch 1, cost is  2.99023
Epoch 2, cost is  2.94371
Epoch 3, cost is  2.90518
Epoch 4, cost is  2.86314
Training took 0.220674 minutes
Weight histogram
[2287 2306 2007 1540 1318 1543 1203  627 1164  180] [ -4.35120016e-02  -3.91572268e-02  -3.48024520e-02  -3.04476771e-02
  -2.60929023e-02  -2.17381275e-02  -1.73833526e-02  -1.30285778e-02
  -8.67380298e-03  -4.31902815e-03   3.57466815e-05]
[2074 1110  912  975 1154 1333 1422 1562 1667 1966] [ -4.35120016e-02  -3.91572268e-02  -3.48024520e-02  -3.04476771e-02
  -2.60929023e-02  -2.17381275e-02  -1.73833526e-02  -1.30285778e-02
  -8.67380298e-03  -4.31902815e-03   3.57466815e-05]
-0.978891
1.16622
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270188 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82628
Epoch 1, cost is  3.7693
Epoch 2, cost is  3.7254
Epoch 3, cost is  3.6851
Epoch 4, cost is  3.65092
Training took 0.202581 minutes
Weight histogram
[1783 1481 1940 1163 1363 1249 1048 1550 3958  665] [ -6.06233403e-02  -5.45574316e-02  -4.84915229e-02  -4.24256142e-02
  -3.63597055e-02  -3.02937968e-02  -2.42278881e-02  -1.81619794e-02
  -1.20960707e-02  -6.03016202e-03   3.57466815e-05]
[4535 1035  965 1055 1127 1214 1382 1449 1591 1847] [ -6.06233403e-02  -5.45574316e-02  -4.84915229e-02  -4.24256142e-02
  -3.63597055e-02  -3.02937968e-02  -2.42278881e-02  -1.81619794e-02
  -1.20960707e-02  -6.03016202e-03   3.57466815e-05]
-1.12279
1.27364
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.85465
Epoch 1, cost is  6.77066
Epoch 2, cost is  6.70151
Epoch 3, cost is  6.6363
Epoch 4, cost is  6.56287
Training took 0.228434 minutes
Weight histogram
[1660 1783 7720 1113  646  423  297  226  171  136] [ -1.02815339e-02  -9.25416446e-03  -8.22679499e-03  -7.19942552e-03
  -6.17205605e-03  -5.14468658e-03  -4.11731711e-03  -3.08994764e-03
  -2.06257817e-03  -1.03520870e-03  -7.83922951e-06]
[5685 2610 2011 1673  940  487  288  232  126  123] [ -1.02815339e-02  -9.25416446e-03  -8.22679499e-03  -7.19942552e-03
  -6.17205605e-03  -5.14468658e-03  -4.11731711e-03  -3.08994764e-03
  -2.06257817e-03  -1.03520870e-03  -7.83922951e-06]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041269 minutes
Epoch 0
Fine tuning took 0.044707 minutes
Epoch 0
Fine tuning took 0.041268 minutes
{'zero': {0: [0.12438423645320197, 0.29187192118226601, 0.10344827586206896, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.34236453201970446, 0.72783251231527091, 0.62561576354679804], 5: [0.35467980295566504, 0.36576354679802958, 0.16871921182266009, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12438423645320197, 0.28817733990147781, 0.11330049261083744, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.37068965517241381, 0.68226600985221675, 0.6219211822660099], 5: [0.35467980295566504, 0.34113300492610837, 0.20443349753694581, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12438423645320197, 0.28817733990147781, 0.10960591133004927, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.36330049261083741, 0.72536945812807885, 0.64655172413793105], 5: [0.35467980295566504, 0.34852216748768472, 0.16502463054187191, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12438423645320197, 0.28325123152709358, 0.094827586206896547, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.52093596059113301, 0.34359605911330049, 0.72906403940886699, 0.5923645320197044], 5: [0.35467980295566504, 0.37315270935960593, 0.17610837438423646, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268129 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.04663
Epoch 1, cost is  2.99023
Epoch 2, cost is  2.94371
Epoch 3, cost is  2.90518
Epoch 4, cost is  2.86314
Training took 0.226340 minutes
Weight histogram
[2287 2306 2007 1540 1318 1543 1203  627 1164  180] [ -4.35120016e-02  -3.91572268e-02  -3.48024520e-02  -3.04476771e-02
  -2.60929023e-02  -2.17381275e-02  -1.73833526e-02  -1.30285778e-02
  -8.67380298e-03  -4.31902815e-03   3.57466815e-05]
[2074 1110  912  975 1154 1333 1422 1562 1667 1966] [ -4.35120016e-02  -3.91572268e-02  -3.48024520e-02  -3.04476771e-02
  -2.60929023e-02  -2.17381275e-02  -1.73833526e-02  -1.30285778e-02
  -8.67380298e-03  -4.31902815e-03   3.57466815e-05]
-0.978891
1.16622
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266105 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82628
Epoch 1, cost is  3.7693
Epoch 2, cost is  3.7254
Epoch 3, cost is  3.6851
Epoch 4, cost is  3.65092
Training took 0.203339 minutes
Weight histogram
[1783 1481 1940 1163 1363 1249 1048 1550 3958  665] [ -6.06233403e-02  -5.45574316e-02  -4.84915229e-02  -4.24256142e-02
  -3.63597055e-02  -3.02937968e-02  -2.42278881e-02  -1.81619794e-02
  -1.20960707e-02  -6.03016202e-03   3.57466815e-05]
[4535 1035  965 1055 1127 1214 1382 1449 1591 1847] [ -6.06233403e-02  -5.45574316e-02  -4.84915229e-02  -4.24256142e-02
  -3.63597055e-02  -3.02937968e-02  -2.42278881e-02  -1.81619794e-02
  -1.20960707e-02  -6.03016202e-03   3.57466815e-05]
-1.12279
1.27364
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78524
Epoch 1, cost is  6.66926
Epoch 2, cost is  6.58728
Epoch 3, cost is  6.50799
Epoch 4, cost is  6.42617
Training took 0.285120 minutes
Weight histogram
[ 623 1144 2184 6735 1513  776  475  325  229  171] [ -1.24916639e-02  -1.12484659e-02  -1.00052679e-02  -8.76206992e-03
  -7.51887191e-03  -6.27567391e-03  -5.03247591e-03  -3.78927791e-03
  -2.54607991e-03  -1.30288191e-03  -5.96839091e-05]
[4997 2391 1916 1672 1417  660  448  314  196  164] [ -1.24916639e-02  -1.12484659e-02  -1.00052679e-02  -8.76206992e-03
  -7.51887191e-03  -6.27567391e-03  -5.03247591e-03  -3.78927791e-03
  -2.54607991e-03  -1.30288191e-03  -5.96839091e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043045 minutes
Epoch 0
Fine tuning took 0.042685 minutes
Epoch 0
Fine tuning took 0.042740 minutes
{'zero': {0: [0.10837438423645321, 0.36699507389162561, 0.12438423645320197, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.3817733990147783, 0.71305418719211822, 0.70812807881773399], 5: [0.35344827586206895, 0.25123152709359609, 0.1625615763546798, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10837438423645321, 0.37438423645320196, 0.10467980295566502, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.3854679802955665, 0.75246305418719217, 0.71305418719211822], 5: [0.35344827586206895, 0.24014778325123154, 0.14285714285714285, 0.15517241379310345], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10837438423645321, 0.37192118226600984, 0.12931034482758622, 0.11945812807881774], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.40763546798029554, 0.74384236453201968, 0.71059113300492616], 5: [0.35344827586206895, 0.22044334975369459, 0.1268472906403941, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10837438423645321, 0.39778325123152708, 0.10467980295566502, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.36699507389162561, 0.7573891625615764, 0.67487684729064035], 5: [0.35344827586206895, 0.23522167487684728, 0.13793103448275862, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271154 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.04663
Epoch 1, cost is  2.99023
Epoch 2, cost is  2.94371
Epoch 3, cost is  2.90518
Epoch 4, cost is  2.86314
Training took 0.229971 minutes
Weight histogram
[2287 2306 2007 1540 1318 1543 1203  627 1164  180] [ -4.35120016e-02  -3.91572268e-02  -3.48024520e-02  -3.04476771e-02
  -2.60929023e-02  -2.17381275e-02  -1.73833526e-02  -1.30285778e-02
  -8.67380298e-03  -4.31902815e-03   3.57466815e-05]
[2074 1110  912  975 1154 1333 1422 1562 1667 1966] [ -4.35120016e-02  -3.91572268e-02  -3.48024520e-02  -3.04476771e-02
  -2.60929023e-02  -2.17381275e-02  -1.73833526e-02  -1.30285778e-02
  -8.67380298e-03  -4.31902815e-03   3.57466815e-05]
-0.978891
1.16622
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.258086 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.82628
Epoch 1, cost is  3.7693
Epoch 2, cost is  3.7254
Epoch 3, cost is  3.6851
Epoch 4, cost is  3.65092
Training took 0.197002 minutes
Weight histogram
[1783 1481 1940 1163 1363 1249 1048 1550 3958  665] [ -6.06233403e-02  -5.45574316e-02  -4.84915229e-02  -4.24256142e-02
  -3.63597055e-02  -3.02937968e-02  -2.42278881e-02  -1.81619794e-02
  -1.20960707e-02  -6.03016202e-03   3.57466815e-05]
[4535 1035  965 1055 1127 1214 1382 1449 1591 1847] [ -6.06233403e-02  -5.45574316e-02  -4.84915229e-02  -4.24256142e-02
  -3.63597055e-02  -3.02937968e-02  -2.42278881e-02  -1.81619794e-02
  -1.20960707e-02  -6.03016202e-03   3.57466815e-05]
-1.12279
1.27364
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.6765
Epoch 1, cost is  6.51134
Epoch 2, cost is  6.41397
Epoch 3, cost is  6.32624
Epoch 4, cost is  6.23727
Training took 0.362045 minutes
Weight histogram
[ 558  838 1426 5357 3200 1217  681  418  283  197] [ -1.41741699e-02  -1.27594284e-02  -1.13446869e-02  -9.92994548e-03
  -8.51520402e-03  -7.10046256e-03  -5.68572110e-03  -4.27097964e-03
  -2.85623818e-03  -1.44149672e-03  -2.67552568e-05]
[4327 2198 1839 1702 1629 1027  582  401  259  211] [ -1.41741699e-02  -1.27594284e-02  -1.13446869e-02  -9.92994548e-03
  -8.51520402e-03  -7.10046256e-03  -5.68572110e-03  -4.27097964e-03
  -2.85623818e-03  -1.44149672e-03  -2.67552568e-05]
-0.0721031
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046265 minutes
Epoch 0
Fine tuning took 0.047723 minutes
Epoch 0
Fine tuning took 0.046564 minutes
{'zero': {0: [0.10344827586206896, 0.34236453201970446, 0.1145320197044335, 0.22536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.3645320197044335, 0.75615763546798032, 0.64162561576354682], 5: [0.35837438423645318, 0.29310344827586204, 0.12931034482758622, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.10344827586206896, 0.35344827586206895, 0.11330049261083744, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.38300492610837439, 0.75, 0.61699507389162567], 5: [0.35837438423645318, 0.26354679802955666, 0.13669950738916256, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.10344827586206896, 0.37684729064039407, 0.1206896551724138, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.37561576354679804, 0.75492610837438423, 0.62931034482758619], 5: [0.35837438423645318, 0.24753694581280788, 0.12438423645320197, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.10344827586206896, 0.37068965517241381, 0.11945812807881774, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53817733990147787, 0.3645320197044335, 0.75123152709359609, 0.66133004926108374], 5: [0.35837438423645318, 0.26477832512315269, 0.12931034482758622, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267234 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.63397
Epoch 1, cost is  3.37164
Epoch 2, cost is  3.29497
Epoch 3, cost is  3.27916
Epoch 4, cost is  3.27216
Training took 0.230447 minutes
Weight histogram
[1810 1261 1889 1877 1840 1727 1958 1177  495  141] [-0.29705769 -0.26742967 -0.23780166 -0.20817364 -0.17854563 -0.14891761
 -0.1192896  -0.08966158 -0.06003357 -0.03040555 -0.00077754]
[ 408  795 1229 1484 1549 1628 1725 1846 1791 1720] [-0.29705769 -0.26742967 -0.23780166 -0.20817364 -0.17854563 -0.14891761
 -0.1192896  -0.08966158 -0.06003357 -0.03040555 -0.00077754]
-12.9313
17.2366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267150 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.92058
Epoch 1, cost is  9.50989
Epoch 2, cost is  9.4978
Epoch 3, cost is  9.58504
Epoch 4, cost is  9.70799
Training took 0.197556 minutes
Weight histogram
[1556 1567 1537 1496 1642 1710 1509 1731 2342 1110] [-0.63869703 -0.57490508 -0.51111313 -0.44732118 -0.38352923 -0.31973728
 -0.25594534 -0.19215339 -0.12836144 -0.06456949 -0.00077754]
[2097 1729 1439 1488 1475 1417 1623 1686 1653 1593] [-0.63869703 -0.57490508 -0.51111313 -0.44732118 -0.38352923 -0.31973728
 -0.25594534 -0.19215339 -0.12836144 -0.06456949 -0.00077754]
-24.7344
24.7747
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.12924
Epoch 1, cost is  3.83453
Epoch 2, cost is  3.70095
Epoch 3, cost is  3.81235
Epoch 4, cost is  4.01009
Training took 0.236180 minutes
Weight histogram
[1809 2371 2512 2155 1800 1215  814  560  420  519] [-0.17713617 -0.1595476  -0.14195903 -0.12437046 -0.10678189 -0.08919332
 -0.07160475 -0.05401618 -0.03642761 -0.01883904 -0.00125047]
[ 873  816 1008 1197 1360 1539 1707 1830 1943 1902] [-0.17713617 -0.1595476  -0.14195903 -0.12437046 -0.10678189 -0.08919332
 -0.07160475 -0.05401618 -0.03642761 -0.01883904 -0.00125047]
-6.86173
7.17654
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042896 minutes
Epoch 0
Fine tuning took 0.042309 minutes
Epoch 0
Fine tuning took 0.046133 minutes
{'zero': {0: [0.21674876847290642, 0.19088669950738915, 0.2229064039408867, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.66502463054187189, 0.66379310344827591, 0.68842364532019706], 5: [0.12192118226600986, 0.14408866995073891, 0.11330049261083744, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21674876847290642, 0.29802955665024633, 0.21798029556650247, 0.29187192118226601], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.52216748768472909, 0.64039408866995073, 0.59359605911330049], 5: [0.12192118226600986, 0.17980295566502463, 0.14162561576354679, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21674876847290642, 0.2376847290640394, 0.23152709359605911, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.61822660098522164, 0.63177339901477836, 0.65640394088669951], 5: [0.12192118226600986, 0.14408866995073891, 0.13669950738916256, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21674876847290642, 0.31157635467980294, 0.22413793103448276, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66133004926108374, 0.49507389162561577, 0.59975369458128081, 0.61576354679802958], 5: [0.12192118226600986, 0.19334975369458129, 0.17610837438423646, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.280567 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.63397
Epoch 1, cost is  3.37164
Epoch 2, cost is  3.29497
Epoch 3, cost is  3.27916
Epoch 4, cost is  3.27216
Training took 0.229866 minutes
Weight histogram
[1810 1261 1889 1877 1840 1727 1958 1177  495  141] [-0.29705769 -0.26742967 -0.23780166 -0.20817364 -0.17854563 -0.14891761
 -0.1192896  -0.08966158 -0.06003357 -0.03040555 -0.00077754]
[ 408  795 1229 1484 1549 1628 1725 1846 1791 1720] [-0.29705769 -0.26742967 -0.23780166 -0.20817364 -0.17854563 -0.14891761
 -0.1192896  -0.08966158 -0.06003357 -0.03040555 -0.00077754]
-12.9313
17.2366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.280123 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.92058
Epoch 1, cost is  9.50989
Epoch 2, cost is  9.4978
Epoch 3, cost is  9.58504
Epoch 4, cost is  9.70799
Training took 0.207755 minutes
Weight histogram
[1556 1567 1537 1496 1642 1710 1509 1731 2342 1110] [-0.63869703 -0.57490508 -0.51111313 -0.44732118 -0.38352923 -0.31973728
 -0.25594534 -0.19215339 -0.12836144 -0.06456949 -0.00077754]
[2097 1729 1439 1488 1475 1417 1623 1686 1653 1593] [-0.63869703 -0.57490508 -0.51111313 -0.44732118 -0.38352923 -0.31973728
 -0.25594534 -0.19215339 -0.12836144 -0.06456949 -0.00077754]
-24.7344
24.7747
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.77821
Epoch 1, cost is  2.92574
Epoch 2, cost is  2.54497
Epoch 3, cost is  2.45293
Epoch 4, cost is  2.45817
Training took 0.263008 minutes
Weight histogram
[1857 2643 2248 2023 1758 1265  880  615  444  442] [-0.11230296 -0.10119887 -0.09009478 -0.0789907  -0.06788661 -0.05678252
 -0.04567843 -0.03457435 -0.02347026 -0.01236617 -0.00126208]
[ 958  798  962 1108 1292 1604 1693 1876 1984 1900] [-0.11230296 -0.10119887 -0.09009478 -0.0789907  -0.06788661 -0.05678252
 -0.04567843 -0.03457435 -0.02347026 -0.01236617 -0.00126208]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044086 minutes
Epoch 0
Fine tuning took 0.043743 minutes
Epoch 0
Fine tuning took 0.043787 minutes
{'zero': {0: [0.22906403940886699, 0.21921182266009853, 0.14532019704433496, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68719211822660098, 0.57389162561576357, 0.63300492610837433, 0.66995073891625612], 5: [0.083743842364532015, 0.20689655172413793, 0.22167487684729065, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22906403940886699, 0.24753694581280788, 0.1539408866995074, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68719211822660098, 0.58374384236453203, 0.65270935960591137, 0.72906403940886699], 5: [0.083743842364532015, 0.16871921182266009, 0.19334975369458129, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22906403940886699, 0.20320197044334976, 0.14285714285714285, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68719211822660098, 0.63300492610837433, 0.6785714285714286, 0.71921182266009853], 5: [0.083743842364532015, 0.16379310344827586, 0.17857142857142858, 0.1206896551724138], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22906403940886699, 0.29187192118226601, 0.21798029556650247, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.68719211822660098, 0.46674876847290642, 0.55911330049261088, 0.6711822660098522], 5: [0.083743842364532015, 0.2413793103448276, 0.2229064039408867, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.260518 minutes
Weight histogram
[ 173  446  567  640 1587 3562 3551 2453 1142   54] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 125  146  208  290  429  728  853 1617 2577 7202] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.63397
Epoch 1, cost is  3.37164
Epoch 2, cost is  3.29497
Epoch 3, cost is  3.27916
Epoch 4, cost is  3.27216
Training took 0.232434 minutes
Weight histogram
[1810 1261 1889 1877 1840 1727 1958 1177  495  141] [-0.29705769 -0.26742967 -0.23780166 -0.20817364 -0.17854563 -0.14891761
 -0.1192896  -0.08966158 -0.06003357 -0.03040555 -0.00077754]
[ 408  795 1229 1484 1549 1628 1725 1846 1791 1720] [-0.29705769 -0.26742967 -0.23780166 -0.20817364 -0.17854563 -0.14891761
 -0.1192896  -0.08966158 -0.06003357 -0.03040555 -0.00077754]
-12.9313
17.2366
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.279809 minutes
Weight histogram
[ 179  574  983 1541 2026 3042 3877 2848  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 249  295  405  553  854 1397  891 1730 2878 6948] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.905351
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  9.92058
Epoch 1, cost is  9.50989
Epoch 2, cost is  9.4978
Epoch 3, cost is  9.58504
Epoch 4, cost is  9.70799
Training took 0.201762 minutes
Weight histogram
[1556 1567 1537 1496 1642 1710 1509 1731 2342 1110] [-0.63869703 -0.57490508 -0.51111313 -0.44732118 -0.38352923 -0.31973728
 -0.25594534 -0.19215339 -0.12836144 -0.06456949 -0.00077754]
[2097 1729 1439 1488 1475 1417 1623 1686 1653 1593] [-0.63869703 -0.57490508 -0.51111313 -0.44732118 -0.38352923 -0.31973728
 -0.25594534 -0.19215339 -0.12836144 -0.06456949 -0.00077754]
-24.7344
24.7747
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.54276
Epoch 1, cost is  2.39387
Epoch 2, cost is  1.85635
Epoch 3, cost is  1.67685
Epoch 4, cost is  1.59117
Training took 0.351320 minutes
Weight histogram
[2363 2726 2305 1899 1534 1288  786  631  424  219] [-0.07301652 -0.06583807 -0.05865962 -0.05148117 -0.04430272 -0.03712427
 -0.02994582 -0.02276737 -0.01558892 -0.00841047 -0.00123202]
[ 968  739  853 1064 1287 1532 1745 1937 2097 1953] [-0.07301652 -0.06583807 -0.05865962 -0.05148117 -0.04430272 -0.03712427
 -0.02994582 -0.02276737 -0.01558892 -0.00841047 -0.00123202]
-3.98509
4.26369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047003 minutes
Epoch 0
Fine tuning took 0.046867 minutes
Epoch 0
Fine tuning took 0.051048 minutes
{'zero': {0: [0.17364532019704434, 0.18226600985221675, 0.10344827586206896, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.58620689655172409, 0.71551724137931039, 0.56650246305418717], 5: [0.12192118226600986, 0.23152709359605911, 0.18103448275862069, 0.25985221674876846], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17364532019704434, 0.17857142857142858, 0.11576354679802955, 0.11945812807881774], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.67241379310344829, 0.76231527093596063, 0.71921182266009853], 5: [0.12192118226600986, 0.14901477832512317, 0.12192118226600986, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17364532019704434, 0.16995073891625614, 0.11576354679802955, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.68103448275862066, 0.70566502463054193, 0.66995073891625612], 5: [0.12192118226600986, 0.14901477832512317, 0.17857142857142858, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17364532019704434, 0.11945812807881774, 0.11822660098522167, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70443349753694584, 0.71305418719211822, 0.72413793103448276, 0.7426108374384236], 5: [0.12192118226600986, 0.16748768472906403, 0.15763546798029557, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272725 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.98594
Epoch 1, cost is  1.89695
Epoch 2, cost is  1.86148
Epoch 3, cost is  1.84276
Epoch 4, cost is  1.83387
Training took 0.231298 minutes
Weight histogram
[3123 1961 2820 2206 2000 1724 1156  639  367  204] [ -1.23040594e-01  -1.10747898e-01  -9.84552018e-02  -8.61625056e-02
  -7.38698094e-02  -6.15771133e-02  -4.92844171e-02  -3.69917209e-02
  -2.46990247e-02  -1.24063285e-02  -1.13632348e-04]
[ 602  703 1072 1380 1604 1841 2050 2206 2343 2399] [ -1.23040594e-01  -1.10747898e-01  -9.84552018e-02  -8.61625056e-02
  -7.38698094e-02  -6.15771133e-02  -4.92844171e-02  -3.69917209e-02
  -2.46990247e-02  -1.24063285e-02  -1.13632348e-04]
-3.78113
6.10712
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268043 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.79722
Epoch 1, cost is  3.67043
Epoch 2, cost is  3.642
Epoch 3, cost is  3.63568
Epoch 4, cost is  3.64292
Training took 0.201811 minutes
Weight histogram
[2188 2059 2821 1860 2277 1912 1399 1126 1660  923] [ -2.10790470e-01  -1.89722786e-01  -1.68655103e-01  -1.47587419e-01
  -1.26519735e-01  -1.05452051e-01  -8.43843675e-02  -6.33166837e-02
  -4.22489999e-02  -2.11813161e-02  -1.13632348e-04]
[1859 1684 1207 1486 1676 1646 2088 2142 2241 2196] [ -2.10790470e-01  -1.89722786e-01  -1.68655103e-01  -1.47587419e-01
  -1.26519735e-01  -1.05452051e-01  -8.43843675e-02  -6.33166837e-02
  -4.22489999e-02  -2.11813161e-02  -1.13632348e-04]
-5.97141
6.26193
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.5624
Epoch 1, cost is  5.38246
Epoch 2, cost is  4.55968
Epoch 3, cost is  4.16803
Epoch 4, cost is  3.92598
Training took 0.229118 minutes
Weight histogram
[2574 2046 1915 1789 1325 1152 1019 1154  833 2393] [-0.08295341 -0.07468115 -0.0664089  -0.05813664 -0.04986438 -0.04159213
 -0.03331987 -0.02504761 -0.01677536 -0.0085031  -0.00023084]
[2464 1398 1113 1261 1345 1498 1661 1859 2006 1595] [-0.08295341 -0.07468115 -0.0664089  -0.05813664 -0.04986438 -0.04159213
 -0.03331987 -0.02504761 -0.01677536 -0.0085031  -0.00023084]
-1.6243
2.76155
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045308 minutes
Epoch 0
Fine tuning took 0.041910 minutes
Epoch 0
Fine tuning took 0.042259 minutes
{'zero': {0: [0.15147783251231528, 0.24753694581280788, 0.20689655172413793, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.62931034482758619, 0.70197044334975367, 0.74507389162561577], 5: [0.2229064039408867, 0.12315270935960591, 0.091133004926108374, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15147783251231528, 0.26354679802955666, 0.20566502463054187, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.61822660098522164, 0.70197044334975367, 0.6785714285714286], 5: [0.2229064039408867, 0.11822660098522167, 0.092364532019704432, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15147783251231528, 0.26231527093596058, 0.20689655172413793, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.62684729064039413, 0.69334975369458129, 0.69581280788177335], 5: [0.2229064039408867, 0.11083743842364532, 0.099753694581280791, 0.088669950738916259], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15147783251231528, 0.26847290640394089, 0.21428571428571427, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62561576354679804, 0.58743842364532017, 0.70443349753694584, 0.70320197044334976], 5: [0.2229064039408867, 0.14408866995073891, 0.081280788177339899, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.258594 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.98594
Epoch 1, cost is  1.89695
Epoch 2, cost is  1.86148
Epoch 3, cost is  1.84276
Epoch 4, cost is  1.83387
Training took 0.226261 minutes
Weight histogram
[3123 1961 2820 2206 2000 1724 1156  639  367  204] [ -1.23040594e-01  -1.10747898e-01  -9.84552018e-02  -8.61625056e-02
  -7.38698094e-02  -6.15771133e-02  -4.92844171e-02  -3.69917209e-02
  -2.46990247e-02  -1.24063285e-02  -1.13632348e-04]
[ 602  703 1072 1380 1604 1841 2050 2206 2343 2399] [ -1.23040594e-01  -1.10747898e-01  -9.84552018e-02  -8.61625056e-02
  -7.38698094e-02  -6.15771133e-02  -4.92844171e-02  -3.69917209e-02
  -2.46990247e-02  -1.24063285e-02  -1.13632348e-04]
-3.78113
6.10712
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.277305 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.79722
Epoch 1, cost is  3.67043
Epoch 2, cost is  3.642
Epoch 3, cost is  3.63568
Epoch 4, cost is  3.64292
Training took 0.200912 minutes
Weight histogram
[2188 2059 2821 1860 2277 1912 1399 1126 1660  923] [ -2.10790470e-01  -1.89722786e-01  -1.68655103e-01  -1.47587419e-01
  -1.26519735e-01  -1.05452051e-01  -8.43843675e-02  -6.33166837e-02
  -4.22489999e-02  -2.11813161e-02  -1.13632348e-04]
[1859 1684 1207 1486 1676 1646 2088 2142 2241 2196] [ -2.10790470e-01  -1.89722786e-01  -1.68655103e-01  -1.47587419e-01
  -1.26519735e-01  -1.05452051e-01  -8.43843675e-02  -6.33166837e-02
  -4.22489999e-02  -2.11813161e-02  -1.13632348e-04]
-5.97141
6.26193
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.47137
Epoch 1, cost is  5.22334
Epoch 2, cost is  4.09284
Epoch 3, cost is  3.47589
Epoch 4, cost is  3.06844
Training took 0.270254 minutes
Weight histogram
[2021 2818 2074 1769 1474 1171 1272 1057 2328  216] [-0.05099323 -0.04592217 -0.04085112 -0.03578007 -0.03070902 -0.02563796
 -0.02056691 -0.01549586 -0.01042481 -0.00535376 -0.0002827 ]
[2622 1459 1219 1314 1317 1485 1628 1773 1871 1512] [-0.05099323 -0.04592217 -0.04085112 -0.03578007 -0.03070902 -0.02563796
 -0.02056691 -0.01549586 -0.01042481 -0.00535376 -0.0002827 ]
-1.2098
1.51827
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043408 minutes
Epoch 0
Fine tuning took 0.043584 minutes
Epoch 0
Fine tuning took 0.044018 minutes
{'zero': {0: [0.11206896551724138, 0.18596059113300492, 0.17610837438423646, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78325123152709364, 0.71551724137931039, 0.69950738916256161, 0.76847290640394084], 5: [0.10467980295566502, 0.098522167487684734, 0.12438423645320197, 0.068965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11206896551724138, 0.17733990147783252, 0.13793103448275862, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78325123152709364, 0.71059113300492616, 0.74137931034482762, 0.78201970443349755], 5: [0.10467980295566502, 0.11206896551724138, 0.1206896551724138, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11206896551724138, 0.17610837438423646, 0.1539408866995074, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78325123152709364, 0.7142857142857143, 0.72906403940886699, 0.76847290640394084], 5: [0.10467980295566502, 0.10960591133004927, 0.11699507389162561, 0.057881773399014777], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11206896551724138, 0.17857142857142858, 0.16995073891625614, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.78325123152709364, 0.70935960591133007, 0.73152709359605916, 0.79802955665024633], 5: [0.10467980295566502, 0.11206896551724138, 0.098522167487684734, 0.048029556650246302], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.278391 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.98594
Epoch 1, cost is  1.89695
Epoch 2, cost is  1.86148
Epoch 3, cost is  1.84276
Epoch 4, cost is  1.83387
Training took 0.230474 minutes
Weight histogram
[3123 1961 2820 2206 2000 1724 1156  639  367  204] [ -1.23040594e-01  -1.10747898e-01  -9.84552018e-02  -8.61625056e-02
  -7.38698094e-02  -6.15771133e-02  -4.92844171e-02  -3.69917209e-02
  -2.46990247e-02  -1.24063285e-02  -1.13632348e-04]
[ 602  703 1072 1380 1604 1841 2050 2206 2343 2399] [ -1.23040594e-01  -1.10747898e-01  -9.84552018e-02  -8.61625056e-02
  -7.38698094e-02  -6.15771133e-02  -4.92844171e-02  -3.69917209e-02
  -2.46990247e-02  -1.24063285e-02  -1.13632348e-04]
-3.78113
6.10712
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.261116 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.79722
Epoch 1, cost is  3.67043
Epoch 2, cost is  3.642
Epoch 3, cost is  3.63568
Epoch 4, cost is  3.64292
Training took 0.200719 minutes
Weight histogram
[2188 2059 2821 1860 2277 1912 1399 1126 1660  923] [ -2.10790470e-01  -1.89722786e-01  -1.68655103e-01  -1.47587419e-01
  -1.26519735e-01  -1.05452051e-01  -8.43843675e-02  -6.33166837e-02
  -4.22489999e-02  -2.11813161e-02  -1.13632348e-04]
[1859 1684 1207 1486 1676 1646 2088 2142 2241 2196] [ -2.10790470e-01  -1.89722786e-01  -1.68655103e-01  -1.47587419e-01
  -1.26519735e-01  -1.05452051e-01  -8.43843675e-02  -6.33166837e-02
  -4.22489999e-02  -2.11813161e-02  -1.13632348e-04]
-5.97141
6.26193
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.35884
Epoch 1, cost is  5.13624
Epoch 2, cost is  3.89439
Epoch 3, cost is  3.10941
Epoch 4, cost is  2.59806
Training took 0.363510 minutes
Weight histogram
[2147 2965 2373 1944 1595 1504 1247 1872  453  100] [-0.03414995 -0.03075964 -0.02736933 -0.02397902 -0.02058872 -0.01719841
 -0.0138081  -0.01041779 -0.00702749 -0.00363718 -0.00024687]
[2866 1478 1212 1204 1324 1453 1545 1612 1851 1655] [-0.03414995 -0.03075964 -0.02736933 -0.02397902 -0.02058872 -0.01719841
 -0.0138081  -0.01041779 -0.00702749 -0.00363718 -0.00024687]
-0.957712
1.41078
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.048190 minutes
Epoch 0
Fine tuning took 0.046482 minutes
Epoch 0
Fine tuning took 0.046981 minutes
{'zero': {0: [0.17733990147783252, 0.22044334975369459, 0.18719211822660098, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.64162561576354682, 0.68472906403940892, 0.68596059113300489], 5: [0.1145320197044335, 0.13793103448275862, 0.12807881773399016, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17733990147783252, 0.18719211822660098, 0.15147783251231528, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.68965517241379315, 0.73275862068965514, 0.76600985221674878], 5: [0.1145320197044335, 0.12315270935960591, 0.11576354679802955, 0.11576354679802955], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17733990147783252, 0.21182266009852216, 0.1625615763546798, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.68103448275862066, 0.70197044334975367, 0.73645320197044339], 5: [0.1145320197044335, 0.10714285714285714, 0.1354679802955665, 0.11699507389162561], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17733990147783252, 0.2105911330049261, 0.16379310344827586, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.66871921182266014, 0.72044334975369462, 0.73645320197044339], 5: [0.1145320197044335, 0.1206896551724138, 0.11576354679802955, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.277212 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.86454
Epoch 1, cost is  1.80596
Epoch 2, cost is  1.77388
Epoch 3, cost is  1.75814
Epoch 4, cost is  1.74369
Training took 0.230155 minutes
Weight histogram
[3397 2438 2421 2347 1699 1644  936  640  390  288] [ -8.95362943e-02  -8.05857293e-02  -7.16351643e-02  -6.26845992e-02
  -5.37340342e-02  -4.47834692e-02  -3.58329041e-02  -2.68823391e-02
  -1.79317741e-02  -8.98120903e-03  -3.06439943e-05]
[ 846  725 1020 1305 1517 1776 1985 2180 2394 2452] [ -8.95362943e-02  -8.05857293e-02  -7.16351643e-02  -6.26845992e-02
  -5.37340342e-02  -4.47834692e-02  -3.58329041e-02  -2.68823391e-02
  -1.79317741e-02  -8.98120903e-03  -3.06439943e-05]
-2.56058
2.96541
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274433 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.19822
Epoch 1, cost is  3.1205
Epoch 2, cost is  3.0991
Epoch 3, cost is  3.08775
Epoch 4, cost is  3.07884
Training took 0.205231 minutes
Weight histogram
[2906 2159 2914 1930 2061 1332  974 1291 1521 1137] [ -1.42235339e-01  -1.28014869e-01  -1.13794400e-01  -9.95739303e-02
  -8.53534608e-02  -7.11329913e-02  -5.69125219e-02  -4.26920524e-02
  -2.84715829e-02  -1.42511135e-02  -3.06439943e-05]
[2280 1472 1025 1335 1639 1637 2034 2178 2354 2271] [ -1.42235339e-01  -1.28014869e-01  -1.13794400e-01  -9.95739303e-02
  -8.53534608e-02  -7.11329913e-02  -5.69125219e-02  -4.26920524e-02
  -2.84715829e-02  -1.42511135e-02  -3.06439943e-05]
-3.84775
4.11825
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74692
Epoch 1, cost is  6.35799
Epoch 2, cost is  5.64003
Epoch 3, cost is  5.02802
Epoch 4, cost is  4.61363
Training took 0.229584 minutes
Weight histogram
[ 661 1848 1519 1550 1234 1337 1407 1224 4932  488] [-0.0558529  -0.05027867 -0.04470444 -0.03913022 -0.03355599 -0.02798176
 -0.02240753 -0.01683331 -0.01125908 -0.00568485 -0.00011062]
[3807 1699 1638 1305 1295 1445 1510 1540 1276  685] [-0.0558529  -0.05027867 -0.04470444 -0.03913022 -0.03355599 -0.02798176
 -0.02240753 -0.01683331 -0.01125908 -0.00568485 -0.00011062]
-0.925005
1.26551
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041651 minutes
Epoch 0
Fine tuning took 0.041991 minutes
Epoch 0
Fine tuning took 0.041328 minutes
{'zero': {0: [0.21674876847290642, 0.20812807881773399, 0.15147783251231528, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.53201970443349755, 0.66009852216748766, 0.73029556650246308], 5: [0.26724137931034481, 0.25985221674876846, 0.18842364532019704, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21674876847290642, 0.22536945812807882, 0.12807881773399016, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.5431034482758621, 0.6711822660098522, 0.72536945812807885], 5: [0.26724137931034481, 0.23152709359605911, 0.20073891625615764, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21674876847290642, 0.22167487684729065, 0.13669950738916256, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.54187192118226601, 0.67487684729064035, 0.7426108374384236], 5: [0.26724137931034481, 0.23645320197044334, 0.18842364532019704, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21674876847290642, 0.20935960591133004, 0.12315270935960591, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51600985221674878, 0.53817733990147787, 0.69458128078817738, 0.73645320197044339], 5: [0.26724137931034481, 0.25246305418719212, 0.18226600985221675, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266094 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.86454
Epoch 1, cost is  1.80596
Epoch 2, cost is  1.77388
Epoch 3, cost is  1.75814
Epoch 4, cost is  1.74369
Training took 0.227526 minutes
Weight histogram
[3397 2438 2421 2347 1699 1644  936  640  390  288] [ -8.95362943e-02  -8.05857293e-02  -7.16351643e-02  -6.26845992e-02
  -5.37340342e-02  -4.47834692e-02  -3.58329041e-02  -2.68823391e-02
  -1.79317741e-02  -8.98120903e-03  -3.06439943e-05]
[ 846  725 1020 1305 1517 1776 1985 2180 2394 2452] [ -8.95362943e-02  -8.05857293e-02  -7.16351643e-02  -6.26845992e-02
  -5.37340342e-02  -4.47834692e-02  -3.58329041e-02  -2.68823391e-02
  -1.79317741e-02  -8.98120903e-03  -3.06439943e-05]
-2.56058
2.96541
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265842 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.19822
Epoch 1, cost is  3.1205
Epoch 2, cost is  3.0991
Epoch 3, cost is  3.08775
Epoch 4, cost is  3.07884
Training took 0.198188 minutes
Weight histogram
[2906 2159 2914 1930 2061 1332  974 1291 1521 1137] [ -1.42235339e-01  -1.28014869e-01  -1.13794400e-01  -9.95739303e-02
  -8.53534608e-02  -7.11329913e-02  -5.69125219e-02  -4.26920524e-02
  -2.84715829e-02  -1.42511135e-02  -3.06439943e-05]
[2280 1472 1025 1335 1639 1637 2034 2178 2354 2271] [ -1.42235339e-01  -1.28014869e-01  -1.13794400e-01  -9.95739303e-02
  -8.53534608e-02  -7.11329913e-02  -5.69125219e-02  -4.26920524e-02
  -2.84715829e-02  -1.42511135e-02  -3.06439943e-05]
-3.84775
4.11825
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.65464
Epoch 1, cost is  6.26632
Epoch 2, cost is  5.5538
Epoch 3, cost is  4.82556
Epoch 4, cost is  4.22933
Training took 0.276183 minutes
Weight histogram
[ 354 1978 2080 1750 1606 1707 1421 3061 2036  207] [-0.03642388 -0.03279739 -0.0291709  -0.02554441 -0.02191792 -0.01829142
 -0.01466493 -0.01103844 -0.00741195 -0.00378545 -0.00015896]
[3529 2070 1733 1304 1286 1365 1413 1393 1344  763] [-0.03642388 -0.03279739 -0.0291709  -0.02554441 -0.02191792 -0.01829142
 -0.01466493 -0.01103844 -0.00741195 -0.00378545 -0.00015896]
-0.823236
0.921125
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046299 minutes
Epoch 0
Fine tuning took 0.046502 minutes
Epoch 0
Fine tuning took 0.043122 minutes
{'zero': {0: [0.20073891625615764, 0.2376847290640394, 0.1539408866995074, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.5357142857142857, 0.65394088669950734, 0.63793103448275867], 5: [0.21921182266009853, 0.22660098522167488, 0.19211822660098521, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20073891625615764, 0.26847290640394089, 0.14655172413793102, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.55665024630541871, 0.68103448275862066, 0.66502463054187189], 5: [0.21921182266009853, 0.1748768472906404, 0.17241379310344829, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20073891625615764, 0.2536945812807882, 0.15147783251231528, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.54064039408866993, 0.6711822660098522, 0.65147783251231528], 5: [0.21921182266009853, 0.20566502463054187, 0.17733990147783252, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20073891625615764, 0.24876847290640394, 0.1354679802955665, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.58004926108374388, 0.56896551724137934, 0.65024630541871919, 0.63423645320197042], 5: [0.21921182266009853, 0.18226600985221675, 0.21428571428571427, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271475 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.86454
Epoch 1, cost is  1.80596
Epoch 2, cost is  1.77388
Epoch 3, cost is  1.75814
Epoch 4, cost is  1.74369
Training took 0.230705 minutes
Weight histogram
[3397 2438 2421 2347 1699 1644  936  640  390  288] [ -8.95362943e-02  -8.05857293e-02  -7.16351643e-02  -6.26845992e-02
  -5.37340342e-02  -4.47834692e-02  -3.58329041e-02  -2.68823391e-02
  -1.79317741e-02  -8.98120903e-03  -3.06439943e-05]
[ 846  725 1020 1305 1517 1776 1985 2180 2394 2452] [ -8.95362943e-02  -8.05857293e-02  -7.16351643e-02  -6.26845992e-02
  -5.37340342e-02  -4.47834692e-02  -3.58329041e-02  -2.68823391e-02
  -1.79317741e-02  -8.98120903e-03  -3.06439943e-05]
-2.56058
2.96541
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268273 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.19822
Epoch 1, cost is  3.1205
Epoch 2, cost is  3.0991
Epoch 3, cost is  3.08775
Epoch 4, cost is  3.07884
Training took 0.204364 minutes
Weight histogram
[2906 2159 2914 1930 2061 1332  974 1291 1521 1137] [ -1.42235339e-01  -1.28014869e-01  -1.13794400e-01  -9.95739303e-02
  -8.53534608e-02  -7.11329913e-02  -5.69125219e-02  -4.26920524e-02
  -2.84715829e-02  -1.42511135e-02  -3.06439943e-05]
[2280 1472 1025 1335 1639 1637 2034 2178 2354 2271] [ -1.42235339e-01  -1.28014869e-01  -1.13794400e-01  -9.95739303e-02
  -8.53534608e-02  -7.11329913e-02  -5.69125219e-02  -4.26920524e-02
  -2.84715829e-02  -1.42511135e-02  -3.06439943e-05]
-3.84775
4.11825
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.52336
Epoch 1, cost is  6.15946
Epoch 2, cost is  5.50675
Epoch 3, cost is  4.70572
Epoch 4, cost is  4.07015
Training took 0.362083 minutes
Weight histogram
[ 434 2214 2547 2184 2173 1815 2620 1783  303  127] [-0.02634971 -0.02372743 -0.02110514 -0.01848285 -0.01586056 -0.01323828
 -0.01061599 -0.0079937  -0.00537142 -0.00274913 -0.00012684]
[3393 2501 1866 1249 1263 1245 1229 1288 1294  872] [-0.02634971 -0.02372743 -0.02110514 -0.01848285 -0.01586056 -0.01323828
 -0.01061599 -0.0079937  -0.00537142 -0.00274913 -0.00012684]
-0.623447
0.829825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045962 minutes
Epoch 0
Fine tuning took 0.046500 minutes
Epoch 0
Fine tuning took 0.046664 minutes
{'zero': {0: [0.16995073891625614, 0.22783251231527094, 0.16502463054187191, 0.17980295566502463], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65147783251231528, 0.53940886699507384, 0.61576354679802958, 0.67364532019704437], 5: [0.17857142857142858, 0.23275862068965517, 0.21921182266009853, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16995073891625614, 0.23029556650246305, 0.15763546798029557, 0.19211822660098521], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65147783251231528, 0.55049261083743839, 0.63300492610837433, 0.67241379310344829], 5: [0.17857142857142858, 0.21921182266009853, 0.20935960591133004, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16995073891625614, 0.23645320197044334, 0.1748768472906404, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65147783251231528, 0.52093596059113301, 0.62438423645320196, 0.69458128078817738], 5: [0.17857142857142858, 0.24261083743842365, 0.20073891625615764, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16995073891625614, 0.2105911330049261, 0.18596059113300492, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.65147783251231528, 0.57758620689655171, 0.61330049261083741, 0.65270935960591137], 5: [0.17857142857142858, 0.21182266009852216, 0.20073891625615764, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272031 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.89054
Epoch 1, cost is  2.8437
Epoch 2, cost is  2.80218
Epoch 3, cost is  2.76792
Epoch 4, cost is  2.73888
Training took 0.224204 minutes
Weight histogram
[3275 2340 2246 1886 1371 1517 1450  678 1239  198] [ -4.60820980e-02  -4.14703136e-02  -3.68585291e-02  -3.22467446e-02
  -2.76349601e-02  -2.30231757e-02  -1.84113912e-02  -1.37996067e-02
  -9.18782226e-03  -4.57603779e-03   3.57466815e-05]
[2224 1160 1003 1154 1398 1524 1679 1819 2145 2094] [ -4.60820980e-02  -4.14703136e-02  -3.68585291e-02  -3.22467446e-02
  -2.76349601e-02  -2.30231757e-02  -1.84113912e-02  -1.37996067e-02
  -9.18782226e-03  -4.57603779e-03   3.57466815e-05]
-1.06369
1.26245
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270141 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.69911
Epoch 1, cost is  3.656
Epoch 2, cost is  3.61994
Epoch 3, cost is  3.58953
Epoch 4, cost is  3.56549
Training took 0.206037 minutes
Weight histogram
[2594 1499 2177 1617 1330 1392 1206 1412 4201  797] [ -6.55786023e-02  -5.90171674e-02  -5.24557325e-02  -4.58942976e-02
  -3.93328627e-02  -3.27714278e-02  -2.62099929e-02  -1.96485580e-02
  -1.30871231e-02  -6.52568821e-03   3.57466815e-05]
[4673 1104 1104 1229 1287 1461 1596 1705 2074 1992] [ -6.55786023e-02  -5.90171674e-02  -5.24557325e-02  -4.58942976e-02
  -3.93328627e-02  -3.27714278e-02  -2.62099929e-02  -1.96485580e-02
  -1.30871231e-02  -6.52568821e-03   3.57466815e-05]
-1.27823
1.38819
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.85686
Epoch 1, cost is  6.77564
Epoch 2, cost is  6.7086
Epoch 3, cost is  6.64439
Epoch 4, cost is  6.57398
Training took 0.229349 minutes
Weight histogram
[1660 1785 9178 1345  766  498  348  263  199  158] [ -1.02815339e-02  -9.25411123e-03  -8.22668852e-03  -7.19926582e-03
  -6.17184311e-03  -5.14442041e-03  -4.11699770e-03  -3.08957500e-03
  -2.06215229e-03  -1.03472959e-03  -7.30688498e-06]
[6636 3045 2341 1947  975  487  288  232  126  123] [ -1.02815339e-02  -9.25411123e-03  -8.22668852e-03  -7.19926582e-03
  -6.17184311e-03  -5.14442041e-03  -4.11699770e-03  -3.08957500e-03
  -2.06215229e-03  -1.03472959e-03  -7.30688498e-06]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040745 minutes
Epoch 0
Fine tuning took 0.044478 minutes
Epoch 0
Fine tuning took 0.041599 minutes
{'zero': {0: [0.31403940886699505, 0.17610837438423646, 0.18226600985221675, 0.28448275862068967], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40270935960591131, 0.62438423645320196, 0.6576354679802956, 0.62561576354679804], 5: [0.28325123152709358, 0.19950738916256158, 0.16009852216748768, 0.089901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31403940886699505, 0.15763546798029557, 0.18842364532019704, 0.27339901477832512], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40270935960591131, 0.64039408866995073, 0.62684729064039413, 0.62315270935960587], 5: [0.28325123152709358, 0.2019704433497537, 0.18472906403940886, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31403940886699505, 0.18596059113300492, 0.15270935960591134, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40270935960591131, 0.61822660098522164, 0.67610837438423643, 0.66871921182266014], 5: [0.28325123152709358, 0.19581280788177341, 0.17118226600985223, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31403940886699505, 0.18842364532019704, 0.20320197044334976, 0.27093596059113301], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40270935960591131, 0.60837438423645318, 0.63669950738916259, 0.64532019704433496], 5: [0.28325123152709358, 0.20320197044334976, 0.16009852216748768, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.279254 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.89054
Epoch 1, cost is  2.8437
Epoch 2, cost is  2.80218
Epoch 3, cost is  2.76792
Epoch 4, cost is  2.73888
Training took 0.233825 minutes
Weight histogram
[3275 2340 2246 1886 1371 1517 1450  678 1239  198] [ -4.60820980e-02  -4.14703136e-02  -3.68585291e-02  -3.22467446e-02
  -2.76349601e-02  -2.30231757e-02  -1.84113912e-02  -1.37996067e-02
  -9.18782226e-03  -4.57603779e-03   3.57466815e-05]
[2224 1160 1003 1154 1398 1524 1679 1819 2145 2094] [ -4.60820980e-02  -4.14703136e-02  -3.68585291e-02  -3.22467446e-02
  -2.76349601e-02  -2.30231757e-02  -1.84113912e-02  -1.37996067e-02
  -9.18782226e-03  -4.57603779e-03   3.57466815e-05]
-1.06369
1.26245
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.259946 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.69911
Epoch 1, cost is  3.656
Epoch 2, cost is  3.61994
Epoch 3, cost is  3.58953
Epoch 4, cost is  3.56549
Training took 0.199216 minutes
Weight histogram
[2594 1499 2177 1617 1330 1392 1206 1412 4201  797] [ -6.55786023e-02  -5.90171674e-02  -5.24557325e-02  -4.58942976e-02
  -3.93328627e-02  -3.27714278e-02  -2.62099929e-02  -1.96485580e-02
  -1.30871231e-02  -6.52568821e-03   3.57466815e-05]
[4673 1104 1104 1229 1287 1461 1596 1705 2074 1992] [ -6.55786023e-02  -5.90171674e-02  -5.24557325e-02  -4.58942976e-02
  -3.93328627e-02  -3.27714278e-02  -2.62099929e-02  -1.96485580e-02
  -1.30871231e-02  -6.52568821e-03   3.57466815e-05]
-1.27823
1.38819
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.7897
Epoch 1, cost is  6.67694
Epoch 2, cost is  6.5974
Epoch 3, cost is  6.52026
Epoch 4, cost is  6.44149
Training took 0.275903 minutes
Weight histogram
[ 623 1144 2185 8085 1840  920  558  379  267  199] [ -1.24916639e-02  -1.12484575e-02  -1.00052511e-02  -8.76204467e-03
  -7.51883825e-03  -6.27563184e-03  -5.03242542e-03  -3.78921900e-03
  -2.54601259e-03  -1.30280617e-03  -5.95997553e-05]
[5837 2794 2240 1948 1599  660  448  314  196  164] [ -1.24916639e-02  -1.12484575e-02  -1.00052511e-02  -8.76204467e-03
  -7.51883825e-03  -6.27563184e-03  -5.03242542e-03  -3.78921900e-03
  -2.54601259e-03  -1.30280617e-03  -5.95997553e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042442 minutes
Epoch 0
Fine tuning took 0.042715 minutes
Epoch 0
Fine tuning took 0.042547 minutes
{'zero': {0: [0.31773399014778325, 0.19950738916256158, 0.22536945812807882, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.65886699507389157, 0.61699507389162567, 0.71305418719211822], 5: [0.27216748768472904, 0.14162561576354679, 0.15763546798029557, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31773399014778325, 0.19827586206896552, 0.22660098522167488, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.66625615763546797, 0.6354679802955665, 0.70566502463054193], 5: [0.27216748768472904, 0.1354679802955665, 0.13793103448275862, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31773399014778325, 0.20566502463054187, 0.18349753694581281, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.66379310344827591, 0.67487684729064035, 0.69211822660098521], 5: [0.27216748768472904, 0.13054187192118227, 0.14162561576354679, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31773399014778325, 0.18472906403940886, 0.23522167487684728, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41009852216748771, 0.66379310344827591, 0.63054187192118227, 0.66995073891625612], 5: [0.27216748768472904, 0.15147783251231528, 0.13423645320197045, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269997 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.89054
Epoch 1, cost is  2.8437
Epoch 2, cost is  2.80218
Epoch 3, cost is  2.76792
Epoch 4, cost is  2.73888
Training took 0.234057 minutes
Weight histogram
[3275 2340 2246 1886 1371 1517 1450  678 1239  198] [ -4.60820980e-02  -4.14703136e-02  -3.68585291e-02  -3.22467446e-02
  -2.76349601e-02  -2.30231757e-02  -1.84113912e-02  -1.37996067e-02
  -9.18782226e-03  -4.57603779e-03   3.57466815e-05]
[2224 1160 1003 1154 1398 1524 1679 1819 2145 2094] [ -4.60820980e-02  -4.14703136e-02  -3.68585291e-02  -3.22467446e-02
  -2.76349601e-02  -2.30231757e-02  -1.84113912e-02  -1.37996067e-02
  -9.18782226e-03  -4.57603779e-03   3.57466815e-05]
-1.06369
1.26245
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.263761 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.69911
Epoch 1, cost is  3.656
Epoch 2, cost is  3.61994
Epoch 3, cost is  3.58953
Epoch 4, cost is  3.56549
Training took 0.203653 minutes
Weight histogram
[2594 1499 2177 1617 1330 1392 1206 1412 4201  797] [ -6.55786023e-02  -5.90171674e-02  -5.24557325e-02  -4.58942976e-02
  -3.93328627e-02  -3.27714278e-02  -2.62099929e-02  -1.96485580e-02
  -1.30871231e-02  -6.52568821e-03   3.57466815e-05]
[4673 1104 1104 1229 1287 1461 1596 1705 2074 1992] [ -6.55786023e-02  -5.90171674e-02  -5.24557325e-02  -4.58942976e-02
  -3.93328627e-02  -3.27714278e-02  -2.62099929e-02  -1.96485580e-02
  -1.30871231e-02  -6.52568821e-03   3.57466815e-05]
-1.27823
1.38819
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.68386
Epoch 1, cost is  6.52328
Epoch 2, cost is  6.42895
Epoch 3, cost is  6.34337
Epoch 4, cost is  6.25783
Training took 0.362302 minutes
Weight histogram
[ 558  838 1427 6084 3986 1455  804  489  330  229] [ -1.41741699e-02  -1.27593708e-02  -1.13445717e-02  -9.92977266e-03
  -8.51497360e-03  -7.10017453e-03  -5.68537547e-03  -4.27057640e-03
  -2.85577734e-03  -1.44097827e-03  -2.61792065e-05]
[5063 2571 2152 1990 1905 1066  582  401  259  211] [ -1.41741699e-02  -1.27593708e-02  -1.13445717e-02  -9.92977266e-03
  -8.51497360e-03  -7.10017453e-03  -5.68537547e-03  -4.27057640e-03
  -2.85577734e-03  -1.44097827e-03  -2.61792065e-05]
-0.0721031
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045810 minutes
Epoch 0
Fine tuning took 0.045757 minutes
Epoch 0
Fine tuning took 0.045975 minutes
{'zero': {0: [0.33866995073891626, 0.11576354679802955, 0.24507389162561577, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.75123152709359609, 0.63423645320197042, 0.60221674876847286], 5: [0.24261083743842365, 0.13300492610837439, 0.1206896551724138, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33866995073891626, 0.11945812807881774, 0.25123152709359609, 0.25], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.73645320197044339, 0.61822660098522164, 0.62068965517241381], 5: [0.24261083743842365, 0.14408866995073891, 0.13054187192118227, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33866995073891626, 0.1145320197044335, 0.23275862068965517, 0.26108374384236455], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.7573891625615764, 0.64532019704433496, 0.61576354679802958], 5: [0.24261083743842365, 0.12807881773399016, 0.12192118226600986, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33866995073891626, 0.083743842364532015, 0.23029556650246305, 0.26970443349753692], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41871921182266009, 0.77709359605911332, 0.63669950738916259, 0.59359605911330049], 5: [0.24261083743842365, 0.13916256157635468, 0.13300492610837439, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264948 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.98415
Epoch 1, cost is  3.73402
Epoch 2, cost is  3.66561
Epoch 3, cost is  3.65429
Epoch 4, cost is  3.65887
Training took 0.230918 minutes
Weight histogram
[2130 1812 1757 2098 2102 1925 2051 1474  680  171] [-0.3284561  -0.29568825 -0.26292039 -0.23015253 -0.19738468 -0.16461682
 -0.13184897 -0.09908111 -0.06631325 -0.0335454  -0.00077754]
[ 483  971 1508 1704 1786 1855 2085 2013 1927 1868] [-0.3284561  -0.29568825 -0.26292039 -0.23015253 -0.19738468 -0.16461682
 -0.13184897 -0.09908111 -0.06631325 -0.0335454  -0.00077754]
-14.8856
19.8122
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264828 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  11.3705
Epoch 1, cost is  10.8587
Epoch 2, cost is  10.8092
Epoch 3, cost is  10.8577
Epoch 4, cost is  10.9473
Training took 0.201830 minutes
Weight histogram
[1705 1848 1755 1816 1692 1825 1782 1967 2310 1525] [-0.71995384 -0.64803621 -0.57611858 -0.50420095 -0.43228332 -0.36036569
 -0.28844806 -0.21653043 -0.1446128  -0.07269517 -0.00077754]
[2552 1637 1637 1701 1631 1743 1905 1869 1794 1756] [-0.71995384 -0.64803621 -0.57611858 -0.50420095 -0.43228332 -0.36036569
 -0.28844806 -0.21653043 -0.1446128  -0.07269517 -0.00077754]
-26.6766
24.8381
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.13189
Epoch 1, cost is  3.85177
Epoch 2, cost is  3.70156
Epoch 3, cost is  3.79783
Epoch 4, cost is  3.96555
Training took 0.229107 minutes
Weight histogram
[1990 2741 2867 2472 2079 1395  940  643  482  591] [-0.17713617 -0.15954683 -0.14195749 -0.12436816 -0.10677882 -0.08918949
 -0.07160015 -0.05401081 -0.03642148 -0.01883214 -0.0012428 ]
[ 995  931 1152 1365 1553 1758 1949 2091 2218 2188] [-0.17713617 -0.15954683 -0.14195749 -0.12436816 -0.10677882 -0.08918949
 -0.07160015 -0.05401081 -0.03642148 -0.01883214 -0.0012428 ]
-6.86173
7.17654
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043338 minutes
Epoch 0
Fine tuning took 0.045523 minutes
Epoch 0
Fine tuning took 0.045813 minutes
{'zero': {0: [0.27955665024630544, 0.22413793103448276, 0.26354679802955666, 0.18719211822660098], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44088669950738918, 0.6711822660098522, 0.60221674876847286, 0.6219211822660099], 5: [0.27955665024630544, 0.10467980295566502, 0.13423645320197045, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27955665024630544, 0.25615763546798032, 0.24014778325123154, 0.18965517241379309], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44088669950738918, 0.59605911330049266, 0.62315270935960587, 0.6576354679802956], 5: [0.27955665024630544, 0.14778325123152711, 0.13669950738916256, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27955665024630544, 0.2413793103448276, 0.26354679802955666, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44088669950738918, 0.61699507389162567, 0.61206896551724133, 0.63054187192118227], 5: [0.27955665024630544, 0.14162561576354679, 0.12438423645320197, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27955665024630544, 0.25, 0.25615763546798032, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.44088669950738918, 0.61083743842364535, 0.64532019704433496, 0.64039408866995073], 5: [0.27955665024630544, 0.13916256157635468, 0.098522167487684734, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.277035 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.98415
Epoch 1, cost is  3.73402
Epoch 2, cost is  3.66561
Epoch 3, cost is  3.65429
Epoch 4, cost is  3.65887
Training took 0.230979 minutes
Weight histogram
[2130 1812 1757 2098 2102 1925 2051 1474  680  171] [-0.3284561  -0.29568825 -0.26292039 -0.23015253 -0.19738468 -0.16461682
 -0.13184897 -0.09908111 -0.06631325 -0.0335454  -0.00077754]
[ 483  971 1508 1704 1786 1855 2085 2013 1927 1868] [-0.3284561  -0.29568825 -0.26292039 -0.23015253 -0.19738468 -0.16461682
 -0.13184897 -0.09908111 -0.06631325 -0.0335454  -0.00077754]
-14.8856
19.8122
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269292 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  11.3705
Epoch 1, cost is  10.8587
Epoch 2, cost is  10.8092
Epoch 3, cost is  10.8577
Epoch 4, cost is  10.9473
Training took 0.201729 minutes
Weight histogram
[1705 1848 1755 1816 1692 1825 1782 1967 2310 1525] [-0.71995384 -0.64803621 -0.57611858 -0.50420095 -0.43228332 -0.36036569
 -0.28844806 -0.21653043 -0.1446128  -0.07269517 -0.00077754]
[2552 1637 1637 1701 1631 1743 1905 1869 1794 1756] [-0.71995384 -0.64803621 -0.57611858 -0.50420095 -0.43228332 -0.36036569
 -0.28844806 -0.21653043 -0.1446128  -0.07269517 -0.00077754]
-26.6766
24.8381
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.77725
Epoch 1, cost is  2.93283
Epoch 2, cost is  2.55391
Epoch 3, cost is  2.47344
Epoch 4, cost is  2.47886
Training took 0.280483 minutes
Weight histogram
[2056 3035 2577 2332 2023 1439 1020  704  509  505] [-0.11230296 -0.1011986  -0.09009424 -0.07898989 -0.06788553 -0.05678117
 -0.04567681 -0.03457246 -0.0234681  -0.01236374 -0.00125938]
[1096  919 1112 1273 1501 1829 1954 2150 2274 2092] [-0.11230296 -0.1011986  -0.09009424 -0.07898989 -0.06788553 -0.05678117
 -0.04567681 -0.03457246 -0.0234681  -0.01236374 -0.00125938]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043323 minutes
Epoch 0
Fine tuning took 0.043601 minutes
Epoch 0
Fine tuning took 0.043497 minutes
{'zero': {0: [0.28694581280788178, 0.24014778325123154, 0.26847290640394089, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.66256157635467983, 0.6711822660098522, 0.72536945812807885], 5: [0.12192118226600986, 0.097290640394088676, 0.060344827586206899, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.28694581280788178, 0.18596059113300492, 0.1748768472906404, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.66256157635467983, 0.7142857142857143, 0.75862068965517238], 5: [0.12192118226600986, 0.15147783251231528, 0.11083743842364532, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.28694581280788178, 0.25862068965517243, 0.23029556650246305, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.61822660098522164, 0.66009852216748766, 0.71059113300492616], 5: [0.12192118226600986, 0.12315270935960591, 0.10960591133004927, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.28694581280788178, 0.23029556650246305, 0.18226600985221675, 0.094827586206896547], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59113300492610843, 0.64039408866995073, 0.71059113300492616, 0.73645320197044339], 5: [0.12192118226600986, 0.12931034482758622, 0.10714285714285714, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.260834 minutes
Weight histogram
[ 173  446  567  640 1587 3578 3692 3394 1996  127] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 7849] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.98415
Epoch 1, cost is  3.73402
Epoch 2, cost is  3.66561
Epoch 3, cost is  3.65429
Epoch 4, cost is  3.65887
Training took 0.228908 minutes
Weight histogram
[2130 1812 1757 2098 2102 1925 2051 1474  680  171] [-0.3284561  -0.29568825 -0.26292039 -0.23015253 -0.19738468 -0.16461682
 -0.13184897 -0.09908111 -0.06631325 -0.0335454  -0.00077754]
[ 483  971 1508 1704 1786 1855 2085 2013 1927 1868] [-0.3284561  -0.29568825 -0.26292039 -0.23015253 -0.19738468 -0.16461682
 -0.13184897 -0.09908111 -0.06631325 -0.0335454  -0.00077754]
-14.8856
19.8122
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271677 minutes
Weight histogram
[ 179  587 1090 2101 3016 3358 3909 2855  980  150] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
[ 250  297  407  560  854 1394  932 1718 3161 8652] [ -7.24884303e-05   1.12847601e-04   2.98183633e-04   4.83519664e-04
   6.68855695e-04   8.54191727e-04   1.03952776e-03   1.22486379e-03
   1.41019982e-03   1.59553585e-03   1.78087188e-03]
-1.12795
0.937578
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  11.3705
Epoch 1, cost is  10.8587
Epoch 2, cost is  10.8092
Epoch 3, cost is  10.8577
Epoch 4, cost is  10.9473
Training took 0.200193 minutes
Weight histogram
[1705 1848 1755 1816 1692 1825 1782 1967 2310 1525] [-0.71995384 -0.64803621 -0.57611858 -0.50420095 -0.43228332 -0.36036569
 -0.28844806 -0.21653043 -0.1446128  -0.07269517 -0.00077754]
[2552 1637 1637 1701 1631 1743 1905 1869 1794 1756] [-0.71995384 -0.64803621 -0.57611858 -0.50420095 -0.43228332 -0.36036569
 -0.28844806 -0.21653043 -0.1446128  -0.07269517 -0.00077754]
-26.6766
24.8381
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.54531
Epoch 1, cost is  2.39442
Epoch 2, cost is  1.87145
Epoch 3, cost is  1.6823
Epoch 4, cost is  1.6045
Training took 0.358597 minutes
Weight histogram
[2655 3118 2646 2167 1757 1486  908  727  486  250] [-0.07301652 -0.06583714 -0.05865775 -0.05147836 -0.04429898 -0.03711959
 -0.02994021 -0.02276082 -0.01558143 -0.00840205 -0.00122266]
[1106  848  986 1219 1472 1760 1993 2212 2395 2209] [-0.07301652 -0.06583714 -0.05865775 -0.05147836 -0.04429898 -0.03711959
 -0.02994021 -0.02276082 -0.01558143 -0.00840205 -0.00122266]
-3.98509
4.26369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046637 minutes
Epoch 0
Fine tuning took 0.046813 minutes
Epoch 0
Fine tuning took 0.047077 minutes
{'zero': {0: [0.19211822660098521, 0.23891625615763548, 0.30788177339901479, 0.29064039408866993], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.63916256157635465, 0.61945812807881773, 0.64778325123152714], 5: [0.1748768472906404, 0.12192118226600986, 0.072660098522167482, 0.061576354679802957], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19211822660098521, 0.1268472906403941, 0.15886699507389163, 0.11945812807881774], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.73891625615763545, 0.73645320197044339, 0.78694581280788178], 5: [0.1748768472906404, 0.13423645320197045, 0.10467980295566502, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19211822660098521, 0.14162561576354679, 0.19950738916256158, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.72413793103448276, 0.67610837438423643, 0.73522167487684731], 5: [0.1748768472906404, 0.13423645320197045, 0.12438423645320197, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19211822660098521, 0.10221674876847291, 0.18472906403940886, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63300492610837433, 0.75369458128078815, 0.70197044334975367, 0.80418719211822665], 5: [0.1748768472906404, 0.14408866995073891, 0.11330049261083744, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270757 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.95706
Epoch 1, cost is  1.85977
Epoch 2, cost is  1.82358
Epoch 3, cost is  1.79995
Epoch 4, cost is  1.78474
Training took 0.226174 minutes
Weight histogram
[3045 2846 2493 2506 2642 1736 1522  781  432  222] [ -1.32718101e-01  -1.19457654e-01  -1.06197207e-01  -9.29367605e-02
  -7.96763136e-02  -6.64158667e-02  -5.31554199e-02  -3.98949730e-02
  -2.66345261e-02  -1.33740792e-02  -1.13632348e-04]
[ 640  785 1220 1536 1772 2038 2307 2432 2545 2950] [ -1.32718101e-01  -1.19457654e-01  -1.06197207e-01  -9.29367605e-02
  -7.96763136e-02  -6.64158667e-02  -5.31554199e-02  -3.98949730e-02
  -2.66345261e-02  -1.33740792e-02  -1.13632348e-04]
-3.93109
6.40209
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267125 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.09924
Epoch 1, cost is  3.96522
Epoch 2, cost is  3.92765
Epoch 3, cost is  3.92098
Epoch 4, cost is  3.92322
Training took 0.203821 minutes
Weight histogram
[2785 2551 2159 2602 2239 2201 1704  968 2005 1036] [ -2.26773381e-01  -2.04107406e-01  -1.81441431e-01  -1.58775457e-01
  -1.36109482e-01  -1.13443507e-01  -9.07775319e-02  -6.81115570e-02
  -4.54455821e-02  -2.27796072e-02  -1.13632348e-04]
[2045 1673 1392 1696 1807 1952 2375 2389 2385 2536] [ -2.26773381e-01  -2.04107406e-01  -1.81441431e-01  -1.58775457e-01
  -1.36109482e-01  -1.13443507e-01  -9.07775319e-02  -6.81115570e-02
  -4.54455821e-02  -2.27796072e-02  -1.13632348e-04]
-7.0726
7.14635
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.5611
Epoch 1, cost is  5.37288
Epoch 2, cost is  4.57335
Epoch 3, cost is  4.18749
Epoch 4, cost is  3.94492
Training took 0.230180 minutes
Weight histogram
[2803 2359 2170 2018 1496 1292 1140 1289  958 2700] [-0.08295341 -0.07468115 -0.0664089  -0.05813664 -0.04986438 -0.04159213
 -0.03331987 -0.02504761 -0.01677536 -0.0085031  -0.00023084]
[2770 1557 1244 1408 1508 1682 1861 2092 2252 1851] [-0.08295341 -0.07468115 -0.0664089  -0.05813664 -0.04986438 -0.04159213
 -0.03331987 -0.02504761 -0.01677536 -0.0085031  -0.00023084]
-1.6243
2.76155
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042324 minutes
Epoch 0
Fine tuning took 0.043176 minutes
Epoch 0
Fine tuning took 0.041954 minutes
{'zero': {0: [0.26231527093596058, 0.24753694581280788, 0.20443349753694581, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47413793103448276, 0.58004926108374388, 0.60467980295566504, 0.6428571428571429], 5: [0.26354679802955666, 0.17241379310344829, 0.19088669950738915, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.26231527093596058, 0.24753694581280788, 0.21674876847290642, 0.16871921182266009], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47413793103448276, 0.53817733990147787, 0.6145320197044335, 0.66379310344827591], 5: [0.26354679802955666, 0.21428571428571427, 0.16871921182266009, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.26231527093596058, 0.25, 0.20566502463054187, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47413793103448276, 0.57389162561576357, 0.60837438423645318, 0.66256157635467983], 5: [0.26354679802955666, 0.17610837438423646, 0.18596059113300492, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.26231527093596058, 0.28201970443349755, 0.22536945812807882, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.47413793103448276, 0.50985221674876846, 0.6071428571428571, 0.6428571428571429], 5: [0.26354679802955666, 0.20812807881773399, 0.16748768472906403, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.262389 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.95706
Epoch 1, cost is  1.85977
Epoch 2, cost is  1.82358
Epoch 3, cost is  1.79995
Epoch 4, cost is  1.78474
Training took 0.231123 minutes
Weight histogram
[3045 2846 2493 2506 2642 1736 1522  781  432  222] [ -1.32718101e-01  -1.19457654e-01  -1.06197207e-01  -9.29367605e-02
  -7.96763136e-02  -6.64158667e-02  -5.31554199e-02  -3.98949730e-02
  -2.66345261e-02  -1.33740792e-02  -1.13632348e-04]
[ 640  785 1220 1536 1772 2038 2307 2432 2545 2950] [ -1.32718101e-01  -1.19457654e-01  -1.06197207e-01  -9.29367605e-02
  -7.96763136e-02  -6.64158667e-02  -5.31554199e-02  -3.98949730e-02
  -2.66345261e-02  -1.33740792e-02  -1.13632348e-04]
-3.93109
6.40209
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267324 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.09924
Epoch 1, cost is  3.96522
Epoch 2, cost is  3.92765
Epoch 3, cost is  3.92098
Epoch 4, cost is  3.92322
Training took 0.207135 minutes
Weight histogram
[2785 2551 2159 2602 2239 2201 1704  968 2005 1036] [ -2.26773381e-01  -2.04107406e-01  -1.81441431e-01  -1.58775457e-01
  -1.36109482e-01  -1.13443507e-01  -9.07775319e-02  -6.81115570e-02
  -4.54455821e-02  -2.27796072e-02  -1.13632348e-04]
[2045 1673 1392 1696 1807 1952 2375 2389 2385 2536] [ -2.26773381e-01  -2.04107406e-01  -1.81441431e-01  -1.58775457e-01
  -1.36109482e-01  -1.13443507e-01  -9.07775319e-02  -6.81115570e-02
  -4.54455821e-02  -2.27796072e-02  -1.13632348e-04]
-7.0726
7.14635
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46962
Epoch 1, cost is  5.22254
Epoch 2, cost is  4.11832
Epoch 3, cost is  3.49729
Epoch 4, cost is  3.0855
Training took 0.278821 minutes
Weight histogram
[2215 3169 2365 1988 1670 1312 1425 1211 2625  245] [-0.05099323 -0.04592217 -0.04085112 -0.03578007 -0.03070902 -0.02563796
 -0.02056691 -0.01549586 -0.01042481 -0.00535376 -0.0002827 ]
[2948 1629 1365 1474 1479 1663 1826 1985 2097 1759] [-0.05099323 -0.04592217 -0.04085112 -0.03578007 -0.03070902 -0.02563796
 -0.02056691 -0.01549586 -0.01042481 -0.00535376 -0.0002827 ]
-1.2098
1.51827
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043874 minutes
Epoch 0
Fine tuning took 0.043795 minutes
Epoch 0
Fine tuning took 0.043363 minutes
{'zero': {0: [0.30665024630541871, 0.29310344827586204, 0.26847290640394089, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.56527093596059108, 0.55665024630541871, 0.60098522167487689], 5: [0.13423645320197045, 0.14162561576354679, 0.1748768472906404, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30665024630541871, 0.29433497536945813, 0.22044334975369459, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.5431034482758621, 0.65147783251231528, 0.66256157635467983], 5: [0.13423645320197045, 0.1625615763546798, 0.12807881773399016, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30665024630541871, 0.27216748768472904, 0.29802955665024633, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.57758620689655171, 0.58251231527093594, 0.64039408866995073], 5: [0.13423645320197045, 0.15024630541871922, 0.11945812807881774, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30665024630541871, 0.30172413793103448, 0.23645320197044334, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.54433497536945807, 0.62684729064039413, 0.68226600985221675], 5: [0.13423645320197045, 0.1539408866995074, 0.13669950738916256, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.271819 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.95706
Epoch 1, cost is  1.85977
Epoch 2, cost is  1.82358
Epoch 3, cost is  1.79995
Epoch 4, cost is  1.78474
Training took 0.230984 minutes
Weight histogram
[3045 2846 2493 2506 2642 1736 1522  781  432  222] [ -1.32718101e-01  -1.19457654e-01  -1.06197207e-01  -9.29367605e-02
  -7.96763136e-02  -6.64158667e-02  -5.31554199e-02  -3.98949730e-02
  -2.66345261e-02  -1.33740792e-02  -1.13632348e-04]
[ 640  785 1220 1536 1772 2038 2307 2432 2545 2950] [ -1.32718101e-01  -1.19457654e-01  -1.06197207e-01  -9.29367605e-02
  -7.96763136e-02  -6.64158667e-02  -5.31554199e-02  -3.98949730e-02
  -2.66345261e-02  -1.33740792e-02  -1.13632348e-04]
-3.93109
6.40209
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.276581 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.09924
Epoch 1, cost is  3.96522
Epoch 2, cost is  3.92765
Epoch 3, cost is  3.92098
Epoch 4, cost is  3.92322
Training took 0.198002 minutes
Weight histogram
[2785 2551 2159 2602 2239 2201 1704  968 2005 1036] [ -2.26773381e-01  -2.04107406e-01  -1.81441431e-01  -1.58775457e-01
  -1.36109482e-01  -1.13443507e-01  -9.07775319e-02  -6.81115570e-02
  -4.54455821e-02  -2.27796072e-02  -1.13632348e-04]
[2045 1673 1392 1696 1807 1952 2375 2389 2385 2536] [ -2.26773381e-01  -2.04107406e-01  -1.81441431e-01  -1.58775457e-01
  -1.36109482e-01  -1.13443507e-01  -9.07775319e-02  -6.81115570e-02
  -4.54455821e-02  -2.27796072e-02  -1.13632348e-04]
-7.0726
7.14635
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.35833
Epoch 1, cost is  5.13441
Epoch 2, cost is  3.91917
Epoch 3, cost is  3.134
Epoch 4, cost is  2.62313
Training took 0.356043 minutes
Weight histogram
[2319 3351 2711 2192 1807 1686 1412 2115  519  113] [-0.03414995 -0.03075964 -0.02736933 -0.02397902 -0.02058872 -0.01719841
 -0.0138081  -0.01041779 -0.00702749 -0.00363718 -0.00024687]
[3222 1651 1361 1356 1488 1633 1732 1808 2075 1899] [-0.03414995 -0.03075964 -0.02736933 -0.02397902 -0.02058872 -0.01719841
 -0.0138081  -0.01041779 -0.00702749 -0.00363718 -0.00024687]
-0.957712
1.41078
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046720 minutes
Epoch 0
Fine tuning took 0.046594 minutes
Epoch 0
Fine tuning took 0.047761 minutes
{'zero': {0: [0.38300492610837439, 0.37807881773399016, 0.27339901477832512, 0.31034482758620691], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4605911330049261, 0.42364532019704432, 0.41995073891625617, 0.50985221674876846], 5: [0.15640394088669951, 0.19827586206896552, 0.30665024630541871, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.38300492610837439, 0.34729064039408869, 0.28817733990147781, 0.2857142857142857], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4605911330049261, 0.49014778325123154, 0.4642857142857143, 0.54802955665024633], 5: [0.15640394088669951, 0.1625615763546798, 0.24753694581280788, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.38300492610837439, 0.34236453201970446, 0.27463054187192121, 0.29802955665024633], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4605911330049261, 0.47660098522167488, 0.46182266009852219, 0.54433497536945807], 5: [0.15640394088669951, 0.18103448275862069, 0.26354679802955666, 0.15763546798029557], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.38300492610837439, 0.31773399014778325, 0.27955665024630544, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4605911330049261, 0.50862068965517238, 0.49014778325123154, 0.5357142857142857], 5: [0.15640394088669951, 0.17364532019704434, 0.23029556650246305, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.281369 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.82655
Epoch 1, cost is  1.76224
Epoch 2, cost is  1.73375
Epoch 3, cost is  1.71567
Epoch 4, cost is  1.70193
Training took 0.220406 minutes
Weight histogram
[3420 2603 3097 2783 1943 1705 1210  722  431  311] [ -9.59578902e-02  -8.63651655e-02  -7.67724409e-02  -6.71797163e-02
  -5.75869917e-02  -4.79942671e-02  -3.84015425e-02  -2.88088178e-02
  -1.92160932e-02  -9.62336861e-03  -3.06439943e-05]
[ 886  801 1149 1453 1686 1977 2241 2433 2619 2980] [ -9.59578902e-02  -8.63651655e-02  -7.67724409e-02  -6.71797163e-02
  -5.75869917e-02  -4.79942671e-02  -3.84015425e-02  -2.88088178e-02
  -1.92160932e-02  -9.62336861e-03  -3.06439943e-05]
-2.73338
3.31842
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.273971 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.28926
Epoch 1, cost is  3.19579
Epoch 2, cost is  3.16492
Epoch 3, cost is  3.14792
Epoch 4, cost is  3.14375
Training took 0.201612 minutes
Weight histogram
[3130 2869 3070 2002 2362 1568 1097 1262 1666 1224] [ -1.51530489e-01  -1.36380505e-01  -1.21230520e-01  -1.06080536e-01
  -9.09305512e-02  -7.57805667e-02  -6.06305821e-02  -4.54805976e-02
  -3.03306131e-02  -1.51806285e-02  -3.06439943e-05]
[2437 1452 1177 1551 1772 1832 2395 2461 2467 2706] [ -1.51530489e-01  -1.36380505e-01  -1.21230520e-01  -1.06080536e-01
  -9.09305512e-02  -7.57805667e-02  -6.06305821e-02  -4.54805976e-02
  -3.03306131e-02  -1.51806285e-02  -3.06439943e-05]
-4.39155
4.30609
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74612
Epoch 1, cost is  6.34724
Epoch 2, cost is  5.62027
Epoch 3, cost is  5.02122
Epoch 4, cost is  4.62033
Training took 0.235853 minutes
Weight histogram
[ 693 2075 1710 1768 1375 1505 1589 1400 5554  556] [-0.0558529  -0.05027848 -0.04470406 -0.03912964 -0.03355522 -0.0279808
 -0.02240639 -0.01683197 -0.01125755 -0.00568313 -0.00010871]
[4257 1922 1817 1445 1447 1604 1686 1707 1452  888] [-0.0558529  -0.05027848 -0.04470406 -0.03912964 -0.03355522 -0.0279808
 -0.02240639 -0.01683197 -0.01125755 -0.00568313 -0.00010871]
-0.925005
1.26551
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041504 minutes
Epoch 0
Fine tuning took 0.041348 minutes
Epoch 0
Fine tuning took 0.041401 minutes
{'zero': {0: [0.30418719211822659, 0.25123152709359609, 0.22906403940886699, 0.28448275862068967], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51847290640394084, 0.56157635467980294, 0.60960591133004927, 0.57635467980295563], 5: [0.17733990147783252, 0.18719211822660098, 0.16133004926108374, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.30418719211822659, 0.27709359605911332, 0.2376847290640394, 0.28078817733990147], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51847290640394084, 0.56896551724137934, 0.58251231527093594, 0.57019704433497542], 5: [0.17733990147783252, 0.1539408866995074, 0.17980295566502463, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.30418719211822659, 0.25123152709359609, 0.21798029556650247, 0.32019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51847290640394084, 0.58374384236453203, 0.60960591133004927, 0.52832512315270941], 5: [0.17733990147783252, 0.16502463054187191, 0.17241379310344829, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.30418719211822659, 0.24753694581280788, 0.22413793103448276, 0.3251231527093596], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51847290640394084, 0.57512315270935965, 0.61699507389162567, 0.53078817733990147], 5: [0.17733990147783252, 0.17733990147783252, 0.15886699507389163, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266920 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.82655
Epoch 1, cost is  1.76224
Epoch 2, cost is  1.73375
Epoch 3, cost is  1.71567
Epoch 4, cost is  1.70193
Training took 0.225591 minutes
Weight histogram
[3420 2603 3097 2783 1943 1705 1210  722  431  311] [ -9.59578902e-02  -8.63651655e-02  -7.67724409e-02  -6.71797163e-02
  -5.75869917e-02  -4.79942671e-02  -3.84015425e-02  -2.88088178e-02
  -1.92160932e-02  -9.62336861e-03  -3.06439943e-05]
[ 886  801 1149 1453 1686 1977 2241 2433 2619 2980] [ -9.59578902e-02  -8.63651655e-02  -7.67724409e-02  -6.71797163e-02
  -5.75869917e-02  -4.79942671e-02  -3.84015425e-02  -2.88088178e-02
  -1.92160932e-02  -9.62336861e-03  -3.06439943e-05]
-2.73338
3.31842
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.273908 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.28926
Epoch 1, cost is  3.19579
Epoch 2, cost is  3.16492
Epoch 3, cost is  3.14792
Epoch 4, cost is  3.14375
Training took 0.201810 minutes
Weight histogram
[3130 2869 3070 2002 2362 1568 1097 1262 1666 1224] [ -1.51530489e-01  -1.36380505e-01  -1.21230520e-01  -1.06080536e-01
  -9.09305512e-02  -7.57805667e-02  -6.06305821e-02  -4.54805976e-02
  -3.03306131e-02  -1.51806285e-02  -3.06439943e-05]
[2437 1452 1177 1551 1772 1832 2395 2461 2467 2706] [ -1.51530489e-01  -1.36380505e-01  -1.21230520e-01  -1.06080536e-01
  -9.09305512e-02  -7.57805667e-02  -6.06305821e-02  -4.54805976e-02
  -3.03306131e-02  -1.51806285e-02  -3.06439943e-05]
-4.39155
4.30609
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.65295
Epoch 1, cost is  6.25577
Epoch 2, cost is  5.52976
Epoch 3, cost is  4.81893
Epoch 4, cost is  4.23819
Training took 0.280443 minutes
Weight histogram
[ 354 2198 2342 1985 1807 1937 1617 3346 2404  235] [-0.03642388 -0.03279739 -0.0291709  -0.02554441 -0.02191792 -0.01829142
 -0.01466493 -0.01103844 -0.00741195 -0.00378545 -0.00015896]
[3949 2344 1931 1450 1444 1532 1575 1563 1513  924] [-0.03642388 -0.03279739 -0.0291709  -0.02554441 -0.02191792 -0.01829142
 -0.01466493 -0.01103844 -0.00741195 -0.00378545 -0.00015896]
-0.823236
0.921125
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043158 minutes
Epoch 0
Fine tuning took 0.043196 minutes
Epoch 0
Fine tuning took 0.043268 minutes
{'zero': {0: [0.31280788177339902, 0.23891625615763548, 0.26724137931034481, 0.34975369458128081], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.58374384236453203, 0.58990147783251234, 0.51600985221674878], 5: [0.14778325123152711, 0.17733990147783252, 0.14285714285714285, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.31280788177339902, 0.19950738916256158, 0.29926108374384236, 0.31157635467980294], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.61206896551724133, 0.55418719211822665, 0.57019704433497542], 5: [0.14778325123152711, 0.18842364532019704, 0.14655172413793102, 0.11822660098522167], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.31280788177339902, 0.25615763546798032, 0.26477832512315269, 0.33497536945812806], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.55172413793103448, 0.58497536945812811, 0.52216748768472909], 5: [0.14778325123152711, 0.19211822660098521, 0.15024630541871922, 0.14285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.31280788177339902, 0.24261083743842365, 0.27216748768472904, 0.3251231527093596], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.53940886699507384, 0.58497536945812811, 0.59482758620689657, 0.55541871921182262], 5: [0.14778325123152711, 0.17241379310344829, 0.13300492610837439, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265146 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.82655
Epoch 1, cost is  1.76224
Epoch 2, cost is  1.73375
Epoch 3, cost is  1.71567
Epoch 4, cost is  1.70193
Training took 0.224741 minutes
Weight histogram
[3420 2603 3097 2783 1943 1705 1210  722  431  311] [ -9.59578902e-02  -8.63651655e-02  -7.67724409e-02  -6.71797163e-02
  -5.75869917e-02  -4.79942671e-02  -3.84015425e-02  -2.88088178e-02
  -1.92160932e-02  -9.62336861e-03  -3.06439943e-05]
[ 886  801 1149 1453 1686 1977 2241 2433 2619 2980] [ -9.59578902e-02  -8.63651655e-02  -7.67724409e-02  -6.71797163e-02
  -5.75869917e-02  -4.79942671e-02  -3.84015425e-02  -2.88088178e-02
  -1.92160932e-02  -9.62336861e-03  -3.06439943e-05]
-2.73338
3.31842
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268584 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.28926
Epoch 1, cost is  3.19579
Epoch 2, cost is  3.16492
Epoch 3, cost is  3.14792
Epoch 4, cost is  3.14375
Training took 0.205416 minutes
Weight histogram
[3130 2869 3070 2002 2362 1568 1097 1262 1666 1224] [ -1.51530489e-01  -1.36380505e-01  -1.21230520e-01  -1.06080536e-01
  -9.09305512e-02  -7.57805667e-02  -6.06305821e-02  -4.54805976e-02
  -3.03306131e-02  -1.51806285e-02  -3.06439943e-05]
[2437 1452 1177 1551 1772 1832 2395 2461 2467 2706] [ -1.51530489e-01  -1.36380505e-01  -1.21230520e-01  -1.06080536e-01
  -9.09305512e-02  -7.57805667e-02  -6.06305821e-02  -4.54805976e-02
  -3.03306131e-02  -1.51806285e-02  -3.06439943e-05]
-4.39155
4.30609
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.52164
Epoch 1, cost is  6.15162
Epoch 2, cost is  5.48445
Epoch 3, cost is  4.70585
Epoch 4, cost is  4.08302
Training took 0.348193 minutes
Weight histogram
[ 434 2436 2872 2484 2459 2077 2913 2062  344  144] [-0.02634971 -0.02372716 -0.0211046  -0.01848205 -0.01585949 -0.01323694
 -0.01061438 -0.00799183 -0.00536927 -0.00274672 -0.00012417]
[3791 2827 2076 1397 1417 1393 1379 1446 1455 1044] [-0.02634971 -0.02372716 -0.0211046  -0.01848205 -0.01585949 -0.01323694
 -0.01061438 -0.00799183 -0.00536927 -0.00274672 -0.00012417]
-0.623447
0.829825
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049651 minutes
Epoch 0
Fine tuning took 0.046554 minutes
Epoch 0
Fine tuning took 0.046750 minutes
{'zero': {0: [0.33128078817733991, 0.28325123152709358, 0.22906403940886699, 0.31403940886699505], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.57019704433497542, 0.64901477832512311, 0.51970443349753692], 5: [0.11330049261083744, 0.14655172413793102, 0.12192118226600986, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33128078817733991, 0.29433497536945813, 0.2376847290640394, 0.30911330049261082], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.56403940886699511, 0.61576354679802958, 0.55541871921182262], 5: [0.11330049261083744, 0.14162561576354679, 0.14655172413793102, 0.1354679802955665], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33128078817733991, 0.26477832512315269, 0.23399014778325122, 0.29926108374384236], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.57019704433497542, 0.64655172413793105, 0.55665024630541871], 5: [0.11330049261083744, 0.16502463054187191, 0.11945812807881774, 0.14408866995073891], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33128078817733991, 0.29187192118226601, 0.2376847290640394, 0.30911330049261082], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55541871921182262, 0.56157635467980294, 0.62438423645320196, 0.55295566502463056], 5: [0.11330049261083744, 0.14655172413793102, 0.13793103448275862, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270652 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.69391
Epoch 1, cost is  2.65401
Epoch 2, cost is  2.61989
Epoch 3, cost is  2.59471
Epoch 4, cost is  2.57144
Training took 0.226920 minutes
Weight histogram
[3501 2609 2635 2274 1605 1474 1803  793 1307  224] [ -4.93701249e-02  -4.44295378e-02  -3.94889506e-02  -3.45483635e-02
  -2.96077763e-02  -2.46671891e-02  -1.97266020e-02  -1.47860148e-02
  -9.84542764e-03  -4.90484048e-03   3.57466815e-05]
[2334 1213 1101 1305 1584 1709 1912 2184 2289 2594] [ -4.93701249e-02  -4.44295378e-02  -3.94889506e-02  -3.45483635e-02
  -2.96077763e-02  -2.46671891e-02  -1.97266020e-02  -1.47860148e-02
  -9.84542764e-03  -4.90484048e-03   3.57466815e-05]
-1.15396
1.31273
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.255098 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.55748
Epoch 1, cost is  3.51026
Epoch 2, cost is  3.4761
Epoch 3, cost is  3.4524
Epoch 4, cost is  3.42845
Training took 0.206407 minutes
Weight histogram
[2098 2833 1656 2404 1478 1681 1310 1447 3950 1393] [ -7.25306571e-02  -6.52740167e-02  -5.80173763e-02  -5.07607359e-02
  -4.35040956e-02  -3.62474552e-02  -2.89908148e-02  -2.17341744e-02
  -1.44775341e-02  -7.22089369e-03   3.57466815e-05]
[4770 1166 1221 1332 1441 1668 1759 2107 2145 2641] [ -7.25306571e-02  -6.52740167e-02  -5.80173763e-02  -5.07607359e-02
  -4.35040956e-02  -3.62474552e-02  -2.89908148e-02  -2.17341744e-02
  -1.44775341e-02  -7.22089369e-03   3.57466815e-05]
-1.41082
1.53966
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.85717
Epoch 1, cost is  6.77644
Epoch 2, cost is  6.71004
Epoch 3, cost is  6.64659
Epoch 4, cost is  6.57701
Training took 0.232368 minutes
Weight histogram
[ 1660  1785 10633  1580   887   574   399   300   227   180] [ -1.02815339e-02  -9.25411123e-03  -8.22668852e-03  -7.19926582e-03
  -6.17184311e-03  -5.14442041e-03  -4.11699770e-03  -3.08957500e-03
  -2.06215229e-03  -1.03472959e-03  -7.30688498e-06]
[7593 3485 2674 2225  992  487  288  232  126  123] [ -1.02815339e-02  -9.25411123e-03  -8.22668852e-03  -7.19926582e-03
  -6.17184311e-03  -5.14442041e-03  -4.11699770e-03  -3.08957500e-03
  -2.06215229e-03  -1.03472959e-03  -7.30688498e-06]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040945 minutes
Epoch 0
Fine tuning took 0.041089 minutes
Epoch 0
Fine tuning took 0.041073 minutes
{'zero': {0: [0.066502463054187194, 0.14778325123152711, 0.10098522167487685, 0.12438423645320197], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.79679802955665024, 0.72044334975369462, 0.77339901477832518], 5: [0.14039408866995073, 0.055418719211822662, 0.17857142857142858, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.066502463054187194, 0.14285714285714285, 0.10344827586206896, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.80172413793103448, 0.71182266009852213, 0.77463054187192115], 5: [0.14039408866995073, 0.055418719211822662, 0.18472906403940886, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.066502463054187194, 0.13423645320197045, 0.1268472906403941, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.80418719211822665, 0.72044334975369462, 0.72290640394088668], 5: [0.14039408866995073, 0.061576354679802957, 0.15270935960591134, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.066502463054187194, 0.14655172413793102, 0.10714285714285714, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.80172413793103448, 0.68349753694581283, 0.75615763546798032], 5: [0.14039408866995073, 0.051724137931034482, 0.20935960591133004, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.280257 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.69391
Epoch 1, cost is  2.65401
Epoch 2, cost is  2.61989
Epoch 3, cost is  2.59471
Epoch 4, cost is  2.57144
Training took 0.226130 minutes
Weight histogram
[3501 2609 2635 2274 1605 1474 1803  793 1307  224] [ -4.93701249e-02  -4.44295378e-02  -3.94889506e-02  -3.45483635e-02
  -2.96077763e-02  -2.46671891e-02  -1.97266020e-02  -1.47860148e-02
  -9.84542764e-03  -4.90484048e-03   3.57466815e-05]
[2334 1213 1101 1305 1584 1709 1912 2184 2289 2594] [ -4.93701249e-02  -4.44295378e-02  -3.94889506e-02  -3.45483635e-02
  -2.96077763e-02  -2.46671891e-02  -1.97266020e-02  -1.47860148e-02
  -9.84542764e-03  -4.90484048e-03   3.57466815e-05]
-1.15396
1.31273
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264370 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.55748
Epoch 1, cost is  3.51026
Epoch 2, cost is  3.4761
Epoch 3, cost is  3.4524
Epoch 4, cost is  3.42845
Training took 0.202114 minutes
Weight histogram
[2098 2833 1656 2404 1478 1681 1310 1447 3950 1393] [ -7.25306571e-02  -6.52740167e-02  -5.80173763e-02  -5.07607359e-02
  -4.35040956e-02  -3.62474552e-02  -2.89908148e-02  -2.17341744e-02
  -1.44775341e-02  -7.22089369e-03   3.57466815e-05]
[4770 1166 1221 1332 1441 1668 1759 2107 2145 2641] [ -7.25306571e-02  -6.52740167e-02  -5.80173763e-02  -5.07607359e-02
  -4.35040956e-02  -3.62474552e-02  -2.89908148e-02  -2.17341744e-02
  -1.44775341e-02  -7.22089369e-03   3.57466815e-05]
-1.41082
1.53966
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79044
Epoch 1, cost is  6.67833
Epoch 2, cost is  6.59977
Epoch 3, cost is  6.52366
Epoch 4, cost is  6.44607
Training took 0.263660 minutes
Weight histogram
[ 623 1144 2186 9430 2170 1065  643  432  305  227] [ -1.24916639e-02  -1.12484042e-02  -1.00051446e-02  -8.76188487e-03
  -7.51862518e-03  -6.27536550e-03  -5.03210582e-03  -3.78884613e-03
  -2.54558645e-03  -1.30232677e-03  -5.90670825e-05]
[6683 3201 2571 2231 1757  660  448  314  196  164] [ -1.24916639e-02  -1.12484042e-02  -1.00051446e-02  -8.76188487e-03
  -7.51862518e-03  -6.27536550e-03  -5.03210582e-03  -3.78884613e-03
  -2.54558645e-03  -1.30232677e-03  -5.90670825e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042937 minutes
Epoch 0
Fine tuning took 0.043565 minutes
Epoch 0
Fine tuning took 0.043208 minutes
{'zero': {0: [0.060344827586206899, 0.13300492610837439, 0.1539408866995074, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81773399014778325, 0.79926108374384242, 0.70812807881773399, 0.64778325123152714], 5: [0.12192118226600986, 0.067733990147783252, 0.13793103448275862, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.060344827586206899, 0.14285714285714285, 0.18349753694581281, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81773399014778325, 0.81280788177339902, 0.67364532019704437, 0.65147783251231528], 5: [0.12192118226600986, 0.044334975369458129, 0.14285714285714285, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.060344827586206899, 0.10960591133004927, 0.1625615763546798, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81773399014778325, 0.83866995073891626, 0.69334975369458129, 0.66625615763546797], 5: [0.12192118226600986, 0.051724137931034482, 0.14408866995073891, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.060344827586206899, 0.14162561576354679, 0.16748768472906403, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.81773399014778325, 0.79187192118226601, 0.66379310344827591, 0.67980295566502458], 5: [0.12192118226600986, 0.066502463054187194, 0.16871921182266009, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.268865 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.69391
Epoch 1, cost is  2.65401
Epoch 2, cost is  2.61989
Epoch 3, cost is  2.59471
Epoch 4, cost is  2.57144
Training took 0.229667 minutes
Weight histogram
[3501 2609 2635 2274 1605 1474 1803  793 1307  224] [ -4.93701249e-02  -4.44295378e-02  -3.94889506e-02  -3.45483635e-02
  -2.96077763e-02  -2.46671891e-02  -1.97266020e-02  -1.47860148e-02
  -9.84542764e-03  -4.90484048e-03   3.57466815e-05]
[2334 1213 1101 1305 1584 1709 1912 2184 2289 2594] [ -4.93701249e-02  -4.44295378e-02  -3.94889506e-02  -3.45483635e-02
  -2.96077763e-02  -2.46671891e-02  -1.97266020e-02  -1.47860148e-02
  -9.84542764e-03  -4.90484048e-03   3.57466815e-05]
-1.15396
1.31273
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265998 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.55748
Epoch 1, cost is  3.51026
Epoch 2, cost is  3.4761
Epoch 3, cost is  3.4524
Epoch 4, cost is  3.42845
Training took 0.203919 minutes
Weight histogram
[2098 2833 1656 2404 1478 1681 1310 1447 3950 1393] [ -7.25306571e-02  -6.52740167e-02  -5.80173763e-02  -5.07607359e-02
  -4.35040956e-02  -3.62474552e-02  -2.89908148e-02  -2.17341744e-02
  -1.44775341e-02  -7.22089369e-03   3.57466815e-05]
[4770 1166 1221 1332 1441 1668 1759 2107 2145 2641] [ -7.25306571e-02  -6.52740167e-02  -5.80173763e-02  -5.07607359e-02
  -4.35040956e-02  -3.62474552e-02  -2.89908148e-02  -2.17341744e-02
  -1.44775341e-02  -7.22089369e-03   3.57466815e-05]
-1.41082
1.53966
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.68549
Epoch 1, cost is  6.52594
Epoch 2, cost is  6.43225
Epoch 3, cost is  6.3485
Epoch 4, cost is  6.26457
Training took 0.353072 minutes
Weight histogram
[ 558  838 1427 6800 4782 1694  927  561  377  261] [ -1.41741699e-02  -1.27593708e-02  -1.13445717e-02  -9.92977266e-03
  -8.51497360e-03  -7.10017453e-03  -5.68537547e-03  -4.27057640e-03
  -2.85577734e-03  -1.44097827e-03  -2.61792065e-05]
[5803 2950 2469 2284 2184 1082  582  401  259  211] [ -1.41741699e-02  -1.27593708e-02  -1.13445717e-02  -9.92977266e-03
  -8.51497360e-03  -7.10017453e-03  -5.68537547e-03  -4.27057640e-03
  -2.85577734e-03  -1.44097827e-03  -2.61792065e-05]
-0.0721031
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046714 minutes
Epoch 0
Fine tuning took 0.046535 minutes
Epoch 0
Fine tuning took 0.046442 minutes
{'zero': {0: [0.087438423645320201, 0.2019704433497537, 0.097290640394088676, 0.083743842364532015], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79926108374384242, 0.72167487684729059, 0.75862068965517238, 0.74876847290640391], 5: [0.11330049261083744, 0.076354679802955669, 0.14408866995073891, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.087438423645320201, 0.2019704433497537, 0.12807881773399016, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79926108374384242, 0.71305418719211822, 0.71674876847290636, 0.75], 5: [0.11330049261083744, 0.084975369458128072, 0.15517241379310345, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.087438423645320201, 0.18226600985221675, 0.11206896551724138, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79926108374384242, 0.74876847290640391, 0.74384236453201968, 0.73152709359605916], 5: [0.11330049261083744, 0.068965517241379309, 0.14408866995073891, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.087438423645320201, 0.19581280788177341, 0.11083743842364532, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.79926108374384242, 0.72783251231527091, 0.73891625615763545, 0.72167487684729059], 5: [0.11330049261083744, 0.076354679802955669, 0.15024630541871922, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.283774 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.31622
Epoch 1, cost is  4.00392
Epoch 2, cost is  3.90203
Epoch 3, cost is  3.8621
Epoch 4, cost is  3.84722
Training took 0.232870 minutes
Weight histogram
[2346 2310 1460 2392 2306 2276 2135 1924  859  217] [-0.35892731 -0.32311233 -0.28729736 -0.25148238 -0.2156674  -0.17985242
 -0.14403745 -0.10822247 -0.07240749 -0.03659252 -0.00077754]
[ 555 1134 1738 1894 1990 2169 2233 2147 2053 2312] [-0.35892731 -0.32311233 -0.28729736 -0.25148238 -0.2156674  -0.17985242
 -0.14403745 -0.10822247 -0.07240749 -0.03659252 -0.00077754]
-17.3462
19.8985
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267988 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.4921
Epoch 1, cost is  11.8513
Epoch 2, cost is  11.7598
Epoch 3, cost is  11.7868
Epoch 4, cost is  11.8615
Training took 0.205224 minutes
Weight histogram
[1997 1748 2107 1924 2206 1773 2155 2087 2281 1972] [ -8.09575081e-01  -7.28695327e-01  -6.47815573e-01  -5.66935819e-01
  -4.86056064e-01  -4.05176310e-01  -3.24296556e-01  -2.43416802e-01
  -1.62537048e-01  -8.16572940e-02  -7.77539855e-04]
[2930 1610 1832 1855 1761 2093 2105 2010 1941 2113] [ -8.09575081e-01  -7.28695327e-01  -6.47815573e-01  -5.66935819e-01
  -4.86056064e-01  -4.05176310e-01  -3.24296556e-01  -2.43416802e-01
  -1.62537048e-01  -8.16572940e-02  -7.77539855e-04]
-27.07
28.4644
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.15056
Epoch 1, cost is  3.88504
Epoch 2, cost is  3.71919
Epoch 3, cost is  3.80923
Epoch 4, cost is  4.01605
Training took 0.233280 minutes
Weight histogram
[2148 3135 3204 2798 2365 1565 1079  724  543  664] [-0.17713617 -0.15954588 -0.14195559 -0.1243653  -0.106775   -0.08918471
 -0.07159442 -0.05400413 -0.03641384 -0.01882355 -0.00123326]
[1118 1052 1298 1540 1747 1979 2198 2353 2496 2444] [-0.17713617 -0.15954588 -0.14195559 -0.1243653  -0.106775   -0.08918471
 -0.07159442 -0.05400413 -0.03641384 -0.01882355 -0.00123326]
-7.17639
7.17654
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042397 minutes
Epoch 0
Fine tuning took 0.042234 minutes
Epoch 0
Fine tuning took 0.043728 minutes
{'zero': {0: [0.2894088669950739, 0.40270935960591131, 0.30665024630541871, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4642857142857143, 0.36699507389162561, 0.34113300492610837, 0.3288177339901478], 5: [0.24630541871921183, 0.23029556650246305, 0.35221674876847292, 0.41379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.2894088669950739, 0.25246305418719212, 0.26108374384236455, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4642857142857143, 0.55049261083743839, 0.53817733990147787, 0.52339901477832518], 5: [0.24630541871921183, 0.19704433497536947, 0.20073891625615764, 0.20935960591133004], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.2894088669950739, 0.28201970443349755, 0.25246305418719212, 0.27586206896551724], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4642857142857143, 0.51970443349753692, 0.51108374384236455, 0.48522167487684731], 5: [0.24630541871921183, 0.19827586206896552, 0.23645320197044334, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.2894088669950739, 0.25985221674876846, 0.24014778325123154, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4642857142857143, 0.58251231527093594, 0.58374384236453203, 0.54556650246305416], 5: [0.24630541871921183, 0.15763546798029557, 0.17610837438423646, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270261 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.31622
Epoch 1, cost is  4.00392
Epoch 2, cost is  3.90203
Epoch 3, cost is  3.8621
Epoch 4, cost is  3.84722
Training took 0.237157 minutes
Weight histogram
[2346 2310 1460 2392 2306 2276 2135 1924  859  217] [-0.35892731 -0.32311233 -0.28729736 -0.25148238 -0.2156674  -0.17985242
 -0.14403745 -0.10822247 -0.07240749 -0.03659252 -0.00077754]
[ 555 1134 1738 1894 1990 2169 2233 2147 2053 2312] [-0.35892731 -0.32311233 -0.28729736 -0.25148238 -0.2156674  -0.17985242
 -0.14403745 -0.10822247 -0.07240749 -0.03659252 -0.00077754]
-17.3462
19.8985
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.279944 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.4921
Epoch 1, cost is  11.8513
Epoch 2, cost is  11.7598
Epoch 3, cost is  11.7868
Epoch 4, cost is  11.8615
Training took 0.205743 minutes
Weight histogram
[1997 1748 2107 1924 2206 1773 2155 2087 2281 1972] [ -8.09575081e-01  -7.28695327e-01  -6.47815573e-01  -5.66935819e-01
  -4.86056064e-01  -4.05176310e-01  -3.24296556e-01  -2.43416802e-01
  -1.62537048e-01  -8.16572940e-02  -7.77539855e-04]
[2930 1610 1832 1855 1761 2093 2105 2010 1941 2113] [ -8.09575081e-01  -7.28695327e-01  -6.47815573e-01  -5.66935819e-01
  -4.86056064e-01  -4.05176310e-01  -3.24296556e-01  -2.43416802e-01
  -1.62537048e-01  -8.16572940e-02  -7.77539855e-04]
-27.07
28.4644
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.78653
Epoch 1, cost is  2.94893
Epoch 2, cost is  2.55268
Epoch 3, cost is  2.47594
Epoch 4, cost is  2.50151
Training took 0.281207 minutes
Weight histogram
[2241 3435 2897 2659 2281 1615 1158  798  574  567] [-0.11230296 -0.10119756 -0.09009215 -0.07898675 -0.06788135 -0.05677594
 -0.04567054 -0.03456514 -0.02345973 -0.01235433 -0.00124893]
[1232 1035 1251 1435 1687 2056 2193 2411 2548 2377] [-0.11230296 -0.10119756 -0.09009215 -0.07898675 -0.06788135 -0.05677594
 -0.04567054 -0.03456514 -0.02345973 -0.01235433 -0.00124893]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043653 minutes
Epoch 0
Fine tuning took 0.043541 minutes
Epoch 0
Fine tuning took 0.043582 minutes
{'zero': {0: [0.18472906403940886, 0.35344827586206895, 0.44581280788177341, 0.37315270935960593], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.40024630541871919, 0.30049261083743845, 0.33004926108374383], 5: [0.25615763546798032, 0.24630541871921183, 0.2536945812807882, 0.29679802955665024], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18472906403940886, 0.20935960591133004, 0.32389162561576357, 0.28448275862068967], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.61330049261083741, 0.51108374384236455, 0.51600985221674878], 5: [0.25615763546798032, 0.17733990147783252, 0.16502463054187191, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18472906403940886, 0.22044334975369459, 0.30418719211822659, 0.29433497536945813], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.56773399014778325, 0.50246305418719217, 0.48645320197044334], 5: [0.25615763546798032, 0.21182266009852216, 0.19334975369458129, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18472906403940886, 0.23891625615763548, 0.3251231527093596, 0.26354679802955666], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55911330049261088, 0.6280788177339901, 0.55418719211822665, 0.5714285714285714], 5: [0.25615763546798032, 0.13300492610837439, 0.1206896551724138, 0.16502463054187191], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.282145 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4297 4494 2310  133] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[ 128  151  207  311  449  760  905 1759 3681 9874] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.31622
Epoch 1, cost is  4.00392
Epoch 2, cost is  3.90203
Epoch 3, cost is  3.8621
Epoch 4, cost is  3.84722
Training took 0.231401 minutes
Weight histogram
[2346 2310 1460 2392 2306 2276 2135 1924  859  217] [-0.35892731 -0.32311233 -0.28729736 -0.25148238 -0.2156674  -0.17985242
 -0.14403745 -0.10822247 -0.07240749 -0.03659252 -0.00077754]
[ 555 1134 1738 1894 1990 2169 2233 2147 2053 2312] [-0.35892731 -0.32311233 -0.28729736 -0.25148238 -0.2156674  -0.17985242
 -0.14403745 -0.10822247 -0.07240749 -0.03659252 -0.00077754]
-17.3462
19.8985
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.280631 minutes
Weight histogram
[ 207  744 1391 2940 3199 4647 4119 2393  539   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 10677] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.03719
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  12.4921
Epoch 1, cost is  11.8513
Epoch 2, cost is  11.7598
Epoch 3, cost is  11.7868
Epoch 4, cost is  11.8615
Training took 0.200841 minutes
Weight histogram
[1997 1748 2107 1924 2206 1773 2155 2087 2281 1972] [ -8.09575081e-01  -7.28695327e-01  -6.47815573e-01  -5.66935819e-01
  -4.86056064e-01  -4.05176310e-01  -3.24296556e-01  -2.43416802e-01
  -1.62537048e-01  -8.16572940e-02  -7.77539855e-04]
[2930 1610 1832 1855 1761 2093 2105 2010 1941 2113] [ -8.09575081e-01  -7.28695327e-01  -6.47815573e-01  -5.66935819e-01
  -4.86056064e-01  -4.05176310e-01  -3.24296556e-01  -2.43416802e-01
  -1.62537048e-01  -8.16572940e-02  -7.77539855e-04]
-27.07
28.4644
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.55444
Epoch 1, cost is  2.41775
Epoch 2, cost is  1.88345
Epoch 3, cost is  1.69319
Epoch 4, cost is  1.6052
Training took 0.361814 minutes
Weight histogram
[2928 3484 3003 2451 1992 1683 1036  819  548  281] [-0.07301652 -0.06583617 -0.05865581 -0.05147546 -0.04429511 -0.03711475
 -0.0299344  -0.02275404 -0.01557369 -0.00839333 -0.00121298]
[1250  955 1120 1381 1659 1986 2246 2497 2694 2437] [-0.07301652 -0.06583617 -0.05865581 -0.05147546 -0.04429511 -0.03711475
 -0.0299344  -0.02275404 -0.01557369 -0.00839333 -0.00121298]
-3.98509
4.26369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046911 minutes
Epoch 0
Fine tuning took 0.047019 minutes
Epoch 0
Fine tuning took 0.047335 minutes
{'zero': {0: [0.19704433497536947, 0.20566502463054187, 0.3460591133004926, 0.29187192118226601], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.56157635467980294, 0.42241379310344829, 0.41009852216748771], 5: [0.18965517241379309, 0.23275862068965517, 0.23152709359605911, 0.29802955665024633], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19704433497536947, 0.17241379310344829, 0.29679802955665024, 0.2857142857142857], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.6711822660098522, 0.57389162561576357, 0.56157635467980294], 5: [0.18965517241379309, 0.15640394088669951, 0.12931034482758622, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19704433497536947, 0.18842364532019704, 0.27216748768472904, 0.25985221674876846], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.62684729064039413, 0.57019704433497542, 0.54187192118226601], 5: [0.18965517241379309, 0.18472906403940886, 0.15763546798029557, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19704433497536947, 0.17118226600985223, 0.37192118226600984, 0.2894088669950739], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61330049261083741, 0.70443349753694584, 0.52463054187192115, 0.56527093596059108], 5: [0.18965517241379309, 0.12438423645320197, 0.10344827586206896, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.266440 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.01843
Epoch 1, cost is  1.92661
Epoch 2, cost is  1.89138
Epoch 3, cost is  1.86895
Epoch 4, cost is  1.85797
Training took 0.235847 minutes
Weight histogram
[3822 3133 2256 2921 2678 2051 1765  938  450  236] [ -1.40079409e-01  -1.26082831e-01  -1.12086254e-01  -9.80896759e-02
  -8.40930983e-02  -7.00965206e-02  -5.60999430e-02  -4.21033653e-02
  -2.81067877e-02  -1.41102100e-02  -1.13632348e-04]
[ 681  880 1360 1712 1987 2268 2501 2667 2896 3298] [ -1.40079409e-01  -1.26082831e-01  -1.12086254e-01  -9.80896759e-02
  -8.40930983e-02  -7.00965206e-02  -5.60999430e-02  -4.21033653e-02
  -2.81067877e-02  -1.41102100e-02  -1.13632348e-04]
-4.27931
6.83226
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.263813 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.30042
Epoch 1, cost is  4.1358
Epoch 2, cost is  4.08828
Epoch 3, cost is  4.07603
Epoch 4, cost is  4.07107
Training took 0.201379 minutes
Weight histogram
[2460 2891 2692 2846 2316 2706 1986 1133 2104 1141] [ -2.48156697e-01  -2.23352390e-01  -1.98548084e-01  -1.73743777e-01
  -1.48939471e-01  -1.24135164e-01  -9.93308580e-02  -7.45265516e-02
  -4.97222452e-02  -2.49179388e-02  -1.13632348e-04]
[2241 1671 1584 1911 1943 2403 2530 2620 2699 2673] [ -2.48156697e-01  -2.23352390e-01  -1.98548084e-01  -1.73743777e-01
  -1.48939471e-01  -1.24135164e-01  -9.93308580e-02  -7.45265516e-02
  -4.97222452e-02  -2.49179388e-02  -1.13632348e-04]
-7.95455
7.57198
... retrieved True_rbm_350-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.5568
Epoch 1, cost is  5.36422
Epoch 2, cost is  4.57102
Epoch 3, cost is  4.19256
Epoch 4, cost is  3.95662
Training took 0.228395 minutes
Weight histogram
[3030 2669 2438 2233 1675 1433 1264 1432 1077 2999] [-0.08295341 -0.0746811  -0.0664088  -0.05813649 -0.04986419 -0.04159188
 -0.03331958 -0.02504727 -0.01677497 -0.00850267 -0.00023036]
[3071 1717 1374 1555 1673 1862 2063 2323 2503 2109] [-0.08295341 -0.0746811  -0.0664088  -0.05813649 -0.04986419 -0.04159188
 -0.03331958 -0.02504727 -0.01677497 -0.00850267 -0.00023036]
-1.6243
2.76155
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042487 minutes
Epoch 0
Fine tuning took 0.042285 minutes
Epoch 0
Fine tuning took 0.042296 minutes
{'zero': {0: [0.42733990147783252, 0.31403940886699505, 0.2376847290640394, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4248768472906404, 0.52955665024630538, 0.59852216748768472, 0.58743842364532017], 5: [0.14778325123152711, 0.15640394088669951, 0.16379310344827586, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.42733990147783252, 0.31157635467980294, 0.20320197044334976, 0.22783251231527094], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4248768472906404, 0.52832512315270941, 0.6280788177339901, 0.58374384236453203], 5: [0.14778325123152711, 0.16009852216748768, 0.16871921182266009, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.42733990147783252, 0.27339901477832512, 0.22044334975369459, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4248768472906404, 0.57512315270935965, 0.62068965517241381, 0.59359605911330049], 5: [0.14778325123152711, 0.15147783251231528, 0.15886699507389163, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.42733990147783252, 0.32142857142857145, 0.2019704433497537, 0.24630541871921183], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4248768472906404, 0.52586206896551724, 0.62931034482758619, 0.56527093596059108], 5: [0.14778325123152711, 0.15270935960591134, 0.16871921182266009, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272479 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.01843
Epoch 1, cost is  1.92661
Epoch 2, cost is  1.89138
Epoch 3, cost is  1.86895
Epoch 4, cost is  1.85797
Training took 0.231609 minutes
Weight histogram
[3822 3133 2256 2921 2678 2051 1765  938  450  236] [ -1.40079409e-01  -1.26082831e-01  -1.12086254e-01  -9.80896759e-02
  -8.40930983e-02  -7.00965206e-02  -5.60999430e-02  -4.21033653e-02
  -2.81067877e-02  -1.41102100e-02  -1.13632348e-04]
[ 681  880 1360 1712 1987 2268 2501 2667 2896 3298] [ -1.40079409e-01  -1.26082831e-01  -1.12086254e-01  -9.80896759e-02
  -8.40930983e-02  -7.00965206e-02  -5.60999430e-02  -4.21033653e-02
  -2.81067877e-02  -1.41102100e-02  -1.13632348e-04]
-4.27931
6.83226
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267762 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.30042
Epoch 1, cost is  4.1358
Epoch 2, cost is  4.08828
Epoch 3, cost is  4.07603
Epoch 4, cost is  4.07107
Training took 0.203562 minutes
Weight histogram
[2460 2891 2692 2846 2316 2706 1986 1133 2104 1141] [ -2.48156697e-01  -2.23352390e-01  -1.98548084e-01  -1.73743777e-01
  -1.48939471e-01  -1.24135164e-01  -9.93308580e-02  -7.45265516e-02
  -4.97222452e-02  -2.49179388e-02  -1.13632348e-04]
[2241 1671 1584 1911 1943 2403 2530 2620 2699 2673] [ -2.48156697e-01  -2.23352390e-01  -1.98548084e-01  -1.73743777e-01
  -1.48939471e-01  -1.24135164e-01  -9.93308580e-02  -7.45265516e-02
  -4.97222452e-02  -2.49179388e-02  -1.13632348e-04]
-7.95455
7.57198
... retrieved True_rbm_350-250_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.46642
Epoch 1, cost is  5.21002
Epoch 2, cost is  4.10812
Epoch 3, cost is  3.49439
Epoch 4, cost is  3.08776
Training took 0.283846 minutes
Weight histogram
[2399 3533 2648 2228 1851 1453 1584 1363 2917  274] [-0.05099323 -0.04592202 -0.0408508  -0.03577959 -0.03070838 -0.02563717
 -0.02056596 -0.01549475 -0.01042354 -0.00535233 -0.00028112]
[3271 1799 1510 1631 1639 1846 2023 2196 2323 2012] [-0.05099323 -0.04592202 -0.0408508  -0.03577959 -0.03070838 -0.02563717
 -0.02056596 -0.01549475 -0.01042354 -0.00535233 -0.00028112]
-1.2098
1.51827
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044187 minutes
Epoch 0
Fine tuning took 0.044106 minutes
Epoch 0
Fine tuning took 0.044046 minutes
{'zero': {0: [0.29802955665024633, 0.22044334975369459, 0.22783251231527094, 0.20073891625615764], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.63054187192118227, 0.59482758620689657, 0.58004926108374388], 5: [0.15147783251231528, 0.14901477832512317, 0.17733990147783252, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.29802955665024633, 0.2229064039408867, 0.18719211822660098, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.62315270935960587, 0.62684729064039413, 0.63423645320197042], 5: [0.15147783251231528, 0.1539408866995074, 0.18596059113300492, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.29802955665024633, 0.21921182266009853, 0.20935960591133004, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.63669950738916259, 0.62684729064039413, 0.62068965517241381], 5: [0.15147783251231528, 0.14408866995073891, 0.16379310344827586, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.29802955665024633, 0.20073891625615764, 0.18472906403940886, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.55049261083743839, 0.62315270935960587, 0.65640394088669951, 0.66995073891625612], 5: [0.15147783251231528, 0.17610837438423646, 0.15886699507389163, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270834 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.01843
Epoch 1, cost is  1.92661
Epoch 2, cost is  1.89138
Epoch 3, cost is  1.86895
Epoch 4, cost is  1.85797
Training took 0.235528 minutes
Weight histogram
[3822 3133 2256 2921 2678 2051 1765  938  450  236] [ -1.40079409e-01  -1.26082831e-01  -1.12086254e-01  -9.80896759e-02
  -8.40930983e-02  -7.00965206e-02  -5.60999430e-02  -4.21033653e-02
  -2.81067877e-02  -1.41102100e-02  -1.13632348e-04]
[ 681  880 1360 1712 1987 2268 2501 2667 2896 3298] [ -1.40079409e-01  -1.26082831e-01  -1.12086254e-01  -9.80896759e-02
  -8.40930983e-02  -7.00965206e-02  -5.60999430e-02  -4.21033653e-02
  -2.81067877e-02  -1.41102100e-02  -1.13632348e-04]
-4.27931
6.83226
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.273993 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.30042
Epoch 1, cost is  4.1358
Epoch 2, cost is  4.08828
Epoch 3, cost is  4.07603
Epoch 4, cost is  4.07107
Training took 0.199442 minutes
Weight histogram
[2460 2891 2692 2846 2316 2706 1986 1133 2104 1141] [ -2.48156697e-01  -2.23352390e-01  -1.98548084e-01  -1.73743777e-01
  -1.48939471e-01  -1.24135164e-01  -9.93308580e-02  -7.45265516e-02
  -4.97222452e-02  -2.49179388e-02  -1.13632348e-04]
[2241 1671 1584 1911 1943 2403 2530 2620 2699 2673] [ -2.48156697e-01  -2.23352390e-01  -1.98548084e-01  -1.73743777e-01
  -1.48939471e-01  -1.24135164e-01  -9.93308580e-02  -7.45265516e-02
  -4.97222452e-02  -2.49179388e-02  -1.13632348e-04]
-7.95455
7.57198
... retrieved True_rbm_350-500_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.35625
Epoch 1, cost is  5.12784
Epoch 2, cost is  3.91097
Epoch 3, cost is  3.14048
Epoch 4, cost is  2.63282
Training took 0.370681 minutes
Weight histogram
[2473 3757 3047 2442 2020 1871 1576 2351  587  126] [-0.03414995 -0.03075915 -0.02736835 -0.02397755 -0.02058674 -0.01719594
 -0.01380514 -0.01041434 -0.00702354 -0.00363274 -0.00024194]
[3572 1827 1509 1507 1652 1813 1921 2007 2300 2142] [-0.03414995 -0.03075915 -0.02736835 -0.02397755 -0.02058674 -0.01719594
 -0.01380514 -0.01041434 -0.00702354 -0.00363274 -0.00024194]
-0.957712
1.41078
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.047196 minutes
Epoch 0
Fine tuning took 0.047298 minutes
Epoch 0
Fine tuning took 0.047402 minutes
{'zero': {0: [0.33497536945812806, 0.32758620689655171, 0.21428571428571427, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.42241379310344829, 0.54187192118226601, 0.43472906403940886], 5: [0.2376847290640394, 0.25, 0.24384236453201971, 0.32142857142857145], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33497536945812806, 0.28201970443349755, 0.18349753694581281, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.50615763546798032, 0.55541871921182262, 0.54556650246305416], 5: [0.2376847290640394, 0.21182266009852216, 0.26108374384236455, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33497536945812806, 0.28817733990147781, 0.17364532019704434, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.48522167487684731, 0.59605911330049266, 0.48275862068965519], 5: [0.2376847290640394, 0.22660098522167488, 0.23029556650246305, 0.29310344827586204], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33497536945812806, 0.28078817733990147, 0.14039408866995073, 0.23029556650246305], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42733990147783252, 0.49261083743842365, 0.62438423645320196, 0.51354679802955661], 5: [0.2376847290640394, 0.22660098522167488, 0.23522167487684728, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.282332 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.8048
Epoch 1, cost is  1.74678
Epoch 2, cost is  1.71505
Epoch 3, cost is  1.69586
Epoch 4, cost is  1.68182
Training took 0.230575 minutes
Weight histogram
[3900 3539 2613 2870 2510 1751 1488  734  525  320] [ -1.00785889e-01  -9.07103643e-02  -8.06348398e-02  -7.05593153e-02
  -6.04837908e-02  -5.04082664e-02  -4.03327419e-02  -3.02572174e-02
  -2.01816929e-02  -1.01061685e-02  -3.06439943e-05]
[ 927  880 1288 1599 1892 2187 2460 2705 2918 3394] [ -1.00785889e-01  -9.07103643e-02  -8.06348398e-02  -7.05593153e-02
  -6.04837908e-02  -5.04082664e-02  -4.03327419e-02  -3.02572174e-02
  -2.01816929e-02  -1.01061685e-02  -3.06439943e-05]
-2.94293
3.82077
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.283527 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.38101
Epoch 1, cost is  3.28849
Epoch 2, cost is  3.2547
Epoch 3, cost is  3.24067
Epoch 4, cost is  3.23037
Training took 0.199570 minutes
Weight histogram
[3366 2945 2755 3031 2480 1987 1341 1132 1893 1345] [ -1.63408548e-01  -1.47070757e-01  -1.30732967e-01  -1.14395177e-01
  -9.80573862e-02  -8.17195958e-02  -6.53818055e-02  -4.90440151e-02
  -3.27062247e-02  -1.63684344e-02  -3.06439943e-05]
[2613 1430 1338 1770 1901 2279 2530 2732 2801 2881] [ -1.63408548e-01  -1.47070757e-01  -1.30732967e-01  -1.14395177e-01
  -9.80573862e-02  -8.17195958e-02  -6.53818055e-02  -4.90440151e-02
  -3.27062247e-02  -1.63684344e-02  -3.06439943e-05]
-4.64101
4.62842
... retrieved True_rbm_350-100_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74608
Epoch 1, cost is  6.34262
Epoch 2, cost is  5.61705
Epoch 3, cost is  5.01514
Epoch 4, cost is  4.61484
Training took 0.233926 minutes
Weight histogram
[ 707 2299 1916 1989 1517 1680 1769 1574 6174  625] [-0.0558529  -0.05027848 -0.04470406 -0.03912964 -0.03355522 -0.0279808
 -0.02240639 -0.01683197 -0.01125755 -0.00568313 -0.00010871]
[4715 2154 2000 1595 1610 1782 1872 1883 1622 1017] [-0.0558529  -0.05027848 -0.04470406 -0.03912964 -0.03355522 -0.0279808
 -0.02240639 -0.01683197 -0.01125755 -0.00568313 -0.00010871]
-0.925005
1.26551
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041623 minutes
Epoch 0
Fine tuning took 0.041938 minutes
Epoch 0
Fine tuning took 0.041712 minutes
{'zero': {0: [0.33866995073891626, 0.2105911330049261, 0.27709359605911332, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5, 0.56527093596059108, 0.59482758620689657, 0.68226600985221675], 5: [0.16133004926108374, 0.22413793103448276, 0.12807881773399016, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33866995073891626, 0.25123152709359609, 0.26231527093596058, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5, 0.54556650246305416, 0.6071428571428571, 0.66502463054187189], 5: [0.16133004926108374, 0.20320197044334976, 0.13054187192118227, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33866995073891626, 0.22413793103448276, 0.26724137931034481, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5, 0.5714285714285714, 0.58990147783251234, 0.66871921182266014], 5: [0.16133004926108374, 0.20443349753694581, 0.14285714285714285, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33866995073891626, 0.22044334975369459, 0.24507389162561577, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.5, 0.6071428571428571, 0.61822660098522164, 0.6785714285714286], 5: [0.16133004926108374, 0.17241379310344829, 0.13669950738916256, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.280279 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.8048
Epoch 1, cost is  1.74678
Epoch 2, cost is  1.71505
Epoch 3, cost is  1.69586
Epoch 4, cost is  1.68182
Training took 0.222784 minutes
Weight histogram
[3900 3539 2613 2870 2510 1751 1488  734  525  320] [ -1.00785889e-01  -9.07103643e-02  -8.06348398e-02  -7.05593153e-02
  -6.04837908e-02  -5.04082664e-02  -4.03327419e-02  -3.02572174e-02
  -2.01816929e-02  -1.01061685e-02  -3.06439943e-05]
[ 927  880 1288 1599 1892 2187 2460 2705 2918 3394] [ -1.00785889e-01  -9.07103643e-02  -8.06348398e-02  -7.05593153e-02
  -6.04837908e-02  -5.04082664e-02  -4.03327419e-02  -3.02572174e-02
  -2.01816929e-02  -1.01061685e-02  -3.06439943e-05]
-2.94293
3.82077
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.263524 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.38101
Epoch 1, cost is  3.28849
Epoch 2, cost is  3.2547
Epoch 3, cost is  3.24067
Epoch 4, cost is  3.23037
Training took 0.200914 minutes
Weight histogram
[3366 2945 2755 3031 2480 1987 1341 1132 1893 1345] [ -1.63408548e-01  -1.47070757e-01  -1.30732967e-01  -1.14395177e-01
  -9.80573862e-02  -8.17195958e-02  -6.53818055e-02  -4.90440151e-02
  -3.27062247e-02  -1.63684344e-02  -3.06439943e-05]
[2613 1430 1338 1770 1901 2279 2530 2732 2801 2881] [ -1.63408548e-01  -1.47070757e-01  -1.30732967e-01  -1.14395177e-01
  -9.80573862e-02  -8.17195958e-02  -6.53818055e-02  -4.90440151e-02
  -3.27062247e-02  -1.63684344e-02  -3.06439943e-05]
-4.64101
4.62842
... retrieved True_rbm_350-250_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.65307
Epoch 1, cost is  6.25355
Epoch 2, cost is  5.5261
Epoch 3, cost is  4.80939
Epoch 4, cost is  4.23352
Training took 0.276405 minutes
Weight histogram
[ 354 2400 2618 2223 2015 2164 1813 3611 2789  263] [-0.03642388 -0.03279739 -0.0291709  -0.02554441 -0.02191792 -0.01829142
 -0.01466493 -0.01103844 -0.00741195 -0.00378545 -0.00015896]
[4369 2626 2133 1605 1604 1708 1740 1740 1667 1058] [-0.03642388 -0.03279739 -0.0291709  -0.02554441 -0.02191792 -0.01829142
 -0.01466493 -0.01103844 -0.00741195 -0.00378545 -0.00015896]
-0.823236
0.921125
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043382 minutes
Epoch 0
Fine tuning took 0.043570 minutes
Epoch 0
Fine tuning took 0.046980 minutes
{'zero': {0: [0.33128078817733991, 0.21551724137931033, 0.19950738916256158, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.60344827586206895, 0.62931034482758619, 0.61206896551724133], 5: [0.15517241379310345, 0.18103448275862069, 0.17118226600985223, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33128078817733991, 0.23645320197044334, 0.21182266009852216, 0.15517241379310345], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.60221674876847286, 0.6354679802955665, 0.65517241379310343], 5: [0.15517241379310345, 0.16133004926108374, 0.15270935960591134, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33128078817733991, 0.22413793103448276, 0.22413793103448276, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.60591133004926112, 0.6071428571428571, 0.6145320197044335], 5: [0.15517241379310345, 0.16995073891625614, 0.16871921182266009, 0.18719211822660098], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33128078817733991, 0.22536945812807882, 0.2019704433497537, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.51354679802955661, 0.60221674876847286, 0.65270935960591137, 0.65147783251231528], 5: [0.15517241379310345, 0.17241379310344829, 0.14532019704433496, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.272871 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  1.8048
Epoch 1, cost is  1.74678
Epoch 2, cost is  1.71505
Epoch 3, cost is  1.69586
Epoch 4, cost is  1.68182
Training took 0.223776 minutes
Weight histogram
[3900 3539 2613 2870 2510 1751 1488  734  525  320] [ -1.00785889e-01  -9.07103643e-02  -8.06348398e-02  -7.05593153e-02
  -6.04837908e-02  -5.04082664e-02  -4.03327419e-02  -3.02572174e-02
  -2.01816929e-02  -1.01061685e-02  -3.06439943e-05]
[ 927  880 1288 1599 1892 2187 2460 2705 2918 3394] [ -1.00785889e-01  -9.07103643e-02  -8.06348398e-02  -7.05593153e-02
  -6.04837908e-02  -5.04082664e-02  -4.03327419e-02  -3.02572174e-02
  -2.01816929e-02  -1.01061685e-02  -3.06439943e-05]
-2.94293
3.82077
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.264296 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.38101
Epoch 1, cost is  3.28849
Epoch 2, cost is  3.2547
Epoch 3, cost is  3.24067
Epoch 4, cost is  3.23037
Training took 0.202687 minutes
Weight histogram
[3366 2945 2755 3031 2480 1987 1341 1132 1893 1345] [ -1.63408548e-01  -1.47070757e-01  -1.30732967e-01  -1.14395177e-01
  -9.80573862e-02  -8.17195958e-02  -6.53818055e-02  -4.90440151e-02
  -3.27062247e-02  -1.63684344e-02  -3.06439943e-05]
[2613 1430 1338 1770 1901 2279 2530 2732 2801 2881] [ -1.63408548e-01  -1.47070757e-01  -1.30732967e-01  -1.14395177e-01
  -9.80573862e-02  -8.17195958e-02  -6.53818055e-02  -4.90440151e-02
  -3.27062247e-02  -1.63684344e-02  -3.06439943e-05]
-4.64101
4.62842
... retrieved True_rbm_350-500_classical1_batch10_lr0.0005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.52136
Epoch 1, cost is  6.15168
Epoch 2, cost is  5.48521
Epoch 3, cost is  4.70017
Epoch 4, cost is  4.07516
Training took 0.357696 minutes
Weight histogram
[ 434 2654 3199 2783 2758 2331 3190 2354  386  161] [-0.02634971 -0.02372716 -0.0211046  -0.01848205 -0.01585949 -0.01323694
 -0.01061438 -0.00799183 -0.00536927 -0.00274672 -0.00012417]
[4187 3168 2286 1551 1572 1544 1534 1606 1624 1178] [-0.02634971 -0.02372716 -0.0211046  -0.01848205 -0.01585949 -0.01323694
 -0.01061438 -0.00799183 -0.00536927 -0.00274672 -0.00012417]
-0.623447
0.83039
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046839 minutes
Epoch 0
Fine tuning took 0.047239 minutes
Epoch 0
Fine tuning took 0.050116 minutes
{'zero': {0: [0.27586206896551724, 0.17857142857142858, 0.21182266009852216, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.60837438423645318, 0.59605911330049266, 0.63054187192118227], 5: [0.14655172413793102, 0.21305418719211822, 0.19211822660098521, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27586206896551724, 0.19950738916256158, 0.2105911330049261, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.60960591133004927, 0.59975369458128081, 0.62684729064039413], 5: [0.14655172413793102, 0.19088669950738915, 0.18965517241379309, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27586206896551724, 0.2019704433497537, 0.20689655172413793, 0.24261083743842365], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.58374384236453203, 0.61822660098522164, 0.61206896551724133], 5: [0.14655172413793102, 0.21428571428571427, 0.1748768472906404, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27586206896551724, 0.19334975369458129, 0.20566502463054187, 0.21921182266009853], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57758620689655171, 0.58620689655172409, 0.59729064039408863, 0.61699507389162567], 5: [0.14655172413793102, 0.22044334975369459, 0.19704433497536947, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.269976 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56886
Epoch 1, cost is  2.53115
Epoch 2, cost is  2.5058
Epoch 3, cost is  2.48114
Epoch 4, cost is  2.45632
Training took 0.225339 minutes
Weight histogram
[3824 3493 2575 2436 1939 1511 1942  944 1342  244] [ -5.18810339e-02  -4.66893559e-02  -4.14976778e-02  -3.63059997e-02
  -3.11143217e-02  -2.59226436e-02  -2.07309656e-02  -1.55392875e-02
  -1.03476094e-02  -5.15593138e-03   3.57466815e-05]
[2436 1258 1202 1478 1718 1919 2098 2494 2579 3068] [ -5.18810339e-02  -4.66893559e-02  -4.14976778e-02  -3.63059997e-02
  -3.11143217e-02  -2.59226436e-02  -2.07309656e-02  -1.55392875e-02
  -1.03476094e-02  -5.15593138e-03   3.57466815e-05]
-1.19089
1.371
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.265363 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.48802
Epoch 1, cost is  3.45253
Epoch 2, cost is  3.42584
Epoch 3, cost is  3.4049
Epoch 4, cost is  3.3851
Training took 0.202175 minutes
Weight histogram
[3333 2378 2248 2298 1802 1653 1567 1413 3498 2085] [ -7.68719688e-02  -6.91811973e-02  -6.14904257e-02  -5.37996542e-02
  -4.61088826e-02  -3.84181111e-02  -3.07273395e-02  -2.30365680e-02
  -1.53457964e-02  -7.65502487e-03   3.57466815e-05]
[4875 1246 1348 1470 1655 1841 2062 2350 2662 2766] [ -7.68719688e-02  -6.91811973e-02  -6.14904257e-02  -5.37996542e-02
  -4.61088826e-02  -3.84181111e-02  -3.07273395e-02  -2.30365680e-02
  -1.53457964e-02  -7.65502487e-03   3.57466815e-05]
-1.55967
1.64721
... retrieved True_rbm_350-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/6/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.8587
Epoch 1, cost is  6.77969
Epoch 2, cost is  6.71467
Epoch 3, cost is  6.65271
Epoch 4, cost is  6.58507
Training took 0.230821 minutes
Weight histogram
[ 1660  1785 12070  1827  1011   651   450   338   256   202] [ -1.02815339e-02  -9.25411123e-03  -8.22668852e-03  -7.19926582e-03
  -6.17184311e-03  -5.14442041e-03  -4.11699770e-03  -3.08957500e-03
  -2.06215229e-03  -1.03472959e-03  -7.30688498e-06]
[8559 3930 3009 2504  992  487  288  232  126  123] [ -1.02815339e-02  -9.25411123e-03  -8.22668852e-03  -7.19926582e-03
  -6.17184311e-03  -5.14442041e-03  -4.11699770e-03  -3.08957500e-03
  -2.06215229e-03  -1.03472959e-03  -7.30688498e-06]
-0.09023
0.162447
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041635 minutes
Epoch 0
Fine tuning took 0.041488 minutes
Epoch 0
Fine tuning took 0.042878 minutes
{'zero': {0: [0.18596059113300492, 0.32635467980295568, 0.082512315270935957, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.58743842364532017, 0.82635467980295563, 0.8214285714285714], 5: [0.084975369458128072, 0.086206896551724144, 0.091133004926108374, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18596059113300492, 0.30418719211822659, 0.10960591133004927, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.61083743842364535, 0.79679802955665024, 0.7931034482758621], 5: [0.084975369458128072, 0.084975369458128072, 0.093596059113300489, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18596059113300492, 0.31280788177339902, 0.096059113300492605, 0.082512315270935957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.6145320197044335, 0.81403940886699511, 0.80788177339901479], 5: [0.084975369458128072, 0.072660098522167482, 0.089901477832512317, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18596059113300492, 0.28078817733990147, 0.11699507389162561, 0.088669950738916259], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72906403940886699, 0.63793103448275867, 0.80295566502463056, 0.80295566502463056], 5: [0.084975369458128072, 0.081280788177339899, 0.080049261083743842, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.267873 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56886
Epoch 1, cost is  2.53115
Epoch 2, cost is  2.5058
Epoch 3, cost is  2.48114
Epoch 4, cost is  2.45632
Training took 0.231548 minutes
Weight histogram
[3824 3493 2575 2436 1939 1511 1942  944 1342  244] [ -5.18810339e-02  -4.66893559e-02  -4.14976778e-02  -3.63059997e-02
  -3.11143217e-02  -2.59226436e-02  -2.07309656e-02  -1.55392875e-02
  -1.03476094e-02  -5.15593138e-03   3.57466815e-05]
[2436 1258 1202 1478 1718 1919 2098 2494 2579 3068] [ -5.18810339e-02  -4.66893559e-02  -4.14976778e-02  -3.63059997e-02
  -3.11143217e-02  -2.59226436e-02  -2.07309656e-02  -1.55392875e-02
  -1.03476094e-02  -5.15593138e-03   3.57466815e-05]
-1.19089
1.371
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.261309 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.48802
Epoch 1, cost is  3.45253
Epoch 2, cost is  3.42584
Epoch 3, cost is  3.4049
Epoch 4, cost is  3.3851
Training took 0.200653 minutes
Weight histogram
[3333 2378 2248 2298 1802 1653 1567 1413 3498 2085] [ -7.68719688e-02  -6.91811973e-02  -6.14904257e-02  -5.37996542e-02
  -4.61088826e-02  -3.84181111e-02  -3.07273395e-02  -2.30365680e-02
  -1.53457964e-02  -7.65502487e-03   3.57466815e-05]
[4875 1246 1348 1470 1655 1841 2062 2350 2662 2766] [ -7.68719688e-02  -6.91811973e-02  -6.14904257e-02  -5.37996542e-02
  -4.61088826e-02  -3.84181111e-02  -3.07273395e-02  -2.30365680e-02
  -1.53457964e-02  -7.65502487e-03   3.57466815e-05]
-1.55967
1.64721
... retrieved True_rbm_350-250_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/7/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79352
Epoch 1, cost is  6.6841
Epoch 2, cost is  6.60691
Epoch 3, cost is  6.53289
Epoch 4, cost is  6.45782
Training took 0.277672 minutes
Weight histogram
[  623  1144  2186 10749  2519  1215   728   487   344   255] [ -1.24916639e-02  -1.12484001e-02  -1.00051363e-02  -8.76187251e-03
  -7.51860871e-03  -6.27534491e-03  -5.03208111e-03  -3.78881731e-03
  -2.54555350e-03  -1.30228970e-03  -5.90259006e-05]
[7539 3614 2903 2520 1892  660  448  314  196  164] [ -1.24916639e-02  -1.12484001e-02  -1.00051363e-02  -8.76187251e-03
  -7.51860871e-03  -6.27534491e-03  -5.03208111e-03  -3.78881731e-03
  -2.54555350e-03  -1.30228970e-03  -5.90259006e-05]
-0.0795347
0.135691
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043266 minutes
Epoch 0
Fine tuning took 0.046814 minutes
Epoch 0
Fine tuning took 0.044562 minutes
{'zero': {0: [0.18965517241379309, 0.23522167487684728, 0.13177339901477833, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.71551724137931039, 0.8214285714285714, 0.87068965517241381], 5: [0.092364532019704432, 0.049261083743842367, 0.046798029556650245, 0.067733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18965517241379309, 0.26970443349753692, 0.1268472906403941, 0.057881773399014777], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.68719211822660098, 0.8288177339901478, 0.87315270935960587], 5: [0.092364532019704432, 0.043103448275862072, 0.044334975369458129, 0.068965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18965517241379309, 0.25862068965517243, 0.12561576354679804, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.68842364532019706, 0.83497536945812811, 0.8719211822660099], 5: [0.092364532019704432, 0.05295566502463054, 0.039408866995073892, 0.066502463054187194], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18965517241379309, 0.25492610837438423, 0.14162561576354679, 0.054187192118226604], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.71798029556650245, 0.68472906403940892, 0.83004926108374388, 0.88916256157635465], 5: [0.092364532019704432, 0.060344827586206899, 0.02832512315270936, 0.056650246305418719], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.258837 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  2.56886
Epoch 1, cost is  2.53115
Epoch 2, cost is  2.5058
Epoch 3, cost is  2.48114
Epoch 4, cost is  2.45632
Training took 0.226042 minutes
Weight histogram
[3824 3493 2575 2436 1939 1511 1942  944 1342  244] [ -5.18810339e-02  -4.66893559e-02  -4.14976778e-02  -3.63059997e-02
  -3.11143217e-02  -2.59226436e-02  -2.07309656e-02  -1.55392875e-02
  -1.03476094e-02  -5.15593138e-03   3.57466815e-05]
[2436 1258 1202 1478 1718 1919 2098 2494 2579 3068] [ -5.18810339e-02  -4.66893559e-02  -4.14976778e-02  -3.63059997e-02
  -3.11143217e-02  -2.59226436e-02  -2.07309656e-02  -1.55392875e-02
  -1.03476094e-02  -5.15593138e-03   3.57466815e-05]
-1.19089
1.371
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.259698 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  3.48802
Epoch 1, cost is  3.45253
Epoch 2, cost is  3.42584
Epoch 3, cost is  3.4049
Epoch 4, cost is  3.3851
Training took 0.204648 minutes
Weight histogram
[3333 2378 2248 2298 1802 1653 1567 1413 3498 2085] [ -7.68719688e-02  -6.91811973e-02  -6.14904257e-02  -5.37996542e-02
  -4.61088826e-02  -3.84181111e-02  -3.07273395e-02  -2.30365680e-02
  -1.53457964e-02  -7.65502487e-03   3.57466815e-05]
[4875 1246 1348 1470 1655 1841 2062 2350 2662 2766] [ -7.68719688e-02  -6.91811973e-02  -6.14904257e-02  -5.37996542e-02
  -4.61088826e-02  -3.84181111e-02  -3.07273395e-02  -2.30365680e-02
  -1.53457964e-02  -7.65502487e-03   3.57466815e-05]
-1.55967
1.64721
... retrieved True_rbm_350-500_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/8/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.69016
Epoch 1, cost is  6.5343
Epoch 2, cost is  6.44367
Epoch 3, cost is  6.36162
Epoch 4, cost is  6.2799
Training took 0.362789 minutes
Weight histogram
[ 558  838 1427 7442 5643 1938 1052  634  425  293] [ -1.41741699e-02  -1.27593082e-02  -1.13444466e-02  -9.92958500e-03
  -8.51472338e-03  -7.09986176e-03  -5.68500014e-03  -4.27013853e-03
  -2.85527691e-03  -1.44041529e-03  -2.55536670e-05]
[6547 3336 2791 2580 2461 1082  582  401  259  211] [ -1.41741699e-02  -1.27593082e-02  -1.13444466e-02  -9.92958500e-03
  -8.51472338e-03  -7.09986176e-03  -5.68500014e-03  -4.27013853e-03
  -2.85527691e-03  -1.44041529e-03  -2.55536670e-05]
-0.0721031
0.113524
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046527 minutes
Epoch 0
Fine tuning took 0.046548 minutes
Epoch 0
Fine tuning took 0.046682 minutes
{'zero': {0: [0.20812807881773399, 0.26847290640394089, 0.19334975369458129, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.69211822660098521, 0.74014778325123154, 0.74753694581280783], 5: [0.091133004926108374, 0.039408866995073892, 0.066502463054187194, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20812807881773399, 0.28694581280788178, 0.16133004926108374, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.66871921182266014, 0.77093596059113301, 0.75], 5: [0.091133004926108374, 0.044334975369458129, 0.067733990147783252, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20812807881773399, 0.30049261083743845, 0.17241379310344829, 0.14778325123152711], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.64532019704433496, 0.74630541871921185, 0.76108374384236455], 5: [0.091133004926108374, 0.054187192118226604, 0.081280788177339899, 0.091133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20812807881773399, 0.29064039408866993, 0.16748768472906403, 0.13916256157635468], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70073891625615758, 0.6576354679802956, 0.77339901477832518, 0.76354679802955661], 5: [0.091133004926108374, 0.051724137931034482, 0.059113300492610835, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.279589 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.63995
Epoch 1, cost is  4.316
Epoch 2, cost is  4.19435
Epoch 3, cost is  4.14475
Epoch 4, cost is  4.11984
Training took 0.225275 minutes
Weight histogram
[2008 2664 2533 1840 2689 2415 2227 2476 1155  243] [-0.39949942 -0.35962723 -0.31975504 -0.27988285 -0.24001067 -0.20013848
 -0.16026629 -0.1203941  -0.08052192 -0.04064973 -0.00077754]
[ 625 1313 1968 2067 2198 2472 2365 2254 2432 2556] [-0.39949942 -0.35962723 -0.31975504 -0.27988285 -0.24001067 -0.20013848
 -0.16026629 -0.1203941  -0.08052192 -0.04064973 -0.00077754]
-18.9301
21.1622
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.270440 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  13.8701
Epoch 1, cost is  13.2749
Epoch 2, cost is  13.1746
Epoch 3, cost is  13.2034
Epoch 4, cost is  13.2798
Training took 0.211480 minutes
Weight histogram
[1914 2117 2073 2120 2301 2295 2344 2249 2289 2573] [ -9.06544149e-01  -8.15967488e-01  -7.25390827e-01  -6.34814166e-01
  -5.44237505e-01  -4.53660844e-01  -3.63084183e-01  -2.72507523e-01
  -1.81930862e-01  -9.13542008e-02  -7.77539855e-04]
[3072 1843 2044 2006 2124 2313 2242 2158 2299 2174] [ -9.06544149e-01  -8.15967488e-01  -7.25390827e-01  -6.34814166e-01
  -5.44237505e-01  -4.53660844e-01  -3.63084183e-01  -2.72507523e-01
  -1.81930862e-01  -9.13542008e-02  -7.77539855e-04]
-30.5205
31.134
... retrieved True_rbm_350-100_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/9/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  5.15024
Epoch 1, cost is  3.89627
Epoch 2, cost is  3.73345
Epoch 3, cost is  3.82646
Epoch 4, cost is  4.00041
Training took 0.230067 minutes
Weight histogram
[2271 3523 3591 3112 2642 1746 1218  806  604  737] [-0.17713617 -0.15954586 -0.14195555 -0.12436524 -0.10677493 -0.08918462
 -0.07159432 -0.05400401 -0.0364137  -0.01882339 -0.00123308]
[1240 1170 1441 1712 1939 2200 2440 2614 2775 2719] [-0.17713617 -0.15954586 -0.14195555 -0.12436524 -0.10677493 -0.08918462
 -0.07159432 -0.05400401 -0.0364137  -0.01882339 -0.00123308]
-7.17639
7.17654
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042108 minutes
Epoch 0
Fine tuning took 0.042478 minutes
Epoch 0
Fine tuning took 0.042149 minutes
{'zero': {0: [0.40640394088669951, 0.32758620689655171, 0.30418719211822659, 0.30418719211822659], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.33620689655172414, 0.41871921182266009, 0.38793103448275862, 0.49384236453201968], 5: [0.25738916256157635, 0.2536945812807882, 0.30788177339901479, 0.2019704433497537], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.40640394088669951, 0.32142857142857145, 0.25862068965517243, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.33620689655172414, 0.46921182266009853, 0.53940886699507384, 0.54802955665024633], 5: [0.25738916256157635, 0.20935960591133004, 0.2019704433497537, 0.18472906403940886], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.40640394088669951, 0.30418719211822659, 0.27216748768472904, 0.28325123152709358], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.33620689655172414, 0.50862068965517238, 0.52463054187192115, 0.50985221674876846], 5: [0.25738916256157635, 0.18719211822660098, 0.20320197044334976, 0.20689655172413793], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.40640394088669951, 0.32389162561576357, 0.26724137931034481, 0.26477832512315269], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.33620689655172414, 0.49384236453201968, 0.51600985221674878, 0.55541871921182262], 5: [0.25738916256157635, 0.18226600985221675, 0.21674876847290642, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274526 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.63995
Epoch 1, cost is  4.316
Epoch 2, cost is  4.19435
Epoch 3, cost is  4.14475
Epoch 4, cost is  4.11984
Training took 0.235063 minutes
Weight histogram
[2008 2664 2533 1840 2689 2415 2227 2476 1155  243] [-0.39949942 -0.35962723 -0.31975504 -0.27988285 -0.24001067 -0.20013848
 -0.16026629 -0.1203941  -0.08052192 -0.04064973 -0.00077754]
[ 625 1313 1968 2067 2198 2472 2365 2254 2432 2556] [-0.39949942 -0.35962723 -0.31975504 -0.27988285 -0.24001067 -0.20013848
 -0.16026629 -0.1203941  -0.08052192 -0.04064973 -0.00077754]
-18.9301
21.1622
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.274978 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  13.8701
Epoch 1, cost is  13.2749
Epoch 2, cost is  13.1746
Epoch 3, cost is  13.2034
Epoch 4, cost is  13.2798
Training took 0.205103 minutes
Weight histogram
[1914 2117 2073 2120 2301 2295 2344 2249 2289 2573] [ -9.06544149e-01  -8.15967488e-01  -7.25390827e-01  -6.34814166e-01
  -5.44237505e-01  -4.53660844e-01  -3.63084183e-01  -2.72507523e-01
  -1.81930862e-01  -9.13542008e-02  -7.77539855e-04]
[3072 1843 2044 2006 2124 2313 2242 2158 2299 2174] [ -9.06544149e-01  -8.15967488e-01  -7.25390827e-01  -6.34814166e-01
  -5.44237505e-01  -4.53660844e-01  -3.63084183e-01  -2.72507523e-01
  -1.81930862e-01  -9.13542008e-02  -7.77539855e-04]
-30.5205
31.134
... retrieved True_rbm_350-250_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/10/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.78998
Epoch 1, cost is  2.96928
Epoch 2, cost is  2.59452
Epoch 3, cost is  2.51646
Epoch 4, cost is  2.53003
Training took 0.282595 minutes
Weight histogram
[2421 3828 3224 2963 2543 1817 1297  890  638  629] [-0.11230296 -0.10119756 -0.09009215 -0.07898675 -0.06788135 -0.05677594
 -0.04567054 -0.03456514 -0.02345973 -0.01235433 -0.00124893]
[1368 1150 1392 1595 1875 2285 2434 2673 2824 2654] [-0.11230296 -0.10119756 -0.09009215 -0.07898675 -0.06788135 -0.05677594
 -0.04567054 -0.03456514 -0.02345973 -0.01235433 -0.00124893]
-4.8072
6.12464
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043571 minutes
Epoch 0
Fine tuning took 0.043235 minutes
Epoch 0
Fine tuning took 0.046843 minutes
{'zero': {0: [0.15886699507389163, 0.18596059113300492, 0.40763546798029554, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.50492610837438423, 0.3891625615763547, 0.41256157635467983], 5: [0.22044334975369459, 0.30911330049261082, 0.20320197044334976, 0.3891625615763547], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15886699507389163, 0.24630541871921183, 0.27709359605911332, 0.25246305418719212], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.57389162561576357, 0.55049261083743839, 0.58004926108374388], 5: [0.22044334975369459, 0.17980295566502463, 0.17241379310344829, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15886699507389163, 0.25862068965517243, 0.28078817733990147, 0.24014778325123154], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.55788177339901479, 0.55295566502463056, 0.56773399014778325], 5: [0.22044334975369459, 0.18349753694581281, 0.16625615763546797, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15886699507389163, 0.2857142857142857, 0.2413793103448276, 0.2413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.62068965517241381, 0.53694581280788178, 0.5923645320197044, 0.60221674876847286], 5: [0.22044334975369459, 0.17733990147783252, 0.16625615763546797, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.285077 minutes
Weight histogram
[ 173  446  567  640 1587 3578 4342 5422 3354  141] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
[  128   151   207   311   449   760   905  1759  3681 11899] [ -7.24884303e-05   1.83056133e-04   4.38600697e-04   6.94145261e-04
   9.49689824e-04   1.20523439e-03   1.46077895e-03   1.71632352e-03
   1.97186808e-03   2.22741264e-03   2.48295721e-03]
-1.86399
1.04236
training layer 1, rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-250_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  4.63995
Epoch 1, cost is  4.316
Epoch 2, cost is  4.19435
Epoch 3, cost is  4.14475
Epoch 4, cost is  4.11984
Training took 0.224554 minutes
Weight histogram
[2008 2664 2533 1840 2689 2415 2227 2476 1155  243] [-0.39949942 -0.35962723 -0.31975504 -0.27988285 -0.24001067 -0.20013848
 -0.16026629 -0.1203941  -0.08052192 -0.04064973 -0.00077754]
[ 625 1313 1968 2067 2198 2472 2365 2254 2432 2556] [-0.39949942 -0.35962723 -0.31975504 -0.27988285 -0.24001067 -0.20013848
 -0.16026629 -0.1203941  -0.08052192 -0.04064973 -0.00077754]
-18.9301
21.1622
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.256852 minutes
Weight histogram
[ 207  744 1391 3148 4151 5409 4212 2398  544   71] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
[  250   297   407   560   854  1394   932  1718  3161 12702] [ -7.24884303e-05   1.32748556e-04   3.37985542e-04   5.43222528e-04
   7.48459515e-04   9.53696501e-04   1.15893349e-03   1.36417047e-03
   1.56940746e-03   1.77464445e-03   1.97988143e-03]
-1.12795
1.07053
training layer 1, rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
... loaded trained layer rbm_250-100_classical1_batch10_lr0.005_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9_dropout0.50
Epoch 0, cost is  13.8701
Epoch 1, cost is  13.2749
Epoch 2, cost is  13.1746
Epoch 3, cost is  13.2034
Epoch 4, cost is  13.2798
Training took 0.203107 minutes
Weight histogram
[1914 2117 2073 2120 2301 2295 2344 2249 2289 2573] [ -9.06544149e-01  -8.15967488e-01  -7.25390827e-01  -6.34814166e-01
  -5.44237505e-01  -4.53660844e-01  -3.63084183e-01  -2.72507523e-01
  -1.81930862e-01  -9.13542008e-02  -7.77539855e-04]
[3072 1843 2044 2006 2124 2313 2242 2158 2299 2174] [ -9.06544149e-01  -8.15967488e-01  -7.25390827e-01  -6.34814166e-01
  -5.44237505e-01  -4.53660844e-01  -3.63084183e-01  -2.72507523e-01
  -1.81930862e-01  -9.13542008e-02  -7.77539855e-04]
-30.5205
31.134
... retrieved True_rbm_350-500_classical1_batch10_lr0.005_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9_dropout0.50 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN2/11/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.55622
Epoch 1, cost is  2.42636
Epoch 2, cost is  1.89408
Epoch 3, cost is  1.69739
Epoch 4, cost is  1.60863
Training took 0.371351 minutes
Weight histogram
[3194 3857 3357 2744 2223 1878 1160  916  608  313] [-0.07301652 -0.06583563 -0.05865474 -0.05147385 -0.04429296 -0.03711207
 -0.02993118 -0.02275029 -0.0155694  -0.00838851 -0.00120763]
[1389 1064 1248 1540 1844 2209 2490 2766 2993 2707] [-0.07301652 -0.06583563 -0.05865474 -0.05147385 -0.04429296 -0.03711207
 -0.02993118 -0.02275029 -0.0155694  -0.00838851 -0.00120763]
-3.98509
4.26369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046396 minutes
Epoch 0
Fine tuning took 0.046254 minutes
Epoch 0
Fine tuning took 0.046729 minutes
{'zero': {0: [0.22906403940886699, 0.13177339901477833, 0.30418719211822659, 0.24753694581280788], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57635467980295563, 0.53201970443349755, 0.52586206896551724, 0.49261083743842365], 5: [0.19458128078817735, 0.33620689655172414, 0.16995073891625614, 0.25985221674876846], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22906403940886699, 0.2376847290640394, 0.34975369458128081, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57635467980295563, 0.59605911330049266, 0.49753694581280788, 0.59729064039408863], 5: [0.19458128078817735, 0.16625615763546797, 0.15270935960591134, 0.14039408866995073], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22906403940886699, 0.2413793103448276, 0.32758620689655171, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57635467980295563, 0.61330049261083741, 0.51970443349753692, 0.53940886699507384], 5: [0.19458128078817735, 0.14532019704433496, 0.15270935960591134, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22906403940886699, 0.27093596059113301, 0.43596059113300495, 0.34482758620689657], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57635467980295563, 0.57266009852216748, 0.42857142857142855, 0.51847290640394084], 5: [0.19458128078817735, 0.15640394088669951, 0.1354679802955665, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}}
