Using gpu device 0: GeForce GT 630
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:153: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/sklearn/preprocessing/data.py:169: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. 
  warnings.warn("Numerical issues were encountered "
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 1 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/usr/lib/python2.7/dist-packages/numpy/core/_methods.py:55: RuntimeWarning: Mean of empty slice.
  warnings.warn("Mean of empty slice.", RuntimeWarning)
/vol/bitbucket/js3611/AssociationLearning/rbm.py:722: UserWarning: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 2 is not part of the computational graph needed to compute the outputs: <TensorType(int64, scalar)>.
To make this warning into an error, you can pass the parameter on_unused_input='raise' to theano.function. To disable it completely, use on_unused_input='ignore'.
  on_unused_input='warn'
/vol/bitbucket/js3611/.virtualenvs/rbm/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
Experiment 1: Interaction between happy/sad children and Secure Parent
Experiment 2: Interaction between happy/sad children and Ambivalent Parent
Experiment 3: Interaction between happy/sad children and Avoidant Parent
... data manager created. project_root: ExperimentADBN5
... moved to /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... initialised associative DBN
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148052 minutes
Weight histogram
[ 34  67 128 159 208 389 379 364 234  63] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
[ 81  77 111 135 144 183 233 302 326 433] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
-0.44884
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.6591
Epoch 1, cost is  2.16357
Epoch 2, cost is  1.81224
Epoch 3, cost is  1.65546
Epoch 4, cost is  1.56546
Training took 0.082684 minutes
Weight histogram
[797 429 307 194  94  62  53  38  21  30] [-0.12928979 -0.11665666 -0.10402352 -0.09139039 -0.07875725 -0.06612412
 -0.05349098 -0.04085785 -0.02822471 -0.01559158 -0.00295844]
[ 69  59  77 104 139 179 217 287 385 509] [-0.12928979 -0.11665666 -0.10402352 -0.09139039 -0.07875725 -0.06612412
 -0.05349098 -0.04085785 -0.02822471 -0.01559158 -0.00295844]
-2.69308
3.4726
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148289 minutes
Weight histogram
[  20   47  169  360  615  834 1020  581  322   82] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
[174 177 226 279 332 396 529 624 792 521] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
-0.600749
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.73834
Epoch 1, cost is  2.11005
Epoch 2, cost is  1.77714
Epoch 3, cost is  1.62809
Epoch 4, cost is  1.54158
Training took 0.080175 minutes
Weight histogram
[1469  905  641  380  206  144  101   81   53   70] [-0.12928979 -0.11663694 -0.10398408 -0.09133123 -0.07867837 -0.06602551
 -0.05337266 -0.0407198  -0.02806695 -0.01541409 -0.00276124]
[149 117 163 211 276 355 455 610 813 901] [-0.12928979 -0.11663694 -0.10398408 -0.09133123 -0.07867837 -0.06602551
 -0.05337266 -0.0407198  -0.02806695 -0.01541409 -0.00276124]
-2.88903
3.92669
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  4.51762
Epoch 1, cost is  2.70855
Epoch 2, cost is  2.2582
Epoch 3, cost is  2.05333
Epoch 4, cost is  1.92589
Training took 0.072684 minutes
Weight histogram
[518 404 306 247 150 129  74  72  65  60] [-0.17946357 -0.16182222 -0.14418087 -0.12653952 -0.10889817 -0.09125682
 -0.07361547 -0.05597412 -0.03833277 -0.02069142 -0.00305007]
[159  83  83 110 141 174 216 269 353 437] [-0.17946357 -0.16182222 -0.14418087 -0.12653952 -0.10889817 -0.09125682
 -0.07361547 -0.05597412 -0.03833277 -0.02069142 -0.00305007]
-3.73078
4.39851
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041977 minutes
Epoch 0
Fine tuning took 0.042480 minutes
Epoch 0
Fine tuning took 0.043375 minutes
{'zero': {0: [0.13423645320197045, 0.15886699507389163, 0.16009852216748768, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.62068965517241381, 0.61330049261083741, 0.60098522167487689], 5: [0.11206896551724138, 0.22044334975369459, 0.22660098522167488, 0.2229064039408867], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13423645320197045, 0.10098522167487685, 0.11083743842364532, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.78694581280788178, 0.77709359605911332, 0.85098522167487689], 5: [0.11206896551724138, 0.11206896551724138, 0.11206896551724138, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13423645320197045, 0.094827586206896547, 0.10591133004926108, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.80295566502463056, 0.79433497536945807, 0.79187192118226601], 5: [0.11206896551724138, 0.10221674876847291, 0.099753694581280791, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13423645320197045, 0.080049261083743842, 0.089901477832512317, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.75369458128078815, 0.81896551724137934, 0.80911330049261088, 0.8423645320197044], 5: [0.11206896551724138, 0.10098522167487685, 0.10098522167487685, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147208 minutes
Weight histogram
[ 34  67 128 159 208 389 379 364 234  63] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
[ 81  77 111 135 144 183 233 302 326 433] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
-0.44884
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.6591
Epoch 1, cost is  2.16357
Epoch 2, cost is  1.81224
Epoch 3, cost is  1.65546
Epoch 4, cost is  1.56546
Training took 0.081867 minutes
Weight histogram
[797 429 307 194  94  62  53  38  21  30] [-0.12928979 -0.11665666 -0.10402352 -0.09139039 -0.07875725 -0.06612412
 -0.05349098 -0.04085785 -0.02822471 -0.01559158 -0.00295844]
[ 69  59  77 104 139 179 217 287 385 509] [-0.12928979 -0.11665666 -0.10402352 -0.09139039 -0.07875725 -0.06612412
 -0.05349098 -0.04085785 -0.02822471 -0.01559158 -0.00295844]
-2.69308
3.4726
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148349 minutes
Weight histogram
[  20   47  169  360  615  834 1020  581  322   82] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
[174 177 226 279 332 396 529 624 792 521] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
-0.600749
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  3.73834
Epoch 1, cost is  2.11005
Epoch 2, cost is  1.77714
Epoch 3, cost is  1.62809
Epoch 4, cost is  1.54158
Training took 0.080000 minutes
Weight histogram
[1469  905  641  380  206  144  101   81   53   70] [-0.12928979 -0.11663694 -0.10398408 -0.09133123 -0.07867837 -0.06602551
 -0.05337266 -0.0407198  -0.02806695 -0.01541409 -0.00276124]
[149 117 163 211 276 355 455 610 813 901] [-0.12928979 -0.11663694 -0.10398408 -0.09133123 -0.07867837 -0.06602551
 -0.05337266 -0.0407198  -0.02806695 -0.01541409 -0.00276124]
-2.88903
3.92669
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  4.41139
Epoch 1, cost is  2.38226
Epoch 2, cost is  1.83627
Epoch 3, cost is  1.56292
Epoch 4, cost is  1.39401
Training took 0.078685 minutes
Weight histogram
[550 399 276 204 177 136  95 100  68  20] [-0.13538978 -0.12215347 -0.10891716 -0.09568085 -0.08244454 -0.06920823
 -0.05597193 -0.04273562 -0.02949931 -0.016263   -0.00302669]
[171  83  87 116 143 178 227 275 333 412] [-0.13538978 -0.12215347 -0.10891716 -0.09568085 -0.08244454 -0.06920823
 -0.05597193 -0.04273562 -0.02949931 -0.016263   -0.00302669]
-2.83934
3.83234
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042908 minutes
Epoch 0
Fine tuning took 0.043020 minutes
Epoch 0
Fine tuning took 0.041711 minutes
{'zero': {0: [0.1145320197044335, 0.12315270935960591, 0.15763546798029557, 0.16625615763546797], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.65024630541871919, 0.54433497536945807, 0.6280788177339901], 5: [0.14162561576354679, 0.22660098522167488, 0.29802955665024633, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1145320197044335, 0.098522167487684734, 0.10221674876847291, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.78940886699507384, 0.7931034482758621, 0.8214285714285714], 5: [0.14162561576354679, 0.11206896551724138, 0.10467980295566502, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1145320197044335, 0.10098522167487685, 0.098522167487684734, 0.075123152709359611], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.76970443349753692, 0.75492610837438423, 0.84113300492610843], 5: [0.14162561576354679, 0.12931034482758622, 0.14655172413793102, 0.083743842364532015], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1145320197044335, 0.088669950738916259, 0.13793103448275862, 0.11576354679802955], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74384236453201968, 0.81403940886699511, 0.76108374384236455, 0.79064039408866993], 5: [0.14162561576354679, 0.097290640394088676, 0.10098522167487685, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146805 minutes
Weight histogram
[ 34  67 128 159 208 389 379 364 234  63] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
[ 81  77 111 135 144 183 233 302 326 433] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
-0.44884
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.26986
Epoch 1, cost is  4.84477
Epoch 2, cost is  3.87455
Epoch 3, cost is  3.34447
Epoch 4, cost is  3.00693
Training took 0.081466 minutes
Weight histogram
[387 333 231 217 182 182 150 108 214  21] [-0.05886312 -0.05300262 -0.04714211 -0.04128161 -0.03542111 -0.02956061
 -0.02370011 -0.0178396  -0.0119791  -0.0061186  -0.0002581 ]
[323 158 140 138 162 175 193 222 241 273] [-0.05886312 -0.05300262 -0.04714211 -0.04128161 -0.03542111 -0.02956061
 -0.02370011 -0.0178396  -0.0119791  -0.0061186  -0.0002581 ]
-0.913047
1.22218
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147611 minutes
Weight histogram
[  20   47  169  360  615  834 1020  581  322   82] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
[174 177 226 279 332 396 529 624 792 521] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
-0.600749
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.408
Epoch 1, cost is  5.13577
Epoch 2, cost is  4.04445
Epoch 3, cost is  3.45522
Epoch 4, cost is  3.07931
Training took 0.084625 minutes
Weight histogram
[523 661 497 442 389 374 347 296 479  42] [-0.05886312 -0.05300064 -0.04713817 -0.0412757  -0.03541322 -0.02955075
 -0.02368827 -0.0178258  -0.01196332 -0.00610085 -0.00023838]
[715 310 274 278 313 350 386 433 478 513] [-0.05886312 -0.05300064 -0.04713817 -0.0412757  -0.03541322 -0.02955075
 -0.02368827 -0.0178258  -0.01196332 -0.00610085 -0.00023838]
-1.05508
1.27692
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  6.48112
Epoch 1, cost is  5.92098
Epoch 2, cost is  5.44663
Epoch 3, cost is  4.87461
Epoch 4, cost is  4.30065
Training took 0.071449 minutes
Weight histogram
[223 233 223 240 159 188 341 366  38  14] [-0.05580953 -0.0502485  -0.04468747 -0.03912644 -0.03356541 -0.02800438
 -0.02244335 -0.01688232 -0.01132129 -0.00576026 -0.00019923]
[431 271 275 240 150 129 125 133 129 142] [-0.05580953 -0.0502485  -0.04468747 -0.03912644 -0.03356541 -0.02800438
 -0.02244335 -0.01688232 -0.01132129 -0.00576026 -0.00019923]
-0.631015
0.809291
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044017 minutes
Epoch 0
Fine tuning took 0.041646 minutes
Epoch 0
Fine tuning took 0.040857 minutes
{'zero': {0: [0.22660098522167488, 0.17610837438423646, 0.13054187192118227, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.66625615763546797, 0.67980295566502458, 0.6711822660098522], 5: [0.15640394088669951, 0.15763546798029557, 0.18965517241379309, 0.13793103448275862], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22660098522167488, 0.13793103448275862, 0.13423645320197045, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.70812807881773399, 0.67487684729064035, 0.71059113300492616], 5: [0.15640394088669951, 0.1539408866995074, 0.19088669950738915, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22660098522167488, 0.15517241379310345, 0.14408866995073891, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.68965517241379315, 0.67733990147783252, 0.71798029556650245], 5: [0.15640394088669951, 0.15517241379310345, 0.17857142857142858, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22660098522167488, 0.14039408866995073, 0.17118226600985223, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61699507389162567, 0.70197044334975367, 0.63054187192118227, 0.70566502463054193], 5: [0.15640394088669951, 0.15763546798029557, 0.19827586206896552, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147192 minutes
Weight histogram
[ 34  67 128 159 208 389 379 364 234  63] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
[ 81  77 111 135 144 183 233 302 326 433] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
-0.44884
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.26986
Epoch 1, cost is  4.84477
Epoch 2, cost is  3.87455
Epoch 3, cost is  3.34447
Epoch 4, cost is  3.00693
Training took 0.082795 minutes
Weight histogram
[387 333 231 217 182 182 150 108 214  21] [-0.05886312 -0.05300262 -0.04714211 -0.04128161 -0.03542111 -0.02956061
 -0.02370011 -0.0178396  -0.0119791  -0.0061186  -0.0002581 ]
[323 158 140 138 162 175 193 222 241 273] [-0.05886312 -0.05300262 -0.04714211 -0.04128161 -0.03542111 -0.02956061
 -0.02370011 -0.0178396  -0.0119791  -0.0061186  -0.0002581 ]
-0.913047
1.22218
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.145926 minutes
Weight histogram
[  20   47  169  360  615  834 1020  581  322   82] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
[174 177 226 279 332 396 529 624 792 521] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
-0.600749
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.408
Epoch 1, cost is  5.13577
Epoch 2, cost is  4.04445
Epoch 3, cost is  3.45522
Epoch 4, cost is  3.07931
Training took 0.081684 minutes
Weight histogram
[523 661 497 442 389 374 347 296 479  42] [-0.05886312 -0.05300064 -0.04713817 -0.0412757  -0.03541322 -0.02955075
 -0.02368827 -0.0178258  -0.01196332 -0.00610085 -0.00023838]
[715 310 274 278 313 350 386 433 478 513] [-0.05886312 -0.05300064 -0.04713817 -0.0412757  -0.03541322 -0.02955075
 -0.02368827 -0.0178258  -0.01196332 -0.00610085 -0.00023838]
-1.05508
1.27692
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.35867
Epoch 1, cost is  5.78968
Epoch 2, cost is  5.3616
Epoch 3, cost is  4.81043
Epoch 4, cost is  4.14643
Training took 0.077581 minutes
Weight histogram
[284 316 364 205 226 333 225  44  18  10] [-0.04141794 -0.03730457 -0.03319119 -0.02907782 -0.02496445 -0.02085107
 -0.0167377  -0.01262432 -0.00851095 -0.00439757 -0.0002842 ]
[396 322 376 213 121 116 116 117 120 128] [-0.04141794 -0.03730457 -0.03319119 -0.02907782 -0.02496445 -0.02085107
 -0.0167377  -0.01262432 -0.00851095 -0.00439757 -0.0002842 ]
-0.652212
0.646161
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043929 minutes
Epoch 0
Fine tuning took 0.044807 minutes
Epoch 0
Fine tuning took 0.041618 minutes
{'zero': {0: [0.21674876847290642, 0.15640394088669951, 0.18226600985221675, 0.2376847290640394], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.66625615763546797, 0.6145320197044335, 0.65517241379310343], 5: [0.13793103448275862, 0.17733990147783252, 0.20320197044334976, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21674876847290642, 0.14532019704433496, 0.13054187192118227, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.65517241379310343, 0.64778325123152714, 0.66625615763546797], 5: [0.13793103448275862, 0.19950738916256158, 0.22167487684729065, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21674876847290642, 0.16995073891625614, 0.17610837438423646, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.64655172413793105, 0.60098522167487689, 0.68719211822660098], 5: [0.13793103448275862, 0.18349753694581281, 0.2229064039408867, 0.11083743842364532], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21674876847290642, 0.12931034482758622, 0.16379310344827586, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64532019704433496, 0.67241379310344829, 0.63793103448275867, 0.69334975369458129], 5: [0.13793103448275862, 0.19827586206896552, 0.19827586206896552, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146633 minutes
Weight histogram
[ 34  67 128 159 208 389 379 364 234  63] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
[ 81  77 111 135 144 183 233 302 326 433] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
-0.44884
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.80581
Epoch 1, cost is  6.68892
Epoch 2, cost is  6.59842
Epoch 3, cost is  6.49509
Epoch 4, cost is  6.36714
Training took 0.082732 minutes
Weight histogram
[1212  284  158  107   76   57   45   35   28   23] [ -1.10378573e-02  -9.93287786e-03  -8.82789845e-03  -7.72291904e-03
  -6.61793963e-03  -5.51296023e-03  -4.40798082e-03  -3.30300141e-03
  -2.19802200e-03  -1.09304259e-03   1.19368206e-05]
[792 273 195 156 131 116 102  90  89  81] [ -1.10378573e-02  -9.93287786e-03  -8.82789845e-03  -7.72291904e-03
  -6.61793963e-03  -5.51296023e-03  -4.40798082e-03  -3.30300141e-03
  -2.19802200e-03  -1.09304259e-03   1.19368206e-05]
-0.192442
0.219575
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148058 minutes
Weight histogram
[  20   47  169  360  615  834 1020  581  322   82] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
[174 177 226 279 332 396 529 624 792 521] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
-0.600749
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.80767
Epoch 1, cost is  6.7012
Epoch 2, cost is  6.63276
Epoch 3, cost is  6.56218
Epoch 4, cost is  6.48575
Training took 0.080033 minutes
Weight histogram
[2415  568  319  214  155  116   90   71   55   47] [ -1.11046555e-02  -9.99279890e-03  -8.88094236e-03  -7.76908581e-03
  -6.65722926e-03  -5.54537272e-03  -4.43351617e-03  -3.32165962e-03
  -2.20980307e-03  -1.09794653e-03   1.39100202e-05]
[1745  632  461  358  315  177  102   90   89   81] [ -1.11046555e-02  -9.99279890e-03  -8.88094236e-03  -7.76908581e-03
  -6.65722926e-03  -5.54537272e-03  -4.43351617e-03  -3.32165962e-03
  -2.20980307e-03  -1.09794653e-03   1.39100202e-05]
-0.192442
0.219575
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(50,)
Epoch 0, cost is  6.71208
Epoch 1, cost is  6.47447
Epoch 2, cost is  6.30152
Epoch 3, cost is  6.15185
Epoch 4, cost is  6.00987
Training took 0.068867 minutes
Weight histogram
[751 424 262 180 125  92  69  51  40  31] [ -2.25583259e-02  -2.02948675e-02  -1.80314090e-02  -1.57679506e-02
  -1.35044921e-02  -1.12410337e-02  -8.97757527e-03  -6.71411683e-03
  -4.45065839e-03  -2.18719995e-03   7.62584896e-05]
[610 253 204 174 156 140 131 127 115 115] [ -2.25583259e-02  -2.02948675e-02  -1.80314090e-02  -1.57679506e-02
  -1.35044921e-02  -1.12410337e-02  -8.97757527e-03  -6.71411683e-03
  -4.45065839e-03  -2.18719995e-03   7.62584896e-05]
-0.0660519
0.0470986
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043870 minutes
Epoch 0
Fine tuning took 0.044208 minutes
Epoch 0
Fine tuning took 0.043150 minutes
{'zero': {0: [0.3460591133004926, 0.15517241379310345, 0.22167487684729065, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.53448275862068961, 0.50369458128078815, 0.53817733990147787], 5: [0.24630541871921183, 0.31034482758620691, 0.27463054187192121, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.3460591133004926, 0.12807881773399016, 0.23399014778325122, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.53817733990147787, 0.49261083743842365, 0.53694581280788178], 5: [0.24630541871921183, 0.33374384236453203, 0.27339901477832512, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.3460591133004926, 0.13300492610837439, 0.2019704433497537, 0.25492610837438423], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.53817733990147787, 0.50615763546798032, 0.50246305418719217], 5: [0.24630541871921183, 0.3288177339901478, 0.29187192118226601, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.3460591133004926, 0.1268472906403941, 0.22783251231527094, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.40763546798029554, 0.56280788177339902, 0.49876847290640391, 0.4963054187192118], 5: [0.24630541871921183, 0.31034482758620691, 0.27339901477832512, 0.26847290640394089], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.147617 minutes
Weight histogram
[ 34  67 128 159 208 389 379 364 234  63] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
[ 81  77 111 135 144 183 233 302 326 433] [ -1.07421329e-04  -2.82311077e-05   5.09591133e-05   1.30149334e-04
   2.09339555e-04   2.88529776e-04   3.67719997e-04   4.46910218e-04
   5.26100439e-04   6.05290660e-04   6.84480881e-04]
-0.44884
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.80581
Epoch 1, cost is  6.68892
Epoch 2, cost is  6.59842
Epoch 3, cost is  6.49509
Epoch 4, cost is  6.36714
Training took 0.080016 minutes
Weight histogram
[1212  284  158  107   76   57   45   35   28   23] [ -1.10378573e-02  -9.93287786e-03  -8.82789845e-03  -7.72291904e-03
  -6.61793963e-03  -5.51296023e-03  -4.40798082e-03  -3.30300141e-03
  -2.19802200e-03  -1.09304259e-03   1.19368206e-05]
[792 273 195 156 131 116 102  90  89  81] [ -1.10378573e-02  -9.93287786e-03  -8.82789845e-03  -7.72291904e-03
  -6.61793963e-03  -5.51296023e-03  -4.40798082e-03  -3.30300141e-03
  -2.19802200e-03  -1.09304259e-03   1.19368206e-05]
-0.192442
0.219575
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(250,)
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.148004 minutes
Weight histogram
[  20   47  169  360  615  834 1020  581  322   82] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
[174 177 226 279 332 396 529 624 792 521] [ -2.14635569e-04  -1.24723924e-04  -3.48122790e-05   5.50993660e-05
   1.45011011e-04   2.34922656e-04   3.24834301e-04   4.14745946e-04
   5.04657591e-04   5.94569236e-04   6.84480881e-04]
-0.600749
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.80767
Epoch 1, cost is  6.7012
Epoch 2, cost is  6.63276
Epoch 3, cost is  6.56218
Epoch 4, cost is  6.48575
Training took 0.080024 minutes
Weight histogram
[2415  568  319  214  155  116   90   71   55   47] [ -1.11046555e-02  -9.99279890e-03  -8.88094236e-03  -7.76908581e-03
  -6.65722926e-03  -5.54537272e-03  -4.43351617e-03  -3.32165962e-03
  -2.20980307e-03  -1.09794653e-03   1.39100202e-05]
[1745  632  461  358  315  177  102   90   89   81] [ -1.11046555e-02  -9.99279890e-03  -8.88094236e-03  -7.76908581e-03
  -6.65722926e-03  -5.54537272e-03  -4.43351617e-03  -3.32165962e-03
  -2.20980307e-03  -1.09794653e-03   1.39100202e-05]
-0.192442
0.219575
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial mean activity for hidden units
(100,)
Epoch 0, cost is  6.56799
Epoch 1, cost is  6.22357
Epoch 2, cost is  6.01444
Epoch 3, cost is  5.84161
Epoch 4, cost is  5.6856
Training took 0.080083 minutes
Weight histogram
[696 430 276 191 133  98  73  54  41  33] [ -2.31174268e-02  -2.08079150e-02  -1.84984032e-02  -1.61888914e-02
  -1.38793796e-02  -1.15698679e-02  -9.26035607e-03  -6.95084428e-03
  -4.64133250e-03  -2.33182071e-03  -2.23089264e-05]
[584 250 199 175 158 146 140 131 120 122] [ -2.31174268e-02  -2.08079150e-02  -1.84984032e-02  -1.61888914e-02
  -1.38793796e-02  -1.15698679e-02  -9.26035607e-03  -6.95084428e-03
  -4.64133250e-03  -2.33182071e-03  -2.23089264e-05]
-0.0683739
0.048653
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043663 minutes
Epoch 0
Fine tuning took 0.043042 minutes
Epoch 0
Fine tuning took 0.042385 minutes
{'zero': {0: [0.33990147783251229, 0.14655172413793102, 0.23891625615763548, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41256157635467983, 0.51847290640394084, 0.50985221674876846, 0.51231527093596063], 5: [0.24753694581280788, 0.33497536945812806, 0.25123152709359609, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.33990147783251229, 0.15640394088669951, 0.24384236453201971, 0.22906403940886699], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41256157635467983, 0.54926108374384242, 0.47536945812807879, 0.52463054187192115], 5: [0.24753694581280788, 0.29433497536945813, 0.28078817733990147, 0.24630541871921183], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.33990147783251229, 0.13423645320197045, 0.23399014778325122, 0.25738916256157635], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41256157635467983, 0.52955665024630538, 0.49261083743842365, 0.51231527093596063], 5: [0.24753694581280788, 0.33620689655172414, 0.27339901477832512, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.33990147783251229, 0.14901477832512317, 0.2229064039408867, 0.23522167487684728], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.41256157635467983, 0.53448275862068961, 0.5, 0.50492610837438423], 5: [0.24753694581280788, 0.31650246305418717, 0.27709359605911332, 0.25985221674876846], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141778 minutes
Weight histogram
[ 56 142 246 405 570 706 782 695 383  65] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
[ 101  117  162  184  261  341  416  559  790 1119] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
-0.526869
0.544969
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.46603
Epoch 1, cost is  1.36473
Epoch 2, cost is  1.3133
Epoch 3, cost is  1.27235
Epoch 4, cost is  1.24093
Training took 0.080187 minutes
Weight histogram
[1463  559  892  511  282  135   74   62   34   38] [-0.16218384 -0.1462613  -0.13033876 -0.11441622 -0.09849368 -0.08257114
 -0.0666486  -0.05072606 -0.03480352 -0.01888098 -0.00295844]
[  86   78  125  170  229  318  448  670  849 1077] [-0.16218384 -0.1462613  -0.13033876 -0.11441622 -0.09849368 -0.08257114
 -0.0666486  -0.05072606 -0.03480352 -0.01888098 -0.00295844]
-3.2974
4.52706
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141516 minutes
Weight histogram
[  23   76  222  499  838 1301 1321 1186  535   74] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
[ 215  243  319  412  505  698  916  751  824 1192] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
-0.725575
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.44284
Epoch 1, cost is  1.34556
Epoch 2, cost is  1.29131
Epoch 3, cost is  1.25284
Epoch 4, cost is  1.21901
Training took 0.081197 minutes
Weight histogram
[1476  819 1640  915  527  271  157  112   75   83] [-0.15762532 -0.14213891 -0.1266525  -0.11116609 -0.09567969 -0.08019328
 -0.06470687 -0.04922046 -0.03373405 -0.01824765 -0.00276124]
[ 178  163  249  339  464  655  944 1123  870 1090] [-0.15762532 -0.14213891 -0.1266525  -0.11116609 -0.09567969 -0.08019328
 -0.06470687 -0.04922046 -0.03373405 -0.01824765 -0.00276124]
-3.81748
4.70887
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.56441
Epoch 1, cost is  2.7874
Epoch 2, cost is  2.33195
Epoch 3, cost is  2.10445
Epoch 4, cost is  1.96465
Training took 0.072313 minutes
Weight histogram
[884 848 702 446 351 254 156 158 130 121] [-0.18266472 -0.16470326 -0.14674179 -0.12878033 -0.11081886 -0.09285739
 -0.07489593 -0.05693446 -0.038973   -0.02101153 -0.00305007]
[322 170 171 224 284 349 447 551 687 845] [-0.18266472 -0.16470326 -0.14674179 -0.12878033 -0.11081886 -0.09285739
 -0.07489593 -0.05693446 -0.038973   -0.02101153 -0.00305007]
-3.75671
4.39851
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042615 minutes
Epoch 0
Fine tuning took 0.043696 minutes
Epoch 0
Fine tuning took 0.041551 minutes
{'zero': {0: [0.14655172413793102, 0.13177339901477833, 0.1625615763546798, 0.18472906403940886], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.61699507389162567, 0.58497536945812811, 0.51477832512315269], 5: [0.11083743842364532, 0.25123152709359609, 0.25246305418719212, 0.30049261083743845], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14655172413793102, 0.11330049261083744, 0.13793103448275862, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.74137931034482762, 0.72783251231527091, 0.74630541871921185], 5: [0.11083743842364532, 0.14532019704433496, 0.13423645320197045, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14655172413793102, 0.10960591133004927, 0.1206896551724138, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.73399014778325122, 0.76724137931034486, 0.74507389162561577], 5: [0.11083743842364532, 0.15640394088669951, 0.11206896551724138, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14655172413793102, 0.089901477832512317, 0.1145320197044335, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7426108374384236, 0.7857142857142857, 0.79679802955665024, 0.74876847290640391], 5: [0.11083743842364532, 0.12438423645320197, 0.088669950738916259, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141775 minutes
Weight histogram
[ 56 142 246 405 570 706 782 695 383  65] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
[ 101  117  162  184  261  341  416  559  790 1119] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
-0.526869
0.544969
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.46603
Epoch 1, cost is  1.36473
Epoch 2, cost is  1.3133
Epoch 3, cost is  1.27235
Epoch 4, cost is  1.24093
Training took 0.081298 minutes
Weight histogram
[1463  559  892  511  282  135   74   62   34   38] [-0.16218384 -0.1462613  -0.13033876 -0.11441622 -0.09849368 -0.08257114
 -0.0666486  -0.05072606 -0.03480352 -0.01888098 -0.00295844]
[  86   78  125  170  229  318  448  670  849 1077] [-0.16218384 -0.1462613  -0.13033876 -0.11441622 -0.09849368 -0.08257114
 -0.0666486  -0.05072606 -0.03480352 -0.01888098 -0.00295844]
-3.2974
4.52706
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140576 minutes
Weight histogram
[  23   76  222  499  838 1301 1321 1186  535   74] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
[ 215  243  319  412  505  698  916  751  824 1192] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
-0.725575
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.44284
Epoch 1, cost is  1.34556
Epoch 2, cost is  1.29131
Epoch 3, cost is  1.25284
Epoch 4, cost is  1.21901
Training took 0.079822 minutes
Weight histogram
[1476  819 1640  915  527  271  157  112   75   83] [-0.15762532 -0.14213891 -0.1266525  -0.11116609 -0.09567969 -0.08019328
 -0.06470687 -0.04922046 -0.03373405 -0.01824765 -0.00276124]
[ 178  163  249  339  464  655  944 1123  870 1090] [-0.15762532 -0.14213891 -0.1266525  -0.11116609 -0.09567969 -0.08019328
 -0.06470687 -0.04922046 -0.03373405 -0.01824765 -0.00276124]
-3.81748
4.70887
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.47063
Epoch 1, cost is  2.48741
Epoch 2, cost is  1.9357
Epoch 3, cost is  1.64011
Epoch 4, cost is  1.45692
Training took 0.078438 minutes
Weight histogram
[1055  816  569  408  350  273  203  199  137   40] [-0.13538978 -0.12215347 -0.10891716 -0.09568085 -0.08244454 -0.06920823
 -0.05597193 -0.04273562 -0.02949931 -0.016263   -0.00302669]
[347 171 176 235 298 364 463 544 662 790] [-0.13538978 -0.12215347 -0.10891716 -0.09568085 -0.08244454 -0.06920823
 -0.05597193 -0.04273562 -0.02949931 -0.016263   -0.00302669]
-2.87015
3.83234
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041380 minutes
Epoch 0
Fine tuning took 0.042367 minutes
Epoch 0
Fine tuning took 0.041924 minutes
{'zero': {0: [0.13177339901477833, 0.1354679802955665, 0.14162561576354679, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.65024630541871919, 0.5923645320197044, 0.60837438423645318], 5: [0.12192118226600986, 0.21428571428571427, 0.26600985221674878, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13177339901477833, 0.12315270935960591, 0.14039408866995073, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.74753694581280783, 0.75246305418719217, 0.72413793103448276], 5: [0.12192118226600986, 0.12931034482758622, 0.10714285714285714, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13177339901477833, 0.097290640394088676, 0.1354679802955665, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.77093596059113301, 0.72044334975369462, 0.7573891625615764], 5: [0.12192118226600986, 0.13177339901477833, 0.14408866995073891, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13177339901477833, 0.11576354679802955, 0.13916256157635468, 0.2536945812807882], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.79187192118226601, 0.75, 0.62438423645320196], 5: [0.12192118226600986, 0.092364532019704432, 0.11083743842364532, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142835 minutes
Weight histogram
[ 56 142 246 405 570 706 782 695 383  65] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
[ 101  117  162  184  261  341  416  559  790 1119] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
-0.526869
0.544969
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.74888
Epoch 1, cost is  2.56174
Epoch 2, cost is  2.41175
Epoch 3, cost is  2.29047
Epoch 4, cost is  2.18971
Training took 0.080215 minutes
Weight histogram
[1081  685  398  486  339  286  250  206  276   43] [-0.08161334 -0.07347782 -0.06534229 -0.05720677 -0.04907124 -0.04093572
 -0.03280019 -0.02466467 -0.01652915 -0.00839362 -0.0002581 ]
[412 221 227 268 325 375 464 515 590 653] [-0.08161334 -0.07347782 -0.06534229 -0.05720677 -0.04907124 -0.04093572
 -0.03280019 -0.02466467 -0.01652915 -0.00839362 -0.0002581 ]
-1.26515
1.68999
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141702 minutes
Weight histogram
[  23   76  222  499  838 1301 1321 1186  535   74] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
[ 215  243  319  412  505  698  916  751  824 1192] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
-0.725575
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.79206
Epoch 1, cost is  2.57663
Epoch 2, cost is  2.40516
Epoch 3, cost is  2.26584
Epoch 4, cost is  2.15498
Training took 0.080088 minutes
Weight histogram
[949 718 686 888 672 522 490 439 635  76] [-0.07765275 -0.06991131 -0.06216988 -0.05442844 -0.046687   -0.03894556
 -0.03120413 -0.02346269 -0.01572125 -0.00797981 -0.00023838]
[906 442 475 566 668 791 551 500 556 620] [-0.07765275 -0.06991131 -0.06216988 -0.05442844 -0.046687   -0.03894556
 -0.03120413 -0.02346269 -0.01572125 -0.00797981 -0.00023838]
-1.37044
1.77565
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.50761
Epoch 1, cost is  5.99122
Epoch 2, cost is  5.56326
Epoch 3, cost is  4.96923
Epoch 4, cost is  4.38813
Training took 0.069630 minutes
Weight histogram
[314 413 425 472 440 405 643 829  80  29] [-0.05580953 -0.05024758 -0.04468564 -0.03912369 -0.03356175 -0.0279998
 -0.02243786 -0.01687591 -0.01131397 -0.00575202 -0.00019008]
[863 573 622 433 276 252 251 262 271 247] [-0.05580953 -0.05024758 -0.04468564 -0.03912369 -0.03356175 -0.0279998
 -0.02243786 -0.01687591 -0.01131397 -0.00575202 -0.00019008]
-0.641502
0.809291
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040749 minutes
Epoch 0
Fine tuning took 0.043696 minutes
Epoch 0
Fine tuning took 0.044176 minutes
{'zero': {0: [0.14778325123152711, 0.17980295566502463, 0.16009852216748768, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.68596059113300489, 0.71551724137931039, 0.66625615763546797], 5: [0.13177339901477833, 0.13423645320197045, 0.12438423645320197, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14778325123152711, 0.1539408866995074, 0.15886699507389163, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.71551724137931039, 0.69950738916256161, 0.69950738916256161], 5: [0.13177339901477833, 0.13054187192118227, 0.14162561576354679, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14778325123152711, 0.18719211822660098, 0.15270935960591134, 0.12192118226600986], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.68103448275862066, 0.72167487684729059, 0.71798029556650245], 5: [0.13177339901477833, 0.13177339901477833, 0.12561576354679804, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14778325123152711, 0.18103448275862069, 0.15640394088669951, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72044334975369462, 0.67980295566502458, 0.71921182266009853, 0.72536945812807885], 5: [0.13177339901477833, 0.13916256157635468, 0.12438423645320197, 0.16133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140770 minutes
Weight histogram
[ 56 142 246 405 570 706 782 695 383  65] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
[ 101  117  162  184  261  341  416  559  790 1119] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
-0.526869
0.544969
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.74888
Epoch 1, cost is  2.56174
Epoch 2, cost is  2.41175
Epoch 3, cost is  2.29047
Epoch 4, cost is  2.18971
Training took 0.081497 minutes
Weight histogram
[1081  685  398  486  339  286  250  206  276   43] [-0.08161334 -0.07347782 -0.06534229 -0.05720677 -0.04907124 -0.04093572
 -0.03280019 -0.02466467 -0.01652915 -0.00839362 -0.0002581 ]
[412 221 227 268 325 375 464 515 590 653] [-0.08161334 -0.07347782 -0.06534229 -0.05720677 -0.04907124 -0.04093572
 -0.03280019 -0.02466467 -0.01652915 -0.00839362 -0.0002581 ]
-1.26515
1.68999
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141111 minutes
Weight histogram
[  23   76  222  499  838 1301 1321 1186  535   74] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
[ 215  243  319  412  505  698  916  751  824 1192] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
-0.725575
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.79206
Epoch 1, cost is  2.57663
Epoch 2, cost is  2.40516
Epoch 3, cost is  2.26584
Epoch 4, cost is  2.15498
Training took 0.083825 minutes
Weight histogram
[949 718 686 888 672 522 490 439 635  76] [-0.07765275 -0.06991131 -0.06216988 -0.05442844 -0.046687   -0.03894556
 -0.03120413 -0.02346269 -0.01572125 -0.00797981 -0.00023838]
[906 442 475 566 668 791 551 500 556 620] [-0.07765275 -0.06991131 -0.06216988 -0.05442844 -0.046687   -0.03894556
 -0.03120413 -0.02346269 -0.01572125 -0.00797981 -0.00023838]
-1.37044
1.77565
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.39298
Epoch 1, cost is  5.8709
Epoch 2, cost is  5.48318
Epoch 3, cost is  4.91342
Epoch 4, cost is  4.2564
Training took 0.079927 minutes
Weight histogram
[377 563 710 600 492 641 517  93  36  21] [-0.04141794 -0.03730335 -0.03318876 -0.02907417 -0.02495958 -0.020845
 -0.01673041 -0.01261582 -0.00850123 -0.00438664 -0.00027205]
[778 696 771 389 233 230 237 244 240 232] [-0.04141794 -0.03730335 -0.03318876 -0.02907417 -0.02495958 -0.020845
 -0.01673041 -0.01261582 -0.00850123 -0.00438664 -0.00027205]
-0.652212
0.668369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042803 minutes
Epoch 0
Fine tuning took 0.042865 minutes
Epoch 0
Fine tuning took 0.043350 minutes
{'zero': {0: [0.15763546798029557, 0.13177339901477833, 0.14778325123152711, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70566502463054193, 0.71674876847290636, 0.68719211822660098, 0.6711822660098522], 5: [0.13669950738916256, 0.15147783251231528, 0.16502463054187191, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15763546798029557, 0.17364532019704434, 0.16625615763546797, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70566502463054193, 0.69211822660098521, 0.6711822660098522, 0.66379310344827591], 5: [0.13669950738916256, 0.13423645320197045, 0.1625615763546798, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15763546798029557, 0.17980295566502463, 0.16748768472906403, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70566502463054193, 0.68103448275862066, 0.67610837438423643, 0.70812807881773399], 5: [0.13669950738916256, 0.13916256157635468, 0.15640394088669951, 0.16625615763546797], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15763546798029557, 0.14532019704433496, 0.1625615763546798, 0.11206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70566502463054193, 0.71798029556650245, 0.66871921182266014, 0.71059113300492616], 5: [0.13669950738916256, 0.13669950738916256, 0.16871921182266009, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141680 minutes
Weight histogram
[ 56 142 246 405 570 706 782 695 383  65] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
[ 101  117  162  184  261  341  416  559  790 1119] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
-0.526869
0.544969
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  6.2156
Epoch 1, cost is  6.06907
Epoch 2, cost is  5.92678
Epoch 3, cost is  5.77508
Epoch 4, cost is  5.61231
Training took 0.080300 minutes
Weight histogram
[ 490  411  383  387 1329  607  209  115   72   47] [ -2.07615718e-02  -1.86842210e-02  -1.66068701e-02  -1.45295192e-02
  -1.24521684e-02  -1.03748175e-02  -8.29746664e-03  -6.22011577e-03
  -4.14276491e-03  -2.06541404e-03   1.19368206e-05]
[1405  435  331  292  289  287  278  265  240  228] [ -2.07615718e-02  -1.86842210e-02  -1.66068701e-02  -1.45295192e-02
  -1.24521684e-02  -1.03748175e-02  -8.29746664e-03  -6.22011577e-03
  -4.14276491e-03  -2.06541404e-03   1.19368206e-05]
-0.254086
0.353367
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141177 minutes
Weight histogram
[  23   76  222  499  838 1301 1321 1186  535   74] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
[ 215  243  319  412  505  698  916  751  824 1192] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
-0.725575
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  6.38078
Epoch 1, cost is  6.27959
Epoch 2, cost is  6.16375
Epoch 3, cost is  6.02875
Epoch 4, cost is  5.89052
Training took 0.083711 minutes
Weight histogram
[ 501  498 1185 2571  542  295  192  130   91   70] [ -1.58055313e-02  -1.42235872e-02  -1.26416430e-02  -1.10596989e-02
  -9.47775477e-03  -7.89581064e-03  -6.31386651e-03  -4.73192237e-03
  -3.14997824e-03  -1.56803411e-03   1.39100202e-05]
[2785  930  662  422  256  241  211  205  175  188] [ -1.58055313e-02  -1.42235872e-02  -1.26416430e-02  -1.10596989e-02
  -9.47775477e-03  -7.89581064e-03  -6.31386651e-03  -4.73192237e-03
  -3.14997824e-03  -1.56803411e-03   1.39100202e-05]
-0.24633
0.368385
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74355
Epoch 1, cost is  6.54117
Epoch 2, cost is  6.38905
Epoch 3, cost is  6.25319
Epoch 4, cost is  6.12407
Training took 0.072094 minutes
Weight histogram
[ 752 1201  750  441  292  205  150  109   85   65] [ -2.25583259e-02  -2.02943558e-02  -1.80303858e-02  -1.57664157e-02
  -1.35024456e-02  -1.12384755e-02  -8.97450545e-03  -6.71053538e-03
  -4.44656530e-03  -2.18259523e-03   8.13748484e-05]
[1672  766  565  179  168  150  147  138  133  132] [ -2.25583259e-02  -2.02943558e-02  -1.80303858e-02  -1.57664157e-02
  -1.35024456e-02  -1.12384755e-02  -8.97450545e-03  -6.71053538e-03
  -4.44656530e-03  -2.18259523e-03   8.13748484e-05]
-0.0677934
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040516 minutes
Epoch 0
Fine tuning took 0.041567 minutes
Epoch 0
Fine tuning took 0.043424 minutes
{'zero': {0: [0.34729064039408869, 0.16748768472906403, 0.25862068965517243, 0.20812807881773399], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42980295566502463, 0.63177339901477836, 0.50369458128078815, 0.39901477832512317], 5: [0.2229064039408867, 0.20073891625615764, 0.2376847290640394, 0.39285714285714285], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.34729064039408869, 0.2019704433497537, 0.24384236453201971, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42980295566502463, 0.6071428571428571, 0.50492610837438423, 0.43842364532019706], 5: [0.2229064039408867, 0.19088669950738915, 0.25123152709359609, 0.3682266009852217], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.34729064039408869, 0.18349753694581281, 0.23029556650246305, 0.22167487684729065], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42980295566502463, 0.61206896551724133, 0.49137931034482757, 0.39655172413793105], 5: [0.2229064039408867, 0.20443349753694581, 0.27832512315270935, 0.3817733990147783], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.34729064039408869, 0.2019704433497537, 0.26847290640394089, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.42980295566502463, 0.58497536945812811, 0.48152709359605911, 0.42610837438423643], 5: [0.2229064039408867, 0.21305418719211822, 0.25, 0.36945812807881773], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141739 minutes
Weight histogram
[ 56 142 246 405 570 706 782 695 383  65] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
[ 101  117  162  184  261  341  416  559  790 1119] [ -1.07421329e-04   5.49501638e-06   1.18411361e-04   2.31327707e-04
   3.44244052e-04   4.57160397e-04   5.70076742e-04   6.82993087e-04
   7.95909432e-04   9.08825777e-04   1.02174212e-03]
-0.526869
0.544969
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  6.2156
Epoch 1, cost is  6.06907
Epoch 2, cost is  5.92678
Epoch 3, cost is  5.77508
Epoch 4, cost is  5.61231
Training took 0.082879 minutes
Weight histogram
[ 490  411  383  387 1329  607  209  115   72   47] [ -2.07615718e-02  -1.86842210e-02  -1.66068701e-02  -1.45295192e-02
  -1.24521684e-02  -1.03748175e-02  -8.29746664e-03  -6.22011577e-03
  -4.14276491e-03  -2.06541404e-03   1.19368206e-05]
[1405  435  331  292  289  287  278  265  240  228] [ -2.07615718e-02  -1.86842210e-02  -1.66068701e-02  -1.45295192e-02
  -1.24521684e-02  -1.03748175e-02  -8.29746664e-03  -6.22011577e-03
  -4.14276491e-03  -2.06541404e-03   1.19368206e-05]
-0.254086
0.353367
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141686 minutes
Weight histogram
[  23   76  222  499  838 1301 1321 1186  535   74] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
[ 215  243  319  412  505  698  916  751  824 1192] [ -2.14635569e-04  -1.14786462e-04  -1.49373547e-05   8.49117525e-05
   1.84760860e-04   2.84609967e-04   3.84459074e-04   4.84308181e-04
   5.84157289e-04   6.84006396e-04   7.83855503e-04]
-0.725575
0.426782
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  6.38078
Epoch 1, cost is  6.27959
Epoch 2, cost is  6.16375
Epoch 3, cost is  6.02875
Epoch 4, cost is  5.89052
Training took 0.079912 minutes
Weight histogram
[ 501  498 1185 2571  542  295  192  130   91   70] [ -1.58055313e-02  -1.42235872e-02  -1.26416430e-02  -1.10596989e-02
  -9.47775477e-03  -7.89581064e-03  -6.31386651e-03  -4.73192237e-03
  -3.14997824e-03  -1.56803411e-03   1.39100202e-05]
[2785  930  662  422  256  241  211  205  175  188] [ -1.58055313e-02  -1.42235872e-02  -1.26416430e-02  -1.10596989e-02
  -9.47775477e-03  -7.89581064e-03  -6.31386651e-03  -4.73192237e-03
  -3.14997824e-03  -1.56803411e-03   1.39100202e-05]
-0.24633
0.368385
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.62094
Epoch 1, cost is  6.32677
Epoch 2, cost is  6.14285
Epoch 3, cost is  5.98539
Epoch 4, cost is  5.84035
Training took 0.078350 minutes
Weight histogram
[ 696 1158  767  468  312  217  160  116   87   69] [ -2.31174268e-02  -2.08074062e-02  -1.84973856e-02  -1.61873650e-02
  -1.38773444e-02  -1.15673239e-02  -9.25730327e-03  -6.94728268e-03
  -4.63726209e-03  -2.32724151e-03  -1.72209202e-05]
[1611  766  588  182  169  154  153  144  142  141] [ -2.31174268e-02  -2.08074062e-02  -1.84973856e-02  -1.61873650e-02
  -1.38773444e-02  -1.15673239e-02  -9.25730327e-03  -6.94728268e-03
  -4.63726209e-03  -2.32724151e-03  -1.72209202e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042384 minutes
Epoch 0
Fine tuning took 0.043342 minutes
Epoch 0
Fine tuning took 0.043035 minutes
{'zero': {0: [0.34359605911330049, 0.18719211822660098, 0.24753694581280788, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4248768472906404, 0.59729064039408863, 0.48029556650246308, 0.40270935960591131], 5: [0.23152709359605911, 0.21551724137931033, 0.27216748768472904, 0.41133004926108374], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.34359605911330049, 0.19211822660098521, 0.24876847290640394, 0.21798029556650247], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4248768472906404, 0.60960591133004927, 0.50369458128078815, 0.42364532019704432], 5: [0.23152709359605911, 0.19827586206896552, 0.24753694581280788, 0.35837438423645318], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.34359605911330049, 0.20443349753694581, 0.22167487684729065, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4248768472906404, 0.58743842364532017, 0.49876847290640391, 0.42241379310344829], 5: [0.23152709359605911, 0.20812807881773399, 0.27955665024630544, 0.37068965517241381], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.34359605911330049, 0.22413793103448276, 0.25492610837438423, 0.19088669950738915], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4248768472906404, 0.57019704433497542, 0.47536945812807879, 0.44950738916256155], 5: [0.23152709359605911, 0.20566502463054187, 0.26970443349753692, 0.35960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141771 minutes
Weight histogram
[  71  200  334  638  798  899  842  956 1070  267] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
[ 113  139  194  236  331  441  598  851 1265 1907] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
-0.565678
0.623485
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.20911
Epoch 1, cost is  1.15298
Epoch 2, cost is  1.1234
Epoch 3, cost is  1.09954
Epoch 4, cost is  1.07956
Training took 0.081183 minutes
Weight histogram
[1963 1369  717 1032  466  264  104   73   46   41] [-0.18376428 -0.1656837  -0.14760311 -0.12952253 -0.11144194 -0.09336136
 -0.07528078 -0.05720019 -0.03911961 -0.02103903 -0.00295844]
[  94  101  155  225  315  471  721  962 1319 1712] [-0.18376428 -0.1656837  -0.14760311 -0.12952253 -0.11144194 -0.09336136
 -0.07528078 -0.05720019 -0.03911961 -0.02103903 -0.00295844]
-3.85298
5.16071
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140134 minutes
Weight histogram
[  30  115  323  765 1199 1583 1469 1581  919  116] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
[ 238  278  383  489  668  928  891  881 1289 2055] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
-0.782244
0.478008
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.2143
Epoch 1, cost is  1.16071
Epoch 2, cost is  1.12529
Epoch 3, cost is  1.10223
Epoch 4, cost is  1.08111
Training took 0.079549 minutes
Weight histogram
[1979 1278  883 1917  967  509  229  146  101   91] [-0.181587   -0.16370442 -0.14582184 -0.12793927 -0.11005669 -0.09217412
 -0.07429154 -0.05640897 -0.03852639 -0.02064381 -0.00276124]
[ 196  206  304  453  631  972 1267  996 1350 1725] [-0.181587   -0.16370442 -0.14582184 -0.12793927 -0.11005669 -0.09217412
 -0.07429154 -0.05640897 -0.03852639 -0.02064381 -0.00276124]
-4.257
5.04583
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.5842
Epoch 1, cost is  2.83254
Epoch 2, cost is  2.37461
Epoch 3, cost is  2.15388
Epoch 4, cost is  2.00931
Training took 0.073037 minutes
Weight histogram
[1348 1241 1051  660  546  373  241  243  191  181] [-0.18266472 -0.16470326 -0.14674179 -0.12878033 -0.11081886 -0.09285739
 -0.07489593 -0.05693446 -0.038973   -0.02101153 -0.00305007]
[ 486  259  263  340  428  527  681  830 1038 1223] [-0.18266472 -0.16470326 -0.14674179 -0.12878033 -0.11081886 -0.09285739
 -0.07489593 -0.05693446 -0.038973   -0.02101153 -0.00305007]
-3.80249
4.39851
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042455 minutes
Epoch 0
Fine tuning took 0.042616 minutes
Epoch 0
Fine tuning took 0.043977 minutes
{'zero': {0: [0.17364532019704434, 0.15270935960591134, 0.17241379310344829, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.51477832512315269, 0.54926108374384242, 0.64901477832512311], 5: [0.14901477832512317, 0.33251231527093594, 0.27832512315270935, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17364532019704434, 0.19334975369458129, 0.17980295566502463, 0.15886699507389163], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.70073891625615758, 0.68965517241379315, 0.7426108374384236], 5: [0.14901477832512317, 0.10591133004926108, 0.13054187192118227, 0.098522167487684734], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17364532019704434, 0.1145320197044335, 0.11576354679802955, 0.10960591133004927], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.72290640394088668, 0.73522167487684731, 0.77586206896551724], 5: [0.14901477832512317, 0.1625615763546798, 0.14901477832512317, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17364532019704434, 0.18719211822660098, 0.1625615763546798, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.69827586206896552, 0.6785714285714286, 0.70935960591133007], 5: [0.14901477832512317, 0.1145320197044335, 0.15886699507389163, 0.13300492610837439], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141250 minutes
Weight histogram
[  71  200  334  638  798  899  842  956 1070  267] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
[ 113  139  194  236  331  441  598  851 1265 1907] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
-0.565678
0.623485
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.20911
Epoch 1, cost is  1.15298
Epoch 2, cost is  1.1234
Epoch 3, cost is  1.09954
Epoch 4, cost is  1.07956
Training took 0.081368 minutes
Weight histogram
[1963 1369  717 1032  466  264  104   73   46   41] [-0.18376428 -0.1656837  -0.14760311 -0.12952253 -0.11144194 -0.09336136
 -0.07528078 -0.05720019 -0.03911961 -0.02103903 -0.00295844]
[  94  101  155  225  315  471  721  962 1319 1712] [-0.18376428 -0.1656837  -0.14760311 -0.12952253 -0.11144194 -0.09336136
 -0.07528078 -0.05720019 -0.03911961 -0.02103903 -0.00295844]
-3.85298
5.16071
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140621 minutes
Weight histogram
[  30  115  323  765 1199 1583 1469 1581  919  116] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
[ 238  278  383  489  668  928  891  881 1289 2055] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
-0.782244
0.478008
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.2143
Epoch 1, cost is  1.16071
Epoch 2, cost is  1.12529
Epoch 3, cost is  1.10223
Epoch 4, cost is  1.08111
Training took 0.079978 minutes
Weight histogram
[1979 1278  883 1917  967  509  229  146  101   91] [-0.181587   -0.16370442 -0.14582184 -0.12793927 -0.11005669 -0.09217412
 -0.07429154 -0.05640897 -0.03852639 -0.02064381 -0.00276124]
[ 196  206  304  453  631  972 1267  996 1350 1725] [-0.181587   -0.16370442 -0.14582184 -0.12793927 -0.11005669 -0.09217412
 -0.07429154 -0.05640897 -0.03852639 -0.02064381 -0.00276124]
-4.257
5.04583
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.48602
Epoch 1, cost is  2.54193
Epoch 2, cost is  1.96175
Epoch 3, cost is  1.66343
Epoch 4, cost is  1.46917
Training took 0.077347 minutes
Weight histogram
[1250 1328  917  633  613  426  293  345  197   73] [-0.13864698 -0.12508495 -0.11152292 -0.09796089 -0.08439886 -0.07083683
 -0.05727481 -0.04371278 -0.03015075 -0.01658872 -0.00302669]
[ 527  257  273  358  456  553  699  826  986 1140] [-0.13864698 -0.12508495 -0.11152292 -0.09796089 -0.08439886 -0.07083683
 -0.05727481 -0.04371278 -0.03015075 -0.01658872 -0.00302669]
-3.06492
4.40344
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042859 minutes
Epoch 0
Fine tuning took 0.044900 minutes
Epoch 0
Fine tuning took 0.041546 minutes
{'zero': {0: [0.13177339901477833, 0.22906403940886699, 0.14532019704433496, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.5714285714285714, 0.6354679802955665, 0.64901477832512311], 5: [0.13669950738916256, 0.19950738916256158, 0.21921182266009853, 0.2376847290640394], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.13177339901477833, 0.14039408866995073, 0.16009852216748768, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.76847290640394084, 0.73891625615763545, 0.75492610837438423], 5: [0.13669950738916256, 0.091133004926108374, 0.10098522167487685, 0.10344827586206896], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.13177339901477833, 0.16502463054187191, 0.13793103448275862, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.72783251231527091, 0.72167487684729059, 0.78448275862068961], 5: [0.13669950738916256, 0.10714285714285714, 0.14039408866995073, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.13177339901477833, 0.11822660098522167, 0.18472906403940886, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73152709359605916, 0.80665024630541871, 0.72413793103448276, 0.72906403940886699], 5: [0.13669950738916256, 0.075123152709359611, 0.091133004926108374, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140766 minutes
Weight histogram
[  71  200  334  638  798  899  842  956 1070  267] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
[ 113  139  194  236  331  441  598  851 1265 1907] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
-0.565678
0.623485
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.08369
Epoch 1, cost is  1.99443
Epoch 2, cost is  1.92501
Epoch 3, cost is  1.85823
Epoch 4, cost is  1.80127
Training took 0.079596 minutes
Weight histogram
[1826  989  903  476  542  392  318  256  304   69] [-0.09502206 -0.08554566 -0.07606927 -0.06659287 -0.05711647 -0.04764008
 -0.03816368 -0.02868729 -0.01921089 -0.00973449 -0.0002581 ]
[ 470  268  319  395  476  613  695  809  971 1059] [-0.09502206 -0.08554566 -0.07606927 -0.06659287 -0.05711647 -0.04764008
 -0.03816368 -0.02868729 -0.01921089 -0.00973449 -0.0002581 ]
-1.49045
1.95435
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140417 minutes
Weight histogram
[  30  115  323  765 1199 1583 1469 1581  919  116] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
[ 238  278  383  489  668  928  891  881 1289 2055] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
-0.782244
0.478008
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.0533
Epoch 1, cost is  1.96751
Epoch 2, cost is  1.88727
Epoch 3, cost is  1.82676
Epoch 4, cost is  1.77173
Training took 0.081033 minutes
Weight histogram
[1600  877 1013  772 1024  782  598  573  737  124] [-0.09247027 -0.08324708 -0.07402389 -0.0648007  -0.05557751 -0.04635432
 -0.03713113 -0.02790794 -0.01868475 -0.00946157 -0.00023838]
[1028  553  668  824  992  598  670  781  936 1050] [-0.09247027 -0.08324708 -0.07402389 -0.0648007  -0.05557751 -0.04635432
 -0.03713113 -0.02790794 -0.01868475 -0.00946157 -0.00023838]
-1.53358
2.06682
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.51716
Epoch 1, cost is  6.01234
Epoch 2, cost is  5.59725
Epoch 3, cost is  4.99449
Epoch 4, cost is  4.4384
Training took 0.069669 minutes
Weight histogram
[ 390  602  619  700  727  646  909 1314  124   44] [-0.05580953 -0.05024695 -0.04468437 -0.03912178 -0.0335592  -0.02799662
 -0.02243404 -0.01687146 -0.01130888 -0.0057463  -0.00018372]
[1265  870  971  637  404  374  378  397  414  365] [-0.05580953 -0.05024695 -0.04468437 -0.03912178 -0.0335592  -0.02799662
 -0.02243404 -0.01687146 -0.01130888 -0.0057463  -0.00018372]
-0.664725
0.874477
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042115 minutes
Epoch 0
Fine tuning took 0.042872 minutes
Epoch 0
Fine tuning took 0.043338 minutes
{'zero': {0: [0.14039408866995073, 0.12192118226600986, 0.16009852216748768, 0.1145320197044335], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.74137931034482762, 0.73768472906403937, 0.75123152709359609], 5: [0.11083743842364532, 0.13669950738916256, 0.10221674876847291, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14039408866995073, 0.10591133004926108, 0.18596059113300492, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.75862068965517238, 0.72290640394088668, 0.79187192118226601], 5: [0.11083743842364532, 0.1354679802955665, 0.091133004926108374, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14039408866995073, 0.12561576354679804, 0.1748768472906404, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.72413793103448276, 0.72290640394088668, 0.77216748768472909], 5: [0.11083743842364532, 0.15024630541871922, 0.10221674876847291, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14039408866995073, 0.098522167487684734, 0.19950738916256158, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.76477832512315269, 0.71674876847290636, 0.76724137931034486], 5: [0.11083743842364532, 0.13669950738916256, 0.083743842364532015, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140998 minutes
Weight histogram
[  71  200  334  638  798  899  842  956 1070  267] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
[ 113  139  194  236  331  441  598  851 1265 1907] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
-0.565678
0.623485
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.08369
Epoch 1, cost is  1.99443
Epoch 2, cost is  1.92501
Epoch 3, cost is  1.85823
Epoch 4, cost is  1.80127
Training took 0.081180 minutes
Weight histogram
[1826  989  903  476  542  392  318  256  304   69] [-0.09502206 -0.08554566 -0.07606927 -0.06659287 -0.05711647 -0.04764008
 -0.03816368 -0.02868729 -0.01921089 -0.00973449 -0.0002581 ]
[ 470  268  319  395  476  613  695  809  971 1059] [-0.09502206 -0.08554566 -0.07606927 -0.06659287 -0.05711647 -0.04764008
 -0.03816368 -0.02868729 -0.01921089 -0.00973449 -0.0002581 ]
-1.49045
1.95435
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140055 minutes
Weight histogram
[  30  115  323  765 1199 1583 1469 1581  919  116] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
[ 238  278  383  489  668  928  891  881 1289 2055] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
-0.782244
0.478008
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.0533
Epoch 1, cost is  1.96751
Epoch 2, cost is  1.88727
Epoch 3, cost is  1.82676
Epoch 4, cost is  1.77173
Training took 0.079885 minutes
Weight histogram
[1600  877 1013  772 1024  782  598  573  737  124] [-0.09247027 -0.08324708 -0.07402389 -0.0648007  -0.05557751 -0.04635432
 -0.03713113 -0.02790794 -0.01868475 -0.00946157 -0.00023838]
[1028  553  668  824  992  598  670  781  936 1050] [-0.09247027 -0.08324708 -0.07402389 -0.0648007  -0.05557751 -0.04635432
 -0.03713113 -0.02790794 -0.01868475 -0.00946157 -0.00023838]
-1.53358
2.06682
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.4053
Epoch 1, cost is  5.89791
Epoch 2, cost is  5.52577
Epoch 3, cost is  4.9662
Epoch 4, cost is  4.32493
Training took 0.079515 minutes
Weight histogram
[ 439  800 1013 1071  764  920  837  144   55   32] [-0.04141794 -0.03730254 -0.03318713 -0.02907172 -0.02495632 -0.02084091
 -0.0167255  -0.0126101  -0.00849469 -0.00437929 -0.00026388]
[1131 1039 1198  576  349  346  357  365  363  351] [-0.04141794 -0.03730254 -0.03318713 -0.02907172 -0.02495632 -0.02084091
 -0.0167255  -0.0126101  -0.00849469 -0.00437929 -0.00026388]
-0.652212
0.668369
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041482 minutes
Epoch 0
Fine tuning took 0.043773 minutes
Epoch 0
Fine tuning took 0.043712 minutes
{'zero': {0: [0.14408866995073891, 0.13177339901477833, 0.14408866995073891, 0.13177339901477833], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.69704433497536944, 0.73891625615763545, 0.74014778325123154], 5: [0.13177339901477833, 0.17118226600985223, 0.11699507389162561, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14408866995073891, 0.11822660098522167, 0.15886699507389163, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.73029556650246308, 0.72660098522167482, 0.75369458128078815], 5: [0.13177339901477833, 0.15147783251231528, 0.1145320197044335, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14408866995073891, 0.11206896551724138, 0.16009852216748768, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.73275862068965514, 0.70812807881773399, 0.74630541871921185], 5: [0.13177339901477833, 0.15517241379310345, 0.13177339901477833, 0.13669950738916256], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14408866995073891, 0.092364532019704432, 0.15270935960591134, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.75369458128078815, 0.72906403940886699, 0.73029556650246308], 5: [0.13177339901477833, 0.1539408866995074, 0.11822660098522167, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141090 minutes
Weight histogram
[  71  200  334  638  798  899  842  956 1070  267] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
[ 113  139  194  236  331  441  598  851 1265 1907] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
-0.565678
0.623485
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.43521
Epoch 1, cost is  5.27113
Epoch 2, cost is  5.10669
Epoch 3, cost is  4.9459
Epoch 4, cost is  4.79278
Training took 0.082118 minutes
Weight histogram
[ 758  810  605  626  543  587 1655  289  131   71] [ -2.87336856e-02  -2.58591234e-02  -2.29845611e-02  -2.01099989e-02
  -1.72354366e-02  -1.43608744e-02  -1.14863122e-02  -8.61174991e-03
  -5.73718767e-03  -2.86262542e-03   1.19368206e-05]
[1830  615  570  537  463  451  416  405  395  393] [ -2.87336856e-02  -2.58591234e-02  -2.29845611e-02  -2.01099989e-02
  -1.72354366e-02  -1.43608744e-02  -1.14863122e-02  -8.61174991e-03
  -5.73718767e-03  -2.86262542e-03   1.19368206e-05]
-0.385056
0.605246
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140114 minutes
Weight histogram
[  30  115  323  765 1199 1583 1469 1581  919  116] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
[ 238  278  383  489  668  928  891  881 1289 2055] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
-0.782244
0.478008
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.72957
Epoch 1, cost is  5.58126
Epoch 2, cost is  5.4312
Epoch 3, cost is  5.28355
Epoch 4, cost is  5.14107
Training took 0.081564 minutes
Weight histogram
[ 676  696  617  669 1135 3154  590  290  168  105] [ -2.27993988e-02  -2.05180679e-02  -1.82367370e-02  -1.59554061e-02
  -1.36740752e-02  -1.13927444e-02  -9.11141349e-03  -6.83008261e-03
  -4.54875173e-03  -2.26742086e-03   1.39100202e-05]
[3891 1040  522  433  408  387  369  355  351  344] [ -2.27993988e-02  -2.05180679e-02  -1.82367370e-02  -1.59554061e-02
  -1.36740752e-02  -1.13927444e-02  -9.11141349e-03  -6.83008261e-03
  -4.54875173e-03  -2.26742086e-03   1.39100202e-05]
-0.482062
0.617323
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.76837
Epoch 1, cost is  6.59284
Epoch 2, cost is  6.45962
Epoch 3, cost is  6.33918
Epoch 4, cost is  6.22382
Training took 0.070202 minutes
Weight histogram
[ 752 1201 1802  827  504  338  242  174  133  102] [ -2.25583259e-02  -2.02943525e-02  -1.80303791e-02  -1.57664057e-02
  -1.35024323e-02  -1.12384589e-02  -8.97448548e-03  -6.71051208e-03
  -4.44653868e-03  -2.18256527e-03   8.14081286e-05]
[2285 1056  694  374  340  319  303  284  280  140] [ -2.25583259e-02  -2.02943525e-02  -1.80303791e-02  -1.57664057e-02
  -1.35024323e-02  -1.12384589e-02  -8.97448548e-03  -6.71051208e-03
  -4.44653868e-03  -2.18256527e-03   8.14081286e-05]
-0.0744421
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040507 minutes
Epoch 0
Fine tuning took 0.044052 minutes
Epoch 0
Fine tuning took 0.041875 minutes
{'zero': {0: [0.12931034482758622, 0.10960591133004927, 0.21182266009852216, 0.26231527093596058], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66256157635467983, 0.51354679802955661, 0.58004926108374388, 0.50985221674876846], 5: [0.20812807881773399, 0.37684729064039407, 0.20812807881773399, 0.22783251231527094], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12931034482758622, 0.10837438423645321, 0.21551724137931033, 0.26724137931034481], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66256157635467983, 0.51477832512315269, 0.53201970443349755, 0.48891625615763545], 5: [0.20812807881773399, 0.37684729064039407, 0.25246305418719212, 0.24384236453201971], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12931034482758622, 0.11945812807881774, 0.19458128078817735, 0.26600985221674878], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66256157635467983, 0.51231527093596063, 0.60098522167487689, 0.51477832512315269], 5: [0.20812807881773399, 0.3682266009852217, 0.20443349753694581, 0.21921182266009853], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12931034482758622, 0.13669950738916256, 0.21674876847290642, 0.29433497536945813], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66256157635467983, 0.48399014778325122, 0.57635467980295563, 0.47906403940886699], 5: [0.20812807881773399, 0.37931034482758619, 0.20689655172413793, 0.22660098522167488], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142745 minutes
Weight histogram
[  71  200  334  638  798  899  842  956 1070  267] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
[ 113  139  194  236  331  441  598  851 1265 1907] [ -1.07421329e-04   2.52385660e-05   1.57898461e-04   2.90558355e-04
   4.23218250e-04   5.55878145e-04   6.88538040e-04   8.21197934e-04
   9.53857829e-04   1.08651772e-03   1.21917762e-03]
-0.565678
0.623485
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.43521
Epoch 1, cost is  5.27113
Epoch 2, cost is  5.10669
Epoch 3, cost is  4.9459
Epoch 4, cost is  4.79278
Training took 0.080418 minutes
Weight histogram
[ 758  810  605  626  543  587 1655  289  131   71] [ -2.87336856e-02  -2.58591234e-02  -2.29845611e-02  -2.01099989e-02
  -1.72354366e-02  -1.43608744e-02  -1.14863122e-02  -8.61174991e-03
  -5.73718767e-03  -2.86262542e-03   1.19368206e-05]
[1830  615  570  537  463  451  416  405  395  393] [ -2.87336856e-02  -2.58591234e-02  -2.29845611e-02  -2.01099989e-02
  -1.72354366e-02  -1.43608744e-02  -1.14863122e-02  -8.61174991e-03
  -5.73718767e-03  -2.86262542e-03   1.19368206e-05]
-0.385056
0.605246
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141198 minutes
Weight histogram
[  30  115  323  765 1199 1583 1469 1581  919  116] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
[ 238  278  383  489  668  928  891  881 1289 2055] [ -2.14635569e-04  -1.01746601e-04   1.11423666e-05   1.24031334e-04
   2.36920302e-04   3.49809270e-04   4.62698238e-04   5.75587206e-04
   6.88476174e-04   8.01365142e-04   9.14254109e-04]
-0.782244
0.478008
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  5.72957
Epoch 1, cost is  5.58126
Epoch 2, cost is  5.4312
Epoch 3, cost is  5.28355
Epoch 4, cost is  5.14107
Training took 0.080261 minutes
Weight histogram
[ 676  696  617  669 1135 3154  590  290  168  105] [ -2.27993988e-02  -2.05180679e-02  -1.82367370e-02  -1.59554061e-02
  -1.36740752e-02  -1.13927444e-02  -9.11141349e-03  -6.83008261e-03
  -4.54875173e-03  -2.26742086e-03   1.39100202e-05]
[3891 1040  522  433  408  387  369  355  351  344] [ -2.27993988e-02  -2.05180679e-02  -1.82367370e-02  -1.59554061e-02
  -1.36740752e-02  -1.13927444e-02  -9.11141349e-03  -6.83008261e-03
  -4.54875173e-03  -2.26742086e-03   1.39100202e-05]
-0.482062
0.617323
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.66093
Epoch 1, cost is  6.40834
Epoch 2, cost is  6.24866
Epoch 3, cost is  6.10995
Epoch 4, cost is  5.98135
Training took 0.078983 minutes
Weight histogram
[ 696 1296 1615  878  540  361  259  185  138  107] [ -2.31174268e-02  -2.08074062e-02  -1.84973856e-02  -1.61873650e-02
  -1.38773444e-02  -1.15673239e-02  -9.25730327e-03  -6.94728268e-03
  -4.63726209e-03  -2.32724151e-03  -1.72209202e-05]
[2184 1058  717  376  344  328  320  297  299  152] [ -2.31174268e-02  -2.08074062e-02  -1.84973856e-02  -1.61873650e-02
  -1.38773444e-02  -1.15673239e-02  -9.25730327e-03  -6.94728268e-03
  -4.63726209e-03  -2.32724151e-03  -1.72209202e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044866 minutes
Epoch 0
Fine tuning took 0.044123 minutes
Epoch 0
Fine tuning took 0.043317 minutes
{'zero': {0: [0.14162561576354679, 0.1145320197044335, 0.20935960591133004, 0.23275862068965517], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66379310344827591, 0.51354679802955661, 0.56896551724137934, 0.52832512315270941], 5: [0.19458128078817735, 0.37192118226600984, 0.22167487684729065, 0.23891625615763548], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14162561576354679, 0.13669950738916256, 0.21305418719211822, 0.21674876847290642], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66379310344827591, 0.48399014778325122, 0.54556650246305416, 0.52709359605911332], 5: [0.19458128078817735, 0.37931034482758619, 0.2413793103448276, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14162561576354679, 0.10837438423645321, 0.22044334975369459, 0.20935960591133004], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66379310344827591, 0.48645320197044334, 0.59729064039408863, 0.54556650246305416], 5: [0.19458128078817735, 0.40517241379310343, 0.18226600985221675, 0.24507389162561577], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14162561576354679, 0.11822660098522167, 0.22536945812807882, 0.24384236453201971], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.66379310344827591, 0.48645320197044334, 0.58004926108374388, 0.49384236453201968], 5: [0.19458128078817735, 0.39532019704433496, 0.19458128078817735, 0.26231527093596058], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142749 minutes
Weight histogram
[  84  237  488  771 1004  920 1026 1543 1645  382] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
[ 120  156  215  281  397  526  762 1095 1772 2776] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
-0.579854
0.69925
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.08837
Epoch 1, cost is  1.0455
Epoch 2, cost is  1.02082
Epoch 3, cost is  1.00427
Epoch 4, cost is  0.987257
Training took 0.080013 minutes
Weight histogram
[2080 1956 1759  708  843  405  169   80   56   44] [-0.202629   -0.18266194 -0.16269489 -0.14272783 -0.12276078 -0.10279372
 -0.08282667 -0.06285961 -0.04289255 -0.0229255  -0.00295844]
[ 102  115  183  273  399  614  972 1245 1794 2403] [-0.202629   -0.18266194 -0.16269489 -0.14272783 -0.12276078 -0.10279372
 -0.08282667 -0.06285961 -0.04289255 -0.0229255  -0.00295844]
-4.32018
5.51809
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141604 minutes
Weight histogram
[  32  127  407  866 1375 1608 1518 2134 1727  331] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
[ 255  310  417  567  784 1084  834 1186 1729 2959] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
-0.865869
0.523501
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.08658
Epoch 1, cost is  1.04436
Epoch 2, cost is  1.02009
Epoch 3, cost is  0.999798
Epoch 4, cost is  0.984234
Training took 0.080047 minutes
Weight histogram
[2127 1912 1576 1398 1623  771  335  170  118   95] [-0.20026758 -0.18051695 -0.16076631 -0.14101568 -0.12126504 -0.10151441
 -0.08176378 -0.06201314 -0.04226251 -0.02251187 -0.00276124]
[ 210  238  363  544  815 1284 1186 1254 1832 2399] [-0.20026758 -0.18051695 -0.16076631 -0.14101568 -0.12126504 -0.10151441
 -0.08176378 -0.06201314 -0.04226251 -0.02251187 -0.00276124]
-4.60198
5.66667
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.58509
Epoch 1, cost is  2.84948
Epoch 2, cost is  2.37721
Epoch 3, cost is  2.1504
Epoch 4, cost is  2.00107
Training took 0.070667 minutes
Weight histogram
[1803 1654 1361  862  775  490  333  329  252  241] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
[ 650  349  355  459  575  704  906 1112 1383 1607] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
-4.03518
4.87094
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041514 minutes
Epoch 0
Fine tuning took 0.044306 minutes
Epoch 0
Fine tuning took 0.042092 minutes
{'zero': {0: [0.16748768472906403, 0.20443349753694581, 0.16379310344827586, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.60344827586206895, 0.67610837438423643, 0.63054187192118227], 5: [0.14039408866995073, 0.19211822660098521, 0.16009852216748768, 0.20812807881773399], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.16748768472906403, 0.10591133004926108, 0.097290640394088676, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.76847290640394084, 0.80295566502463056, 0.80665024630541871], 5: [0.14039408866995073, 0.12561576354679804, 0.099753694581280791, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.16748768472906403, 0.14285714285714285, 0.10344827586206896, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.74630541871921185, 0.79187192118226601, 0.75492610837438423], 5: [0.14039408866995073, 0.11083743842364532, 0.10467980295566502, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.16748768472906403, 0.11945812807881774, 0.094827586206896547, 0.061576354679802957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69211822660098521, 0.74753694581280783, 0.82389162561576357, 0.85344827586206895], 5: [0.14039408866995073, 0.13300492610837439, 0.081280788177339899, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140759 minutes
Weight histogram
[  84  237  488  771 1004  920 1026 1543 1645  382] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
[ 120  156  215  281  397  526  762 1095 1772 2776] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
-0.579854
0.69925
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.08837
Epoch 1, cost is  1.0455
Epoch 2, cost is  1.02082
Epoch 3, cost is  1.00427
Epoch 4, cost is  0.987257
Training took 0.079982 minutes
Weight histogram
[2080 1956 1759  708  843  405  169   80   56   44] [-0.202629   -0.18266194 -0.16269489 -0.14272783 -0.12276078 -0.10279372
 -0.08282667 -0.06285961 -0.04289255 -0.0229255  -0.00295844]
[ 102  115  183  273  399  614  972 1245 1794 2403] [-0.202629   -0.18266194 -0.16269489 -0.14272783 -0.12276078 -0.10279372
 -0.08282667 -0.06285961 -0.04289255 -0.0229255  -0.00295844]
-4.32018
5.51809
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142377 minutes
Weight histogram
[  32  127  407  866 1375 1608 1518 2134 1727  331] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
[ 255  310  417  567  784 1084  834 1186 1729 2959] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
-0.865869
0.523501
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.08658
Epoch 1, cost is  1.04436
Epoch 2, cost is  1.02009
Epoch 3, cost is  0.999798
Epoch 4, cost is  0.984234
Training took 0.082984 minutes
Weight histogram
[2127 1912 1576 1398 1623  771  335  170  118   95] [-0.20026758 -0.18051695 -0.16076631 -0.14101568 -0.12126504 -0.10151441
 -0.08176378 -0.06201314 -0.04226251 -0.02251187 -0.00276124]
[ 210  238  363  544  815 1284 1186 1254 1832 2399] [-0.20026758 -0.18051695 -0.16076631 -0.14101568 -0.12126504 -0.10151441
 -0.08176378 -0.06201314 -0.04226251 -0.02251187 -0.00276124]
-4.60198
5.66667
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.49248
Epoch 1, cost is  2.55716
Epoch 2, cost is  1.98958
Epoch 3, cost is  1.69431
Epoch 4, cost is  1.4994
Training took 0.078756 minutes
Weight histogram
[1755 1727 1175  854  786  570  412  461  261   99] [-0.13890298 -0.12531535 -0.11172772 -0.09814009 -0.08455246 -0.07096483
 -0.05737721 -0.04378958 -0.03020195 -0.01661432 -0.00302669]
[ 705  346  368  484  611  741  929 1108 1305 1503] [-0.13890298 -0.12531535 -0.11172772 -0.09814009 -0.08455246 -0.07096483
 -0.05737721 -0.04378958 -0.03020195 -0.01661432 -0.00302669]
-3.74219
4.4139
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042700 minutes
Epoch 0
Fine tuning took 0.044056 minutes
Epoch 0
Fine tuning took 0.043947 minutes
{'zero': {0: [0.14532019704433496, 0.2229064039408867, 0.17241379310344829, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.57019704433497542, 0.63793103448275867, 0.59975369458128081], 5: [0.10837438423645321, 0.20689655172413793, 0.18965517241379309, 0.25862068965517243], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.14532019704433496, 0.1354679802955665, 0.14901477832512317, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.74384236453201968, 0.76970443349753692, 0.7857142857142857], 5: [0.10837438423645321, 0.1206896551724138, 0.081280788177339899, 0.093596059113300489], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.14532019704433496, 0.13916256157635468, 0.10344827586206896, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.74014778325123154, 0.78201970443349755, 0.78448275862068961], 5: [0.10837438423645321, 0.1206896551724138, 0.1145320197044335, 0.12438423645320197], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.14532019704433496, 0.12192118226600986, 0.14901477832512317, 0.098522167487684734], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74630541871921185, 0.76354679802955661, 0.75492610837438423, 0.78694581280788178], 5: [0.10837438423645321, 0.1145320197044335, 0.096059113300492605, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140322 minutes
Weight histogram
[  84  237  488  771 1004  920 1026 1543 1645  382] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
[ 120  156  215  281  397  526  762 1095 1772 2776] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
-0.579854
0.69925
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.74946
Epoch 1, cost is  1.69133
Epoch 2, cost is  1.64482
Epoch 3, cost is  1.60004
Epoch 4, cost is  1.56394
Training took 0.081257 minutes
Weight histogram
[1988 1868 1140  898  544  521  380  324  263  174] [-0.10670821 -0.0960632  -0.08541819 -0.07477318 -0.06412817 -0.05348316
 -0.04283814 -0.03219313 -0.02154812 -0.01090311 -0.0002581 ]
[ 514  319  395  507  665  792  956 1123 1326 1503] [-0.10670821 -0.0960632  -0.08541819 -0.07477318 -0.06412817 -0.05348316
 -0.04283814 -0.03219313 -0.02154812 -0.01090311 -0.0002581 ]
-1.74138
2.23781
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141499 minutes
Weight histogram
[  32  127  407  866 1375 1608 1518 2134 1727  331] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
[ 255  310  417  567  784 1084  834 1186 1729 2959] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
-0.865869
0.523501
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.7238
Epoch 1, cost is  1.67241
Epoch 2, cost is  1.62848
Epoch 3, cost is  1.58857
Epoch 4, cost is  1.55315
Training took 0.083628 minutes
Weight histogram
[1962 1599 1035 1076  987 1066  731  666  773  230] [-0.10446468 -0.09404205 -0.08361942 -0.07319679 -0.06277416 -0.05235153
 -0.0419289  -0.03150627 -0.02108364 -0.01066101 -0.00023838]
[1110  662  834 1078  835  758  917 1080 1315 1536] [-0.10446468 -0.09404205 -0.08361942 -0.07319679 -0.06277416 -0.05235153
 -0.0419289  -0.03150627 -0.02108364 -0.01066101 -0.00023838]
-1.80743
2.4009
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.52251
Epoch 1, cost is  6.02306
Epoch 2, cost is  5.60802
Epoch 3, cost is  5.00459
Epoch 4, cost is  4.46824
Training took 0.071807 minutes
Weight histogram
[ 462  795  813  926 1027  877 1169 1802  169   60] [-0.05580953 -0.05024695 -0.04468437 -0.03912178 -0.0335592  -0.02799662
 -0.02243404 -0.01687146 -0.01130888 -0.0057463  -0.00018372]
[1643 1156 1324  838  539  500  502  531  563  504] [-0.05580953 -0.05024695 -0.04468437 -0.03912178 -0.0335592  -0.02799662
 -0.02243404 -0.01687146 -0.01130888 -0.0057463  -0.00018372]
-0.715445
0.874477
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043181 minutes
Epoch 0
Fine tuning took 0.041268 minutes
Epoch 0
Fine tuning took 0.041921 minutes
{'zero': {0: [0.21305418719211822, 0.10467980295566502, 0.10960591133004927, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.76354679802955661, 0.71182266009852213, 0.73152709359605916], 5: [0.15517241379310345, 0.13177339901477833, 0.17857142857142858, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21305418719211822, 0.077586206896551727, 0.097290640394088676, 0.094827586206896547], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.7857142857142857, 0.75369458128078815, 0.77093596059113301], 5: [0.15517241379310345, 0.13669950738916256, 0.14901477832512317, 0.13423645320197045], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21305418719211822, 0.096059113300492605, 0.11576354679802955, 0.13300492610837439], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.77463054187192115, 0.72044334975369462, 0.73768472906403937], 5: [0.15517241379310345, 0.12931034482758622, 0.16379310344827586, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21305418719211822, 0.081280788177339899, 0.093596059113300489, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63177339901477836, 0.76724137931034486, 0.74137931034482762, 0.76724137931034486], 5: [0.15517241379310345, 0.15147783251231528, 0.16502463054187191, 0.13054187192118227], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.143489 minutes
Weight histogram
[  84  237  488  771 1004  920 1026 1543 1645  382] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
[ 120  156  215  281  397  526  762 1095 1772 2776] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
-0.579854
0.69925
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.74946
Epoch 1, cost is  1.69133
Epoch 2, cost is  1.64482
Epoch 3, cost is  1.60004
Epoch 4, cost is  1.56394
Training took 0.083155 minutes
Weight histogram
[1988 1868 1140  898  544  521  380  324  263  174] [-0.10670821 -0.0960632  -0.08541819 -0.07477318 -0.06412817 -0.05348316
 -0.04283814 -0.03219313 -0.02154812 -0.01090311 -0.0002581 ]
[ 514  319  395  507  665  792  956 1123 1326 1503] [-0.10670821 -0.0960632  -0.08541819 -0.07477318 -0.06412817 -0.05348316
 -0.04283814 -0.03219313 -0.02154812 -0.01090311 -0.0002581 ]
-1.74138
2.23781
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141649 minutes
Weight histogram
[  32  127  407  866 1375 1608 1518 2134 1727  331] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
[ 255  310  417  567  784 1084  834 1186 1729 2959] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
-0.865869
0.523501
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.7238
Epoch 1, cost is  1.67241
Epoch 2, cost is  1.62848
Epoch 3, cost is  1.58857
Epoch 4, cost is  1.55315
Training took 0.079846 minutes
Weight histogram
[1962 1599 1035 1076  987 1066  731  666  773  230] [-0.10446468 -0.09404205 -0.08361942 -0.07319679 -0.06277416 -0.05235153
 -0.0419289  -0.03150627 -0.02108364 -0.01066101 -0.00023838]
[1110  662  834 1078  835  758  917 1080 1315 1536] [-0.10446468 -0.09404205 -0.08361942 -0.07319679 -0.06277416 -0.05235153
 -0.0419289  -0.03150627 -0.02108364 -0.01066101 -0.00023838]
-1.80743
2.4009
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.41207
Epoch 1, cost is  5.91118
Epoch 2, cost is  5.54459
Epoch 3, cost is  4.9829
Epoch 4, cost is  4.3632
Training took 0.077860 minutes
Weight histogram
[ 503 1031 1349 1517 1028 1196 1159  199   75   43] [-0.04141794 -0.03730252 -0.03318709 -0.02907167 -0.02495624 -0.02084082
 -0.01672539 -0.01260997 -0.00849454 -0.00437912 -0.00026369]
[1465 1341 1653  776  472  461  476  489  492  475] [-0.04141794 -0.03730252 -0.03318709 -0.02907167 -0.02495624 -0.02084082
 -0.01672539 -0.01260997 -0.00849454 -0.00437912 -0.00026369]
-0.68208
0.684132
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041177 minutes
Epoch 0
Fine tuning took 0.043829 minutes
Epoch 0
Fine tuning took 0.041663 minutes
{'zero': {0: [0.22660098522167488, 0.1145320197044335, 0.11822660098522167, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.72167487684729059, 0.71798029556650245, 0.70197044334975367], 5: [0.13423645320197045, 0.16379310344827586, 0.16379310344827586, 0.16379310344827586], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22660098522167488, 0.10837438423645321, 0.11945812807881774, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.71305418719211822, 0.74507389162561577, 0.72906403940886699], 5: [0.13423645320197045, 0.17857142857142858, 0.1354679802955665, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22660098522167488, 0.093596059113300489, 0.12807881773399016, 0.14285714285714285], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.76108374384236455, 0.69827586206896552, 0.71059113300492616], 5: [0.13423645320197045, 0.14532019704433496, 0.17364532019704434, 0.14655172413793102], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22660098522167488, 0.093596059113300489, 0.13177339901477833, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63916256157635465, 0.74014778325123154, 0.72290640394088668, 0.69704433497536944], 5: [0.13423645320197045, 0.16625615763546797, 0.14532019704433496, 0.1539408866995074], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140680 minutes
Weight histogram
[  84  237  488  771 1004  920 1026 1543 1645  382] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
[ 120  156  215  281  397  526  762 1095 1772 2776] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
-0.579854
0.69925
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.65792
Epoch 1, cost is  4.53102
Epoch 2, cost is  4.41848
Epoch 3, cost is  4.30857
Epoch 4, cost is  4.21171
Training took 0.080321 minutes
Weight histogram
[ 994  898  970  954  792  695  793 1676  229   99] [ -3.66822518e-02  -3.30128330e-02  -2.93434141e-02  -2.56739952e-02
  -2.20045764e-02  -1.83351575e-02  -1.46657386e-02  -1.09963198e-02
  -7.32690091e-03  -3.65748205e-03   1.19368206e-05]
[2130  841  766  660  616  572  587  611  647  670] [ -3.66822518e-02  -3.30128330e-02  -2.93434141e-02  -2.56739952e-02
  -2.20045764e-02  -1.83351575e-02  -1.46657386e-02  -1.09963198e-02
  -7.32690091e-03  -3.65748205e-03   1.19368206e-05]
-0.503843
0.739498
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142816 minutes
Weight histogram
[  32  127  407  866 1375 1608 1518 2134 1727  331] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
[ 255  310  417  567  784 1084  834 1186 1729 2959] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
-0.865869
0.523501
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.98616
Epoch 1, cost is  4.84856
Epoch 2, cost is  4.71956
Epoch 3, cost is  4.59874
Epoch 4, cost is  4.48559
Training took 0.083322 minutes
Weight histogram
[ 840  838  881  900  797 1091 3658  681  288  151] [ -3.01196147e-02  -2.71062622e-02  -2.40929097e-02  -2.10795573e-02
  -1.80662048e-02  -1.50528523e-02  -1.20394999e-02  -9.02614739e-03
  -6.01279492e-03  -2.99944245e-03   1.39100202e-05]
[4663  868  663  608  567  552  547  543  545  569] [ -3.01196147e-02  -2.71062622e-02  -2.40929097e-02  -2.10795573e-02
  -1.80662048e-02  -1.50528523e-02  -1.20394999e-02  -9.02614739e-03
  -6.01279492e-03  -2.99944245e-03   1.39100202e-05]
-0.601055
0.786185
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.78714
Epoch 1, cost is  6.63575
Epoch 2, cost is  6.52234
Epoch 3, cost is  6.42018
Epoch 4, cost is  6.32535
Training took 0.070987 minutes
Weight histogram
[ 752 1201 2672 1323  752  488  343  245  183  141] [ -2.25583259e-02  -2.02941450e-02  -1.80299640e-02  -1.57657831e-02
  -1.35016022e-02  -1.12374213e-02  -8.97324033e-03  -6.70905941e-03
  -4.44487848e-03  -2.18069755e-03   8.34833772e-05]
[2924 1362  946  594  541  508  485  320  280  140] [ -2.25583259e-02  -2.02941450e-02  -1.80299640e-02  -1.57657831e-02
  -1.35016022e-02  -1.12374213e-02  -8.97324033e-03  -6.70905941e-03
  -4.44487848e-03  -2.18069755e-03   8.34833772e-05]
-0.0778338
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044189 minutes
Epoch 0
Fine tuning took 0.041870 minutes
Epoch 0
Fine tuning took 0.042085 minutes
{'zero': {0: [0.25738916256157635, 0.2376847290640394, 0.21428571428571427, 0.29679802955665024], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.55049261083743839, 0.6428571428571429, 0.47044334975369456], 5: [0.25985221674876846, 0.21182266009852216, 0.14285714285714285, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25738916256157635, 0.20689655172413793, 0.23522167487684728, 0.29556650246305421], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.57389162561576357, 0.62931034482758619, 0.43719211822660098], 5: [0.25985221674876846, 0.21921182266009853, 0.1354679802955665, 0.26724137931034481], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25738916256157635, 0.20935960591133004, 0.23399014778325122, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.56403940886699511, 0.59359605911330049, 0.44211822660098521], 5: [0.25985221674876846, 0.22660098522167488, 0.17241379310344829, 0.25123152709359609], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25738916256157635, 0.19334975369458129, 0.23029556650246305, 0.31034482758620691], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48275862068965519, 0.60344827586206895, 0.62931034482758619, 0.43226600985221675], 5: [0.25985221674876846, 0.20320197044334976, 0.14039408866995073, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141533 minutes
Weight histogram
[  84  237  488  771 1004  920 1026 1543 1645  382] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
[ 120  156  215  281  397  526  762 1095 1772 2776] [ -1.07421329e-04   4.05291481e-05   1.88479625e-04   3.36430102e-04
   4.84380579e-04   6.32331055e-04   7.80281532e-04   9.28232009e-04
   1.07618249e-03   1.22413296e-03   1.37208344e-03]
-0.579854
0.69925
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.65792
Epoch 1, cost is  4.53102
Epoch 2, cost is  4.41848
Epoch 3, cost is  4.30857
Epoch 4, cost is  4.21171
Training took 0.082556 minutes
Weight histogram
[ 994  898  970  954  792  695  793 1676  229   99] [ -3.66822518e-02  -3.30128330e-02  -2.93434141e-02  -2.56739952e-02
  -2.20045764e-02  -1.83351575e-02  -1.46657386e-02  -1.09963198e-02
  -7.32690091e-03  -3.65748205e-03   1.19368206e-05]
[2130  841  766  660  616  572  587  611  647  670] [ -3.66822518e-02  -3.30128330e-02  -2.93434141e-02  -2.56739952e-02
  -2.20045764e-02  -1.83351575e-02  -1.46657386e-02  -1.09963198e-02
  -7.32690091e-03  -3.65748205e-03   1.19368206e-05]
-0.503843
0.739498
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141441 minutes
Weight histogram
[  32  127  407  866 1375 1608 1518 2134 1727  331] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
[ 255  310  417  567  784 1084  834 1186 1729 2959] [ -2.14635569e-04  -9.68006556e-05   2.10342580e-05   1.38869171e-04
   2.56704085e-04   3.74538999e-04   4.92373912e-04   6.10208826e-04
   7.28043739e-04   8.45878653e-04   9.63713566e-04]
-0.865869
0.523501
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.98616
Epoch 1, cost is  4.84856
Epoch 2, cost is  4.71956
Epoch 3, cost is  4.59874
Epoch 4, cost is  4.48559
Training took 0.082152 minutes
Weight histogram
[ 840  838  881  900  797 1091 3658  681  288  151] [ -3.01196147e-02  -2.71062622e-02  -2.40929097e-02  -2.10795573e-02
  -1.80662048e-02  -1.50528523e-02  -1.20394999e-02  -9.02614739e-03
  -6.01279492e-03  -2.99944245e-03   1.39100202e-05]
[4663  868  663  608  567  552  547  543  545  569] [ -3.01196147e-02  -2.71062622e-02  -2.40929097e-02  -2.10795573e-02
  -1.80662048e-02  -1.50528523e-02  -1.20394999e-02  -9.02614739e-03
  -6.01279492e-03  -2.99944245e-03   1.39100202e-05]
-0.601055
0.786185
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.69329
Epoch 1, cost is  6.47747
Epoch 2, cost is  6.34464
Epoch 3, cost is  6.23206
Epoch 4, cost is  6.12652
Training took 0.077340 minutes
Weight histogram
[ 696 1297 2406 1403  809  522  367  260  193  147] [ -2.31174268e-02  -2.08071785e-02  -1.84969302e-02  -1.61866819e-02
  -1.38764336e-02  -1.15661853e-02  -9.25593698e-03  -6.94568868e-03
  -4.63544038e-03  -2.32519208e-03  -1.49437756e-05]
[2788 1361  969  596  558  519  513  345  299  152] [ -2.31174268e-02  -2.08071785e-02  -1.84969302e-02  -1.61866819e-02
  -1.38764336e-02  -1.15661853e-02  -9.25593698e-03  -6.94568868e-03
  -4.63544038e-03  -2.32519208e-03  -1.49437756e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042517 minutes
Epoch 0
Fine tuning took 0.044133 minutes
Epoch 0
Fine tuning took 0.041549 minutes
{'zero': {0: [0.22536945812807882, 0.18965517241379309, 0.2229064039408867, 0.30541871921182268], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.58004926108374388, 0.6576354679802956, 0.45443349753694579], 5: [0.26477832512315269, 0.23029556650246305, 0.11945812807881774, 0.24014778325123154], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22536945812807882, 0.17610837438423646, 0.19704433497536947, 0.3288177339901478], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.58497536945812811, 0.66379310344827591, 0.45935960591133007], 5: [0.26477832512315269, 0.23891625615763548, 0.13916256157635468, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22536945812807882, 0.17610837438423646, 0.19827586206896552, 0.30665024630541871], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.59359605911330049, 0.66871921182266014, 0.45812807881773399], 5: [0.26477832512315269, 0.23029556650246305, 0.13300492610837439, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22536945812807882, 0.18596059113300492, 0.19950738916256158, 0.32635467980295568], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.50985221674876846, 0.61576354679802958, 0.68596059113300489, 0.44088669950738918], 5: [0.26477832512315269, 0.19827586206896552, 0.1145320197044335, 0.23275862068965517], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140718 minutes
Weight histogram
[  99  271  576  858 1043  998 1212 2002 2343  723] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
[ 126  173  227  316  449  599  949 1370 2282 3634] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
-0.59863
0.75234
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.00322
Epoch 1, cost is  0.965248
Epoch 2, cost is  0.944234
Epoch 3, cost is  0.931935
Epoch 4, cost is  0.918277
Training took 0.081464 minutes
Weight histogram
[2834 2151 1808 1296 1052  511  261   99   68   45] [-0.21944265 -0.19779423 -0.17614581 -0.15449739 -0.13284897 -0.11120055
 -0.08955213 -0.06790371 -0.04625528 -0.02460686 -0.00295844]
[ 108  128  209  314  483  763 1141 1619 2272 3088] [-0.21944265 -0.19779423 -0.17614581 -0.15449739 -0.13284897 -0.11120055
 -0.08955213 -0.06790371 -0.04625528 -0.02460686 -0.00295844]
-4.76054
5.51809
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141122 minutes
Weight histogram
[  34  148  470  960 1601 1668 1723 2963 2315  268] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 265  335  455  609  884 1160  876 1308 2375 3883] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.548489
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.995223
Epoch 1, cost is  0.953202
Epoch 2, cost is  0.932185
Epoch 3, cost is  0.916831
Epoch 4, cost is  0.905152
Training took 0.084952 minutes
Weight histogram
[2736 2236 1615 1494 2060 1093  456  221  133  106] [-0.21634193 -0.19498386 -0.17362579 -0.15226772 -0.13090965 -0.10955158
 -0.08819351 -0.06683544 -0.04547738 -0.02411931 -0.00276124]
[ 224  264  420  631 1008 1461 1165 1702 2294 2981] [-0.21634193 -0.19498386 -0.17362579 -0.15226772 -0.13090965 -0.10955158
 -0.08819351 -0.06683544 -0.04547738 -0.02411931 -0.00276124]
-4.83245
5.75459
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.59417
Epoch 1, cost is  2.86327
Epoch 2, cost is  2.40795
Epoch 3, cost is  2.19275
Epoch 4, cost is  2.04692
Training took 0.071550 minutes
Weight histogram
[2287 2085 1642 1086  969  615  413  409  318  301] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
[ 814  439  449  577  722  885 1141 1398 1743 1957] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
-4.03518
5.17421
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043192 minutes
Epoch 0
Fine tuning took 0.043513 minutes
Epoch 0
Fine tuning took 0.044028 minutes
{'zero': {0: [0.22660098522167488, 0.12561576354679804, 0.14901477832512317, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.66256157635467983, 0.66133004926108374, 0.68226600985221675], 5: [0.13916256157635468, 0.21182266009852216, 0.18965517241379309, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22660098522167488, 0.12438423645320197, 0.15270935960591134, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.73645320197044339, 0.71674876847290636, 0.73522167487684731], 5: [0.13916256157635468, 0.13916256157635468, 0.13054187192118227, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22660098522167488, 0.15270935960591134, 0.097290640394088676, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.73768472906403937, 0.78940886699507384, 0.7931034482758621], 5: [0.13916256157635468, 0.10960591133004927, 0.11330049261083744, 0.10714285714285714], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22660098522167488, 0.12315270935960591, 0.15763546798029557, 0.16009852216748768], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.63423645320197042, 0.7068965517241379, 0.69950738916256161, 0.70073891625615758], 5: [0.13916256157635468, 0.16995073891625614, 0.14285714285714285, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142436 minutes
Weight histogram
[  99  271  576  858 1043  998 1212 2002 2343  723] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
[ 126  173  227  316  449  599  949 1370 2282 3634] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
-0.59863
0.75234
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.00322
Epoch 1, cost is  0.965248
Epoch 2, cost is  0.944234
Epoch 3, cost is  0.931935
Epoch 4, cost is  0.918277
Training took 0.080040 minutes
Weight histogram
[2834 2151 1808 1296 1052  511  261   99   68   45] [-0.21944265 -0.19779423 -0.17614581 -0.15449739 -0.13284897 -0.11120055
 -0.08955213 -0.06790371 -0.04625528 -0.02460686 -0.00295844]
[ 108  128  209  314  483  763 1141 1619 2272 3088] [-0.21944265 -0.19779423 -0.17614581 -0.15449739 -0.13284897 -0.11120055
 -0.08955213 -0.06790371 -0.04625528 -0.02460686 -0.00295844]
-4.76054
5.51809
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.139587 minutes
Weight histogram
[  34  148  470  960 1601 1668 1723 2963 2315  268] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 265  335  455  609  884 1160  876 1308 2375 3883] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.548489
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.995223
Epoch 1, cost is  0.953202
Epoch 2, cost is  0.932185
Epoch 3, cost is  0.916831
Epoch 4, cost is  0.905152
Training took 0.079638 minutes
Weight histogram
[2736 2236 1615 1494 2060 1093  456  221  133  106] [-0.21634193 -0.19498386 -0.17362579 -0.15226772 -0.13090965 -0.10955158
 -0.08819351 -0.06683544 -0.04547738 -0.02411931 -0.00276124]
[ 224  264  420  631 1008 1461 1165 1702 2294 2981] [-0.21634193 -0.19498386 -0.17362579 -0.15226772 -0.13090965 -0.10955158
 -0.08819351 -0.06683544 -0.04547738 -0.02411931 -0.00276124]
-4.83245
5.75459
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.50099
Epoch 1, cost is  2.56077
Epoch 2, cost is  1.99721
Epoch 3, cost is  1.69247
Epoch 4, cost is  1.49661
Training took 0.079056 minutes
Weight histogram
[2203 2064 1502 1096  976  722  532  578  327  125] [-0.13948034 -0.12583497 -0.11218961 -0.09854424 -0.08489888 -0.07125351
 -0.05760815 -0.04396279 -0.03031742 -0.01667206 -0.00302669]
[ 886  436  465  610  772  938 1169 1378 1636 1835] [-0.13948034 -0.12583497 -0.11218961 -0.09854424 -0.08489888 -0.07125351
 -0.05760815 -0.04396279 -0.03031742 -0.01667206 -0.00302669]
-3.74219
4.4139
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041397 minutes
Epoch 0
Fine tuning took 0.044928 minutes
Epoch 0
Fine tuning took 0.043261 minutes
{'zero': {0: [0.18103448275862069, 0.12931034482758622, 0.14532019704433496, 0.084975369458128072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.66995073891625612, 0.59729064039408863, 0.6576354679802956], 5: [0.093596059113300489, 0.20073891625615764, 0.25738916256157635, 0.25738916256157635], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18103448275862069, 0.15517241379310345, 0.12315270935960591, 0.12315270935960591], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.75862068965517238, 0.78940886699507384, 0.77709359605911332], 5: [0.093596059113300489, 0.086206896551724144, 0.087438423645320201, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18103448275862069, 0.14532019704433496, 0.12561576354679804, 0.11945812807881774], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.72660098522167482, 0.75615763546798032, 0.76600985221674878], 5: [0.093596059113300489, 0.12807881773399016, 0.11822660098522167, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18103448275862069, 0.14532019704433496, 0.10714285714285714, 0.14901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72536945812807885, 0.75369458128078815, 0.81034482758620685, 0.76600985221674878], 5: [0.093596059113300489, 0.10098522167487685, 0.082512315270935957, 0.084975369458128072], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141610 minutes
Weight histogram
[  99  271  576  858 1043  998 1212 2002 2343  723] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
[ 126  173  227  316  449  599  949 1370 2282 3634] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
-0.59863
0.75234
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.52644
Epoch 1, cost is  1.49027
Epoch 2, cost is  1.45415
Epoch 3, cost is  1.42909
Epoch 4, cost is  1.40342
Training took 0.079840 minutes
Weight histogram
[2411 1989 1664 1416  647  695  448  362  259  234] [-0.11701655 -0.1053407  -0.09366486 -0.08198901 -0.07031317 -0.05863732
 -0.04696148 -0.03528563 -0.02360979 -0.01193394 -0.0002581 ]
[ 550  366  475  618  819  985 1215 1426 1672 1999] [-0.11701655 -0.1053407  -0.09366486 -0.08198901 -0.07031317 -0.05863732
 -0.04696148 -0.03528563 -0.02360979 -0.01193394 -0.0002581 ]
-1.9649
2.46171
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142781 minutes
Weight histogram
[  34  148  470  960 1601 1668 1723 2963 2315  268] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 265  335  455  609  884 1160  876 1308 2375 3883] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.548489
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.52128
Epoch 1, cost is  1.48052
Epoch 2, cost is  1.45146
Epoch 3, cost is  1.424
Epoch 4, cost is  1.39868
Training took 0.081849 minutes
Weight histogram
[2411 1846 1723 1218  971 1230  900  736  630  485] [-0.11428571 -0.10288098 -0.09147625 -0.08007151 -0.06866678 -0.05726205
 -0.04585731 -0.03445258 -0.02304784 -0.01164311 -0.00023838]
[1180  755  996 1205  780  928 1162 1391 1677 2076] [-0.11428571 -0.10288098 -0.09147625 -0.08007151 -0.06866678 -0.05726205
 -0.04585731 -0.03445258 -0.02304784 -0.01164311 -0.00023838]
-2.07541
2.80701
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.52468
Epoch 1, cost is  6.02835
Epoch 2, cost is  5.61553
Epoch 3, cost is  5.01072
Epoch 4, cost is  4.4924
Training took 0.071956 minutes
Weight histogram
[ 542  979 1005 1146 1336 1113 1418 2295  215   76] [-0.05580953 -0.05024695 -0.04468437 -0.03912178 -0.0335592  -0.02799662
 -0.02243404 -0.01687146 -0.01130888 -0.0057463  -0.00018372]
[2012 1427 1686 1044  674  623  624  666  713  656] [-0.05580953 -0.05024695 -0.04468437 -0.03912178 -0.0335592  -0.02799662
 -0.02243404 -0.01687146 -0.01130888 -0.0057463  -0.00018372]
-0.744946
0.87504
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041374 minutes
Epoch 0
Fine tuning took 0.043432 minutes
Epoch 0
Fine tuning took 0.042076 minutes
{'zero': {0: [0.24876847290640394, 0.12315270935960591, 0.14655172413793102, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6219211822660099, 0.71921182266009853, 0.73768472906403937, 0.76354679802955661], 5: [0.12931034482758622, 0.15763546798029557, 0.11576354679802955, 0.15024630541871922], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24876847290640394, 0.083743842364532015, 0.14532019704433496, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6219211822660099, 0.74630541871921185, 0.75246305418719217, 0.77093596059113301], 5: [0.12931034482758622, 0.16995073891625614, 0.10221674876847291, 0.14778325123152711], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24876847290640394, 0.084975369458128072, 0.12807881773399016, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6219211822660099, 0.76600985221674878, 0.74753694581280783, 0.77339901477832518], 5: [0.12931034482758622, 0.14901477832512317, 0.12438423645320197, 0.12931034482758622], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24876847290640394, 0.088669950738916259, 0.12807881773399016, 0.089901477832512317], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6219211822660099, 0.7573891625615764, 0.73768472906403937, 0.77093596059113301], 5: [0.12931034482758622, 0.1539408866995074, 0.13423645320197045, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142317 minutes
Weight histogram
[  99  271  576  858 1043  998 1212 2002 2343  723] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
[ 126  173  227  316  449  599  949 1370 2282 3634] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
-0.59863
0.75234
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.52644
Epoch 1, cost is  1.49027
Epoch 2, cost is  1.45415
Epoch 3, cost is  1.42909
Epoch 4, cost is  1.40342
Training took 0.083224 minutes
Weight histogram
[2411 1989 1664 1416  647  695  448  362  259  234] [-0.11701655 -0.1053407  -0.09366486 -0.08198901 -0.07031317 -0.05863732
 -0.04696148 -0.03528563 -0.02360979 -0.01193394 -0.0002581 ]
[ 550  366  475  618  819  985 1215 1426 1672 1999] [-0.11701655 -0.1053407  -0.09366486 -0.08198901 -0.07031317 -0.05863732
 -0.04696148 -0.03528563 -0.02360979 -0.01193394 -0.0002581 ]
-1.9649
2.46171
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141846 minutes
Weight histogram
[  34  148  470  960 1601 1668 1723 2963 2315  268] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 265  335  455  609  884 1160  876 1308 2375 3883] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.548489
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.52128
Epoch 1, cost is  1.48052
Epoch 2, cost is  1.45146
Epoch 3, cost is  1.424
Epoch 4, cost is  1.39868
Training took 0.079839 minutes
Weight histogram
[2411 1846 1723 1218  971 1230  900  736  630  485] [-0.11428571 -0.10288098 -0.09147625 -0.08007151 -0.06866678 -0.05726205
 -0.04585731 -0.03445258 -0.02304784 -0.01164311 -0.00023838]
[1180  755  996 1205  780  928 1162 1391 1677 2076] [-0.11428571 -0.10288098 -0.09147625 -0.08007151 -0.06866678 -0.05726205
 -0.04585731 -0.03445258 -0.02304784 -0.01164311 -0.00023838]
-2.07541
2.80701
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.4146
Epoch 1, cost is  5.91724
Epoch 2, cost is  5.55558
Epoch 3, cost is  4.99622
Epoch 4, cost is  4.39193
Training took 0.080991 minutes
Weight histogram
[ 580 1242 1636 2008 1301 1471 1481  257   95   54] [-0.04141794 -0.03730252 -0.03318709 -0.02907167 -0.02495624 -0.02084082
 -0.01672539 -0.01260997 -0.00849454 -0.00437912 -0.00026369]
[1795 1638 2113  978  594  576  600  613  623  595] [-0.04141794 -0.03730252 -0.03318709 -0.02907167 -0.02495624 -0.02084082
 -0.01672539 -0.01260997 -0.00849454 -0.00437912 -0.00026369]
-0.747541
0.684132
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043743 minutes
Epoch 0
Fine tuning took 0.044929 minutes
Epoch 0
Fine tuning took 0.042262 minutes
{'zero': {0: [0.27339901477832512, 0.15640394088669951, 0.12931034482758622, 0.088669950738916259], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.72044334975369462, 0.73275862068965514, 0.77955665024630538], 5: [0.15147783251231528, 0.12315270935960591, 0.13793103448275862, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.27339901477832512, 0.11083743842364532, 0.14532019704433496, 0.086206896551724144], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.75369458128078815, 0.73768472906403937, 0.7573891625615764], 5: [0.15147783251231528, 0.1354679802955665, 0.11699507389162561, 0.15640394088669951], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.27339901477832512, 0.14901477832512317, 0.15024630541871922, 0.10837438423645321], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.72906403940886699, 0.72536945812807885, 0.76847290640394084], 5: [0.15147783251231528, 0.12192118226600986, 0.12438423645320197, 0.12315270935960591], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.27339901477832512, 0.13177339901477833, 0.14532019704433496, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57512315270935965, 0.74014778325123154, 0.74384236453201968, 0.76724137931034486], 5: [0.15147783251231528, 0.12807881773399016, 0.11083743842364532, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141342 minutes
Weight histogram
[  99  271  576  858 1043  998 1212 2002 2343  723] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
[ 126  173  227  316  449  599  949 1370 2282 3634] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
-0.59863
0.75234
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.10693
Epoch 1, cost is  4.01879
Epoch 2, cost is  3.93611
Epoch 3, cost is  3.86113
Epoch 4, cost is  3.78728
Training took 0.084296 minutes
Weight histogram
[1273 1113 1155 1097 1186  917  839 2014  400  131] [ -4.42447811e-02  -3.98191093e-02  -3.53934375e-02  -3.09677657e-02
  -2.65420940e-02  -2.21164222e-02  -1.76907504e-02  -1.32650786e-02
  -8.83940677e-03  -4.41373498e-03   1.19368206e-05]
[2356 1032  873  785  726  753  796  865  943  996] [ -4.42447811e-02  -3.98191093e-02  -3.53934375e-02  -3.09677657e-02
  -2.65420940e-02  -2.21164222e-02  -1.76907504e-02  -1.32650786e-02
  -8.83940677e-03  -4.41373498e-03   1.19368206e-05]
-0.597307
0.898025
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140265 minutes
Weight histogram
[  34  148  470  960 1601 1668 1723 2963 2315  268] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 265  335  455  609  884 1160  876 1308 2375 3883] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.548489
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.36824
Epoch 1, cost is  4.26603
Epoch 2, cost is  4.17056
Epoch 3, cost is  4.08224
Epoch 4, cost is  3.99968
Training took 0.083913 minutes
Weight histogram
[1151  909 1069 1013 1150 1009 1703 3460  483  203] [ -3.74371260e-02  -3.36920224e-02  -2.99469188e-02  -2.62018152e-02
  -2.24567116e-02  -1.87116080e-02  -1.49665044e-02  -1.12214008e-02
  -7.47629719e-03  -3.73119359e-03   1.39100202e-05]
[4981  974  818  748  728  714  723  776  822  866] [ -3.74371260e-02  -3.36920224e-02  -2.99469188e-02  -2.62018152e-02
  -2.24567116e-02  -1.87116080e-02  -1.49665044e-02  -1.12214008e-02
  -7.47629719e-03  -3.73119359e-03   1.39100202e-05]
-0.694973
0.901988
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.79791
Epoch 1, cost is  6.6592
Epoch 2, cost is  6.55651
Epoch 3, cost is  6.46459
Epoch 4, cost is  6.37931
Training took 0.070559 minutes
Weight histogram
[ 752 1202 3397 1913 1027  647  450  319  237  181] [ -2.25583259e-02  -2.02939437e-02  -1.80295616e-02  -1.57651794e-02
  -1.35007973e-02  -1.12364151e-02  -8.97203295e-03  -6.70765079e-03
  -4.44326864e-03  -2.17888648e-03   8.54956816e-05]
[3671 1719 1236  851  776  647  485  320  280  140] [ -2.25583259e-02  -2.02939437e-02  -1.80295616e-02  -1.57651794e-02
  -1.35007973e-02  -1.12364151e-02  -8.97203295e-03  -6.70765079e-03
  -4.44326864e-03  -2.17888648e-03   8.54956816e-05]
-0.0778338
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043289 minutes
Epoch 0
Fine tuning took 0.040806 minutes
Epoch 0
Fine tuning took 0.041689 minutes
{'zero': {0: [0.23029556650246305, 0.18472906403940886, 0.26108374384236455, 0.16502463054187191], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.52586206896551724, 0.62931034482758619, 0.53448275862068961], 5: [0.15517241379310345, 0.2894088669950739, 0.10960591133004927, 0.30049261083743845], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23029556650246305, 0.15270935960591134, 0.29187192118226601, 0.18103448275862069], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.50123152709359609, 0.57019704433497542, 0.51970443349753692], 5: [0.15517241379310345, 0.3460591133004926, 0.13793103448275862, 0.29926108374384236], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23029556650246305, 0.15517241379310345, 0.30172413793103448, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.52339901477832518, 0.56773399014778325, 0.52463054187192115], 5: [0.15517241379310345, 0.32142857142857145, 0.13054187192118227, 0.29679802955665024], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23029556650246305, 0.16379310344827586, 0.26970443349753692, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6145320197044335, 0.51477832512315269, 0.60837438423645318, 0.51354679802955661], 5: [0.15517241379310345, 0.32142857142857145, 0.12192118226600986, 0.31650246305418717], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140389 minutes
Weight histogram
[  99  271  576  858 1043  998 1212 2002 2343  723] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
[ 126  173  227  316  449  599  949 1370 2282 3634] [ -1.07421329e-04   4.88262660e-05   2.05073861e-04   3.61321455e-04
   5.17569050e-04   6.73816645e-04   8.30064240e-04   9.86311834e-04
   1.14255943e-03   1.29880702e-03   1.45505462e-03]
-0.59863
0.75234
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.10693
Epoch 1, cost is  4.01879
Epoch 2, cost is  3.93611
Epoch 3, cost is  3.86113
Epoch 4, cost is  3.78728
Training took 0.080299 minutes
Weight histogram
[1273 1113 1155 1097 1186  917  839 2014  400  131] [ -4.42447811e-02  -3.98191093e-02  -3.53934375e-02  -3.09677657e-02
  -2.65420940e-02  -2.21164222e-02  -1.76907504e-02  -1.32650786e-02
  -8.83940677e-03  -4.41373498e-03   1.19368206e-05]
[2356 1032  873  785  726  753  796  865  943  996] [ -4.42447811e-02  -3.98191093e-02  -3.53934375e-02  -3.09677657e-02
  -2.65420940e-02  -2.21164222e-02  -1.76907504e-02  -1.32650786e-02
  -8.83940677e-03  -4.41373498e-03   1.19368206e-05]
-0.597307
0.898025
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141702 minutes
Weight histogram
[  34  148  470  960 1601 1668 1723 2963 2315  268] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 265  335  455  609  884 1160  876 1308 2375 3883] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.548489
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  4.36824
Epoch 1, cost is  4.26603
Epoch 2, cost is  4.17056
Epoch 3, cost is  4.08224
Epoch 4, cost is  3.99968
Training took 0.081790 minutes
Weight histogram
[1151  909 1069 1013 1150 1009 1703 3460  483  203] [ -3.74371260e-02  -3.36920224e-02  -2.99469188e-02  -2.62018152e-02
  -2.24567116e-02  -1.87116080e-02  -1.49665044e-02  -1.12214008e-02
  -7.47629719e-03  -3.73119359e-03   1.39100202e-05]
[4981  974  818  748  728  714  723  776  822  866] [ -3.74371260e-02  -3.36920224e-02  -2.99469188e-02  -2.62018152e-02
  -2.24567116e-02  -1.87116080e-02  -1.49665044e-02  -1.12214008e-02
  -7.47629719e-03  -3.73119359e-03   1.39100202e-05]
-0.694973
0.901988
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.71137
Epoch 1, cost is  6.51531
Epoch 2, cost is  6.39725
Epoch 3, cost is  6.29731
Epoch 4, cost is  6.20416
Training took 0.078056 minutes
Weight histogram
[ 696 1299 3037 2030 1105  699  482  338  251  188] [ -2.31174268e-02  -2.08069315e-02  -1.84964363e-02  -1.61859410e-02
  -1.38754458e-02  -1.15649506e-02  -9.25445531e-03  -6.94396007e-03
  -4.63346482e-03  -2.32296958e-03  -1.24743328e-05]
[3501 1716 1264  860  798  677  513  345  299  152] [ -2.31174268e-02  -2.08069315e-02  -1.84964363e-02  -1.61859410e-02
  -1.38754458e-02  -1.15649506e-02  -9.25445531e-03  -6.94396007e-03
  -4.63346482e-03  -2.32296958e-03  -1.24743328e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044457 minutes
Epoch 0
Fine tuning took 0.043702 minutes
Epoch 0
Fine tuning took 0.043103 minutes
{'zero': {0: [0.21305418719211822, 0.18472906403940886, 0.28078817733990147, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.53940886699507384, 0.6280788177339901, 0.52216748768472909], 5: [0.17980295566502463, 0.27586206896551724, 0.091133004926108374, 0.27093596059113301], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21305418719211822, 0.16871921182266009, 0.28201970443349755, 0.18596059113300492], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.54187192118226601, 0.61083743842364535, 0.5357142857142857], 5: [0.17980295566502463, 0.2894088669950739, 0.10714285714285714, 0.27832512315270935], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21305418719211822, 0.17980295566502463, 0.2894088669950739, 0.18349753694581281], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.5357142857142857, 0.58620689655172409, 0.51724137931034486], 5: [0.17980295566502463, 0.28448275862068967, 0.12438423645320197, 0.29926108374384236], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21305418719211822, 0.16625615763546797, 0.26970443349753692, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6071428571428571, 0.52709359605911332, 0.6219211822660099, 0.52339901477832518], 5: [0.17980295566502463, 0.30665024630541871, 0.10837438423645321, 0.28325123152709358], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140858 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2391 3191  894] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 132  180  245  345  482  701 1055 1717 2678 4615] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.619555
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.929474
Epoch 1, cost is  0.892248
Epoch 2, cost is  0.872928
Epoch 3, cost is  0.863273
Epoch 4, cost is  0.847749
Training took 0.082477 minutes
Weight histogram
[3572 2462 2019 1816  982  730  323  122   78   46] [-0.23419961 -0.2110755  -0.18795138 -0.16482726 -0.14170315 -0.11857903
 -0.09545491 -0.07233079 -0.04920668 -0.02608256 -0.00295844]
[ 112  142  235  354  567  955 1289 1950 2790 3756] [-0.23419961 -0.2110755  -0.18795138 -0.16482726 -0.14170315 -0.11857903
 -0.09545491 -0.07233079 -0.04920668 -0.02608256 -0.00295844]
-5.46377
5.59467
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140065 minutes
Weight histogram
[  34  148  470  960 1601 1668 1731 3292 3708  563] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 276  358  472  674  967 1110  999 1709 2815 4795] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.555757
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.933666
Epoch 1, cost is  0.898863
Epoch 2, cost is  0.881279
Epoch 3, cost is  0.866491
Epoch 4, cost is  0.854238
Training took 0.079743 minutes
Weight histogram
[3762 2293 2033 1698 1872 1378  629  247  154  109] [-0.22909935 -0.20646554 -0.18383173 -0.16119792 -0.1385641  -0.11593029
 -0.09329648 -0.07066267 -0.04802886 -0.02539505 -0.00276124]
[ 235  287  473  716 1186 1458 1327 2016 2789 3688] [-0.22909935 -0.20646554 -0.18383173 -0.16119792 -0.1385641  -0.11593029
 -0.09329648 -0.07066267 -0.04802886 -0.02539505 -0.00276124]
-5.00253
5.75459
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.603
Epoch 1, cost is  2.89243
Epoch 2, cost is  2.42205
Epoch 3, cost is  2.19733
Epoch 4, cost is  2.04525
Training took 0.072458 minutes
Weight histogram
[2659 2519 2011 1326 1167  730  505  486  385  362] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
[ 978  532  542  700  875 1069 1377 1680 2091 2306] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
-4.03518
5.24361
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040599 minutes
Epoch 0
Fine tuning took 0.043568 minutes
Epoch 0
Fine tuning took 0.040743 minutes
{'zero': {0: [0.18965517241379309, 0.15763546798029557, 0.20443349753694581, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.60098522167487689, 0.6071428571428571, 0.60344827586206895], 5: [0.1625615763546798, 0.2413793103448276, 0.18842364532019704, 0.19827586206896552], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.18965517241379309, 0.15147783251231528, 0.15517241379310345, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.74137931034482762, 0.72290640394088668, 0.71921182266009853], 5: [0.1625615763546798, 0.10714285714285714, 0.12192118226600986, 0.11945812807881774], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.18965517241379309, 0.14532019704433496, 0.13054187192118227, 0.12561576354679804], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.74384236453201968, 0.76600985221674878, 0.74753694581280783], 5: [0.1625615763546798, 0.11083743842364532, 0.10344827586206896, 0.1268472906403941], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.18965517241379309, 0.14655172413793102, 0.17980295566502463, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64778325123152714, 0.70443349753694584, 0.71182266009852213, 0.7573891625615764], 5: [0.1625615763546798, 0.14901477832512317, 0.10837438423645321, 0.097290640394088676], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140928 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2391 3191  894] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 132  180  245  345  482  701 1055 1717 2678 4615] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.619555
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.929474
Epoch 1, cost is  0.892248
Epoch 2, cost is  0.872928
Epoch 3, cost is  0.863273
Epoch 4, cost is  0.847749
Training took 0.079894 minutes
Weight histogram
[3572 2462 2019 1816  982  730  323  122   78   46] [-0.23419961 -0.2110755  -0.18795138 -0.16482726 -0.14170315 -0.11857903
 -0.09545491 -0.07233079 -0.04920668 -0.02608256 -0.00295844]
[ 112  142  235  354  567  955 1289 1950 2790 3756] [-0.23419961 -0.2110755  -0.18795138 -0.16482726 -0.14170315 -0.11857903
 -0.09545491 -0.07233079 -0.04920668 -0.02608256 -0.00295844]
-5.46377
5.59467
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142680 minutes
Weight histogram
[  34  148  470  960 1601 1668 1731 3292 3708  563] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 276  358  472  674  967 1110  999 1709 2815 4795] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.555757
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.933666
Epoch 1, cost is  0.898863
Epoch 2, cost is  0.881279
Epoch 3, cost is  0.866491
Epoch 4, cost is  0.854238
Training took 0.079800 minutes
Weight histogram
[3762 2293 2033 1698 1872 1378  629  247  154  109] [-0.22909935 -0.20646554 -0.18383173 -0.16119792 -0.1385641  -0.11593029
 -0.09329648 -0.07066267 -0.04802886 -0.02539505 -0.00276124]
[ 235  287  473  716 1186 1458 1327 2016 2789 3688] [-0.22909935 -0.20646554 -0.18383173 -0.16119792 -0.1385641  -0.11593029
 -0.09329648 -0.07066267 -0.04802886 -0.02539505 -0.00276124]
-5.00253
5.75459
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.51627
Epoch 1, cost is  2.59858
Epoch 2, cost is  2.03182
Epoch 3, cost is  1.72789
Epoch 4, cost is  1.53982
Training took 0.080662 minutes
Weight histogram
[2636 2436 1846 1323 1162  865  645  695  392  150] [-0.13948034 -0.12583497 -0.11218961 -0.09854424 -0.08489888 -0.07125351
 -0.05760815 -0.04396279 -0.03031742 -0.01667206 -0.00302669]
[1065  528  562  739  929 1129 1409 1655 1961 2173] [-0.13948034 -0.12583497 -0.11218961 -0.09854424 -0.08489888 -0.07125351
 -0.05760815 -0.04396279 -0.03031742 -0.01667206 -0.00302669]
-3.74219
4.4139
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041487 minutes
Epoch 0
Fine tuning took 0.044105 minutes
Epoch 0
Fine tuning took 0.042979 minutes
{'zero': {0: [0.1354679802955665, 0.15517241379310345, 0.17241379310344829, 0.14039408866995073], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.60221674876847286, 0.62561576354679804, 0.62315270935960587], 5: [0.15640394088669951, 0.24261083743842365, 0.2019704433497537, 0.23645320197044334], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1354679802955665, 0.10714285714285714, 0.18103448275862069, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.79802955665024633, 0.71674876847290636, 0.7068965517241379], 5: [0.15640394088669951, 0.094827586206896547, 0.10221674876847291, 0.13177339901477833], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1354679802955665, 0.11576354679802955, 0.10467980295566502, 0.10960591133004927], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.77216748768472909, 0.79187192118226601, 0.74137931034482762], 5: [0.15640394088669951, 0.11206896551724138, 0.10344827586206896, 0.14901477832512317], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1354679802955665, 0.11330049261083744, 0.1625615763546798, 0.19581280788177341], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70812807881773399, 0.80049261083743839, 0.73768472906403937, 0.66502463054187189], 5: [0.15640394088669951, 0.086206896551724144, 0.099753694581280791, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141815 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2391 3191  894] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 132  180  245  345  482  701 1055 1717 2678 4615] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.619555
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.37515
Epoch 1, cost is  1.34546
Epoch 2, cost is  1.32087
Epoch 3, cost is  1.29961
Epoch 4, cost is  1.28046
Training took 0.082903 minutes
Weight histogram
[2954 2462 2088 1408 1077  661  555  411  281  253] [-0.12607567 -0.11349391 -0.10091216 -0.0883304  -0.07574864 -0.06316688
 -0.05058513 -0.03800337 -0.02542161 -0.01283985 -0.0002581 ]
[ 579  413  541  754  946 1194 1420 1746 2083 2474] [-0.12607567 -0.11349391 -0.10091216 -0.0883304  -0.07574864 -0.06316688
 -0.05058513 -0.03800337 -0.02542161 -0.01283985 -0.0002581 ]
-2.19717
2.58879
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141246 minutes
Weight histogram
[  34  148  470  960 1601 1668 1731 3292 3708  563] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 276  358  472  674  967 1110  999 1709 2815 4795] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.555757
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.3913
Epoch 1, cost is  1.36083
Epoch 2, cost is  1.33847
Epoch 3, cost is  1.31419
Epoch 4, cost is  1.29297
Training took 0.082586 minutes
Weight histogram
[3128 2214 1857 1353 1247 1269 1075  792  690  550] [-0.12321857 -0.11092055 -0.09862253 -0.08632451 -0.07402649 -0.06172847
 -0.04943046 -0.03713244 -0.02483442 -0.0125364  -0.00023838]
[1239  842 1131 1167  895 1118 1364 1726 2105 2588] [-0.12321857 -0.11092055 -0.09862253 -0.08632451 -0.07402649 -0.06172847
 -0.04943046 -0.03713244 -0.02483442 -0.0125364  -0.00023838]
-2.29274
3.06677
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.5275
Epoch 1, cost is  6.0343
Epoch 2, cost is  5.62777
Epoch 3, cost is  5.02258
Epoch 4, cost is  4.49583
Training took 0.070677 minutes
Weight histogram
[ 605 1174 1198 1364 1651 1346 1678 2782  260   92] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
[2383 1708 2060 1245  808  742  753  805  870  776] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
-0.744946
0.911377
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044312 minutes
Epoch 0
Fine tuning took 0.041147 minutes
Epoch 0
Fine tuning took 0.040972 minutes
{'zero': {0: [0.19088669950738915, 0.19334975369458129, 0.13793103448275862, 0.13054187192118227], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57635467980295563, 0.63916256157635465, 0.70320197044334976, 0.72413793103448276], 5: [0.23275862068965517, 0.16748768472906403, 0.15886699507389163, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19088669950738915, 0.14901477832512317, 0.14532019704433496, 0.092364532019704432], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57635467980295563, 0.65024630541871919, 0.69088669950738912, 0.75615763546798032], 5: [0.23275862068965517, 0.20073891625615764, 0.16379310344827586, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19088669950738915, 0.15886699507389163, 0.1206896551724138, 0.10591133004926108], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57635467980295563, 0.66995073891625612, 0.70812807881773399, 0.75246305418719217], 5: [0.23275862068965517, 0.17118226600985223, 0.17118226600985223, 0.14162561576354679], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19088669950738915, 0.13054187192118227, 0.12807881773399016, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.57635467980295563, 0.69088669950738912, 0.71674876847290636, 0.73522167487684731], 5: [0.23275862068965517, 0.17857142857142858, 0.15517241379310345, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.146320 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2391 3191  894] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 132  180  245  345  482  701 1055 1717 2678 4615] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.619555
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.37515
Epoch 1, cost is  1.34546
Epoch 2, cost is  1.32087
Epoch 3, cost is  1.29961
Epoch 4, cost is  1.28046
Training took 0.086838 minutes
Weight histogram
[2954 2462 2088 1408 1077  661  555  411  281  253] [-0.12607567 -0.11349391 -0.10091216 -0.0883304  -0.07574864 -0.06316688
 -0.05058513 -0.03800337 -0.02542161 -0.01283985 -0.0002581 ]
[ 579  413  541  754  946 1194 1420 1746 2083 2474] [-0.12607567 -0.11349391 -0.10091216 -0.0883304  -0.07574864 -0.06316688
 -0.05058513 -0.03800337 -0.02542161 -0.01283985 -0.0002581 ]
-2.19717
2.58879
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140892 minutes
Weight histogram
[  34  148  470  960 1601 1668 1731 3292 3708  563] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 276  358  472  674  967 1110  999 1709 2815 4795] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.555757
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.3913
Epoch 1, cost is  1.36083
Epoch 2, cost is  1.33847
Epoch 3, cost is  1.31419
Epoch 4, cost is  1.29297
Training took 0.084126 minutes
Weight histogram
[3128 2214 1857 1353 1247 1269 1075  792  690  550] [-0.12321857 -0.11092055 -0.09862253 -0.08632451 -0.07402649 -0.06172847
 -0.04943046 -0.03713244 -0.02483442 -0.0125364  -0.00023838]
[1239  842 1131 1167  895 1118 1364 1726 2105 2588] [-0.12321857 -0.11092055 -0.09862253 -0.08632451 -0.07402649 -0.06172847
 -0.04943046 -0.03713244 -0.02483442 -0.0125364  -0.00023838]
-2.29274
3.06677
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.41813
Epoch 1, cost is  5.92433
Epoch 2, cost is  5.56804
Epoch 3, cost is  5.01464
Epoch 4, cost is  4.40529
Training took 0.078486 minutes
Weight histogram
[ 622 1470 1939 2502 1575 1742 1806  314  115   65] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
[2117 1936 2578 1188  716  687  722  735  757  714] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
-0.751311
0.684235
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044415 minutes
Epoch 0
Fine tuning took 0.042211 minutes
Epoch 0
Fine tuning took 0.042470 minutes
{'zero': {0: [0.19827586206896552, 0.16871921182266009, 0.12192118226600986, 0.096059113300492605], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.66871921182266014, 0.73275862068965514, 0.76477832512315269], 5: [0.23645320197044334, 0.1625615763546798, 0.14532019704433496, 0.13916256157635468], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19827586206896552, 0.18349753694581281, 0.1268472906403941, 0.1206896551724138], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.6280788177339901, 0.70566502463054193, 0.71921182266009853], 5: [0.23645320197044334, 0.18842364532019704, 0.16748768472906403, 0.16009852216748768], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19827586206896552, 0.1539408866995074, 0.13300492610837439, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.66502463054187189, 0.72290640394088668, 0.74507389162561577], 5: [0.23645320197044334, 0.18103448275862069, 0.14408866995073891, 0.12561576354679804], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19827586206896552, 0.13916256157635468, 0.12438423645320197, 0.1268472906403941], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56527093596059108, 0.66502463054187189, 0.72290640394088668, 0.72783251231527091], 5: [0.23645320197044334, 0.19581280788177341, 0.15270935960591134, 0.14532019704433496], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141891 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2391 3191  894] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 132  180  245  345  482  701 1055 1717 2678 4615] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.619555
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.708
Epoch 1, cost is  3.63849
Epoch 2, cost is  3.57877
Epoch 3, cost is  3.52224
Epoch 4, cost is  3.46577
Training took 0.081479 minutes
Weight histogram
[1642 1410 1307 1300 1310 1213 1066 1974  763  165] [ -5.09079434e-02  -4.58159554e-02  -4.07239674e-02  -3.56319793e-02
  -3.05399913e-02  -2.54480033e-02  -2.03560153e-02  -1.52640272e-02
  -1.01720392e-02  -5.08005120e-03   1.19368206e-05]
[2553 1181  971  875  888  953 1042 1143 1246 1298] [ -5.09079434e-02  -4.58159554e-02  -4.07239674e-02  -3.56319793e-02
  -3.05399913e-02  -2.54480033e-02  -2.03560153e-02  -1.52640272e-02
  -1.01720392e-02  -5.08005120e-03   1.19368206e-05]
-0.658815
1.02115
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140125 minutes
Weight histogram
[  34  148  470  960 1601 1668 1731 3292 3708  563] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 276  358  472  674  967 1110  999 1709 2815 4795] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.555757
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.9093
Epoch 1, cost is  3.83336
Epoch 2, cost is  3.76437
Epoch 3, cost is  3.70089
Epoch 4, cost is  3.63611
Training took 0.085299 minutes
Weight histogram
[1422 1267 1212 1145 1255 1365 1219 4247  783  260] [ -4.41288464e-02  -3.97145707e-02  -3.53002951e-02  -3.08860195e-02
  -2.64717438e-02  -2.20574682e-02  -1.76431925e-02  -1.32289169e-02
  -8.81464126e-03  -4.40036562e-03   1.39100202e-05]
[5224 1086  943  880  864  884  958 1027 1127 1182] [ -4.41288464e-02  -3.97145707e-02  -3.53002951e-02  -3.08860195e-02
  -2.64717438e-02  -2.20574682e-02  -1.76431925e-02  -1.32289169e-02
  -8.81464126e-03  -4.40036562e-03   1.39100202e-05]
-0.75047
0.972288
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.80568
Epoch 1, cost is  6.67606
Epoch 2, cost is  6.58091
Epoch 3, cost is  6.49605
Epoch 4, cost is  6.41619
Training took 0.071045 minutes
Weight histogram
[ 752 1202 3987 2596 1322  818  563  395  293  222] [ -2.25583259e-02  -2.02937727e-02  -1.80292196e-02  -1.57646664e-02
  -1.35001133e-02  -1.12355601e-02  -8.97100694e-03  -6.70645378e-03
  -4.44190062e-03  -2.17734746e-03   8.72056989e-05]
[4502 2120 1566 1134  956  647  485  320  280  140] [ -2.25583259e-02  -2.02937727e-02  -1.80292196e-02  -1.57646664e-02
  -1.35001133e-02  -1.12355601e-02  -8.97100694e-03  -6.70645378e-03
  -4.44190062e-03  -2.17734746e-03   8.72056989e-05]
-0.0778338
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043141 minutes
Epoch 0
Fine tuning took 0.041659 minutes
Epoch 0
Fine tuning took 0.042316 minutes
{'zero': {0: [0.21428571428571427, 0.17733990147783252, 0.15270935960591134, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.67487684729064035, 0.67733990147783252, 0.6071428571428571], 5: [0.14285714285714285, 0.14778325123152711, 0.16995073891625614, 0.29187192118226601], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.21428571428571427, 0.20443349753694581, 0.17241379310344829, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.63423645320197042, 0.66748768472906406, 0.60837438423645318], 5: [0.14285714285714285, 0.16133004926108374, 0.16009852216748768, 0.27463054187192121], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.21428571428571427, 0.18965517241379309, 0.14162561576354679, 0.093596059113300489], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.65394088669950734, 0.70566502463054193, 0.6280788177339901], 5: [0.14285714285714285, 0.15640394088669951, 0.15270935960591134, 0.27832512315270935], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.21428571428571427, 0.19827586206896552, 0.15147783251231528, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6428571428571429, 0.6428571428571429, 0.66748768472906406, 0.59113300492610843], 5: [0.14285714285714285, 0.15886699507389163, 0.18103448275862069, 0.30541871921182268], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141038 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2391 3191  894] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 132  180  245  345  482  701 1055 1717 2678 4615] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.619555
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.708
Epoch 1, cost is  3.63849
Epoch 2, cost is  3.57877
Epoch 3, cost is  3.52224
Epoch 4, cost is  3.46577
Training took 0.080052 minutes
Weight histogram
[1642 1410 1307 1300 1310 1213 1066 1974  763  165] [ -5.09079434e-02  -4.58159554e-02  -4.07239674e-02  -3.56319793e-02
  -3.05399913e-02  -2.54480033e-02  -2.03560153e-02  -1.52640272e-02
  -1.01720392e-02  -5.08005120e-03   1.19368206e-05]
[2553 1181  971  875  888  953 1042 1143 1246 1298] [ -5.09079434e-02  -4.58159554e-02  -4.07239674e-02  -3.56319793e-02
  -3.05399913e-02  -2.54480033e-02  -2.03560153e-02  -1.52640272e-02
  -1.01720392e-02  -5.08005120e-03   1.19368206e-05]
-0.658815
1.02115
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142435 minutes
Weight histogram
[  34  148  470  960 1601 1668 1731 3292 3708  563] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 276  358  472  674  967 1110  999 1709 2815 4795] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.555757
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.9093
Epoch 1, cost is  3.83336
Epoch 2, cost is  3.76437
Epoch 3, cost is  3.70089
Epoch 4, cost is  3.63611
Training took 0.082553 minutes
Weight histogram
[1422 1267 1212 1145 1255 1365 1219 4247  783  260] [ -4.41288464e-02  -3.97145707e-02  -3.53002951e-02  -3.08860195e-02
  -2.64717438e-02  -2.20574682e-02  -1.76431925e-02  -1.32289169e-02
  -8.81464126e-03  -4.40036562e-03   1.39100202e-05]
[5224 1086  943  880  864  884  958 1027 1127 1182] [ -4.41288464e-02  -3.97145707e-02  -3.53002951e-02  -3.08860195e-02
  -2.64717438e-02  -2.20574682e-02  -1.76431925e-02  -1.32289169e-02
  -8.81464126e-03  -4.40036562e-03   1.39100202e-05]
-0.75047
0.972288
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.72475
Epoch 1, cost is  6.54201
Epoch 2, cost is  6.43264
Epoch 3, cost is  6.34148
Epoch 4, cost is  6.25655
Training took 0.077592 minutes
Weight histogram
[ 696 1299 3521 2755 1429  888  601  422  308  231] [ -2.31174268e-02  -2.08067977e-02  -1.84961686e-02  -1.61855395e-02
  -1.38749103e-02  -1.15642812e-02  -9.25365213e-03  -6.94302303e-03
  -4.63239392e-03  -2.32176481e-03  -1.11357003e-05]
[4314 2113 1594 1153  990  677  513  345  299  152] [ -2.31174268e-02  -2.08067977e-02  -1.84961686e-02  -1.61855395e-02
  -1.38749103e-02  -1.15642812e-02  -9.25365213e-03  -6.94302303e-03
  -4.63239392e-03  -2.32176481e-03  -1.11357003e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044274 minutes
Epoch 0
Fine tuning took 0.043567 minutes
Epoch 0
Fine tuning took 0.043351 minutes
{'zero': {0: [0.19581280788177341, 0.16625615763546797, 0.15024630541871922, 0.084975369458128072], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.70566502463054193, 0.69211822660098521, 0.60467980295566504], 5: [0.14655172413793102, 0.12807881773399016, 0.15763546798029557, 0.31034482758620691], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19581280788177341, 0.16748768472906403, 0.12438423645320197, 0.087438423645320201], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.66871921182266014, 0.71305418719211822, 0.58990147783251234], 5: [0.14655172413793102, 0.16379310344827586, 0.1625615763546798, 0.32266009852216748], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19581280788177341, 0.15517241379310345, 0.1354679802955665, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.68965517241379315, 0.72290640394088668, 0.60221674876847286], 5: [0.14655172413793102, 0.15517241379310345, 0.14162561576354679, 0.30049261083743845], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19581280788177341, 0.14408866995073891, 0.1539408866995074, 0.065270935960591137], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.6576354679802956, 0.69334975369458129, 0.70197044334975367, 0.58990147783251234], 5: [0.14655172413793102, 0.1625615763546798, 0.14408866995073891, 0.34482758620689657], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.157038 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2398 4370 1733] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 137  187  254  379  532  777 1159 1960 3329 5461] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.647837
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.877017
Epoch 1, cost is  0.847957
Epoch 2, cost is  0.831985
Epoch 3, cost is  0.81731
Epoch 4, cost is  0.80725
Training took 0.087905 minutes
Weight histogram
[4005 2877 2661 1787 1146 1017  411  135   87   49] [-0.24672845 -0.22235145 -0.19797445 -0.17359745 -0.14922045 -0.12484345
 -0.10046645 -0.07608944 -0.05171244 -0.02733544 -0.00295844]
[ 117  153  257  396  649 1090 1541 2286 3242 4444] [-0.24672845 -0.22235145 -0.19797445 -0.17359745 -0.14922045 -0.12484345
 -0.10046645 -0.07608944 -0.05171244 -0.02733544 -0.00295844]
-5.72461
6.11379
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.155850 minutes
Weight histogram
[  34  148  470  960 1601 1668 1742 3702 5045  830] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 289  371  514  718 1055 1050 1099 1995 3284 5825] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.584559
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.877528
Epoch 1, cost is  0.844808
Epoch 2, cost is  0.828349
Epoch 3, cost is  0.81721
Epoch 4, cost is  0.807431
Training took 0.085241 minutes
Weight histogram
[3963 2743 2667 1769 1683 1996  783  308  170  118] [-0.24352781 -0.21945116 -0.1953745  -0.17129784 -0.14722118 -0.12314453
 -0.09906787 -0.07499121 -0.05091455 -0.0268379  -0.00276124]
[ 243  314  512  807 1348 1436 1602 2347 3144 4447] [-0.24352781 -0.21945116 -0.1953745  -0.17129784 -0.14722118 -0.12314453
 -0.09906787 -0.07499121 -0.05091455 -0.0268379  -0.00276124]
-5.19414
5.75459
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.60583
Epoch 1, cost is  2.89347
Epoch 2, cost is  2.42105
Epoch 3, cost is  2.19559
Epoch 4, cost is  2.04116
Training took 0.075835 minutes
Weight histogram
[3007 2996 2364 1555 1366  856  591  566  452  422] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
[1144  623  635  820 1032 1248 1608 1967 2441 2657] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
-4.03518
5.24361
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042596 minutes
Epoch 0
Fine tuning took 0.048968 minutes
Epoch 0
Fine tuning took 0.043697 minutes
{'zero': {0: [0.15517241379310345, 0.16133004926108374, 0.17733990147783252, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.57758620689655171, 0.59605911330049266, 0.59729064039408863], 5: [0.14285714285714285, 0.26108374384236455, 0.22660098522167488, 0.23029556650246305], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15517241379310345, 0.098522167487684734, 0.077586206896551727, 0.10591133004926108], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.80418719211822665, 0.81650246305418717, 0.78940886699507384], 5: [0.14285714285714285, 0.097290640394088676, 0.10591133004926108, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15517241379310345, 0.10837438423645321, 0.10714285714285714, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.77832512315270941, 0.7573891625615764, 0.77339901477832518], 5: [0.14285714285714285, 0.11330049261083744, 0.1354679802955665, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15517241379310345, 0.11576354679802955, 0.098522167487684734, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.70197044334975367, 0.78448275862068961, 0.80665024630541871, 0.76108374384236455], 5: [0.14285714285714285, 0.099753694581280791, 0.094827586206896547, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141140 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2398 4370 1733] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 137  187  254  379  532  777 1159 1960 3329 5461] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.647837
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.877017
Epoch 1, cost is  0.847957
Epoch 2, cost is  0.831985
Epoch 3, cost is  0.81731
Epoch 4, cost is  0.80725
Training took 0.083506 minutes
Weight histogram
[4005 2877 2661 1787 1146 1017  411  135   87   49] [-0.24672845 -0.22235145 -0.19797445 -0.17359745 -0.14922045 -0.12484345
 -0.10046645 -0.07608944 -0.05171244 -0.02733544 -0.00295844]
[ 117  153  257  396  649 1090 1541 2286 3242 4444] [-0.24672845 -0.22235145 -0.19797445 -0.17359745 -0.14922045 -0.12484345
 -0.10046645 -0.07608944 -0.05171244 -0.02733544 -0.00295844]
-5.72461
6.11379
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140205 minutes
Weight histogram
[  34  148  470  960 1601 1668 1742 3702 5045  830] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 289  371  514  718 1055 1050 1099 1995 3284 5825] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.584559
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.877528
Epoch 1, cost is  0.844808
Epoch 2, cost is  0.828349
Epoch 3, cost is  0.81721
Epoch 4, cost is  0.807431
Training took 0.081503 minutes
Weight histogram
[3963 2743 2667 1769 1683 1996  783  308  170  118] [-0.24352781 -0.21945116 -0.1953745  -0.17129784 -0.14722118 -0.12314453
 -0.09906787 -0.07499121 -0.05091455 -0.0268379  -0.00276124]
[ 243  314  512  807 1348 1436 1602 2347 3144 4447] [-0.24352781 -0.21945116 -0.1953745  -0.17129784 -0.14722118 -0.12314453
 -0.09906787 -0.07499121 -0.05091455 -0.0268379  -0.00276124]
-5.19414
5.75459
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.51777
Epoch 1, cost is  2.60783
Epoch 2, cost is  2.03142
Epoch 3, cost is  1.72523
Epoch 4, cost is  1.53545
Training took 0.080698 minutes
Weight histogram
[3004 2842 2168 1569 1359 1010  775  815  458  175] [-0.14000282 -0.1263052  -0.11260759 -0.09890998 -0.08521237 -0.07151475
 -0.05781714 -0.04411953 -0.03042192 -0.0167243  -0.00302669]
[1245  619  659  867 1087 1322 1647 1925 2282 2522] [-0.14000282 -0.1263052  -0.11260759 -0.09890998 -0.08521237 -0.07151475
 -0.05781714 -0.04411953 -0.03042192 -0.0167243  -0.00302669]
-3.74219
4.4139
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043307 minutes
Epoch 0
Fine tuning took 0.046515 minutes
Epoch 0
Fine tuning took 0.046205 minutes
{'zero': {0: [0.12931034482758622, 0.16379310344827586, 0.16009852216748768, 0.19950738916256158], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.58497536945812811, 0.55788177339901479, 0.58004926108374388], 5: [0.12561576354679804, 0.25123152709359609, 0.28201970443349755, 0.22044334975369459], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.12931034482758622, 0.094827586206896547, 0.096059113300492605, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.80665024630541871, 0.80788177339901479, 0.7857142857142857], 5: [0.12561576354679804, 0.098522167487684734, 0.096059113300492605, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.12931034482758622, 0.11576354679802955, 0.1206896551724138, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.76354679802955661, 0.75123152709359609, 0.75369458128078815], 5: [0.12561576354679804, 0.1206896551724138, 0.12807881773399016, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.12931034482758622, 0.084975369458128072, 0.082512315270935957, 0.067733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74507389162561577, 0.80788177339901479, 0.80788177339901479, 0.82266009852216748], 5: [0.12561576354679804, 0.10714285714285714, 0.10960591133004927, 0.10960591133004927], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141137 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2398 4370 1733] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 137  187  254  379  532  777 1159 1960 3329 5461] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.647837
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.26802
Epoch 1, cost is  1.24266
Epoch 2, cost is  1.22392
Epoch 3, cost is  1.2052
Epoch 4, cost is  1.1912
Training took 0.083503 minutes
Weight histogram
[3569 2492 2361 1932 1412  719  667  436  321  266] [-0.13381761 -0.12046166 -0.10710571 -0.09374976 -0.08039381 -0.06703785
 -0.0536819  -0.04032595 -0.02697    -0.01361405 -0.0002581 ]
[ 605  449  616  861 1082 1385 1682 2042 2471 2982] [-0.13381761 -0.12046166 -0.10710571 -0.09374976 -0.08039381 -0.06703785
 -0.0536819  -0.04032595 -0.02697    -0.01361405 -0.0002581 ]
-2.3772
2.71179
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.156225 minutes
Weight histogram
[  34  148  470  960 1601 1668 1742 3702 5045  830] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 289  371  514  718 1055 1050 1099 1995 3284 5825] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.584559
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.2847
Epoch 1, cost is  1.26169
Epoch 2, cost is  1.24169
Epoch 3, cost is  1.22547
Epoch 4, cost is  1.20857
Training took 0.089394 minutes
Weight histogram
[3374 2624 2100 1960 1412 1204 1296  874  754  602] [-0.13212158 -0.11893326 -0.10574494 -0.09255662 -0.0793683  -0.06617998
 -0.05299166 -0.03980334 -0.02661502 -0.0134267  -0.00023838]
[1287  924 1269 1122 1005 1298 1607 2038 2543 3107] [-0.13212158 -0.11893326 -0.10574494 -0.09255662 -0.0793683  -0.06617998
 -0.05299166 -0.03980334 -0.02661502 -0.0134267  -0.00023838]
-2.52593
3.25964
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.52935
Epoch 1, cost is  6.03719
Epoch 2, cost is  5.63102
Epoch 3, cost is  5.039
Epoch 4, cost is  4.52034
Training took 0.077653 minutes
Weight histogram
[ 664 1363 1399 1587 1963 1581 1935 3269  306  108] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
[2744 1974 2424 1460  947  866  879  939 1020  922] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
-0.744946
0.920039
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044255 minutes
Epoch 0
Fine tuning took 0.042888 minutes
Epoch 0
Fine tuning took 0.043037 minutes
{'zero': {0: [0.15640394088669951, 0.16748768472906403, 0.098522167487684734, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.67487684729064035, 0.74384236453201968, 0.70566502463054193], 5: [0.23275862068965517, 0.15763546798029557, 0.15763546798029557, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15640394088669951, 0.13793103448275862, 0.14039408866995073, 0.11699507389162561], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.67364532019704437, 0.71059113300492616, 0.66995073891625612], 5: [0.23275862068965517, 0.18842364532019704, 0.14901477832512317, 0.21305418719211822], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15640394088669951, 0.15640394088669951, 0.11822660098522167, 0.11822660098522167], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.6785714285714286, 0.7142857142857143, 0.69827586206896552], 5: [0.23275862068965517, 0.16502463054187191, 0.16748768472906403, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15640394088669951, 0.13177339901477833, 0.10344827586206896, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.69334975369458129, 0.75615763546798032, 0.71305418719211822], 5: [0.23275862068965517, 0.1748768472906404, 0.14039408866995073, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.157366 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2398 4370 1733] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 137  187  254  379  532  777 1159 1960 3329 5461] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.647837
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.26802
Epoch 1, cost is  1.24266
Epoch 2, cost is  1.22392
Epoch 3, cost is  1.2052
Epoch 4, cost is  1.1912
Training took 0.085867 minutes
Weight histogram
[3569 2492 2361 1932 1412  719  667  436  321  266] [-0.13381761 -0.12046166 -0.10710571 -0.09374976 -0.08039381 -0.06703785
 -0.0536819  -0.04032595 -0.02697    -0.01361405 -0.0002581 ]
[ 605  449  616  861 1082 1385 1682 2042 2471 2982] [-0.13381761 -0.12046166 -0.10710571 -0.09374976 -0.08039381 -0.06703785
 -0.0536819  -0.04032595 -0.02697    -0.01361405 -0.0002581 ]
-2.3772
2.71179
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141574 minutes
Weight histogram
[  34  148  470  960 1601 1668 1742 3702 5045  830] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 289  371  514  718 1055 1050 1099 1995 3284 5825] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.584559
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.2847
Epoch 1, cost is  1.26169
Epoch 2, cost is  1.24169
Epoch 3, cost is  1.22547
Epoch 4, cost is  1.20857
Training took 0.082739 minutes
Weight histogram
[3374 2624 2100 1960 1412 1204 1296  874  754  602] [-0.13212158 -0.11893326 -0.10574494 -0.09255662 -0.0793683  -0.06617998
 -0.05299166 -0.03980334 -0.02661502 -0.0134267  -0.00023838]
[1287  924 1269 1122 1005 1298 1607 2038 2543 3107] [-0.13212158 -0.11893326 -0.10574494 -0.09255662 -0.0793683  -0.06617998
 -0.05299166 -0.03980334 -0.02661502 -0.0134267  -0.00023838]
-2.52593
3.25964
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.42014
Epoch 1, cost is  5.92896
Epoch 2, cost is  5.57395
Epoch 3, cost is  5.02748
Epoch 4, cost is  4.42916
Training took 0.078465 minutes
Weight histogram
[ 651 1718 2244 2986 1847 2009 2136  373  135   76] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
[2436 2230 3043 1401  841  802  847  859  888  828] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
-0.751311
0.685368
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044482 minutes
Epoch 0
Fine tuning took 0.042284 minutes
Epoch 0
Fine tuning took 0.041694 minutes
{'zero': {0: [0.15886699507389163, 0.15270935960591134, 0.1354679802955665, 0.14408866995073891], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.66009852216748766, 0.72290640394088668, 0.66379310344827591], 5: [0.22906403940886699, 0.18719211822660098, 0.14162561576354679, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15886699507389163, 0.13423645320197045, 0.11699507389162561, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.69704433497536944, 0.7142857142857143, 0.65147783251231528], 5: [0.22906403940886699, 0.16871921182266009, 0.16871921182266009, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15886699507389163, 0.14655172413793102, 0.10591133004926108, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.66995073891625612, 0.7426108374384236, 0.65024630541871919], 5: [0.22906403940886699, 0.18349753694581281, 0.15147783251231528, 0.17980295566502463], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15886699507389163, 0.13177339901477833, 0.14408866995073891, 0.14655172413793102], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61206896551724133, 0.70073891625615758, 0.71921182266009853, 0.64901477832512311], 5: [0.22906403940886699, 0.16748768472906403, 0.13669950738916256, 0.20443349753694581], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140616 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2398 4370 1733] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 137  187  254  379  532  777 1159 1960 3329 5461] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.647837
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.40105
Epoch 1, cost is  3.35003
Epoch 2, cost is  3.29822
Epoch 3, cost is  3.25246
Epoch 4, cost is  3.2064
Training took 0.082711 minutes
Weight histogram
[2036 1792 1590 1418 1388 1524 1209 1126 1896  196] [ -5.65475933e-02  -5.08916403e-02  -4.52356873e-02  -3.95797343e-02
  -3.39237813e-02  -2.82678283e-02  -2.26118752e-02  -1.69559222e-02
  -1.12999692e-02  -5.64401619e-03   1.19368206e-05]
[2733 1282 1076  983 1058 1174 1285 1425 1519 1640] [ -5.65475933e-02  -5.08916403e-02  -4.52356873e-02  -3.95797343e-02
  -3.39237813e-02  -2.82678283e-02  -2.26118752e-02  -1.69559222e-02
  -1.12999692e-02  -5.64401619e-03   1.19368206e-05]
-0.736193
1.11764
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141212 minutes
Weight histogram
[  34  148  470  960 1601 1668 1742 3702 5045  830] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 289  371  514  718 1055 1050 1099 1995 3284 5825] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.584559
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.57089
Epoch 1, cost is  3.51321
Epoch 2, cost is  3.45885
Epoch 3, cost is  3.40326
Epoch 4, cost is  3.35421
Training took 0.080758 minutes
Weight histogram
[1720 1565 1438 1333 1439 1406 1420 4190 1367  322] [ -5.03553823e-02  -4.53184530e-02  -4.02815238e-02  -3.52445946e-02
  -3.02076654e-02  -2.51707361e-02  -2.01338069e-02  -1.50968777e-02
  -1.00599484e-02  -5.02301921e-03   1.39100202e-05]
[5411 1208 1042 1008 1001 1091 1188 1307 1409 1535] [ -5.03553823e-02  -4.53184530e-02  -4.02815238e-02  -3.52445946e-02
  -3.02076654e-02  -2.51707361e-02  -2.01338069e-02  -1.50968777e-02
  -1.00599484e-02  -5.02301921e-03   1.39100202e-05]
-0.795588
1.06213
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.81164
Epoch 1, cost is  6.68823
Epoch 2, cost is  6.59828
Epoch 3, cost is  6.51807
Epoch 4, cost is  6.44344
Training took 0.069432 minutes
Weight histogram
[ 752 1202 4444 3370 1639 1000  679  474  351  264] [ -2.25583259e-02  -2.02937727e-02  -1.80292196e-02  -1.57646664e-02
  -1.35001133e-02  -1.12355601e-02  -8.97100694e-03  -6.70645378e-03
  -4.44190062e-03  -2.17734746e-03   8.72056989e-05]
[5408 2554 1918 1443  980  647  485  320  280  140] [ -2.25583259e-02  -2.02937727e-02  -1.80292196e-02  -1.57646664e-02
  -1.35001133e-02  -1.12355601e-02  -8.97100694e-03  -6.70645378e-03
  -4.44190062e-03  -2.17734746e-03   8.72056989e-05]
-0.0778338
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.040263 minutes
Epoch 0
Fine tuning took 0.040898 minutes
Epoch 0
Fine tuning took 0.041356 minutes
{'zero': {0: [0.19827586206896552, 0.11206896551724138, 0.18226600985221675, 0.20689655172413793], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.65147783251231528, 0.6576354679802956, 0.59359605911330049], 5: [0.19088669950738915, 0.23645320197044334, 0.16009852216748768, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19827586206896552, 0.10591133004926108, 0.18103448275862069, 0.2229064039408867], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.70073891625615758, 0.6428571428571429, 0.6145320197044335], 5: [0.19088669950738915, 0.19334975369458129, 0.17610837438423646, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19827586206896552, 0.10098522167487685, 0.2019704433497537, 0.21305418719211822], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.70320197044334976, 0.65886699507389157, 0.61822660098522164], 5: [0.19088669950738915, 0.19581280788177341, 0.13916256157635468, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19827586206896552, 0.10714285714285714, 0.16748768472906403, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.61083743842364535, 0.69704433497536944, 0.68596059113300489, 0.62438423645320196], 5: [0.19088669950738915, 0.19581280788177341, 0.14655172413793102, 0.16995073891625614], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142075 minutes
Weight histogram
[ 108  314  634  948 1083 1038 1549 2398 4370 1733] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
[ 137  187  254  379  532  777 1159 1960 3329 5461] [ -1.07421329e-04   5.65451257e-05   2.20511580e-04   3.84478035e-04
   5.48444489e-04   7.12410943e-04   8.76377398e-04   1.04034385e-03
   1.20431031e-03   1.36827676e-03   1.53224322e-03]
-0.647837
0.810074
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.40105
Epoch 1, cost is  3.35003
Epoch 2, cost is  3.29822
Epoch 3, cost is  3.25246
Epoch 4, cost is  3.2064
Training took 0.080107 minutes
Weight histogram
[2036 1792 1590 1418 1388 1524 1209 1126 1896  196] [ -5.65475933e-02  -5.08916403e-02  -4.52356873e-02  -3.95797343e-02
  -3.39237813e-02  -2.82678283e-02  -2.26118752e-02  -1.69559222e-02
  -1.12999692e-02  -5.64401619e-03   1.19368206e-05]
[2733 1282 1076  983 1058 1174 1285 1425 1519 1640] [ -5.65475933e-02  -5.08916403e-02  -4.52356873e-02  -3.95797343e-02
  -3.39237813e-02  -2.82678283e-02  -2.26118752e-02  -1.69559222e-02
  -1.12999692e-02  -5.64401619e-03   1.19368206e-05]
-0.736193
1.11764
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140066 minutes
Weight histogram
[  34  148  470  960 1601 1668 1742 3702 5045  830] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
[ 289  371  514  718 1055 1050 1099 1995 3284 5825] [ -2.14635569e-04  -9.12358810e-05   3.21638072e-05   1.55563495e-04
   2.78963184e-04   4.02362872e-04   5.25762560e-04   6.49162248e-04
   7.72561936e-04   8.95961624e-04   1.01936131e-03]
-0.927654
0.584559
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.57089
Epoch 1, cost is  3.51321
Epoch 2, cost is  3.45885
Epoch 3, cost is  3.40326
Epoch 4, cost is  3.35421
Training took 0.083476 minutes
Weight histogram
[1720 1565 1438 1333 1439 1406 1420 4190 1367  322] [ -5.03553823e-02  -4.53184530e-02  -4.02815238e-02  -3.52445946e-02
  -3.02076654e-02  -2.51707361e-02  -2.01338069e-02  -1.50968777e-02
  -1.00599484e-02  -5.02301921e-03   1.39100202e-05]
[5411 1208 1042 1008 1001 1091 1188 1307 1409 1535] [ -5.03553823e-02  -4.53184530e-02  -4.02815238e-02  -3.52445946e-02
  -3.02076654e-02  -2.51707361e-02  -2.01338069e-02  -1.50968777e-02
  -1.00599484e-02  -5.02301921e-03   1.39100202e-05]
-0.795588
1.06213
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.73468
Epoch 1, cost is  6.56212
Epoch 2, cost is  6.45933
Epoch 3, cost is  6.37452
Epoch 4, cost is  6.29534
Training took 0.078502 minutes
Weight histogram
[ 696 1299 3838 3599 1779 1088  725  507  369  275] [ -2.31174268e-02  -2.08067977e-02  -1.84961686e-02  -1.61855395e-02
  -1.38749103e-02  -1.15642812e-02  -9.25365213e-03  -6.94302303e-03
  -4.63239392e-03  -2.32176481e-03  -1.11357003e-05]
[5196 2548 1952 1472 1021  677  513  345  299  152] [ -2.31174268e-02  -2.08067977e-02  -1.84961686e-02  -1.61855395e-02
  -1.38749103e-02  -1.15642812e-02  -9.25365213e-03  -6.94302303e-03
  -4.63239392e-03  -2.32176481e-03  -1.11357003e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044272 minutes
Epoch 0
Fine tuning took 0.044346 minutes
Epoch 0
Fine tuning took 0.042098 minutes
{'zero': {0: [0.20935960591133004, 0.1145320197044335, 0.18349753694581281, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.69211822660098521, 0.64162561576354682, 0.65147783251231528], 5: [0.19211822660098521, 0.19334975369458129, 0.1748768472906404, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.20935960591133004, 0.11822660098522167, 0.18596059113300492, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.6785714285714286, 0.61083743842364535, 0.62931034482758619], 5: [0.19211822660098521, 0.20320197044334976, 0.20320197044334976, 0.19211822660098521], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.20935960591133004, 0.091133004926108374, 0.18965517241379309, 0.17733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.70812807881773399, 0.64655172413793105, 0.64778325123152714], 5: [0.19211822660098521, 0.20073891625615764, 0.16379310344827586, 0.1748768472906404], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.20935960591133004, 0.12807881773399016, 0.2019704433497537, 0.15640394088669951], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.59852216748768472, 0.66256157635467983, 0.6145320197044335, 0.64408866995073888], 5: [0.19211822660098521, 0.20935960591133004, 0.18349753694581281, 0.19950738916256158], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140502 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3123 5982 1016] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 141  204  263  380  585  830 1320 2228 3733 6516] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.832506
Epoch 1, cost is  0.801389
Epoch 2, cost is  0.786699
Epoch 3, cost is  0.77721
Epoch 4, cost is  0.766794
Training took 0.079907 minutes
Weight histogram
[4177 3849 2237 2397 1511 1219  492  171   95   52] [-0.25852904 -0.23297198 -0.20741492 -0.18185786 -0.1563008  -0.13074374
 -0.10518668 -0.07962962 -0.05407256 -0.0285155  -0.00295844]
[ 121  166  274  439  733 1212 1773 2615 3832 5035] [-0.25852904 -0.23297198 -0.20741492 -0.18185786 -0.1563008  -0.13074374
 -0.10518668 -0.07962962 -0.05407256 -0.0285155  -0.00295844]
-5.88611
6.31421
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141590 minutes
Weight histogram
[  34  153  468  966 1625 1668 1787 4538 6046  940] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 298  383  539  747 1146 1056 1292 2106 3980 6678] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.586567
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.833518
Epoch 1, cost is  0.802566
Epoch 2, cost is  0.786509
Epoch 3, cost is  0.774662
Epoch 4, cost is  0.765062
Training took 0.080873 minutes
Weight histogram
[4036 3961 2306 2096 1767 2384 1020  350  182  123] [-0.2556662  -0.2303757  -0.2050852  -0.17979471 -0.15450421 -0.12921372
 -0.10392322 -0.07863273 -0.05334223 -0.02805173 -0.00276124]
[ 251  338  553  895 1540 1384 1831 2649 3715 5069] [-0.2556662  -0.2303757  -0.2050852  -0.17979471 -0.15450421 -0.12921372
 -0.10392322 -0.07863273 -0.05334223 -0.02805173 -0.00276124]
-5.29482
5.79506
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.60358
Epoch 1, cost is  2.88846
Epoch 2, cost is  2.4215
Epoch 3, cost is  2.20387
Epoch 4, cost is  2.04897
Training took 0.074069 minutes
Weight histogram
[3368 3438 2721 1806 1569  973  679  644  520  482] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
[1309  712  731  943 1182 1430 1845 2254 2796 2998] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
-4.03518
5.24361
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042118 minutes
Epoch 0
Fine tuning took 0.041468 minutes
Epoch 0
Fine tuning took 0.043826 minutes
{'zero': {0: [0.17610837438423646, 0.20443349753694581, 0.16748768472906403, 0.15024630541871922], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69458128078817738, 0.56527093596059108, 0.55295566502463056, 0.61822660098522164], 5: [0.12931034482758622, 0.23029556650246305, 0.27955665024630544, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17610837438423646, 0.14162561576354679, 0.11576354679802955, 0.11330049261083744], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69458128078817738, 0.77463054187192115, 0.77216748768472909, 0.78448275862068961], 5: [0.12931034482758622, 0.083743842364532015, 0.11206896551724138, 0.10221674876847291], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17610837438423646, 0.10591133004926108, 0.13423645320197045, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69458128078817738, 0.79187192118226601, 0.73645320197044339, 0.78817733990147787], 5: [0.12931034482758622, 0.10221674876847291, 0.12931034482758622, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17610837438423646, 0.16133004926108374, 0.12931034482758622, 0.14532019704433496], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.69458128078817738, 0.76970443349753692, 0.74014778325123154, 0.77709359605911332], 5: [0.12931034482758622, 0.068965517241379309, 0.13054187192118227, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141811 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3123 5982 1016] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 141  204  263  380  585  830 1320 2228 3733 6516] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.832506
Epoch 1, cost is  0.801389
Epoch 2, cost is  0.786699
Epoch 3, cost is  0.77721
Epoch 4, cost is  0.766794
Training took 0.080239 minutes
Weight histogram
[4177 3849 2237 2397 1511 1219  492  171   95   52] [-0.25852904 -0.23297198 -0.20741492 -0.18185786 -0.1563008  -0.13074374
 -0.10518668 -0.07962962 -0.05407256 -0.0285155  -0.00295844]
[ 121  166  274  439  733 1212 1773 2615 3832 5035] [-0.25852904 -0.23297198 -0.20741492 -0.18185786 -0.1563008  -0.13074374
 -0.10518668 -0.07962962 -0.05407256 -0.0285155  -0.00295844]
-5.88611
6.31421
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141513 minutes
Weight histogram
[  34  153  468  966 1625 1668 1787 4538 6046  940] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 298  383  539  747 1146 1056 1292 2106 3980 6678] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.586567
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.833518
Epoch 1, cost is  0.802566
Epoch 2, cost is  0.786509
Epoch 3, cost is  0.774662
Epoch 4, cost is  0.765062
Training took 0.080178 minutes
Weight histogram
[4036 3961 2306 2096 1767 2384 1020  350  182  123] [-0.2556662  -0.2303757  -0.2050852  -0.17979471 -0.15450421 -0.12921372
 -0.10392322 -0.07863273 -0.05334223 -0.02805173 -0.00276124]
[ 251  338  553  895 1540 1384 1831 2649 3715 5069] [-0.2556662  -0.2303757  -0.2050852  -0.17979471 -0.15450421 -0.12921372
 -0.10392322 -0.07863273 -0.05334223 -0.02805173 -0.00276124]
-5.29482
5.79506
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.5161
Epoch 1, cost is  2.60345
Epoch 2, cost is  2.03152
Epoch 3, cost is  1.73599
Epoch 4, cost is  1.53476
Training took 0.077567 minutes
Weight histogram
[3357 3269 2525 1798 1553 1147  897  928  526  200] [-0.14000282 -0.1263052  -0.11260759 -0.09890998 -0.08521237 -0.07151475
 -0.05781714 -0.04411953 -0.03042192 -0.0167243  -0.00302669]
[1425  710  755  996 1243 1518 1887 2206 2602 2858] [-0.14000282 -0.1263052  -0.11260759 -0.09890998 -0.08521237 -0.07151475
 -0.05781714 -0.04411953 -0.03042192 -0.0167243  -0.00302669]
-3.74219
4.4139
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.041602 minutes
Epoch 0
Fine tuning took 0.045245 minutes
Epoch 0
Fine tuning took 0.044182 minutes
{'zero': {0: [0.15270935960591134, 0.15763546798029557, 0.1625615763546798, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.61576354679802958, 0.62438423645320196, 0.63054187192118227], 5: [0.12315270935960591, 0.22660098522167488, 0.21305418719211822, 0.21182266009852216], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.15270935960591134, 0.14039408866995073, 0.14162561576354679, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.77586206896551724, 0.80788177339901479, 0.80911330049261088], 5: [0.12315270935960591, 0.083743842364532015, 0.050492610837438424, 0.099753694581280791], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.15270935960591134, 0.11576354679802955, 0.10098522167487685, 0.088669950738916259], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.78201970443349755, 0.80788177339901479, 0.80295566502463056], 5: [0.12315270935960591, 0.10221674876847291, 0.091133004926108374, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.15270935960591134, 0.11083743842364532, 0.16625615763546797, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.72413793103448276, 0.81896551724137934, 0.75862068965517238, 0.78940886699507384], 5: [0.12315270935960591, 0.070197044334975367, 0.075123152709359611, 0.11330049261083744], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140895 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3123 5982 1016] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 141  204  263  380  585  830 1320 2228 3733 6516] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.18239
Epoch 1, cost is  1.16342
Epoch 2, cost is  1.14512
Epoch 3, cost is  1.12951
Epoch 4, cost is  1.11732
Training took 0.080986 minutes
Weight histogram
[4010 3120 2775 2116 1521  829  715  494  343  277] [-0.14082214 -0.12676574 -0.11270933 -0.09865293 -0.08459652 -0.07054012
 -0.05648371 -0.04242731 -0.02837091 -0.0143145  -0.0002581 ]
[ 629  487  682  961 1213 1554 1913 2346 2888 3527] [-0.14082214 -0.12676574 -0.11270933 -0.09865293 -0.08459652 -0.07054012
 -0.05648371 -0.04242731 -0.02837091 -0.0143145  -0.0002581 ]
-2.50161
2.88508
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141659 minutes
Weight histogram
[  34  153  468  966 1625 1668 1787 4538 6046  940] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 298  383  539  747 1146 1056 1292 2106 3980 6678] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.586567
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.20486
Epoch 1, cost is  1.18444
Epoch 2, cost is  1.17061
Epoch 3, cost is  1.15668
Epoch 4, cost is  1.14147
Training took 0.080739 minutes
Weight histogram
[3956 3244 2515 2016 1456 1202 1446  957  801  632] [-0.13846606 -0.12464329 -0.11082052 -0.09699776 -0.08317499 -0.06935222
 -0.05552945 -0.04170668 -0.02788391 -0.01406114 -0.00023838]
[1331 1002 1401 1070 1127 1463 1853 2344 3003 3631] [-0.13846606 -0.12464329 -0.11082052 -0.09699776 -0.08317499 -0.06935222
 -0.05552945 -0.04170668 -0.02788391 -0.01406114 -0.00023838]
-2.7409
3.44418
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.53282
Epoch 1, cost is  6.04675
Epoch 2, cost is  5.6435
Epoch 3, cost is  5.04919
Epoch 4, cost is  4.53209
Training took 0.072151 minutes
Weight histogram
[ 706 1562 1592 1814 2281 1819 2186 3763  353  124] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
[3102 2243 2789 1671 1085  986 1004 1073 1170 1077] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
-0.744946
0.974803
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043382 minutes
Epoch 0
Fine tuning took 0.042211 minutes
Epoch 0
Fine tuning took 0.040830 minutes
{'zero': {0: [0.23275862068965517, 0.18842364532019704, 0.18842364532019704, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56896551724137934, 0.58497536945812811, 0.65640394088669951, 0.66995073891625612], 5: [0.19827586206896552, 0.22660098522167488, 0.15517241379310345, 0.1625615763546798], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23275862068965517, 0.16625615763546797, 0.15763546798029557, 0.1748768472906404], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56896551724137934, 0.60960591133004927, 0.66871921182266014, 0.64408866995073888], 5: [0.19827586206896552, 0.22413793103448276, 0.17364532019704434, 0.18103448275862069], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23275862068965517, 0.17980295566502463, 0.14532019704433496, 0.1625615763546798], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56896551724137934, 0.57019704433497542, 0.68103448275862066, 0.65886699507389157], 5: [0.19827586206896552, 0.25, 0.17364532019704434, 0.17857142857142858], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23275862068965517, 0.15640394088669951, 0.13669950738916256, 0.15147783251231528], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56896551724137934, 0.58620689655172409, 0.68842364532019706, 0.6711822660098522], 5: [0.19827586206896552, 0.25738916256157635, 0.1748768472906404, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141966 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3123 5982 1016] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 141  204  263  380  585  830 1320 2228 3733 6516] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.18239
Epoch 1, cost is  1.16342
Epoch 2, cost is  1.14512
Epoch 3, cost is  1.12951
Epoch 4, cost is  1.11732
Training took 0.083437 minutes
Weight histogram
[4010 3120 2775 2116 1521  829  715  494  343  277] [-0.14082214 -0.12676574 -0.11270933 -0.09865293 -0.08459652 -0.07054012
 -0.05648371 -0.04242731 -0.02837091 -0.0143145  -0.0002581 ]
[ 629  487  682  961 1213 1554 1913 2346 2888 3527] [-0.14082214 -0.12676574 -0.11270933 -0.09865293 -0.08459652 -0.07054012
 -0.05648371 -0.04242731 -0.02837091 -0.0143145  -0.0002581 ]
-2.50161
2.88508
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141521 minutes
Weight histogram
[  34  153  468  966 1625 1668 1787 4538 6046  940] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 298  383  539  747 1146 1056 1292 2106 3980 6678] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.586567
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.20486
Epoch 1, cost is  1.18444
Epoch 2, cost is  1.17061
Epoch 3, cost is  1.15668
Epoch 4, cost is  1.14147
Training took 0.079689 minutes
Weight histogram
[3956 3244 2515 2016 1456 1202 1446  957  801  632] [-0.13846606 -0.12464329 -0.11082052 -0.09699776 -0.08317499 -0.06935222
 -0.05552945 -0.04170668 -0.02788391 -0.01406114 -0.00023838]
[1331 1002 1401 1070 1127 1463 1853 2344 3003 3631] [-0.13846606 -0.12464329 -0.11082052 -0.09699776 -0.08317499 -0.06935222
 -0.05552945 -0.04170668 -0.02788391 -0.01406114 -0.00023838]
-2.7409
3.44418
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.42458
Epoch 1, cost is  5.93886
Epoch 2, cost is  5.58633
Epoch 3, cost is  5.03572
Epoch 4, cost is  4.44492
Training took 0.077186 minutes
Weight histogram
[ 675 1960 2547 3477 2119 2271 2476  432  156   87] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
[2752 2525 3506 1609  969  917  972  984 1021  945] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
-0.777106
0.685368
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042319 minutes
Epoch 0
Fine tuning took 0.043502 minutes
Epoch 0
Fine tuning took 0.044015 minutes
{'zero': {0: [0.23522167487684728, 0.19088669950738915, 0.15763546798029557, 0.1539408866995074], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56280788177339902, 0.55049261083743839, 0.66502463054187189, 0.69458128078817738], 5: [0.2019704433497537, 0.25862068965517243, 0.17733990147783252, 0.15147783251231528], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.23522167487684728, 0.18965517241379309, 0.16871921182266009, 0.16748768472906403], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56280788177339902, 0.58374384236453203, 0.6280788177339901, 0.64162561576354682], 5: [0.2019704433497537, 0.22660098522167488, 0.20320197044334976, 0.19088669950738915], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.23522167487684728, 0.2019704433497537, 0.18719211822660098, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56280788177339902, 0.56034482758620685, 0.63177339901477836, 0.67980295566502458], 5: [0.2019704433497537, 0.2376847290640394, 0.18103448275862069, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.23522167487684728, 0.18226600985221675, 0.19581280788177341, 0.13669950738916256], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.56280788177339902, 0.58990147783251234, 0.6071428571428571, 0.71059113300492616], 5: [0.2019704433497537, 0.22783251231527094, 0.19704433497536947, 0.15270935960591134], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141000 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3123 5982 1016] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 141  204  263  380  585  830 1320 2228 3733 6516] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.1531
Epoch 1, cost is  3.1163
Epoch 2, cost is  3.07424
Epoch 3, cost is  3.03556
Epoch 4, cost is  2.99964
Training took 0.080499 minutes
Weight histogram
[2419 2161 1822 1631 1604 1588 1449 1172 2125  229] [ -6.17165230e-02  -5.55436770e-02  -4.93708311e-02  -4.31979851e-02
  -3.70251391e-02  -3.08522931e-02  -2.46794471e-02  -1.85066011e-02
  -1.23337551e-02  -6.16090916e-03   1.19368206e-05]
[2886 1382 1146 1123 1222 1390 1553 1663 1806 2029] [ -6.17165230e-02  -5.55436770e-02  -4.93708311e-02  -4.31979851e-02
  -3.70251391e-02  -3.08522931e-02  -2.46794471e-02  -1.85066011e-02
  -1.23337551e-02  -6.16090916e-03   1.19368206e-05]
-0.812964
1.19056
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141119 minutes
Weight histogram
[  34  153  468  966 1625 1668 1787 4538 6046  940] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 298  383  539  747 1146 1056 1292 2106 3980 6678] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.586567
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.29875
Epoch 1, cost is  3.25157
Epoch 2, cost is  3.20331
Epoch 3, cost is  3.16731
Epoch 4, cost is  3.12003
Training took 0.082127 minutes
Weight histogram
[2046 1864 1739 1612 1471 1528 1668 2188 3726  383] [ -5.59067689e-02  -5.03147010e-02  -4.47226331e-02  -3.91305652e-02
  -3.35384973e-02  -2.79464294e-02  -2.23543615e-02  -1.67622937e-02
  -1.11702258e-02  -5.57815787e-03   1.39100202e-05]
[5580 1297 1143 1120 1181 1297 1435 1571 1734 1867] [ -5.59067689e-02  -5.03147010e-02  -4.47226331e-02  -3.91305652e-02
  -3.35384973e-02  -2.79464294e-02  -2.23543615e-02  -1.67622937e-02
  -1.11702258e-02  -5.57815787e-03   1.39100202e-05]
-0.877303
1.15811
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.81599
Epoch 1, cost is  6.69787
Epoch 2, cost is  6.61143
Epoch 3, cost is  6.53401
Epoch 4, cost is  6.46233
Training took 0.072638 minutes
Weight histogram
[ 752 1202 4758 4253 1976 1189  798  556  409  307] [ -2.25583259e-02  -2.02937727e-02  -1.80292196e-02  -1.57646664e-02
  -1.35001133e-02  -1.12355601e-02  -8.97100694e-03  -6.70645378e-03
  -4.44190062e-03  -2.17734746e-03   8.72056989e-05]
[6360 3016 2285 1687  980  647  485  320  280  140] [ -2.25583259e-02  -2.02937727e-02  -1.80292196e-02  -1.57646664e-02
  -1.35001133e-02  -1.12355601e-02  -8.97100694e-03  -6.70645378e-03
  -4.44190062e-03  -2.17734746e-03   8.72056989e-05]
-0.0778338
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042912 minutes
Epoch 0
Fine tuning took 0.041143 minutes
Epoch 0
Fine tuning took 0.043837 minutes
{'zero': {0: [0.093596059113300489, 0.1145320197044335, 0.1268472906403941, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.67733990147783252, 0.67487684729064035, 0.57019704433497542], 5: [0.16871921182266009, 0.20812807881773399, 0.19827586206896552, 0.23152709359605911], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.093596059113300489, 0.13669950738916256, 0.13300492610837439, 0.17857142857142858], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.66009852216748766, 0.67733990147783252, 0.54802955665024633], 5: [0.16871921182266009, 0.20320197044334976, 0.18965517241379309, 0.27339901477832512], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.093596059113300489, 0.14532019704433496, 0.1354679802955665, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.6219211822660099, 0.64778325123152714, 0.58620689655172409], 5: [0.16871921182266009, 0.23275862068965517, 0.21674876847290642, 0.25246305418719212], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.093596059113300489, 0.1206896551724138, 0.12192118226600986, 0.19827586206896552], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.65640394088669951, 0.69458128078817738, 0.56773399014778325], 5: [0.16871921182266009, 0.2229064039408867, 0.18349753694581281, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141235 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3123 5982 1016] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 141  204  263  380  585  830 1320 2228 3733 6516] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.1531
Epoch 1, cost is  3.1163
Epoch 2, cost is  3.07424
Epoch 3, cost is  3.03556
Epoch 4, cost is  2.99964
Training took 0.084009 minutes
Weight histogram
[2419 2161 1822 1631 1604 1588 1449 1172 2125  229] [ -6.17165230e-02  -5.55436770e-02  -4.93708311e-02  -4.31979851e-02
  -3.70251391e-02  -3.08522931e-02  -2.46794471e-02  -1.85066011e-02
  -1.23337551e-02  -6.16090916e-03   1.19368206e-05]
[2886 1382 1146 1123 1222 1390 1553 1663 1806 2029] [ -6.17165230e-02  -5.55436770e-02  -4.93708311e-02  -4.31979851e-02
  -3.70251391e-02  -3.08522931e-02  -2.46794471e-02  -1.85066011e-02
  -1.23337551e-02  -6.16090916e-03   1.19368206e-05]
-0.812964
1.19056
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140325 minutes
Weight histogram
[  34  153  468  966 1625 1668 1787 4538 6046  940] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 298  383  539  747 1146 1056 1292 2106 3980 6678] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.586567
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.29875
Epoch 1, cost is  3.25157
Epoch 2, cost is  3.20331
Epoch 3, cost is  3.16731
Epoch 4, cost is  3.12003
Training took 0.088997 minutes
Weight histogram
[2046 1864 1739 1612 1471 1528 1668 2188 3726  383] [ -5.59067689e-02  -5.03147010e-02  -4.47226331e-02  -3.91305652e-02
  -3.35384973e-02  -2.79464294e-02  -2.23543615e-02  -1.67622937e-02
  -1.11702258e-02  -5.57815787e-03   1.39100202e-05]
[5580 1297 1143 1120 1181 1297 1435 1571 1734 1867] [ -5.59067689e-02  -5.03147010e-02  -4.47226331e-02  -3.91305652e-02
  -3.35384973e-02  -2.79464294e-02  -2.23543615e-02  -1.67622937e-02
  -1.11702258e-02  -5.57815787e-03   1.39100202e-05]
-0.877303
1.15811
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74141
Epoch 1, cost is  6.57634
Epoch 2, cost is  6.47901
Epoch 3, cost is  6.39739
Epoch 4, cost is  6.3216
Training took 0.085156 minutes
Weight histogram
[ 696 1299 4014 4544 2153 1296  853  595  430  320] [ -2.31174268e-02  -2.08067977e-02  -1.84961686e-02  -1.61855395e-02
  -1.38749103e-02  -1.15642812e-02  -9.25365213e-03  -6.94302303e-03
  -4.63239392e-03  -2.32176481e-03  -1.11357003e-05]
[6126 3010 2329 1728 1021  677  513  345  299  152] [ -2.31174268e-02  -2.08067977e-02  -1.84961686e-02  -1.61855395e-02
  -1.38749103e-02  -1.15642812e-02  -9.25365213e-03  -6.94302303e-03
  -4.63239392e-03  -2.32176481e-03  -1.11357003e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.049191 minutes
Epoch 0
Fine tuning took 0.045323 minutes
Epoch 0
Fine tuning took 0.041984 minutes
{'zero': {0: [0.092364532019704432, 0.13423645320197045, 0.15270935960591134, 0.20320197044334976], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.67610837438423643, 0.70935960591133007, 0.56157635467980294], 5: [0.15886699507389163, 0.18965517241379309, 0.13793103448275862, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.092364532019704432, 0.13300492610837439, 0.16009852216748768, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.6576354679802956, 0.68842364532019706, 0.57389162561576357], 5: [0.15886699507389163, 0.20935960591133004, 0.15147783251231528, 0.25], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.092364532019704432, 0.13177339901477833, 0.16502463054187191, 0.19334975369458129], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.66009852216748766, 0.68472906403940892, 0.55049261083743839], 5: [0.15886699507389163, 0.20812807881773399, 0.15024630541871922, 0.25615763546798032], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.092364532019704432, 0.13177339901477833, 0.13669950738916256, 0.21551724137931033], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.74876847290640391, 0.64655172413793105, 0.73891625615763545, 0.54926108374384242], 5: [0.15886699507389163, 0.22167487684729065, 0.12438423645320197, 0.23522167487684728], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.161417 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3278 7643 1225] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 146  211  285  410  608  922 1503 2509 4416 7215] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.800522
Epoch 1, cost is  0.775877
Epoch 2, cost is  0.759939
Epoch 3, cost is  0.74907
Epoch 4, cost is  0.741501
Training took 0.086054 minutes
Weight histogram
[4564 3853 3645 2088 2002 1144  573  198  103   55] [-0.27042261 -0.24367619 -0.21692977 -0.19018336 -0.16343694 -0.13669053
 -0.10994411 -0.08319769 -0.05645128 -0.02970486 -0.00295844]
[ 125  175  296  474  818 1314 2009 2866 4376 5772] [-0.27042261 -0.24367619 -0.21692977 -0.19018336 -0.16343694 -0.13669053
 -0.10994411 -0.08319769 -0.05645128 -0.02970486 -0.00295844]
-6.15462
6.31421
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.154373 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5098 7323 1109] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 306  393  559  794 1169 1041 1421 2440 4415 7712] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.79734
Epoch 1, cost is  0.766489
Epoch 2, cost is  0.754305
Epoch 3, cost is  0.741004
Epoch 4, cost is  0.734725
Training took 0.112174 minutes
Weight histogram
[4656 3768 3491 2248 1977 2151 1237  392  201  129] [-0.26652601 -0.24014954 -0.21377306 -0.18739658 -0.1610201  -0.13464363
 -0.10826715 -0.08189067 -0.05551419 -0.02913772 -0.00276124]
[ 260  354  598  976 1701 1340 2076 2972 4223 5750] [-0.26652601 -0.24014954 -0.21377306 -0.18739658 -0.1610201  -0.13464363
 -0.10826715 -0.08189067 -0.05551419 -0.02913772 -0.00276124]
-5.71266
5.91341
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.61036
Epoch 1, cost is  2.91948
Epoch 2, cost is  2.45339
Epoch 3, cost is  2.22903
Epoch 4, cost is  2.08204
Training took 0.097080 minutes
Weight histogram
[3771 3863 3046 2070 1768 1087  767  727  583  543] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
[1475  804  824 1070 1337 1616 2083 2547 3152 3317] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
-4.03518
5.24361
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044826 minutes
Epoch 0
Fine tuning took 0.042454 minutes
Epoch 0
Fine tuning took 0.043848 minutes
{'zero': {0: [0.24014778325123154, 0.15886699507389163, 0.17118226600985223, 0.15270935960591134], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.6280788177339901, 0.61699507389162567, 0.65270935960591137], 5: [0.11822660098522167, 0.21305418719211822, 0.21182266009852216, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.24014778325123154, 0.12807881773399016, 0.11576354679802955, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.74876847290640391, 0.78325123152709364, 0.81650246305418717], 5: [0.11822660098522167, 0.12315270935960591, 0.10098522167487685, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.24014778325123154, 0.13916256157635468, 0.11822660098522167, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.72783251231527091, 0.77955665024630538, 0.82389162561576357], 5: [0.11822660098522167, 0.13300492610837439, 0.10221674876847291, 0.078817733990147784], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.24014778325123154, 0.11945812807881774, 0.13423645320197045, 0.10098522167487685], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.64162561576354682, 0.72044334975369462, 0.74876847290640391, 0.8214285714285714], 5: [0.11822660098522167, 0.16009852216748768, 0.11699507389162561, 0.077586206896551727], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140859 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3278 7643 1225] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 146  211  285  410  608  922 1503 2509 4416 7215] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.800522
Epoch 1, cost is  0.775877
Epoch 2, cost is  0.759939
Epoch 3, cost is  0.74907
Epoch 4, cost is  0.741501
Training took 0.083989 minutes
Weight histogram
[4564 3853 3645 2088 2002 1144  573  198  103   55] [-0.27042261 -0.24367619 -0.21692977 -0.19018336 -0.16343694 -0.13669053
 -0.10994411 -0.08319769 -0.05645128 -0.02970486 -0.00295844]
[ 125  175  296  474  818 1314 2009 2866 4376 5772] [-0.27042261 -0.24367619 -0.21692977 -0.19018336 -0.16343694 -0.13669053
 -0.10994411 -0.08319769 -0.05645128 -0.02970486 -0.00295844]
-6.15462
6.31421
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.155380 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5098 7323 1109] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 306  393  559  794 1169 1041 1421 2440 4415 7712] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.79734
Epoch 1, cost is  0.766489
Epoch 2, cost is  0.754305
Epoch 3, cost is  0.741004
Epoch 4, cost is  0.734725
Training took 0.112218 minutes
Weight histogram
[4656 3768 3491 2248 1977 2151 1237  392  201  129] [-0.26652601 -0.24014954 -0.21377306 -0.18739658 -0.1610201  -0.13464363
 -0.10826715 -0.08189067 -0.05551419 -0.02913772 -0.00276124]
[ 260  354  598  976 1701 1340 2076 2972 4223 5750] [-0.26652601 -0.24014954 -0.21377306 -0.18739658 -0.1610201  -0.13464363
 -0.10826715 -0.08189067 -0.05551419 -0.02913772 -0.00276124]
-5.71266
5.91341
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.52249
Epoch 1, cost is  2.62188
Epoch 2, cost is  2.03005
Epoch 3, cost is  1.72667
Epoch 4, cost is  1.5308
Training took 0.083579 minutes
Weight histogram
[3794 3681 2802 2034 1739 1298 1016 1045  591  225] [-0.14000282 -0.1263052  -0.11260759 -0.09890998 -0.08521237 -0.07151475
 -0.05781714 -0.04411953 -0.03042192 -0.0167243  -0.00302669]
[1606  801  853 1126 1402 1706 2120 2479 2918 3214] [-0.14000282 -0.1263052  -0.11260759 -0.09890998 -0.08521237 -0.07151475
 -0.05781714 -0.04411953 -0.03042192 -0.0167243  -0.00302669]
-3.74219
4.4139
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.046501 minutes
Epoch 0
Fine tuning took 0.044699 minutes
Epoch 0
Fine tuning took 0.043070 minutes
{'zero': {0: [0.1625615763546798, 0.18842364532019704, 0.18965517241379309, 0.16379310344827586], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.56403940886699511, 0.59605911330049266, 0.60221674876847286], 5: [0.12315270935960591, 0.24753694581280788, 0.21428571428571427, 0.23399014778325122], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.1625615763546798, 0.10098522167487685, 0.089901477832512317, 0.10344827586206896], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.79802955665024633, 0.83990147783251234, 0.84482758620689657], 5: [0.12315270935960591, 0.10098522167487685, 0.070197044334975367, 0.051724137931034482], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.1625615763546798, 0.13793103448275862, 0.11822660098522167, 0.10714285714285714], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.72660098522167482, 0.79679802955665024, 0.78694581280788178], 5: [0.12315270935960591, 0.1354679802955665, 0.084975369458128072, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.1625615763546798, 0.10098522167487685, 0.087438423645320201, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.82389162561576357, 0.84359605911330049, 0.8854679802955665], 5: [0.12315270935960591, 0.075123152709359611, 0.068965517241379309, 0.036945812807881777], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142283 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3278 7643 1225] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 146  211  285  410  608  922 1503 2509 4416 7215] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.12083
Epoch 1, cost is  1.103
Epoch 2, cost is  1.08942
Epoch 3, cost is  1.07588
Epoch 4, cost is  1.06651
Training took 0.080528 minutes
Weight histogram
[4352 3676 2888 2567 1733  974  820  529  398  288] [-0.14763187 -0.13289449 -0.11815711 -0.10341974 -0.08868236 -0.07394498
 -0.05920761 -0.04447023 -0.02973285 -0.01499547 -0.0002581 ]
[ 647  525  746 1061 1353 1696 2156 2667 3259 4115] [-0.14763187 -0.13289449 -0.11815711 -0.10341974 -0.08868236 -0.07394498
 -0.05920761 -0.04447023 -0.02973285 -0.01499547 -0.0002581 ]
-2.62846
3.02335
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141669 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5098 7323 1109] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 306  393  559  794 1169 1041 1421 2440 4415 7712] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.13644
Epoch 1, cost is  1.12027
Epoch 2, cost is  1.10415
Epoch 3, cost is  1.0934
Epoch 4, cost is  1.08217
Training took 0.083254 minutes
Weight histogram
[4374 3462 2789 2391 1776 1340 1537 1052  866  663] [-0.14570601 -0.13115925 -0.11661249 -0.10206572 -0.08751896 -0.07297219
 -0.05842543 -0.04387867 -0.0293319  -0.01478514 -0.00023838]
[1368 1079 1523 1035 1258 1602 2086 2714 3372 4213] [-0.14570601 -0.13115925 -0.11661249 -0.10206572 -0.08751896 -0.07297219
 -0.05842543 -0.04387867 -0.0293319  -0.01478514 -0.00023838]
-2.96932
3.61332
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.53401
Epoch 1, cost is  6.04886
Epoch 2, cost is  5.64941
Epoch 3, cost is  5.05944
Epoch 4, cost is  4.54488
Training took 0.070972 minutes
Weight histogram
[ 738 1767 1785 2050 2589 2063 2438 4255  400  140] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
[3458 2513 3160 1880 1223 1110 1129 1207 1320 1225] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
-0.744946
0.974803
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042487 minutes
Epoch 0
Fine tuning took 0.043245 minutes
Epoch 0
Fine tuning took 0.040798 minutes
{'zero': {0: [0.36699507389162561, 0.17241379310344829, 0.1539408866995074, 0.16995073891625614], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45320197044334976, 0.68842364532019706, 0.63916256157635465, 0.65886699507389157], 5: [0.17980295566502463, 0.13916256157635468, 0.20689655172413793, 0.17118226600985223], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.36699507389162561, 0.17980295566502463, 0.15517241379310345, 0.15763546798029557], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45320197044334976, 0.68349753694581283, 0.63916256157635465, 0.65886699507389157], 5: [0.17980295566502463, 0.13669950738916256, 0.20566502463054187, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.36699507389162561, 0.17610837438423646, 0.14778325123152711, 0.16133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45320197044334976, 0.68226600985221675, 0.6576354679802956, 0.66256157635467983], 5: [0.17980295566502463, 0.14162561576354679, 0.19458128078817735, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.36699507389162561, 0.16133004926108374, 0.13300492610837439, 0.17610837438423646], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.45320197044334976, 0.69211822660098521, 0.6428571428571429, 0.65640394088669951], 5: [0.17980295566502463, 0.14655172413793102, 0.22413793103448276, 0.16748768472906403], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.143836 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3278 7643 1225] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 146  211  285  410  608  922 1503 2509 4416 7215] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.12083
Epoch 1, cost is  1.103
Epoch 2, cost is  1.08942
Epoch 3, cost is  1.07588
Epoch 4, cost is  1.06651
Training took 0.080200 minutes
Weight histogram
[4352 3676 2888 2567 1733  974  820  529  398  288] [-0.14763187 -0.13289449 -0.11815711 -0.10341974 -0.08868236 -0.07394498
 -0.05920761 -0.04447023 -0.02973285 -0.01499547 -0.0002581 ]
[ 647  525  746 1061 1353 1696 2156 2667 3259 4115] [-0.14763187 -0.13289449 -0.11815711 -0.10341974 -0.08868236 -0.07394498
 -0.05920761 -0.04447023 -0.02973285 -0.01499547 -0.0002581 ]
-2.62846
3.02335
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141240 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5098 7323 1109] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 306  393  559  794 1169 1041 1421 2440 4415 7712] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.13644
Epoch 1, cost is  1.12027
Epoch 2, cost is  1.10415
Epoch 3, cost is  1.0934
Epoch 4, cost is  1.08217
Training took 0.079535 minutes
Weight histogram
[4374 3462 2789 2391 1776 1340 1537 1052  866  663] [-0.14570601 -0.13115925 -0.11661249 -0.10206572 -0.08751896 -0.07297219
 -0.05842543 -0.04387867 -0.0293319  -0.01478514 -0.00023838]
[1368 1079 1523 1035 1258 1602 2086 2714 3372 4213] [-0.14570601 -0.13115925 -0.11661249 -0.10206572 -0.08751896 -0.07297219
 -0.05842543 -0.04387867 -0.0293319  -0.01478514 -0.00023838]
-2.96932
3.61332
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.42581
Epoch 1, cost is  5.94246
Epoch 2, cost is  5.59255
Epoch 3, cost is  5.05077
Epoch 4, cost is  4.46168
Training took 0.080516 minutes
Weight histogram
[ 692 2198 2851 3980 2388 2529 2820  492  177   98] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
[3067 2821 3974 1822 1092 1032 1099 1108 1154 1056] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
-0.777643
0.685368
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043306 minutes
Epoch 0
Fine tuning took 0.041848 minutes
Epoch 0
Fine tuning took 0.041418 minutes
{'zero': {0: [0.32758620689655171, 0.23275862068965517, 0.15517241379310345, 0.12931034482758622], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.6280788177339901, 0.64655172413793105, 0.68103448275862066], 5: [0.18103448275862069, 0.13916256157635468, 0.19827586206896552, 0.18965517241379309], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.32758620689655171, 0.16625615763546797, 0.16502463054187191, 0.13423645320197045], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.69704433497536944, 0.60221674876847286, 0.66502463054187189], 5: [0.18103448275862069, 0.13669950738916256, 0.23275862068965517, 0.20073891625615764], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.32758620689655171, 0.18472906403940886, 0.16995073891625614, 0.14162561576354679], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.66625615763546797, 0.61083743842364535, 0.66379310344827591], 5: [0.18103448275862069, 0.14901477832512317, 0.21921182266009853, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.32758620689655171, 0.18349753694581281, 0.18103448275862069, 0.1354679802955665], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.49137931034482757, 0.66625615763546797, 0.6071428571428571, 0.67610837438423643], 5: [0.18103448275862069, 0.15024630541871922, 0.21182266009852216, 0.18842364532019704], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141456 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3278 7643 1225] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 146  211  285  410  608  922 1503 2509 4416 7215] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.96144
Epoch 1, cost is  2.93241
Epoch 2, cost is  2.89023
Epoch 3, cost is  2.8661
Epoch 4, cost is  2.83286
Training took 0.082026 minutes
Weight histogram
[2907 2435 2193 1818 1716 1664 1636 1309 2282  265] [ -6.64282069e-02  -5.97841925e-02  -5.31401781e-02  -4.64961638e-02
  -3.98521494e-02  -3.32081350e-02  -2.65641207e-02  -1.99201063e-02
  -1.32760919e-02  -6.63207755e-03   1.19368206e-05]
[3031 1459 1225 1244 1412 1594 1759 1938 2148 2415] [ -6.64282069e-02  -5.97841925e-02  -5.31401781e-02  -4.64961638e-02
  -3.98521494e-02  -3.32081350e-02  -2.65641207e-02  -1.99201063e-02
  -1.32760919e-02  -6.63207755e-03   1.19368206e-05]
-0.879439
1.24408
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.141877 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5098 7323 1109] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 306  393  559  794 1169 1041 1421 2440 4415 7712] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.07916
Epoch 1, cost is  3.03967
Epoch 2, cost is  3.00378
Epoch 3, cost is  2.96707
Epoch 4, cost is  2.93333
Training took 0.082727 minutes
Weight histogram
[2445 2191 1923 1803 1695 1684 1770 1870 4420  449] [ -6.10096343e-02  -5.49072799e-02  -4.88049254e-02  -4.27025710e-02
  -3.66002166e-02  -3.04978621e-02  -2.43955077e-02  -1.82931533e-02
  -1.21907988e-02  -6.08844441e-03   1.39100202e-05]
[5721 1389 1250 1238 1352 1502 1661 1872 2039 2226] [ -6.10096343e-02  -5.49072799e-02  -4.88049254e-02  -4.27025710e-02
  -3.66002166e-02  -3.04978621e-02  -2.43955077e-02  -1.82931533e-02
  -1.21907988e-02  -6.08844441e-03   1.39100202e-05]
-0.939369
1.25154
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.81887
Epoch 1, cost is  6.70412
Epoch 2, cost is  6.62095
Epoch 3, cost is  6.54586
Epoch 4, cost is  6.4768
Training took 0.069044 minutes
Weight histogram
[ 752 1202 4926 5251 2330 1385  920  640  468  351] [ -2.25583259e-02  -2.02937281e-02  -1.80291303e-02  -1.57645326e-02
  -1.34999348e-02  -1.12353370e-02  -8.97073923e-03  -6.70614145e-03
  -4.44154367e-03  -2.17694589e-03   8.76518898e-05]
[7321 3482 2662 1908  980  647  485  320  280  140] [ -2.25583259e-02  -2.02937281e-02  -1.80291303e-02  -1.57645326e-02
  -1.34999348e-02  -1.12353370e-02  -8.97073923e-03  -6.70614145e-03
  -4.44154367e-03  -2.17694589e-03   8.76518898e-05]
-0.0778338
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044060 minutes
Epoch 0
Fine tuning took 0.042613 minutes
Epoch 0
Fine tuning took 0.042820 minutes
{'zero': {0: [0.11330049261083744, 0.077586206896551727, 0.14039408866995073, 0.11083743842364532], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.74507389162561577, 0.64039408866995073, 0.66502463054187189], 5: [0.14901477832512317, 0.17733990147783252, 0.21921182266009853, 0.22413793103448276], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.11330049261083744, 0.048029556650246302, 0.17118226600985223, 0.099753694581280791], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.78078817733990147, 0.63669950738916259, 0.65886699507389157], 5: [0.14901477832512317, 0.17118226600985223, 0.19211822660098521, 0.2413793103448276], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.11330049261083744, 0.059113300492610835, 0.17733990147783252, 0.094827586206896547], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.76600985221674878, 0.60960591133004927, 0.66256157635467983], 5: [0.14901477832512317, 0.1748768472906404, 0.21305418719211822, 0.24261083743842365], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.11330049261083744, 0.071428571428571425, 0.16995073891625614, 0.10221674876847291], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.73768472906403937, 0.75123152709359609, 0.63793103448275867, 0.69211822660098521], 5: [0.14901477832512317, 0.17733990147783252, 0.19211822660098521, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142843 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3278 7643 1225] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 146  211  285  410  608  922 1503 2509 4416 7215] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.96144
Epoch 1, cost is  2.93241
Epoch 2, cost is  2.89023
Epoch 3, cost is  2.8661
Epoch 4, cost is  2.83286
Training took 0.082667 minutes
Weight histogram
[2907 2435 2193 1818 1716 1664 1636 1309 2282  265] [ -6.64282069e-02  -5.97841925e-02  -5.31401781e-02  -4.64961638e-02
  -3.98521494e-02  -3.32081350e-02  -2.65641207e-02  -1.99201063e-02
  -1.32760919e-02  -6.63207755e-03   1.19368206e-05]
[3031 1459 1225 1244 1412 1594 1759 1938 2148 2415] [ -6.64282069e-02  -5.97841925e-02  -5.31401781e-02  -4.64961638e-02
  -3.98521494e-02  -3.32081350e-02  -2.65641207e-02  -1.99201063e-02
  -1.32760919e-02  -6.63207755e-03   1.19368206e-05]
-0.879439
1.24408
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140549 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5098 7323 1109] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 306  393  559  794 1169 1041 1421 2440 4415 7712] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.963188
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  3.07916
Epoch 1, cost is  3.03967
Epoch 2, cost is  3.00378
Epoch 3, cost is  2.96707
Epoch 4, cost is  2.93333
Training took 0.080319 minutes
Weight histogram
[2445 2191 1923 1803 1695 1684 1770 1870 4420  449] [ -6.10096343e-02  -5.49072799e-02  -4.88049254e-02  -4.27025710e-02
  -3.66002166e-02  -3.04978621e-02  -2.43955077e-02  -1.82931533e-02
  -1.21907988e-02  -6.08844441e-03   1.39100202e-05]
[5721 1389 1250 1238 1352 1502 1661 1872 2039 2226] [ -6.10096343e-02  -5.49072799e-02  -4.88049254e-02  -4.27025710e-02
  -3.66002166e-02  -3.04978621e-02  -2.43955077e-02  -1.82931533e-02
  -1.21907988e-02  -6.08844441e-03   1.39100202e-05]
-0.939369
1.25154
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.74733
Epoch 1, cost is  6.58702
Epoch 2, cost is  6.49251
Epoch 3, cost is  6.41409
Epoch 4, cost is  6.34096
Training took 0.077873 minutes
Weight histogram
[ 696 1299 4059 5585 2548 1510  985  686  491  366] [ -2.31174268e-02  -2.08067115e-02  -1.84959962e-02  -1.61852809e-02
  -1.38745657e-02  -1.15638504e-02  -9.25313510e-03  -6.94241982e-03
  -4.63170454e-03  -2.32098925e-03  -1.02739732e-05]
[7071 3479 2710 1958 1021  677  513  345  299  152] [ -2.31174268e-02  -2.08067115e-02  -1.84959962e-02  -1.61852809e-02
  -1.38745657e-02  -1.15638504e-02  -9.25313510e-03  -6.94241982e-03
  -4.63170454e-03  -2.32098925e-03  -1.02739732e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043178 minutes
Epoch 0
Fine tuning took 0.044394 minutes
Epoch 0
Fine tuning took 0.041645 minutes
{'zero': {0: [0.088669950738916259, 0.073891625615763554, 0.16625615763546797, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.76600985221674878, 0.64901477832512311, 0.70443349753694584], 5: [0.13300492610837439, 0.16009852216748768, 0.18472906403940886, 0.21428571428571427], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.088669950738916259, 0.067733990147783252, 0.16379310344827586, 0.064039408866995079], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.75123152709359609, 0.66748768472906406, 0.75246305418719217], 5: [0.13300492610837439, 0.18103448275862069, 0.16871921182266009, 0.18349753694581281], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.088669950738916259, 0.065270935960591137, 0.18226600985221675, 0.070197044334975367], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.74876847290640391, 0.61699507389162567, 0.73522167487684731], 5: [0.13300492610837439, 0.18596059113300492, 0.20073891625615764, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.088669950738916259, 0.071428571428571425, 0.16379310344827586, 0.076354679802955669], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77832512315270941, 0.76354679802955661, 0.67364532019704437, 0.71798029556650245], 5: [0.13300492610837439, 0.16502463054187191, 0.1625615763546798, 0.20566502463054187], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140833 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3423 9339 1409] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 150  215  293  432  650  987 1637 2818 4984 8084] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.773563
Epoch 1, cost is  0.74295
Epoch 2, cost is  0.728527
Epoch 3, cost is  0.71807
Epoch 4, cost is  0.713495
Training took 0.082417 minutes
Weight histogram
[5578 4333 3096 3016 1956 1152  729  227  105   58] [-0.28003734 -0.25232945 -0.22462156 -0.19691367 -0.16920578 -0.14149789
 -0.11379    -0.08608211 -0.05837422 -0.03066633 -0.00295844]
[ 128  183  320  514  920 1417 2191 3355 4692 6530] [-0.28003734 -0.25232945 -0.22462156 -0.19691367 -0.16920578 -0.14149789
 -0.11379    -0.08608211 -0.05837422 -0.03066633 -0.00295844]
-6.43964
6.31421
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140026 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5745 8530 1280] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 312  408  580  847 1223 1023 1495 2826 4925 8636] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.97075
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.764158
Epoch 1, cost is  0.737294
Epoch 2, cost is  0.72497
Epoch 3, cost is  0.71136
Epoch 4, cost is  0.704972
Training took 0.083248 minutes
Weight histogram
[5404 4200 3290 3037 1898 2177 1449  476  208  136] [-0.27719939 -0.24975557 -0.22231176 -0.19486794 -0.16742413 -0.13998031
 -0.1125365  -0.08509268 -0.05764887 -0.03020505 -0.00276124]
[ 266  374  633 1067 1715 1462 2276 3321 4686 6475] [-0.27719939 -0.24975557 -0.22231176 -0.19486794 -0.16742413 -0.13998031
 -0.1125365  -0.08509268 -0.05764887 -0.03020505 -0.00276124]
-5.89963
6.32529
... retrieved True_rbm_200-50_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/0/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.60923
Epoch 1, cost is  2.91339
Epoch 2, cost is  2.44172
Epoch 3, cost is  2.21074
Epoch 4, cost is  2.05306
Training took 0.070751 minutes
Weight histogram
[4133 4319 3371 2322 1983 1206  857  807  649  603] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
[1640  893  920 1194 1496 1799 2317 2841 3494 3656] [-0.18341476 -0.16537829 -0.14734182 -0.12930535 -0.11126888 -0.09323241
 -0.07519594 -0.05715947 -0.03912301 -0.02108654 -0.00305007]
-4.03518
5.24361
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044517 minutes
Epoch 0
Fine tuning took 0.043927 minutes
Epoch 0
Fine tuning took 0.042396 minutes
{'zero': {0: [0.19211822660098521, 0.17241379310344829, 0.17364532019704434, 0.2019704433497537], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.60221674876847286, 0.60221674876847286, 0.60098522167487689], 5: [0.13054187192118227, 0.22536945812807882, 0.22413793103448276, 0.19704433497536947], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.19211822660098521, 0.098522167487684734, 0.11822660098522167, 0.078817733990147784], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.77093596059113301, 0.75369458128078815, 0.80665024630541871], 5: [0.13054187192118227, 0.13054187192118227, 0.12807881773399016, 0.1145320197044335], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.19211822660098521, 0.13423645320197045, 0.1354679802955665, 0.10960591133004927], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.75985221674876846, 0.74507389162561577, 0.80418719211822665], 5: [0.13054187192118227, 0.10591133004926108, 0.11945812807881774, 0.086206896551724144], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.19211822660098521, 0.12192118226600986, 0.15517241379310345, 0.087438423645320201], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.67733990147783252, 0.72290640394088668, 0.7142857142857143, 0.78448275862068961], 5: [0.13054187192118227, 0.15517241379310345, 0.13054187192118227, 0.12807881773399016], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140219 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3423 9339 1409] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 150  215  293  432  650  987 1637 2818 4984 8084] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.773563
Epoch 1, cost is  0.74295
Epoch 2, cost is  0.728527
Epoch 3, cost is  0.71807
Epoch 4, cost is  0.713495
Training took 0.083167 minutes
Weight histogram
[5578 4333 3096 3016 1956 1152  729  227  105   58] [-0.28003734 -0.25232945 -0.22462156 -0.19691367 -0.16920578 -0.14149789
 -0.11379    -0.08608211 -0.05837422 -0.03066633 -0.00295844]
[ 128  183  320  514  920 1417 2191 3355 4692 6530] [-0.28003734 -0.25232945 -0.22462156 -0.19691367 -0.16920578 -0.14149789
 -0.11379    -0.08608211 -0.05837422 -0.03066633 -0.00295844]
-6.43964
6.31421
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140334 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5745 8530 1280] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 312  408  580  847 1223 1023 1495 2826 4925 8636] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.97075
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.01_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  0.764158
Epoch 1, cost is  0.737294
Epoch 2, cost is  0.72497
Epoch 3, cost is  0.71136
Epoch 4, cost is  0.704972
Training took 0.082264 minutes
Weight histogram
[5404 4200 3290 3037 1898 2177 1449  476  208  136] [-0.27719939 -0.24975557 -0.22231176 -0.19486794 -0.16742413 -0.13998031
 -0.1125365  -0.08509268 -0.05764887 -0.03020505 -0.00276124]
[ 266  374  633 1067 1715 1462 2276 3321 4686 6475] [-0.27719939 -0.24975557 -0.22231176 -0.19486794 -0.16742413 -0.13998031
 -0.1125365  -0.08509268 -0.05764887 -0.03020505 -0.00276124]
-5.89963
6.32529
... retrieved True_rbm_200-100_classical1_batch10_lr0.01_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/1/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  4.52345
Epoch 1, cost is  2.6266
Epoch 2, cost is  2.05243
Epoch 3, cost is  1.73408
Epoch 4, cost is  1.53436
Training took 0.079173 minutes
Weight histogram
[4196 4064 3138 2266 1942 1447 1130 1154  665  248] [-0.14000282 -0.1263052  -0.11260759 -0.09890998 -0.08521237 -0.07151475
 -0.05781714 -0.04411953 -0.03042192 -0.0167243  -0.00302669]
[1787  891  951 1257 1559 1904 2355 2756 3233 3557] [-0.14000282 -0.1263052  -0.11260759 -0.09890998 -0.08521237 -0.07151475
 -0.05781714 -0.04411953 -0.03042192 -0.0167243  -0.00302669]
-3.74219
4.4139
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.043012 minutes
Epoch 0
Fine tuning took 0.042732 minutes
Epoch 0
Fine tuning took 0.043436 minutes
{'zero': {0: [0.17980295566502463, 0.19581280788177341, 0.20073891625615764, 0.17241379310344829], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.56773399014778325, 0.53201970443349755, 0.55788177339901479], 5: [0.10591133004926108, 0.23645320197044334, 0.26724137931034481, 0.26970443349753692], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.17980295566502463, 0.10837438423645321, 0.11083743842364532, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.80295566502463056, 0.79556650246305416, 0.84729064039408863], 5: [0.10591133004926108, 0.088669950738916259, 0.093596059113300489, 0.071428571428571425], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.17980295566502463, 0.11699507389162561, 0.1206896551724138, 0.097290640394088676], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.79679802955665024, 0.73645320197044339, 0.79802955665024633], 5: [0.10591133004926108, 0.086206896551724144, 0.14285714285714285, 0.10467980295566502], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.17980295566502463, 0.10837438423645321, 0.11330049261083744, 0.046798029556650245], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7142857142857143, 0.80911330049261088, 0.80049261083743839, 0.88300492610837433], 5: [0.10591133004926108, 0.082512315270935957, 0.086206896551724144, 0.070197044334975367], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140742 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3423 9339 1409] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 150  215  293  432  650  987 1637 2818 4984 8084] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.05943
Epoch 1, cost is  1.04367
Epoch 2, cost is  1.03063
Epoch 3, cost is  1.01949
Epoch 4, cost is  1.00801
Training took 0.082947 minutes
Weight histogram
[4869 3995 3205 2784 1995 1295  814  571  427  295] [-0.15424316 -0.13884465 -0.12344614 -0.10804764 -0.09264913 -0.07725063
 -0.06185212 -0.04645361 -0.03105511 -0.0156566  -0.0002581 ]
[ 662  561  808 1146 1495 1881 2369 2938 3741 4649] [-0.15424316 -0.13884465 -0.12344614 -0.10804764 -0.09264913 -0.07725063
 -0.06185212 -0.04645361 -0.03105511 -0.0156566  -0.0002581 ]
-2.75391
3.16297
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.139645 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5745 8530 1280] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 312  408  580  847 1223 1023 1495 2826 4925 8636] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.97075
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.08723
Epoch 1, cost is  1.06907
Epoch 2, cost is  1.05504
Epoch 3, cost is  1.04308
Epoch 4, cost is  1.03565
Training took 0.082529 minutes
Weight histogram
[4737 3878 3394 2402 1959 1585 1571 1143  916  690] [-0.15217891 -0.13698486 -0.12179081 -0.10659675 -0.0914027  -0.07620864
 -0.06101459 -0.04582054 -0.03062648 -0.01543243 -0.00023838]
[1407 1147 1569 1066 1379 1779 2348 2982 3857 4741] [-0.15217891 -0.13698486 -0.12179081 -0.10659675 -0.0914027  -0.07620864
 -0.06101459 -0.04582054 -0.03062648 -0.01543243 -0.00023838]
-3.1457
3.74954
... retrieved True_rbm_200-50_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/2/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.53407
Epoch 1, cost is  6.04951
Epoch 2, cost is  5.65001
Epoch 3, cost is  5.06195
Epoch 4, cost is  4.55657
Training took 0.069703 minutes
Weight histogram
[ 770 1973 1979 2281 2908 2298 2689 4749  447  156] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
[3813 2782 3527 2098 1359 1233 1255 1343 1474 1366] [-0.05580953 -0.05024556 -0.04468158 -0.03911761 -0.03355364 -0.02798967
 -0.0224257  -0.01686173 -0.01129776 -0.00573378 -0.00016981]
-0.744946
0.974803
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.042403 minutes
Epoch 0
Fine tuning took 0.041886 minutes
Epoch 0
Fine tuning took 0.043164 minutes
{'zero': {0: [0.25123152709359609, 0.18226600985221675, 0.18842364532019704, 0.22413793103448276], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48399014778325122, 0.64162561576354682, 0.62684729064039413, 0.59975369458128081], 5: [0.26477832512315269, 0.17610837438423646, 0.18472906403940886, 0.17610837438423646], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.25123152709359609, 0.16748768472906403, 0.19827586206896552, 0.20566502463054187], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48399014778325122, 0.63177339901477836, 0.63300492610837433, 0.61699507389162567], 5: [0.26477832512315269, 0.20073891625615764, 0.16871921182266009, 0.17733990147783252], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.25123152709359609, 0.15147783251231528, 0.18596059113300492, 0.18842364532019704], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48399014778325122, 0.62931034482758619, 0.62438423645320196, 0.6428571428571429], 5: [0.26477832512315269, 0.21921182266009853, 0.18965517241379309, 0.16871921182266009], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.25123152709359609, 0.18596059113300492, 0.19827586206896552, 0.17364532019704434], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.48399014778325122, 0.62068965517241381, 0.63177339901477836, 0.65270935960591137], 5: [0.26477832512315269, 0.19334975369458129, 0.16995073891625614, 0.17364532019704434], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140433 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3423 9339 1409] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 150  215  293  432  650  987 1637 2818 4984 8084] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.05943
Epoch 1, cost is  1.04367
Epoch 2, cost is  1.03063
Epoch 3, cost is  1.01949
Epoch 4, cost is  1.00801
Training took 0.083939 minutes
Weight histogram
[4869 3995 3205 2784 1995 1295  814  571  427  295] [-0.15424316 -0.13884465 -0.12344614 -0.10804764 -0.09264913 -0.07725063
 -0.06185212 -0.04645361 -0.03105511 -0.0156566  -0.0002581 ]
[ 662  561  808 1146 1495 1881 2369 2938 3741 4649] [-0.15424316 -0.13884465 -0.12344614 -0.10804764 -0.09264913 -0.07725063
 -0.06185212 -0.04645361 -0.03105511 -0.0156566  -0.0002581 ]
-2.75391
3.16297
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.140264 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5745 8530 1280] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 312  408  580  847 1223 1023 1495 2826 4925 8636] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.97075
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  1.08723
Epoch 1, cost is  1.06907
Epoch 2, cost is  1.05504
Epoch 3, cost is  1.04308
Epoch 4, cost is  1.03565
Training took 0.083845 minutes
Weight histogram
[4737 3878 3394 2402 1959 1585 1571 1143  916  690] [-0.15217891 -0.13698486 -0.12179081 -0.10659675 -0.0914027  -0.07620864
 -0.06101459 -0.04582054 -0.03062648 -0.01543243 -0.00023838]
[1407 1147 1569 1066 1379 1779 2348 2982 3857 4741] [-0.15217891 -0.13698486 -0.12179081 -0.10659675 -0.0914027  -0.07620864
 -0.06101459 -0.04582054 -0.03062648 -0.01543243 -0.00023838]
-3.1457
3.74954
... retrieved True_rbm_200-100_classical1_batch10_lr0.001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/3/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.42563
Epoch 1, cost is  5.9429
Epoch 2, cost is  5.59474
Epoch 3, cost is  5.05835
Epoch 4, cost is  4.46992
Training took 0.078558 minutes
Weight histogram
[ 707 2445 3137 4493 2654 2774 3181  552  198  109] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
[3382 3117 4445 2033 1218 1147 1223 1233 1287 1165] [-0.04141794 -0.0373011  -0.03318427 -0.02906743 -0.02495059 -0.02083376
 -0.01671692 -0.01260008 -0.00848324 -0.00436641 -0.00024957]
-0.777643
0.685368
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.045015 minutes
Epoch 0
Fine tuning took 0.044883 minutes
Epoch 0
Fine tuning took 0.045845 minutes
{'zero': {0: [0.22906403940886699, 0.17857142857142858, 0.18842364532019704, 0.20443349753694581], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.63793103448275867, 0.64655172413793105, 0.61330049261083741], 5: [0.27463054187192121, 0.18349753694581281, 0.16502463054187191, 0.18226600985221675], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.22906403940886699, 0.17980295566502463, 0.18226600985221675, 0.21428571428571427], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.64532019704433496, 0.61822660098522164, 0.59113300492610843], 5: [0.27463054187192121, 0.1748768472906404, 0.19950738916256158, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.22906403940886699, 0.17980295566502463, 0.17118226600985223, 0.19704433497536947], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.63793103448275867, 0.64778325123152714, 0.5923645320197044], 5: [0.27463054187192121, 0.18226600985221675, 0.18103448275862069, 0.2105911330049261], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.22906403940886699, 0.17857142857142858, 0.18965517241379309, 0.17118226600985223], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.4963054187192118, 0.63669950738916259, 0.62684729064039413, 0.63423645320197042], 5: [0.27463054187192121, 0.18472906403940886, 0.18349753694581281, 0.19458128078817735], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142174 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3423 9339 1409] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 150  215  293  432  650  987 1637 2818 4984 8084] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.79311
Epoch 1, cost is  2.76317
Epoch 2, cost is  2.73037
Epoch 3, cost is  2.70844
Epoch 4, cost is  2.67473
Training took 0.081932 minutes
Weight histogram
[3323 2750 2512 2118 1816 1773 1828 1434 2394  302] [ -7.07015768e-02  -6.36302254e-02  -5.65588741e-02  -4.94875227e-02
  -4.24161713e-02  -3.53448200e-02  -2.82734686e-02  -2.12021173e-02
  -1.41307659e-02  -7.05941454e-03   1.19368206e-05]
[3159 1528 1297 1392 1583 1813 1972 2224 2496 2786] [ -7.07015768e-02  -6.36302254e-02  -5.65588741e-02  -4.94875227e-02
  -4.24161713e-02  -3.53448200e-02  -2.82734686e-02  -2.12021173e-02
  -1.41307659e-02  -7.05941454e-03   1.19368206e-05]
-0.927951
1.29049
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.142730 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5745 8530 1280] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 312  408  580  847 1223 1023 1495 2826 4925 8636] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.97075
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.89232
Epoch 1, cost is  2.85837
Epoch 2, cost is  2.83346
Epoch 3, cost is  2.79425
Epoch 4, cost is  2.77254
Training took 0.087941 minutes
Weight histogram
[3006 2468 2199 2009 1866 1733 1899 1874 4707  514] [ -6.53307438e-02  -5.87962784e-02  -5.22618130e-02  -4.57273476e-02
  -3.91928823e-02  -3.26584169e-02  -2.61239515e-02  -1.95894861e-02
  -1.30550207e-02  -6.52055536e-03   1.39100202e-05]
[5842 1484 1353 1353 1536 1720 1918 2151 2361 2557] [ -6.53307438e-02  -5.87962784e-02  -5.22618130e-02  -4.57273476e-02
  -3.91928823e-02  -3.26584169e-02  -2.61239515e-02  -1.95894861e-02
  -1.30550207e-02  -6.52055536e-03   1.39100202e-05]
-1.00042
1.3375
... retrieved True_rbm_200-50_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/4/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.82229
Epoch 1, cost is  6.7109
Epoch 2, cost is  6.63003
Epoch 3, cost is  6.55799
Epoch 4, cost is  6.49054
Training took 0.073959 minutes
Weight histogram
[ 752 1202 4926 6379 2707 1590 1045  725  528  396] [ -2.25583259e-02  -2.02937220e-02  -1.80291181e-02  -1.57645142e-02
  -1.34999103e-02  -1.12353064e-02  -8.97070256e-03  -6.70609867e-03
  -4.44149477e-03  -2.17689088e-03   8.77130078e-05]
[8286 3954 3045 2113  980  647  485  320  280  140] [ -2.25583259e-02  -2.02937220e-02  -1.80291181e-02  -1.57645142e-02
  -1.34999103e-02  -1.12353064e-02  -8.97070256e-03  -6.70609867e-03
  -4.44149477e-03  -2.17689088e-03   8.77130078e-05]
-0.0778338
0.0811374
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044439 minutes
Epoch 0
Fine tuning took 0.045721 minutes
Epoch 0
Fine tuning took 0.043739 minutes
{'zero': {0: [0.055418719211822662, 0.096059113300492605, 0.11083743842364532, 0.082512315270935957], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.88177339901477836, 0.78325123152709364, 0.81650246305418717], 5: [0.15147783251231528, 0.022167487684729065, 0.10591133004926108, 0.10098522167487685], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.055418719211822662, 0.098522167487684734, 0.092364532019704432, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.87931034482758619, 0.77955665024630538, 0.81280788177339902], 5: [0.15147783251231528, 0.022167487684729065, 0.12807881773399016, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.055418719211822662, 0.089901477832512317, 0.091133004926108374, 0.076354679802955669], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.86945812807881773, 0.78817733990147787, 0.82758620689655171], 5: [0.15147783251231528, 0.04064039408866995, 0.1206896551724138, 0.096059113300492605], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.055418719211822662, 0.10467980295566502, 0.072660098522167482, 0.067733990147783252], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.7931034482758621, 0.86945812807881773, 0.81280788177339902, 0.83743842364532017], 5: [0.15147783251231528, 0.025862068965517241, 0.1145320197044335, 0.094827586206896547], 6: [0.0, 0.0, 0.0, 0.0]}}
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... initialising association layer
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.155634 minutes
Weight histogram
[ 114  328  692  980 1108 1137 1720 3423 9339 1409] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
[ 150  215  293  432  650  987 1637 2818 4984 8084] [ -1.07421329e-04   6.16842102e-05   2.30789749e-04   3.99895288e-04
   5.69000827e-04   7.38106366e-04   9.07211905e-04   1.07631744e-03
   1.24542298e-03   1.41452852e-03   1.58363406e-03]
-0.660645
0.812784
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.79311
Epoch 1, cost is  2.76317
Epoch 2, cost is  2.73037
Epoch 3, cost is  2.70844
Epoch 4, cost is  2.67473
Training took 0.085108 minutes
Weight histogram
[3323 2750 2512 2118 1816 1773 1828 1434 2394  302] [ -7.07015768e-02  -6.36302254e-02  -5.65588741e-02  -4.94875227e-02
  -4.24161713e-02  -3.53448200e-02  -2.82734686e-02  -2.12021173e-02
  -1.41307659e-02  -7.05941454e-03   1.19368206e-05]
[3159 1528 1297 1392 1583 1813 1972 2224 2496 2786] [ -7.07015768e-02  -6.36302254e-02  -5.65588741e-02  -4.94875227e-02
  -4.24161713e-02  -3.53448200e-02  -2.82734686e-02  -2.12021173e-02
  -1.41307659e-02  -7.05941454e-03   1.19368206e-05]
-0.927951
1.29049
training layer 0, rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
... loaded trained layer rbm_625-250_classical1_batch10_lr0.0001_nesterov0.9_wd0.0001_sparsity_t0.1_c0.01_d0.9
Epoch 0, cost is  nan
Epoch 1, cost is  nan
Epoch 2, cost is  nan
Epoch 3, cost is  nan
Epoch 4, cost is  nan
Training took 0.154859 minutes
Weight histogram
[  34  153  468  966 1625 1668 1806 5745 8530 1280] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
[ 312  408  580  847 1223 1023 1495 2826 4925 8636] [ -2.14635569e-04  -9.07955517e-05   3.30444658e-05   1.56884483e-04
   2.80724501e-04   4.04564518e-04   5.28404536e-04   6.52244553e-04
   7.76084571e-04   8.99924588e-04   1.02376461e-03]
-0.97075
0.617135
training layer 1, rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
... loaded trained layer rbm_250-100_classical1_batch10_lr0.0001_classical0.5_wd0.0001_sparsity_t0.1_c0.1_d0.9
Epoch 0, cost is  2.89232
Epoch 1, cost is  2.85837
Epoch 2, cost is  2.83346
Epoch 3, cost is  2.79425
Epoch 4, cost is  2.77254
Training took 0.084769 minutes
Weight histogram
[3006 2468 2199 2009 1866 1733 1899 1874 4707  514] [ -6.53307438e-02  -5.87962784e-02  -5.22618130e-02  -4.57273476e-02
  -3.91928823e-02  -3.26584169e-02  -2.61239515e-02  -1.95894861e-02
  -1.30550207e-02  -6.52055536e-03   1.39100202e-05]
[5842 1484 1353 1353 1536 1720 1918 2151 2361 2557] [ -6.53307438e-02  -5.87962784e-02  -5.22618130e-02  -4.57273476e-02
  -3.91928823e-02  -3.26584169e-02  -2.61239515e-02  -1.95894861e-02
  -1.30550207e-02  -6.52055536e-03   1.39100202e-05]
-1.00042
1.3375
... retrieved True_rbm_200-100_classical1_batch10_lr0.0001_nesterov0.5_wd0.0001_sparsity_t0.1_c0.01_d0.9 from /vol/bitbucket/js3611/AssociationLearning/data/ExperimentADBN5/5/association_layer/2_2
... top layer RBM loaded
Epoch 0, cost is  6.75217
Epoch 1, cost is  6.59705
Epoch 2, cost is  6.50606
Epoch 3, cost is  6.43045
Epoch 4, cost is  6.35957
Training took 0.084109 minutes
Weight histogram
[ 696 1299 4059 6633 2965 1732 1122  777  555  412] [ -2.31174268e-02  -2.08066879e-02  -1.84959491e-02  -1.61852102e-02
  -1.38744713e-02  -1.15637325e-02  -9.25299363e-03  -6.94225477e-03
  -4.63151591e-03  -2.32077705e-03  -1.00381940e-05]
[8022 3952 3099 2170 1021  677  513  345  299  152] [ -2.31174268e-02  -2.08066879e-02  -1.84959491e-02  -1.61852102e-02
  -1.38744713e-02  -1.15637325e-02  -9.25299363e-03  -6.94225477e-03
  -4.63151591e-03  -2.32077705e-03  -1.00381940e-05]
-0.0699412
0.0734844
initialise reconstruction by active_h
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
... Sparsity: setting initial bias for Stochastic Binary Hidden Unit
Epoch 0
Fine tuning took 0.044747 minutes
Epoch 0
Fine tuning took 0.042340 minutes
Epoch 0
Fine tuning took 0.044024 minutes
{'zero': {0: [0.050492610837438424, 0.075123152709359611, 0.10467980295566502, 0.096059113300492605], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77709359605911332, 0.88669950738916259, 0.78201970443349755, 0.78201970443349755], 5: [0.17241379310344829, 0.038177339901477834, 0.11330049261083744, 0.12192118226600986], 6: [0.0, 0.0, 0.0, 0.0]}, 'v_noisy_active_h': {0: [0.050492610837438424, 0.080049261083743842, 0.082512315270935957, 0.091133004926108374], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77709359605911332, 0.88423645320197042, 0.80418719211822665, 0.80049261083743839], 5: [0.17241379310344829, 0.035714285714285712, 0.11330049261083744, 0.10837438423645321], 6: [0.0, 0.0, 0.0, 0.0]}, 'binomial0.1': {0: [0.050492610837438424, 0.078817733990147784, 0.086206896551724144, 0.077586206896551727], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77709359605911332, 0.88054187192118227, 0.78694581280788178, 0.81650246305418717], 5: [0.17241379310344829, 0.04064039408866995, 0.1268472906403941, 0.10591133004926108], 6: [0.0, 0.0, 0.0, 0.0]}, 'active_h': {0: [0.050492610837438424, 0.078817733990147784, 0.097290640394088676, 0.081280788177339899], 1: [0.0, 0.0, 0.0, 0.0], 2: [0.0, 0.0, 0.0, 0.0], 3: [0.0, 0.0, 0.0, 0.0], 4: [0.77709359605911332, 0.8928571428571429, 0.78694581280788178, 0.82635467980295563], 5: [0.17241379310344829, 0.02832512315270936, 0.11576354679802955, 0.092364532019704432], 6: [0.0, 0.0, 0.0, 0.0]}}
